text,summary
"The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . The idea is using quantizers with clipping (denoted in the paper with Q(x,k)) and some additional operators like shift (denoted with shift(x)) and stochastic rounding. The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations. <sep> After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization. They introduce the additional operators needed for training in such network. Since quantization may loss some information, the authors need to quantize the signals in the network around the dynamic range in order not to ""kill"" the signal. The authors describe how to do that. Afterward, as in other techniques for quantization, they describe how to initialize the network values. Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values (""orientations"") and not the absolute values and (2) small values in errors are negligible. <sep> Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works. The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively. For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network. The authors investigate mainly the bitwidth of errors and gradients. <sep> In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference. For inference only, other works has more to offer but this is a promising technique for learning. The things that are still missing in this work are some power reduction estimates as well as area reduction estimations. This will give the hardware community a clear vision of how such methods may be implemented both in data centers as well as on end portable devices.","High quality paper, appreciated by reviewers, likely to be of substantial interest to the community. It's worth an oral to facilitate a group discussion."
"-------------- <sep> Summary: <sep> -------------- <sep> This paper presents a series of experiments on language emergence through referential games between two agents. They ground these experiments in both fully-specified symbolic worlds and through raw, entangled, visual observations of simple synthetic scenes. They provide rich analysis of the emergent languages the agents produce under different experimental conditions. This analysis (especially on raw pixel images) make up the primary contribution of this work. <sep> -------------- <sep> Evaluation: <sep> -------------- <sep> Overall I think the paper makes some interesting contributions with respect to the line of recent 'language emergence' papers. The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings, coming to the (perhaps uncontroversial) finding that varying the environment and restrictions on language result in variations in the learned communication protocols. <sep> In the context of existing literature, the novelty of this work is somewhat limited -- consisting primarily of the extension of multi-agent reference games to raw-pixel inputs. While this is a non-trivial extension, other works have demonstrated language learning in similar referring-expression contexts (essentially modeling only the listener model [Hermann et.al 2017]). <sep> I have a number of requests for clarification in the weaknesses section which I think would improve my understanding of this work and result in a stronger submission if included by the authors. <sep> -------------- <sep> Strengths: <sep> -------------- <sep> - Clear writing and document structure. <sep> - Extensive experimental setting tweaks which ablate the information and regularity available to the agents. The discussion of the resulting languages is appropriate and provides some interesting insights. <sep> - A number of novel analyses are presented to evaluate the learned languages and perceptual systems. <sep> -------------- <sep> Weaknesses: <sep> -------------- <sep> - How stable are the reported trends / languages across multiple runs within the same experimental setting? The variance of REINFORCE policy gradients (especially without a baseline) plus the general stochasticity of SGD on randomly initialized networks leads me to believe that multiple training runs of these agents might result is significantly different codes / performance. I am interested in hearing the author's experiences in this regard and if multiple runs present similar quantitative and qualitative results. I admit that expecting identical codes is unrealistic, but the form of the codes (i.e. primarily encoding position) might be consistent even if the individual mappings are not). <sep> - I don't recall seeing descriptions of the inference-time procedure used to evaluate training / test accuracy. I will assume argmax decoding for both speaker and listener. Please clarify or let me know if I missed something. <sep> - There is ambiguity in how the ""protocol size"" metric is computed. In Table 1, it is defined as 'the effective number of unique message used'. This comes back to my question about decoding I suppose, but does this count the 'inference-time' messages or those produced during training? <sep> Furthermore, Table 2 redefines ""protocol size"" as the percentage of novel message. I assume this is an editing error given the values presented and take these columns as counts. It also seems ""protocol size"" is replaced with the term ""lexicon"" from 4.1 onward. <sep> - I'm surprised by how well the agents generalize in the raw pixel data experiments. In fact, it seems that across all games the test accuracy remains very close to the train accuracy. <sep> Given the dataset is created by taking all combinations of color / shape and then sampling 100 location / floor color variations, it is unlikely that a shape / color combo has not been seen in training. Such that the only novel variations are likely location and floor color. However, taking Game A as an example, the probe classifiers are relatively poor at these attributes -- indicating the speaker's representation is not capturing these attributes well. Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape? <sep> I think some additional analysis of this setting might shed some light on this issue. One thought is to compute upper-bounds based on ground truth attributes. Consider a model which knows shape perfectly, but cannot predict other attributes beyond chance. To compute the performance of such a model, you could take the candidate set, remove any instances not matching the ground truth shape, and then pick randomly from the remaining instances. Something similar could be repeated for all attributes independently as well as their combinations -- obviously culminating in 100% accuracy given all 4. It could be that by dataset construction, object location and shape are sufficient to achieve high accuracy because the odds of seeing the same shape at the same location (but different color) is very low. <sep> Given these are operations on annotations and don't require time-consuming model training, I hope to see this analysis in the rebuttal to put the results into appropriate context. <sep> - What is random chance for the position and floor color probe classifiers? I don't think it is mentioned how many locations / floor colors are used in generation. <sep> - Relatively minor complaint: Both agents are trained via the REINFORCE policy gradient update rule; however, the listener agent makes a fairly standard classification decision and could be trained with a standard cross-entropy loss. That is to say, the listener policy need not make intermediate discrete policy decisions. This decision to withhold available supervision is not discussed in the paper (as far as I noticed), could the authors speak to this point? <sep> -------------- <sep> Curiosities: <sep> -------------- <sep> - I got the impression from the results (specifically the lack of discussion about message length) that in these experiments agents always issued full length messages even though they did not need to do so. If true, could the authors give some intuition as to why? If untrue, what sort of distribution of lengths do you observe? <sep> - There is no long term planning involved in this problem, so why use reinforcement learning over some sort of differentiable sampler? With some re-parameterization (i.e. Gumbel-Softmax), this model could be end-to-end differentiable. <sep> -------------- <sep> Minor errors: <sep> -------------- <sep> [2.2 paragraph 1] LSTM citation should not be in inline form. <sep> [3 paragraph 1] 'Note that these representations do care some' -> carry <sep> [3.3.1 last paragraph] 'still able comprehend' --> to <sep> ------- <sep> Edit <sep> ------- <sep> Updating rating from 6 to 7.","Important problem (analyzing the properties of emergent languages in multi-agent reference games), a number of interesting analyses (both with symbolic and pixel inputs), reaching a finding that varying the environment and restrictions on language result in variations in the learned communication protocols (which in hindsight is that not surprising, but that's hindsight). While the pixel experiments are not done with real images, it's an interesting addition the literature nonetheless."
"The authors provide a method for learning from demonstrations where several modalities of the same task are given. The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations. <sep> The paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming. They put the this specific work in the right context of imitation learning and IRL. Afterward, the authors argue that deterministic network cannot adequately several modalities. The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs. I find that recent paper by Tamar et al 2016. on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network. Even the control task is very similar to the current proposed task in this paper. <sep> The authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. Again, Tamar et al. 2016 deals with this 3 points. <sep> I went over the math. It seems right and valid. Indeed, SNN is a good choice for adding (Bayesian) context to a task. Also, I see the advantage of referring only to the ""good"" quantiles when needed. It is indeed a good method for dealing with the variance. <sep> I must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task). <sep> My concerns are as follows: <sep> 1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task. For me, this is not the pain point in this tasks. the pain point is knowing when tasks are begin and end. <sep> 2) I'm not sure, and I haven't seen evidence in the paper (or other references) that SNN is the only (optimal?) method for this context. Why not adding (non Bayesian) context (not label) to the task will not work as well? <sep> 3) the robot task is impressive. but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage? I know how hard is to make robotic tasks work... <sep> 4) I'm not sure that the comparison of the suggested architecture to one without any underlying additional variable Z or context (i.e., non-Bayesian setup) is fair. ""Vanilla"" NN indeed may fail miserably . So, the comparison should be to any other work that can deal with ""similar environment but different details"". <sep> To summarize, I like the work and I can see clearly the motivation. But I think some more work is needed in this work: comparing to the right current state of the art, and show that in principal (by demonstrating on other simpler simulations domains) that this method is better than other methods.","This paper presents a sampling inference method for learning in multi-modal demonstration scenarios. Reference to imitation learning causes some confusion with the IRL domain, where this terminology is usually encountered. Providing a real application to robot reaching, while a relatively simple task in robotics, increases the difficulty and complexity of the demonstration. That makes it impressive, but also difficult to unpick the contributions and reproduce even the first demonstration. It's understandable at a meeting on learning representations that the reviewers wanted to understand why existing methods for learning multi-modal distributions would not work, and get a better understanding of the tradeoffs and limitations of the proposed method. The CVAE comparison added to the appendix during the rebuttal period just pushed this paper over the bar. The demonstration is simplified, so much easier to reproduce, making it more feasible others will attempt to reproduce the claims made here."
"Summary: <sep> The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. <sep> This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way: <sep> - weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity <sep> - the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations <sep> This way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers. <sep> Review: <sep> The paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. <sep> I only have a couple of questions/comments: <sep> 1) I'm not familiar with the term m-specific (""Matrices B, G and A are m-specific."") and didn't find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description. <sep> 2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs? <sep> 3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015) <sep> 4) Figure 5 caption has a typo: ""acrruacy"" <sep> References: <sep> Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. ""Binaryconnect: Training deep neural networks with binary weights during propagations."" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015. <sep> Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. ""Neural networks with few multiplications."" arXiv preprint arXiv:1510.03009 (2015). <sep> Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. ""Xnor-net: Imagenet classification using binary convolutional neural networks."" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.","The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy. The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments. One concern brought up was the applicability to deeper network structures. This was acknowledged by the authors to be a subject of future work. Another issue raised was the question of theoretical vs. actual speedup. Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations. The reviewers were consistent in their support of the paper. I follow their recommendation: Accept."
"This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.  However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story. <sep> I will be happy to revisit the rating if the experimental section is enriched. <sep> Pros: <sep> - very easy to follow idea and model <sep> - simple merge or RL and SL in an end-to-end trainable model <sep> - improvements over previous solutions <sep> Cons: <sep> - K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. <sep> - TSP experiments show that ""in distribution"" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). <sep> - in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments <sep> Side notes: <sep> - DCN is already quite commonly used abbreviation for ""Deep Classifier Network"" as well as ""Dynamic Capacity Network"", thus might be a good idea to find different name. <sep> - please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example: <sep> Graph Neural Network Nowak et al. (2017) <sep> should be <sep> Graph Neural Network (Nowak et al. (2017)) <sep> # After the update <sep> Evaluation section has been updated threefold: <sep> - TSP experiments are now in the appendix rather than main part of the paper <sep> - k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering <sep> - Knapsack problem has been added <sep> Paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. I can see the benefit of trainable approach here, the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that. <sep> I increased rating for the paper, however in order to put the ""clear accept"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).","The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks, mimicking a class of standard algorithms. The paper is clearly written, and the experiments are diverse. It also seems to point in the direction of a wider class of algorithm-inspired neural net architectures."
"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Pipeline: -Data are augmented with domain-specific transformations. For instance, in the case of MNIST, rotations with different degrees are applied. All data are then labelled as ""original"" or ""transformed by ...(specific transformation)"". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels. -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion. -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer. <sep> Detailed Comments: <sep> (*) Pros <sep> -The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST. <sep> -Use of ACOL and GAR is interesting, also the idea to make ""labeled"" data from unlabelled ones by using data augmentation. <sep> (*) Cons <sep> -minor: in the title, I find the expression ""unsupervised clustering"" uselessly redundant since clustering is by definition unsupervised. <sep> -Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits. This is not a very challenging task. <sep> And just because something works on MNIST, does not mean it works in general. <sep> What are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)? <sep> -This is not clear what is novel here since ACOL and GAR already exist. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. <sep> My main problem  was about the lack of novelty. The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv.  The other issue concerned the validation of the approach on databases other than MNIST. The author also addressed this point, and I changed my scores accordingly.","The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance. I would recommend removing the term ""unsupervised"" in clustering, as it is redundant. Clustering is, by default, assumed to be unsupervised. <sep> There is some interest in extending this to non-vision domains, however this is beyond the scope of the current work."
"########## UPDATED AFTER AUTHOR RESPONSE ########## <sep> Thanks for the good revision and response that addressed most of my concerns. I am bumping up my score. <sep> ############################################### <sep> This paper presents a Disentangled Inferred Prior (DIP-VAE) method for learning disentangled features from unlabeled observations following the VAE framework. The basic idea of DIP-VAE is to enforce the aggregated posterior q(z) = E_x [q(z | x)] to be close to an identity matrix as implied by the commonly chosen standard normal prior p(z). The authors propose to moment-match q(z) given it is hard to minimize the KL-divergence between q(z) and p(z). This leads to one additional term to the regular VAE objective (in two parts, on- and off-diagonal). It has the similar property as beta-VAE (Higgins et al. 2017) but without sacrificing the reconstruction quality. Empirically the authors demonstrate that DIP-VAE can effectively learn disentangled features, perform comparably better than beta-VAE and at the same time retain the reconstruction quality close to regular VAE (beta-VAE with beta = 1). <sep> The paper is overall well-written with minor issues (listed below). I think the idea of enforcing an aggregated (marginalized) posterior q(z) to be close to the standard normal prior p(z) makes sense, as opposed to enforcing each individual posterior q(z|x) to be close to p(z) as (beta-)VAE objective suggests. I would like to make some connection to some work on understanding VAE objective (Hoffman & Johnson 2016, ELBO surgery: yet another way to carve up the variational evidence lower bound) where they derived something along the same line of an aggregated posterior q(z). In Hoffman & Johnson, it is shown that KL(q(z) | p(z)) is in fact buried in ELBO, and the inequality gap in Eq (3) is basically a mutual information term between z and n (the index of the data point). Similar observations have led to the development of VAMP-prior (Tomczak & Welling 2017, VAE with a VampPrior). Following the derivation in Hoffman & Johnson, DIP-VAE is basically adding a regularization parameter to the KL(q(z) | p(z)) term in standard ELBO. I think this interpretation is complementary to (and in my opinion, more clear than) the one that's described in the paper. <sep> My concerns are mostly regarding the empirical studies: <sep> 1. One of my main concern is on the empirical results in Table 1. The disentanglement metric score for beta-VAE is suspiciously lower than what's reported in Higgins et al., where they reported a 99.23% disentanglement metric score on 2D shape dataset. I understand the linear classier is different, but still the difference is too large to ignore. Hence my current more neutral review rating. <sep> 2. Regarding the correlational plots (the bottom row of Table 3 and 4), I don't think I can see any clear patterns (especially on CelebA). I wonder what's the point of including them here and if there is a point, please explain them clearly in the paper. <sep> 3. Figure 2 is also a little confusing to me. If I understand the procedure correctly, a good disentangled feature would imply smaller correlations to other features (i.e., the numbers in Figure 2 should be smaller for better disentangled features). However, looking at Figure 2 and many other plots in the appendix, I don't think DIP-VAE has a clear win here. Is my understanding correct? If so, what exactly are you trying to convey in Figure 2? <sep> Minor comments: <sep> 1. In Eq (6) I think there are typos in terms of the definition of Cov_q(z)(z)? It appears as only the second term in Eq (5). <sep> 2. Hyperparameter subsection in section 3: Shouldn't \\lambda_od be larger if the entanglement is mainly reflected in the off-diagonal entries? Why the opposite? <sep> 3. Can you elaborate on how a running estimate of Cov_p(x)(\\mu(x)) is maintained (following Eq (6)). It's not very clear at the current state of the paper. <sep> 4. Can we have error bars in Table 2? Some of the numbers are possibly hitting the error floor. <sep> 5. Table 5 and 6 are not very necessary, unless there is a clear point.","Thank you for submitting you paper to *CONF*. The reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger. <sep> The authors may want to consider adding a discussion about whether the simple standard Gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations. Alternatives, such as sparse priors, might be more sensible if a model-based solution to this problem is sought."
"In this article, the authors offer a way to decrease the variance of the gradient estimation in the training of neural networks. <sep> They start in the Introduction and Section 2 by explaining the multiple uses of random connection weights in deep learning and how the computational cost often restricts their use to a single randomly sampled set of weights per minibatch, which results to higher-variance gradient estimatos than could be achieved otherwise. In Section 3 the authors offer to get the benefits of multiple weights without most of the cost, when the distribution of the weights is symmetric and fully factorized, by multiplying sampled-once random perturbations of the weights by a rank-1 random sign matrix. This efficient mechanism is only twice as costly as a single random perturbation, and the authors show how to efficiently parallelize it on GPUs, thereby also allowing GPU-ization of evolution strategies (something so far difficult toachieve). Of note, they provide a theoretical analysis in Section 3.2, proving the actual variance reduction of their efficient pseudo-sampling scheme. In Section 4 they provide quite varied empirical analysis: they confirm their theoretical results on four architectures; they show its use it to regularise on language models; they apply it on large minibatch settings where high variance is a main problem; and on evolution strategies. <sep> While it is a rather simple idea which could be summarised much earlier in the  single equation (3), I really like the thoroughness and the clarity of the exposure of the idea. Too many papers in our community skimp on details and on formalism, and it is a delight to see things exposed so clearly -- even accompanied with a proof. <sep> However, the painful part: while I am convinced by the idea and love its detailed exposure, and the gradient variance reduction is made very clear, the experimental impact in terms of accuracy (or perplexity) is, sadly,  not very convincing. Nowhere in the text did I find a clear rationale of why it is beneficial to reduce the variance of the gradient. The numerical results in Table 1 and Table 2 also do not show a clear improvement: Flipout does not provide the best accuracy. The gain in wall clock could be a factor, but would need to be measured on the figures more clearly. And the validation errors in Figure 2 for Evolution strategies seem to be worse than backprop.The main text itself also only claims performance ""comparable to the other methods"".  The only visible gain is on the lower part Figure 2.a on a ConvNet. <sep> This makes me wonder if the authors could do a better job of putting forward the actual advantages of their methods on the end-results: could wall clock measure be put more forward, to justify the extra work? This would, in my mind, strongly improve the case for publication of this article. <sep> A few improvement suggestions: <sep> * Could put earlier more emphasis of superiority to Local Reparameterization Trick in terms of architecture, not wait until Section 2.2 and section 4.1 <sep> *Should also put more emphasis on limitations, not wait until 3.1. <sep> * Proposition 1 is quite straightforward, not sure it deserves a proposition, but it's elegant to put it forward. <sep> * Footnote 1 on re-using the matrices is indeed practical, but also somewhat surprising in terms of bias risks. Could it be explained in more depth, maybe by the random permutations of the minibatches making the bias non systematic and cancelling out? <sep> * Theorem 1: For readability could merge the expectations on the joint distribution as E_{x, \\hat \\delta W} , rather than separate expectations with the conditional distributions. <sep> * Theorem 1: could the authors provide a clearer intuitive explanation of the \\beta term alone, not only as part of \\alpha + \\beta, especially as it plays such a key role, being the only one that does not disappear? And how do they explain their empirical observation that \\beta is close to 0? Any intuition on that? <sep> * Experiments: I salute the authors for providing all the details in exhaustive manner in the Appendix. Very commendable. <sep> * Experiments: I like the empirical verification of the theory. Very neat to see. <sep> Minor typo: <sep> * page 2 last paragraph, ""evolution strategies"" is plural but the verbs are used in singular (""is black box"", ""It doesn't"", ""generates"")","Thank you for submitting you paper to *CONF*. The idea is simple, but easy to implement and effective. The paper examines the performance fairly thoroughly across a number of different scenarios showing that the method consistently reduces variance. How this translates into final performance is complex of course, but faster convergence is demonstrated and the revised experiments in table 2 show that it can lead to improvements in accuracy."
"This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that: <sep> (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. <sep> (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. <sep> (3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer. <sep> The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge. <sep> The second observation is much less clear to me. Specifically, <sep> a. The author claim that ""A sufficient condition for \\delta u to be the same in both cases is L'(x = f(u)) ~ L'(x = g(u))"". However, I'm not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. <sep> b. Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified. <sep> c. For BNNs, where both the weights and activations are binarized, shouldn't we compare weights*activations to (binarized weights)*(binarized activations)? <sep> d. To make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3. <sep> e. It is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. <sep> The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized. <sep> To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. <sep> %%% After Author's response %%% <sep> a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation). <sep> Following the author's response and revisions, I have raised my grade.","This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy. Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we've seen much empirical success of binarization schemes. This paper shows that the continuous angles and dot products are well approximated in the discretized network. The paper concludes with an input rotation trick to fix discretization failures in the first layer. <sep> Overall, the contribution seems substantial, and the reviewers haven't found any significant issues. One reviewer wasn't convinced of the problem's importance, but I disagree here. I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions. I recommend acceptance."
"This paper proposes a new method of detecting in vs. out of distribution samples. Most existing approaches for this deal with detecting out of distributions at *test time* by augmenting input data and or temperature scaling the softmax and applying a simple classification rule based on the output. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure. <sep> The authors propose to train a generator network in combination with the classifier and an adversarial discriminator. The generator is trained to produce images that (1) fools a standard GAN discriminator and (2) has high entropy (as enforced with the pull-away term from the EBGAN). Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. <sep> The model is evaluated on CIFAR-10 and SVNH, where several out of distribution datasets are used in each case. Performance gains are clear with respect to the baseline methods. <sep> This paper is clearly written, proposes a simple model and seems to outperform current methods. One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. <sep> I have some clarifying questions below: <sep> - Figure 4 is unclear: does ""Confidence loss with original GAN"" refer to the method where the classifier is pretrained and then ""Joint confidence loss"" is with joint training? What does ""Confidence loss (KL on SVHN/CIFAR-10)"" refer to? <sep> - Why does the join training improve the ability of the model to generalize to out-of-distribution datasets not seen during training? <sep> - Why is the pull away term necessary and how does the model perform without it? Most GAN models are able to stably train without such explicit terms such as the pull away or batch discrimination. Is the proposed model unstable without the pull-away term? <sep> - How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional ""out of distribution"" class? This exact approach has been used to do semi supervised learning with GANS [1][2]. More generally, could the authors comment on how this approach is related to these semi-supervised approaches? <sep> - Did you try combining the classifier and discriminator into one model as in [1][2]? <sep> [1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583) <sep> [2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)","Meta score: 6 <sep> The paper approaches the problem of identifying out-of-distribution data by modifying the objective function to include a generative term. Experiments on a number of image datasets. <sep> Pros: <sep> - clearly expressed idea, well-supported by experimentation <sep> - good experimental results <sep> - well-written <sep> Cons: <sep> - slightly limited novelty <sep> - could be improved by linking to work on semi-supervised learning approaches using GANs <sep> The authors note that *CONF* submission 267 (https://openreview.net/forum?id=H1VGkIxRZ) covers similar ground to theirs."
"The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences. The outputs are evaluated by ROUGE-L and test perplexity. There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences. <sep> This paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The main weakness is that I would have liked to see more analysis and comparisons in the evaluation. <sep> Evaluation: <sep> Currently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task? <sep> Extractiveness analysis: <sep> I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores? This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help. <sep> A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not. <sep> I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here. <sep> Overall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work. <sep> ---- <sep> After reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper. This is a very nice contribution.","This paper presents a new multi-document summarization task of trying to write a wikipedia article based on its sources. Reviewers found the paper and the task clear to understand and well-explained. The modeling aspects are clear as well, although lacking justification. Reviewers are split on the originality of the task, saying that it is certainly big, but wondering if that makes it difficult to compare with. The main split was the feeling that ""the paper presents strong quantitative results and qualitative examples. "" versus a frustration that the experimental results did not take into account extractive baselines or analysis. However the authors provide a significantly updated version of the work targeting many of these concerns, which does alleviate some of the main issues. For these reasons, despite one low review, my recommendation is that this work be accepted as a very interesting contribution."
"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip. The paper describes the use additional mechanisms for synchronization and memory loading. <sep> The evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).  With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads. Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise. <sep> Storing weights persistent on chip should give a sharp benefit when all weights fit on the chip. One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those. <sep> To summarize, the paper adds the ability to support pruning over persistent RNNs. However, Narang et. al., 2017 already explore this idea, although briefly. Furthermore, the gains from the sparsity appear rather limited over real applications. I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads). Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial. However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.","The reviewers find the work interesting and well made, but are concerned that *CONF* is not the right venue for the work. I will recommend that the paper be accepted, but ask the authors to add the NMT results to the main paper (any other non-synthetic applications they could add would be helpful)."
"This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. <sep> The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing. <sep> In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. <sep> What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. <sep> Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.","This paper adapts (Nachum et al 2017) to continuous control via TRPO. The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of ""building atop a closely related work""), nontrivial, and shows empirical promise. The reviewers would like more exploration of the sensitivity of the hyper-parameters."
"Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation? <sep> The fact that it's an obvious question does not mean that it's a question that's worthless. In fact, I am glad someone asked this question and tried to answer it. <sep> Pros: <sep> - Clear presentation, easy to follow. <sep> - Very interesting, but obvious, question is explored. <sep> - The paper is very clear, and uses building blocks which have been analyzed before, which leaves the authors free to explore their interactions rather than each individual building block's property. <sep> - Results are shown on two tasks (classification / segmentation) rather than just one (the obvious one would have been to only discuss results on classification), and relatively intuitive results are shown (i.e., more bits = better performance). What is perhaps not obvious is how much impact does doubling the bandwidth have (i.e., initially it means more, then later on it plateaus, but much earlier than expected). <sep> - Joint training of compression + other tasks. As far as I know this is the first paper to talk about this particular scenario. <sep> - I like the fact that classical codecs were not completely discarded (there's a comparison with JPEG 2K). <sep> - The discussion section is of particular interest, discussing openly the pros/cons of the method (I wish more papers would be as straightforward as this one). <sep> Cons: <sep> - I would have liked to have a discussion on the effect of the encoder network. Only one architecture/variant was used. <sep> - For PSNR, SSIM and MS-SSIM I would like a bit more clarity whether these were done channel-wise, or on the grayscale channel. <sep> - While runtime is given as pro, it would be nice for those not familiar with the methods to provide some runtime numbers (i.e., breakdown how much time does it take to encode and how much time does it take to classify or segment, but in seconds, not flops). For example, Figure 6 could be augmented with actual runtime in seconds. <sep> - I wish the authors did a ctrl+F for ""??"" and fixed all the occurrences. <sep> - One of the things that would be cool to add later on but I wished to have beeyn covered is whether it's possible to learn not only to compress, but also downscale. In particular, the input to ResNet et al for classification is fixed sized, so the question is -- would it be possible to produced a compact representation to be used for classification given arbitrary image resolutions, and if yes, would it have any benefit? <sep> General comments: <sep> - The classification bits are all open source, which is very good. However, there are very few neural net compression methods which are open sourced. Would you be inclined to open source the code for your implementation? It would be a great service to the community if yes (and I realize that it could already be open sourced -- feel free to not answer if it may lead to break anonymity, but please take this into consideration).","Some reviewers seem to assign novelty to the compression and classification formulation; however, semi-supervised autoencoders have been used for a long time. Taking the compression task more seriously as is done in this paper is less explored. <sep> The paper provides some extensive experimental evaluation and was edited to make the paper more concise at the request of reviewers. One reviewer had a particularly strong positive rating, due to the quality of the presentation, experiments and discussion. I think the community would like this work and it should be accepted."
"To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. The purpose of the transformation is to erase the high-frequency signals potentially embedded by an adversarial attack. <sep> Strong points: <sep> * To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). <sep> * The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work. <sep> * The proposed approach really helps in a black-box scenario (Figure 4). As explained below, the presented investigation is however insufficient to assess whether the proposed defense helps in a true white-box scenario. <sep> Weak points: <sep> * The black-box versus white-box terminology is not appropriate, and confusing. In general, black-box means that the adversary ignores everything from the decision process. Hence, in this case, the adversary does not know about the classification model, nor the defensive method, when used. This corresponds to Figure 3. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. Assimilating the parameters of the transform to a secret key is not correct because those parameters could be inferred by presenting many image samples to the transform and looking at the outcome of the transformation (which is supposed to be available in a 'white-box' paradigm) for those samples. <sep> * Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5. <sep> * The paper does not discuss the impact of the denfense strategy on the classification performance in absence of adversity. <sep> * The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without <sep> Security through Obscurity' at https://arxiv.org/abs/1612.01401. <sep> * In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge. Designing an attack in case of a non differentiable transformation is obviously not trivial since back-propagation can not be used. However, since the proposed transformation primarily aim at removing the high frequency pattern induced by the attack, one could for example design an attack that account for a (linear and differentiable) low-pass filter transformation. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation. <sep> * If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all? <sep> Overall, the works investigates an interesting idea, but lacks maturity to be accepted. Therefore, I would only recommend acceptation if room. <sep> Minor issues: <sep> Typo on p7: to change*s* <sep> Clarify poor formulations: <sep> * p1: 'enforce model-specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization schemes'. <sep> * p1: 'too simple to remove adversarial perturbations from input images sufficiently","A well written paper proposing some reasonable approaches to counter adversarial images. Proposed approaches include non-differentiable and randomized methods. Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray ""box"" settings. The approach appears to be a plausible defence strategy. One reviewers is a hold out on acceptance, but is open to the idea. The authors responded to the points of this reviewer sufficiently. The AC recommends accept."
"UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]). <sep> The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates. <sep> Pros: <sep> - Task of reducing computation by skipping inputs is interesting <sep> - Model is novel and interesting <sep> - Experiments on multiple tasks and datasets confirm the efficacy of the method <sep> - Skipping behavior can be controlled via an auxiliary loss term <sep> - Paper is clearly written <sep> Cons: <sep> - Missing comparison to prior work on sequential MNIST <sep> - Low performance on Charades dataset, no comparison to prior work <sep> - No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification <sep> The task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy. <sep> However, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work. <sep> On permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task. <sep> For Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on ""fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge"", and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don't expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else. <sep> In a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks. <sep> Though experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results. <sep> On the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author's own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments. <sep> References <sep> [1] Le et al, ""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"", arXiv 2015 <sep> [2] Arjovsky et al, ""Unitary Evolution Recurrent Neural Networks"", ICML 2016 <sep> [3] Cooijmans et al, ""Recurrent Batch Normalization"", *CONF* 2017 <sep> [4] Zhang et al, ""Architectural Complexity Measures of Recurrent Neural Networks"", NIPS 2016 <sep> [5] Sigurdsson et al, ""Hollywood in homes: Crowdsourcing data collection for activity understanding"", ECCV 2016 <sep> [6] Sigurdsson et al, ""Asynchronous temporal fields for action recognition"", CVPR 2017",This paper explores what might be characterized as an adaptive form of ZoneOut. <sep> With the improvements and clarifications added to the paper during the rebuttal the paper could be accepted.
"This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its ""nonlinear part"" to push its values in the negative direction of the loss gradient.  The authors demonstrate this using a Taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference.  Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way. <sep> Some weaknesses of the paper on the other hand — some of the parts of the paper (e.g. on weight sharing) are only somewhat related to the main topic of the paper. In fact, the authors moved the connection to SGD to the appendix, which I thought would be *more* related.   Additionally, parts of the paper are not as clearly written as they could be and lack rigor.  This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly.  The explanation following is also handwavey despite claims to being formal. <sep> Some other lower level thoughts: <sep> * Regarding weight sharing for residual layers, I don't understand why we can draw the conclusion that the initial gradient explosion is responsible for the lower generalization capability of the model with shared weights.  Are there other papers in literature that have shown this connection? <sep> * The name ""cosine loss"" suggests that this function is actually being minimized by a training procedure, but it is just a value that is being plotted… perhaps just call it the cosine? <sep> * I recommend that the authors also check out Figurnov et al CVPR 2017 (""Spatially Adaptive Computation Time for Residual Networks"") which proposes an ""adaptive"" version of ResNet based on the intuition of adaptive inference. <sep> * The plots in the later parts of the paper are quite small and hard to read.  They are also spaced together too tightly (horizontally), making it difficult to immediately see what each plot is supposed to represent via the y-axis label. <sep> * Finally, the citations need to be fixed (use \\citep{} instead of \\cite{})","The paper presents an interesting view of ResNets and the findings should be of broad interest. R1 did not update their score/review, but I am satisfied with the author response, and recommend this paper for acceptance."
"Summary: <sep> - This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. <sep> Contribution: <sep> - This paper proposes a new object counting module which operates on a graph of object proposals. <sep> Clarity: <sep> - The paper is well written and clarity is good. Figure 2 & 3 helps the readers understand the core algorithm. <sep> Pros: <sep> - De-duplication modules of inter and intra object edges are interesting. <sep> - The proposed method improves the baseline by 5% on counting questions. <sep> Cons: <sep> - The proposed model is pretty hand-crafted. I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016). <sep> - One major bottleneck of the model is that the proposals are not jointly finetuned. So if the proposals are missing a single object, this cannot really be counted. In short, if the proposals don't have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals. The paper didn't study what is the recall of the proposals and how sensitive the threshold is. <sep> - The paper doesn't study a simple baseline that just does NMS on the proposal domain. <sep> - The paper doesn't compare experiment numbers with (Chattopadhyay et al., 2017). <sep> - The proposed algorithm doesn't handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges). This is similar to a density map approach and the problem is that the model doesn't develop a notion of instance. <sep> - Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions. <sep> - Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL <sep> Conclusion: <sep> - I feel that the motivation is good, but the proposed model is too hand-crafted. Also, key experiments are missing: 1) NMS baseline 2) Comparison with VQA counting work  (Chattopadhyay et al., 2017). Therefore I recommend reject. <sep> References: <sep> - Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. *CONF* 2017. <sep> - Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. *CONF* 2016. <sep> Update: <sep> Thank you for the rebuttal. The paper is revised and I saw NMS baseline is added. I understood the reason not to compare with certain related work. The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy. However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.","Initially this paper received mixed reviews. After reading the author response, R1 and and R3 recommend acceptance. <sep> R2, who recommended rejecting the paper, did not participate in discussions, did not respond to author explanations, did not respond to AC emails, and did not submit a final recommendation. This AC does not agree with the concerns raised by R2 (e.g. I don't find this model to be unprincipled). <sep> The concerns raised by R1 and R3 were important (especially e.g. comparisons to NMS) and the authors have done a good job adding the required experiments and providing explanations. <sep> Please update the manuscript incorporating all feedback received here, including comparisons reported to the concurrent *CONF* submission on counting."
"SUMMARY <sep> The paper considers the problem of using cycle GANs to decipher text encrypted with historical ciphers. Also it presents some theory to address the problem that discriminating between the discrete data and continuous prediction is too simple. The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables. <sep> The log loss of the GAN is replaced by a quadratic loss and a regularization of the Jacobian of the discriminator. Experiments show that the method is very effective. <sep> REVIEW <sep> The paper considers an interesting and fairly original problem and the overall discussion of ciphers is quite nice. Unfortunately, my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments (contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3, which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part). Another important concern is with the proof: there seems to be an unmotivated additional assumption that appears in the middle of the proof of Proposition 1 + some steps need to be clarified (see comment 16 below). <sep> The experiments do not have any simple baseline, which is somewhat unfortunate. <sep> DETAILED COMMENTS: <sep> 1- The paper makes a few bold and debatable statements: <sep> line 9 of section 1 <sep> ""Such hand-crafted features have fallen out of favor (Goodfellow et al., 2016) as a result of their demonstrated inferiority to features learned directly from data in end-to-end learning frameworks such as neural networks"" <sep> This is certainly an overstatement and although it might be true for specific types of inputs it is not universally true, most deep architectures rely on a human-in-the-loop and there are number of areas where human crafted feature are arguably still relevant, if only to specify what is the input of a deep network: there are many domains where the notion of raw data does not make sense, and, when it does, it is usually associated with a sensing device that has been designed by a human and which implicitly imposes what the data is based on human expertise. <sep> 2- In the last paragraph of the introduction, the paper says that previous work has only worked on vocabularies of 26 characters while the current paper tackles word level ciphers with 200 words. But, isn't this just a matter of scalability and only possible with very large amounts of text? Is it really because of an intrinsic limitation or lack of scalability of previous approaches or just because the authors of the corresponding papers did not care to present larger scale experiments? <sep> 3- The discussion at the top of page 5 is difficult to follow. What do you mean when you say ""this motivates the benefits of having strong curvature globally, as opposed to linearly between etc"" <sep> Which curvature are we talking about? and what how does the ""as opposed to linearly"" mean? Should we understand ""as opposed to having curvature linearly interpolated between etc"" or ""as opposed to having a linear function""? Please clarify. <sep> 4- In the same paragraph: what does ""a region that has not seen the Jacobian norm applied to it"" mean? How is a norm applied to a region? I guess that what you mean is that the generator G might creates samples in a part of the space where the function F has not yet been learned and is essentially close to 0. Is this what you mean? <sep> 5- I do not understand why the paper introduces WGAN since in the end it does not use them but uses a quadratic loss, introduced in the first display of section 4.3. <sep> 6- The paper makes a theoretical contribution which supports replacing the sample y by a sample drawn from a region around y. But it seems that this is not used in the experiment and that the authors consider that the introduction of the embedding is a substitution for this. Indeed, in the last paragraph of section 3.1, the paper says ""we make the assumption that the training of the embedding vectors approximates random sampling similar to what is described in Proposition 1"". This does not make any sense to me because the embedding vectors map each y deterministically to a single point, and so the distribution on the corresponding vectors is still a fixed discrete distribution. This gives me this impression that the proposed theory does not match what is used in the experiments. <sep> (The last sentence of section 3.1, which is commenting on this and could perhaps clarify the situation is ill formed with two verbs.) <sep> 7- In the definitions: ""A discriminator is said to perform uninformative discrimination"" etc. -> It seems that the choice of the word uninformative would be misleading: an uninformative discrimination would be a discrimination that completely fails, while what the condition is saying it that it cannot perform perfect discrimination. I would thus suggest to call this ""imperfect discrimination"". <sep> 8- It seems that the same embedding is used in X space and in Y space (from equations 6 and 7). Is there any reason for that? I would seem more natural to me to introduce two different embeddings since the objects are a priori different... <sep> Actually I don't understand how the embeddings can be the same in the Vignere code case since time taken into account one one side. <sep> 9- On the 5th line after equation (7), the paper says ""the embeddings... are trained to minimize L_GAN and L_cyc, meaning... and are easy to discriminate"" -> This last part of the sentence seems wrong to me. The discriminator is trying to maximize L_GAN and so minimizing w.r.t. to the embedding is precisely trying to prevent to the discriminator to tell apart too easily the true elements from the estimated ones. <sep> In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings. This might be connected to the discussion at the top of page 5. Since there are no experiments with alpha different than the default value = 10, this is difficult to assess. <sep> 10-The Vigenere cipher is explained again at the end of section 4.2 when it has already been presented in section 1.1 <sep> 11- Concerning results in Table 2: I do not see why it would not be possible to compare the performance of the method with classical frequency analysis, at least for the character case. <sep> 12- At the beginning of section 4.3, the text says that the log loss was replaced with the quadratic loss, but without giving any reason. Could you explain why. <sep> 13- The only comparison of results with and without embeddings is presented in the curves of figure 3, for Brown-W with a vocabulary of 200 words. In that case it helps. Could the authors report systematically results about all cases? (I guess this might however be the only hard case...) <sep> 14- It would be useful to have a brief reminder of the architecture of the neural network (right now the reader is just refered to Zhu et al., 2017): how many layers, how many convolution layers etc. <sep> The same comment applies for the way the position of the letter/word in the text appear is in encoded in a feature that is provided as input to the neural network: it would be nice if the paper could provide a few details here and be more self contained. (The fact that the engineering of the time feature can ""dramatically"" improve the performance of the network should be an argument to convince the authors that hand-crafted feature have not fallen out of favor completely yet...) <sep> 15- I disagree with the statement made in the conclusion that the proposed work ""empirically confirms [...] that the use of continuous relaxation of discrete variable facilitates [...] and prevents [...]"" because for me the proposed implementation does not use at all the theoretical idea of continuous relaxation proposed in the paper, unless there is a major point that I am missing. <sep> 16- I have two issues with the proof in the appendix a) after the first display of the last page the paper makes an additional assumption which is not announced in the statement of the theorem, which is that two specific inequality hold... <sep> Unless I am mistaken this assumption is never proven (later or earlier). Given that this inequality is just ""the right inequality to get the proof go through"" and given that there are no explanation for why this assumption is reasonable, to me this invalidates the proof. The step of going from G(S_y) to S_(G(y)) seems delicate... <sep> b) If we accept these inequalities, the determinant of the Jacobian (the notation is not defined) of F at (x_bar) disappears from the equations, as if it could be assumed to be greater than one. If this is indeed the case, please provide a justification of this step. <sep> 17- A way to address the issue of trivial discrimination in GANs with discrete data has been proposed in <sep> Luc, P., Couprie, C., Chintala, S., & Verbeek, J. (2016). Semantic segmentation using adversarial networks. arXiv preprint arXiv:1611.08408. <sep> The authors should probably reference this paper. <sep> 18- Clarification of the Jacobian regularization: in equation (3), the Jacobian computed seems to be w.r.t D composed with F while in equation (8) it is only the Jacobian of D. Which equation is the correct one? <sep> TYPOS: <sep> Proposition 1: the if-then statement is broken into two sentences separated by a full point and a carriage return. <sep> sec. 4.3 line 10 we use a cycle loss *with a regularization coefficient* lambda=1 (a piece of the sentence is missing) <sep> sec. 4.3 lines 12-13 the learning rates given are the same at startup and after ""warming up""... <sep> In the appendix: <sep> 3rd line of proof of prop 1: I don' understand ""countably infinite finite sequences of vectors lying in the vertices of the simplex"" -> what is countable infinite here? The vertices?","this work adapts cycle GAN to the problem of decipherment with some success. it's still an early result, but all the reviewers have found it to be interesting and worthwhile for publication."
"Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn't contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient. <sep> Contribution: <sep> - The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN. <sep> Pros: <sep> - Models that dynamically decide the amount of computation make intuitive sense and are of general interests. <sep> - The paper presents solid experimentation on various text classification and question answering datasets. <sep> - The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks). <sep> - The paper is well written, and the presentation is good. <sep> Cons: <sep> - Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN. <sep> - The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps. <sep> - Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental. <sep> Questions: <sep> - Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup. <sep> - One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary. <sep> Conclusion: <sep> - Based on the comments above, I recommend Accept","this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.) the reviewers found the submission very positive. <sep> please, do not forget to include all the result and discussion on the proposed approach's relationship to VCRNN which was presented at the same conference just a year ago."
"# Summary and Assessment <sep> The paper addresses an important issue–that of making learning of recurrent networks tractable for sequence lengths well beyond 1'000s of time steps. A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor. <sep> The authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator. Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively. <sep> In the following, the authors explore the landscape of RNNs satisfying the necessary conditions. The performance is investigated in terms of wall clock time. Further, experimental results of problems with previously untacked sequence lengths are reported. <sep> The paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies. <sep> To me, the execution seems sound. The experiments back up the claim. <sep> ## Minor <sep> - I challenge the claim that thousands and millions of time steps are a common issue in ""robotics, remote sensing, control systems, speech recognition, medicine and finance"", as claimed in the first paragraph of the introduction. IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I'd appreciate a few examples where this is a case to better justify the method.","Paper presents a way in which linear RNNs can be computed (fprop, bprop) using parallel scan. They show big improvements in speedups and show application on really long sequences. Reviews were generally favorable."
"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized. The paper is rigorous and ideas are clearly stated. The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation. I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. <sep> 1. The framework uses the class information, i.e., ""only data samples from the normal class are used for training"", but it is still considered unsupervised. Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information. I would like to see a plot of the sample energy as a function of the number of data points. Is there an elbow that indicates the threshold cut? Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 – LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data). <sep> 2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network? <sep> 3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results? <sep> 4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model). Those approaches should at least be discussed in the related work, if not compared against. <sep> 5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better. Could you provide a comparison with EM? <sep> 6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant? Does it well describe the new space? Do you normalize the features (the output of the dimension reduction and the representation error are quite different)? Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians. <sep> 7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets. <sep> 8. The authors mention that ""we can clearly see from Fig. 3a that DAGMM is able to well separate ..."" - it is not clear to me, it does look better than the other ones, but not clear. If there is a clear separation from a different view, show that one instead. We don't need the same view for all methods. <sep> 9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them. This seems very drastic! <sep> Minor comments: <sep> 1. Fig.1: what dimension reduction did you use? Add axis labels. <sep> 2. ""DAGMM preserves the key information of an input sample"" - what does key information mean? <sep> 3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE. Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined. They are the best in terms of precision. <sep> 4. Is the error in Table 2 averaged over multiple runs? If yes, how many? <sep> Quality – The paper is thoroughly written, and the ideas are clearly presented. It can be further improved as mentioned in the comments. <sep> Clarity – The paper is very well written with clear statements, a pleasure to read. <sep> Originality – Fairly original, but it still needs some work to justify it better. <sep> Significance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.",+ Empirically convincing and clearly explained application: a novel deep learning architecture and approach is shown to significantly outperform state-of-the-art in unsupervised anomaly detection. <sep> - No clear theoretical foundation and justification is provided for the approach <sep> - Connexion and differentiation from prior work on simulataneous learning representation and fitting a Gaussian mixture to it would deserve a much more thorough discussion / treatment.
"Summary <sep> --- <sep> This paper proposes a new model called SCAN (Symbol-Concept Association Network) for hierarchical concept learning. It trains one VAE on images then another one on symbols and aligns their latent spaces. This allows for symbol2image and image2symbol inference. But it also allows for generalization to new concepts composed from existing concepts using logical operators. Experiments show that SCAN generates images which correspond to provided concept labels and span the space of concepts which match these labels. <sep> The model starts with a beta-VAE trained on images (x) from the relevant domain (in this case, simple scenes generated from DeepMind Lab which vary across a few known dimensions). This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. SCAN optimizes the ELBO plus a KL term which pushes the latent distribution of the y VAE toward the latent distribution of the x (image) VAE. This aligns the latent representations so now a symbol can be encoded into a latent distribution z and decoded as an image. <sep> One nice property of the learned latent representation is that more specific concepts have more specific latent representations. Consider latent distributions z1 and z2 for a more general symbol {red} and a more specific symbol {red, suitcase}. Fewer dimensions of z2 have high variance than dimensions of z1. For example, the latent space could encode red and suitcase in two dimensions (as binary attributes). z1 would have high variance on all dimensions but the one which encodes red and z2 would have high variance on all dimensions but red and suitcase. In the reported experiments some of the dimensions do seem to be interpretable attributes (figure 5 right). <sep> SCAN also pays particular attention to hierarchical concepts. Another very simple model (1d convolution layer) is learned to mimic logical operators. Normally a SCAN encoder takes {red} as input and the decoder reconstructs {red}. Now another model is trained that takes ""{red} AND {suitcase}"" as input and reconstructs {red, suitcase}. The two input concepts {red} and {suitcase} are each encoded by a pre-trained SCAN encoder and then those two distributions are combined into one by a simple 1d convolution module trained to implement the AND operator (or IGNORE/IN COMMON). This allows images of concepts like {small, red, suitcase} to be generated even if small red suitcases are not in the training data. <sep> Experiments provide some basic verification and analysis of the method: <sep> 1) Qualitatively, concept samples are correct and diverse, generating images with all configurations of attributes not specified by the input concept. <sep> 2) As SCAN sees more diverse examples of a concept (e.g. suitcases of all colors instead of just red ones) it starts to generate more diverse image samples of that concept. <sep> 3) SCAN samples/representations are more accurate (generate images of the right concept) and more diverse (far from a uniform prior in a KL sense) than JMVAE and TELBO baselines. <sep> 4) SCAN is also compared to SCAN_U, which uses an image beta-VAE that learned an entangled (Unstructured) representation. SCAN_U performed worse than SCAN <sep> and baselines. <sep> 5) Concepts expressed as logical combinations of other concepts generalize well for both the SCAN representation and the baseline representations. <sep> Strengths <sep> --- <sep> The idea of concept learning considered here is novel and satisfying. It imposing logical, hierarchical structure on latent representations in a general way. This suggests opportunities for inserting prior information and adds interpretability to the latent space. <sep> Weaknesses <sep> --- <sep> I think this paper is missing some important evaluation. <sep> Role/Nature of Disentangled Features not Clear (major): <sep> * Disentangled features seem to be very important for SCAN to work well (SCAN vs SCAN_U). It seems that the only difference between the unstructured (entangled) and the structured (disentangled) visual VAE is the color space of the input (RGB vs HSV). If so, this should be stated more clearly in the main paper. What role did beta-VAE (tuning beta) as opposed to plain VAE play in learning disentangled features? <sep> * What color space was used for the JMVAE and TELBO baselines? Training these with HSV seems especially important for establishing a good comparison, but it would be good to report results for HSV and RGB for all models. <sep> * How specific is the HSV trick to this domain? Would it matter for natural images? <sep> * How would a latent representation learned via supervision perform? (Maybe explicitly align dimensions of z to red/suitcase/small with supervision through some mechanism. c.f. ""Discovering Hidden Factors of Variation in Deep Networks"" by Cheung et al.) <sep> Evaluation of sample complexity (major): <sep> * One of the main benefits of SCAN is that it works with less training data. There should be a more systematic evaluation of this claim. In particular, I would like to see a Number of Examples vs Performance (Accuracy/Diversity) plot for both SCAN and the baselines. <sep> Minor questions/comments/concerns: <sep> * What do the logical operators learn that the hand-specified versions do not? <sep> * Does training SCAN with the structure provided by the logical operators lead to improved performance? <sep> * There seems to be a mistake in figure 5 unless I interpreted it incorrectly. The right side doesn't match the left side. During the middle stage of training object hues vary on the left, but floor color becomes less specific on the right. Shouldn't object color become less specific? <sep> Prelimary Evaluation <sep> --- <sep> This clear and well written paper describes an interesting and novel way of learning a model of hierarchical concepts. It's missing some evaluation that would help establish the sample complexity benefit more precisely (a claimed contribution) and add important details about unsupervised disentangled representations. I would happy to increase my rating if these are addressed.","This paper initially received borderline reviews. The main concern raised by all reviewers was a limited experimental evaluation (synthetic only). In rebuttal, the authors provided new results on the CelebA dataset, which turned the first reviewer positive. The AC agrees there is merit to this approach, and generally appreciates the idea of compositional concept learning."
"This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible strategies. One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the assigned pseudo-labels. Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to any of the known categories. In practice this second solution is analogous to the first, but a general 'distractor' class is added. Finally the third technique learns to weight the samples according to their distance to the original prototypes. <sep> These strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting multiple times a large dataset), then they are used on a final target task with again few labeled data and large unlabeled samples but beloning to a different set of categories. <sep> + the paper is well written, well organized and overall easy to read <sep> +/-  this work builds largely on previous work. It introduces only some small technical novelty inspired by soft-k-means clustering that anyway seems to be effective. <sep> + different aspect of the problem are analyzed by varying the number of disctractors and varying the level of semantic relatedness between the source and the target sets <sep> Few notes and questions <sep> 1) why for the omniglot experiment the table reports the error results? It would be better to present accuracy as for the other tables/experiments <sep> 2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because actually there is a training phase also at test time. <sep> 3) although the paper indicate that there are different other few-shot methods that could be applicable here, <sep> no other approach is considered besides the prothotipical network and its variants. An further external reference could be used to give an idea of what would be the experimental result at least in the supervised case.","The paper extends the earlier work on Prototypical networks to semi-supervised setting. Reviewers largely agree that the paper is well-written. There are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, I recommend acceptance."
"This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy. The writing is clear, concise and easy to follow. <sep> An important argument in favour of using complex-valued networks is said to be the propagation of phase information. However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat. CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise. It definitely does not preserve phase, like modReLU would. <sep> This makes me wonder whether the ""complex numbers"" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in ""Compressing neural networks with the hashing trick"" by Chen et al.). Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this. <sep> The image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach. The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated. It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague. Once again, a more in-depth analysis of this phase behavior would be very welcome. <sep> I'm on the fence about this work: I like the ideas and they are explained well, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). <sep> Comments: <sep> - The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work. Maybe some subsection titles would help make it feel a bit more cohesive. <sep> - page 3: ""(cite a couple of them)"" should be replaced by some actual references :) <sep> - Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer? It would be nice to discuss computational cost as well. <sep> REVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments. In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network. <sep> Regarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this. With this in mind, it is of course completely fine that the results are not better than for real-valued networks.","The paper received mostly positive comments from experts. To summarize: <sep> Pros: <sep> -- The paper provides complex counterparts for typical architectures / optimization strategies used by real valued networks. <sep> Cons: <sep> -- Although the authors include plots explaining how nonlinearities transform phase, intuition about how phase gets processed can be improved. <sep> -- Improving evaluations: Wisdom et al. computes log magnitude; real valued networks may not be suited for computing real / complex numbers which have a large dynamic range, like the complex spectra. So please compare performance by estimating magnitude as in Wisdom et al. <sep> -- Please add computational cost, in terms of the number of multiplies and adds, to the final version of the paper. <sep> I am recommending that the paper be accepted based on these reviews."
"The paper proposes ""wavelet pooling"" as an alternative for traditional subsampling methods, e.g. max/average/global pooling, etc., within convolutional neural networks. <sep> Experiments on the MNIST, CIFAR-10, SHVN and KDEF datasets, shows the proposed wavelet-based method has competitive performance with existing methods while still being able to address the overfitting behavior of max pooling. <sep> Strong points <sep> - The method is sound and well motivated. <sep> - The proposes method achieves competitive performance. <sep> Weak points <sep> - No information about added computational costs is given. <sep> - Experiments are conducted in relatively low-scale datasets. <sep> Overall the method is well presented and properly motivated. The paper as a good flow and is easy to follow. The authors effectively demonstrate with few toy examples the weaknesses of traditional methods, i.e max pooling and average pooling. Moreover, their extended evaluation on several datasets show the performance of the proposed method in different scenarios. <sep> My main concerns with the manuscript are the following. <sep> Compared to traditional methods, the proposed methods seems to require higher computation costs. In a deep neural network setting where operations are conducted a large number of times, this is a of importance. However, no indication is given on what are the added computation costs of the proposed method and how that compares to existing methods. A comparison on that regard would strengthen the paper. <sep> In many of the experiments, the manuscript stresses the overfitting behavior of max pooling. This makes me wonder whether this is caused by the fact that experiments are conducted or relatively smaller datasets. While the currently tested datasets are a good indication of the performance of the proposed method, an evaluation on a large scale scenario, e.g. ILSVRC'12, could solidify the message sent by this manuscript. Moreover, it would increase the relevance of this work in the computer vision community. <sep> Finally, related to the presentation, I would recommend presenting the plots, i.e. Fig. 8,10,12,14, for the training and validation image subsets in two separate plots. Currently, results for training and validation sets are mixed in the same plot, and due to the clutter it is not possible to see the trends clearly. <sep> Similarly, I would recommend referring to the Tables added in the paper when discussing the performance of the proposed method w.r.t. traditional alternatives. <sep> I encourage the authors to address my concerns in their rebuttal","The idea of using wavelet pooling is novel and will bring many interesting research work in this direction. But more thorough experimental justification such as those recommended by the reviewers would make the paper better. Overall, the committee feels this paper will bring value to the conference."
"This paper presents a new 2nd-order algorithm that implicitly uses curvature information, and it shows the intuition behind the approximation schemes in the algorithms and also validates the heuristics in various experiments.  The method involves using Neumann Series and Richardson iteration to avoid Hessian-vector product in second order method for NN.  In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization.  The numerical examples are relatively clear and easy to figure out details. <sep> 1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer.  For example, one simple experiment would be showing how it works for convex problems, e.g., logistic regression.  Realistic DNN systems are very complex, and evaluating the method in a simple setting would help a lot in determining what if anything is novel about the method. <sep> 2. Also, for deep learning problems, it would be more convincing to see how different initialization can affect the performances. <sep> 3. Although the authors present their algorithm as a second order method at beginning, the final algorithm is kind of like a complex momentum SGD with limited memory.  Rather than simply throwing out a new method with a new name, it would be helpful to understand what the steps of this method are implicitly doing.  Please explain more about this. <sep> 4. It said that the algorithm is hyperparameter free except for learning rate.  However, it is hard to see why there is no need to tune other hyperparameters, e.g., Cubic Regularizer, Repulsive Regularizer.  The effect/sensitivity of hyperparameters for second order methods are quite different than hyperparameters for first order methods, and it is of interest to know how hyperparameters for implicit second order methods perform. <sep> 5. For Section 4.2, the well know benefit by using large batch size to train models is that it could reduce training time and epochs.  However, from Table 3, there is no such phenomenon.  Please explain.",Pros: <sep> + Clearly written paper. <sep> + Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases. <sep> + Thorough evaluation against the state of the art. <sep> Cons: <sep> - No theoretical guarantees for the algorithm. <sep> This paper belongs in *CONF* if there is enough space.
"Summary of paper: <sep> The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. They compare this optimization method with two baselines in MNIST and CIFAR, and provide an analysis of the decision boundaries by their adversarial examples, the baselines and non-altered examples. <sep> Review summary: <sep> I think this paper is interesting. The novelty of the attack is a bit dim, since it seems it's just the straightforward attack against the region cls defense. The authors fail to include the most standard baseline attack, namely FSGM. The authors also miss the most standard defense, training with adversarial examples. As well, the considered attacks are in L2 norm, and the distortion is measured in L2, while the defenses measure distortion in L_\\infty (see detailed comments for the significance of this if considering white-box defenses). The provided analysis is insightful, though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks. <sep> If the authors add FSGM to the batch of experiments (especially section 4.1) and address some of the objections I will consider updating my score. <sep> A more detailed review follows. <sep> Detailed comments: <sep> - I think the novelty of the attack is not very strong. The authors essentially develop an attack targeted to the region cls defense. Designing an attack for a specific defense is very well established in the literature, and the fact that the attack fools this specific defense is not surprising. <sep> - I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack (such as PGD or region based), or for defenses that know this is a likely attack (see the following comment as well). If the authors want to make the second claim, training the network with adversarial examples coming from OptMargin is missing. <sep> - The attacks are all based in L2, in the sense that the look for they measure perturbation in an L2 sense (as the paper evaluation does), while the defenses are all L_\\infty based (since the region classifier method samples from a hypercube, and PGD uses an L_\\infty perturbation limit). This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack. <sep> - The simplest most standard baseline of all (FSGM) is missing. This is important to compare properly with previous work. <sep> - The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack. This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack. <sep> - I think the authors rush to conclude that ""a small ball around a given input distance can be misleading"". Wether balls are in L2 or L_\\infty, or another norm makes a big difference in defense and attacks, given that they are only equivalent to a multiplicative factor of sqrt(d) where d is the dimension of the space, and we are dealing with very high dimensional problems. I find the analysis made by the authors to be very simplistic. <sep> - The analysis of section 4.1 is interesting, it was insightful and to the best of my knowledge novel. Again I would ask the authors to make these plots for FSGM. Since FSGM is known to be robust to small random perturbations, I would be surprised that for a majority of random directions, the adversarial examples are brought back to the original class. <sep> - I think a bit more analysis is needed in section 4.2. Do the authors think that this distinguishability can lead to a defense that uses these statistics? If so, how? <sep> - I think the analysis of section 5 is fairly trivial. Distinguishability in high dimensions is an easy problem (as any GAN experiment confirms, see for example Arjovsky & Bottou, *CONF* 2017), so it's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries. <sep> - Will the authors release code to reproduce all their experiments and methods? <sep> Minor comments: <sep> - The justification of why OptStrong is missing from Table2 (last three sentences of 3.3) should be summarized in the caption of table 2 (even just pointing to the text), otherwise a first reader will mistake this for the omission of a baseline. <sep> - I think it's important to state in table 1 what is the amount of distortion noticeable by a human. <sep> ========================================= <sep> After the rebuttal I've updated my score, due to the addition of FSGM added as a baseline and a few clarifications. I now understand more the claims of the paper, and their experiments towards them. I still think the novelty, significance of the claims and protocol are still perhaps borderline for publication (though I'm leaning towards acceptance), but I don't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence.","Authors propose an approach to generation of adversarial examples that jointly examine the effects to classification within a local neighborhood, to yield a more robust example. This idea is taken a step further for defense, whereby the classification boundaries within a local neighborhood of a presented example are examined to determine if the data was adversarially generated or not. <sep> Pro: <sep> - The idea of examining local neighborhoods around data points appears new and interesting. <sep> - Evaluation and investigation is thorough and insightful. <sep> - Authors made reasonable attempts to address reviewer concerns. <sep> Con <sep> - Generation of adversarial examples an incremental improvement over prior methods"
"In this paper, the authors interpret the training of GAN by potential field and inspired from which to provide new training procedure for GAN. They claim that under the condition that global optima are achieved for discriminator and generator in each iteration, the Coulomb GAN converges to the global solution. <sep> I think there are several points need to be addressed. <sep> 1, I agree that the ""model collapsing"" is due to converging to a local Nash Equilibrium. However, there are more reasons besides the drawback of the loss function, which is emphasized in the paper. Leave the stochastic gradient descent optimization algorithm apart (since most of the neural networks are trained in this way), the parametrization and the richness of discriminator family play a vital role in the model collapsing issue. In fact, even with KL-divergence in which log operation is involved, if one can select reasonable parametrization, e.g., directly handling in function space, the saddle point optimization is convex-concave, which means under the same assumption made in the paper, there is only one global Nash Equilibrium. On the other hand, the richness of the discriminator also important in the training of GAN. I did not get the point about the drawback of III. If indeed as the paper considered in the ideal case, the discriminator is rich enough, III cannot happen. <sep> The model collapsing is not just because loss function in training GAN. It is caused by the twist of these three issues listed above. Modifying the loss can avoid partially model collapsing, however, it is not appropriate to claim that the proposed algorithm is 'provable'. The assumption in this paper is too restricted, and the discussion is unfair to the existing variants of GAN, e.g., GMMN or Wasserstein GAN, which under some assumptions, there is also only one global Nash Equilibrium. <sep> 2, In the training procedure, the discriminator family is important as we discussed. The paper claims that the reason to introduce the extra discriminator is reducing variance. However, such parametrization will introduce bias too. The bias and variance tradeoff should be explicitly discussed here. Ideally, it should contain all the functions formed with Plummer kernel, but not too large (otherwise, it will increase the sample complexity.). Which function family used in the paper is not clear. <sep> 3, As the authors already realized, the GMMN is one closely related model. It will be more convincing to add the comparison with GMMN. <sep> In sum, this paper provides an interesting perspective modeling GAN from the potential field, however, there are several issues need to be addressed. I expect to see the reply of the authors regarding the mentioned issues.","The paper provides an interesting take on GAN training based on Coulomb dynamics. The proposed formulation is theoretically well motivated and authors provide guarantees for convergence. Reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results. The method addresses mode collapse issue but still lacks in sample quality. Nevertheless, reviewers agree that this is a good step towards the understanding of GAN training."
"Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. The application is to predict errors made by students on programming tasks. This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds). <sep> --- <sep> Quality: The experiments compare the three proposed neural network architectures with two syntax-based architectures. It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings. <sep> Clarity: The paper is clearly written. <sep> Originality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). <sep> Significance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand. <sep> --- <sep> Some questions/comments: <sep> - Do we need to add the print statements for any new programs that the students submit? What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated? <sep> --- <sep> References <sep> Cai, J., Shin, R., & Song, D. (2017). Making Neural Programming Architectures Generalize via Recursion. In International Conference on Learning Representations (*CONF*).",PROS: <sep> 1. Interesting and clearly useful idea <sep> 2. The paper is clearly written. <sep> 3. This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). <sep> 4. This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand. <sep> CONS: <sep> 1. The paper has some clarity issues which the authors have promised to fix. <sep> ---
"The authors present a novel evolution scheme applied to neural network architecture search. It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm. To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. To speed up the search, they focus on finding cells instead of an entire network. In evaluation time, they insert these cells between layers of a network comparable in size to known networks. They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance. <sep> The method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. It could lead to new insight on automating design of neural networks for given problems. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The paper presents a good work and is well articulated. However, it could benefit from additional details and a deeper analysis of the results. <sep> The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. The method is also appealing for its use of some kind of emergence between two levels of hierarchy. In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules. Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm. Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy? Are the cells found interpretable? The authors should try to give their opinion about the design obtained. <sep> The implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description. During evaluation, what is a step? A batch or an epoch or other? <sep> The method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. It raises questions on the usability of the method for small labs. At some point, we will have to use insights from this search to stop early, when no improvement is expected. Also, authors claim that their method consume less computation time than reinforcement learning. This should be supported by some quantitative results. <sep> The paper would greatly benefit from a deeper comparison over other techniques. For instance, it could describe more the advantages over reinforcement learning. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. It could have taken more spaces in the paper. <sep> I am also concerned the computational efficiency of the results obtained with this method on current processors. Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand. Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand. Does the solution obtained with the optimization can be run as efficiently? A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper. This is a general comment over this kind of approach, but I think it should be addressed.","PROS: <sep> 1. Overall, the paper is well-written, clear in its exposition and technically sound. <sep> 2. With some caveats, an independent team concluded that the results were ""largely reproducible"" <sep> 3. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. <sep> 4. The implementation seems technically sound. <sep> CONS: <sep> 1. The results were a bit over-stated (the authors promise to correct) <sep> 2. Could benefit from more comparison with other approaches (e.g. RL)"
"Summary: <sep> The authors propose a method to make exploration in really sparse reward tasks more efficient. They propose a method called Workflow Guided Exploration (WGE) which is learnt from demonstrations but is environment agnostic. Episodes are generated by first turning demonstrations to a workflow lattice. This lattice encodes actions which are in some sense similar to those in the demonstration. By rolling out episodes which are randomly sampled from this set of similar actions for each encountered state, it is claimed that other methods like Behavor Cloning + RL (BC-then-RL) can be outperformed in terms of number of sample complexity since high reward episodes can be sampled with much higher probability. <sep> A novel NN architecture (DOMNet) is also presented which can embed structured documents like HTML webpages. <sep> Comments: <sep> - The paper is well-written and relevant literature is cited and discussed. <sep> - My main concern is that while imitation learning and inverse reinforcement learning are mentioned and discussed in related work section as classes of algorithms for incorporating prior information there is no baseline experiment using either of these methods. Note that the work of Ross and Bagnell, 2010, 2011 (cited in the paper) establish theoretically that Behavior Cloning does not work in such situations due to the non-iid data generation process in such sequential decision-making settings (the mistakes grow quadratically in the length of the horizon). Their proposed algorithm DAgger fixes this (the mistakes by the policy are linear in the horizon length) by using an iterative procedure where the learnt policy from the previous iteration is executed and expert demonstrations on the visited states are recorded, the new data thus generated is added to the previous data and a new policy retrained. Dagger and related methods like Aggrevate provide sample-efficient ways of exploring the environment near where the initial demonstrations were given. WGE is aiming to do the same: explore near demonstration states. <sep> - The problem with putting in the replay buffer only episodes which yield high reward is that extrapolation will inevitably lead the learnt policy towards parts of the state space where there is actually low reward but since no support is present the policy makes such mistakes. <sep> - Therefore would be good to have Dagger or a similar imitation learning algorithm be used as a baseline in the experiments. <sep> - Similar concerns with IRL methods not being used as baselines. <sep> Update: Review score updated after discussion with authors below.",PROS: <sep> 1. well-written and clear <sep> 2. added extra comparison to dagger which shows success <sep> 3. SOTA results on open ai benchmark problem and comparison to relevant related work (Shi 2017) <sep> 4. practical applications <sep> 5. created new dataset to test harder aspects of the problem <sep> CONS: <sep> 1. the algorithmic novelty is somewhat limited <sep> 2. some indication of scalability to real-world tasks is provided but it is limited
"This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. The novelty of this work is a refined aggregation process, which is improved in three ways: <sep> a) Gaussian instead of Laplace noise is used to achieve differential privacy. <sep> b) Queries to the aggregator are ""filtered"" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong. <sep> c) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query. <sep> I think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. On this basis, I think the paper should be accepted. However, I think some clarification is needed with regard to item c above: <sep> Theorem 2 gives a data-dependent privacy guarantee. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low. This data-dependent privacy guarantee is likely to be much tighter than the data-independent guarantee. <sep> However, since the privacy guarantee now depends on the data, it is itself sensitive information. How is this issue resolved? If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. This would resemble the ""privacy odometer"" setting of Rogers-Roth-Ullman-Vadhan [ https://arxiv.org/abs/1605.08294 ]. <sep> Another way to resolve this would be to have an output-dependent privacy guarantee. That is, the privacy guarantee would depend only on public information, rather than the private data. The widely-used ""sparse vector"" technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this. <sep> In any case, this is an important issue that needs to be clarified, as it is not clear to me how this is resolved. <sep> The algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a ""student"" being trained using sensitive data with queries being answered in a differentially private manner. And, in particular, these works also filter out uninformative queries using the sparse vector technique. It would be helpful to add a comparison.","This paper extends last year's paper on PATE to large-scale, real-world datasets. The model works by training multiple ""teacher"" models -- one per dataset, where a dataset might be for example, one user's data -- and then distilling those models into a student model. The teachers are all trained on disjoint data. Differential privacy is guaranteed by aggregating the teacher responses with added noise. The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query. The new results beat the old results convincingly on a variety of measures. <sep> Quality and Clarity: The reviewers and I thought the paper was well written. <sep> Originality: In some sense this work is incremental, extending and improving the existing PATE framework. However, the extensions and new analysis are non-trivial and the results are good. <sep> PROS: <sep> 1. Well written though difficult in places for somebody like myself who is not involved in this area. <sep> 2. Much improved scalability to real datasets <sep> 3. Good theoretical analysis supporting the extensions. <sep> 4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results) <sep> CONS: <sep> 1. Perhaps a little dense for the non-expert"
"After reading rebuttals from the authors: The authors have addressed all of my concerns. THe additional experiments are a good addition. <sep> ************************ <sep> The authors provide an algorithm-agnostic active learning algorithm for multi-class classification. The core technique is to construct a coreset of points whose labels inform the labels of other points.  The coreset construction requires one to construct a set of  points which can cover the entire dataset. While this is NP-hard problem in general, the greedy algorithm is 2-approximate. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried. The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. <sep> The experimental results are convincing enough to show that it outperforms other active learning algorithms. However, I have a few major and minor comments. <sep> Major comments: <sep> 1. The proof of Lemma 1 is incomplete. We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label. The proof of lemma 1 only establishes the Lipschitz constant of the CNN function. Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function. <sep> 2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset. Why is this term included. Is this term not equal to 0? <sep> 3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning. <sep> UPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf <sep> Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf <sep> A bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf <sep> 4.  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function. I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper. For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training. <sep> Minor-comment: <sep> 1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. It would have been great had the authors mentioned that u_j \\in {0,1}. <sep> 2. The authors write on page 4, ""Moreover, zero training error can be enforced by converting average loss into maximal loss"". It is not clear to me what the authors mean here. For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that? Why would that result in zero training error? <sep> On the whole this is interesting work and the results are very nice. But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified. Also, important references in active learning literature are missing.","The effectiveness of active learning techniques for training modern deep learning pipelines in a label efficient manner is certainly a very well motivated topic. The reviewers unanimously found the contributions of this paper to be of interest, particularly nice empirical gains over several natural baselines."
"Update: Based on the discussions and the revisions, I have improved my rating. However I still feel like the novelty is somewhat limited, hence the recommendation. <sep> ====================== <sep> The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM to determine when a smartphone enters or exits a building, then using the change in barometric pressure from the entrance of the building to indoor location. Overall the methodology is a fairly simple application of existing methods to a problem, and  there remain some methodological issues (see below). <sep> General Comments <sep> - The claim that the bmp280 device is in most smartphones today doesn't seem to be backed up by the ""comScore"" reference (a simple ranking of manufacturers).  Please provide the original source for this information. <sep> - Almost all exciting results based on RNNs are achieved with LSTMs, so calling an RNN with LSTM hidden units a new name IOLSTM seems rather strange - this is simply an LSTM. <sep> - There exist models for modelling multiple levels of abstraction, such as the contextual LSTM of [1]. This would be much more satisfying that the two level approach taken here, would likely perform better, would replace the need for the clustering method, and would solve issues such as the user being on the roof.  The only caveat is that it may require an encoding of the building (through a one-hot encoding) to ensure that the relationship between the floor height and barometric pressure is learnt. For unseen buildings a background class could be used, the estimators as used before, or aggregation of the other buildings by turning the whole vector on. <sep> - It's not clear if a bias of 1 was added to the forget gate of the LSTM or not. This has been shown to improve results [2]. <sep> - Overall the whole pipeline feels very ad-hoc, with many hand-tuned parameters.  Notwithstanding the network architecture, here I'm referring to the window for the barometric pressure, the Jaccard distance threshold, the binary mask lengths, and the time window for selecting p0. <sep> - Are there plans to release the data and/or the code for the experiments? Currently the results would be impossible to reproduce. <sep> - The typo of accuracy given by the authors is somewhat worrying, given that the result is repeated several times in the paper. <sep> Typographical Issues <sep> - Page 1:  ""floor-level accuracy"" back ticks <sep> - Page  4:   Figure  4.1→Figure  1;  Nawarathne  et  al  Nawarathne  et  al.→Nawarathne et al. <sep> - Page 6:  ""carpet to carpet"" back ticks <sep> - Table 2:  What does -4+ mean? <sep> - References.  The references should have capitalisation where appropriate.For example,  Iodetector→IODetector,  wi-fi→Wi-Fi,  apple→Apple, iphone→iPhone, i→I etc. <sep> [1]  Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, and LarryHeck. Contextual LSTM (CLSTM) models for large scale NLP tasks. arXivpreprint arXiv:1602.06291, 2016. <sep> [2]  Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.  An empirical exploration of recurrent network architectures.  InProceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2342–2350,2015","Reviewers agree that the paper is well done and addresses an interesting problem, but uses fairly standard ML techniques. <sep> The authors have responded to rebuttals with careful revisions, and improved results."
"In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. Here, the expressive power refers to the rank of tensor decomposition, i.e., the number of latent components. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks. It is proved that the space of TT-type networks with rank O(r)  can be complex as the same as the space of CP-type networks with rank poly(r). <sep> The paper is clearly written and easy to follow. <sep> The contribution is clear and it is distinguished from previous studies. <sep> Though I enjoyed reading this paper, I have several concerns. <sep> 1. The authors compare the complexity of TT representation with CP representation (and HT representation). However, CP representation does not have universality (i.e., some tensors cannot be expressed by CP representation with finite rank, see [1]), this comparison may not make sense. It seems the comparison with Tucker-type representation makes much more sense because it has universality. <sep> 2. Connecting RNN and TT representation is a bit confusing. Specifically, I found two gaps. <sep> (a) RNNs reuse the same parameter against all the input x_1 to x_d. This means that G_1 to G_d in Figure 1 are all the same. That's why RNNs can handle size-varying sequences. <sep> (b) Standard RNNs do not use the multilinear units shown in Figure 3, but use a simple addition of an input and the output from the previous layer (i.e., h_t = f(Wx_t + Vh_{t-1}), where h_t is the t-th hidden unit, x_t is the t-th input, W and V are weights, and f is an activation function.) <sep> Due to the gaps, the analysis used in this paper seems not applicable to RNNs. If this is true, the story of this paper is somewhat misleading. Or, is your theory still applicable? <sep> [1] Hackbusch, Wolfgang. Tensor spaces and numerical tensor calculus. Vol. 42. Springer Science & Business Media, 2012.","This paper offers a theoretical and empirical analysis of the expressivity of RNNs, in particular in comparison to TT decomposition. The reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations. More critical reviewers argued the comparison basis with CP networks was not ""fair"" in that their shallowness restricted their expressivity w.r.t. TT. The experiments could be strengthened by making the explanations surrounding the set up clearer. This paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author. From reading the reviews and the author responses, I am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at *CONF*."
"The paper proposes an approach to exploration based on initializing a value function to 1 everywhere, then letting the value decay back toward zero as the state space is explored. I like the idea a lot. I don't really like the paper, though. I'd really like to see a strong theoretical and/or empirical justification for it, and both are lacking. On the theoretical side, can a bound be proven for this approach, even in the tabular case? On the empirical side, there are more (and more recent!) testbeds that have come to define the field---the mountain car problem is just not sufficient to convincingly argue that the method scales and generalizes. My intuition is that such an approach ought to be effective, but I really want to see additional evidence. Given the availability of so many RL testbeds, I worry that it had been tried but failed. <sep> Detailed comments: <sep> ""Where γ is"" -> "", <newline> where γ is"". <sep> ""The other alternative"" -> ""The alternative""? <sep> ""without learning a model (Mongillo et al., 2014)."": Seems like an odd choice for a citation for model-free RL. Perhaps select the paper that first used the term? Or an RL survey? <sep> Right before Section 1.1, put a period after the Q-learning update equation. <sep> ""new states may"" -> ""new states, may"". <sep> ""such approaches leads"" -> ""such approaches lead"". <sep> ""they still fails"" -> ""they still fail"". <sep> ""evaluated with respect only to its immediate outcome"": Not so. Several of the cited papers use counters to determine which states are ""known"" and then solve an MDP to direct exploration past immediate outcomes. <sep> "" exploration bonus(Little & Sommer, 2014)"" -> "" exploration bonus (Little & Sommer, 2014)"". <sep> ""n a model-free settings."" -> ""n model-free settings."". <sep> "" Therefore, a satisfying approach for propagating directed exploration in model-free reinforcement learning is still missing. "": I think you should cite http://research.cs.rutgers.edu/~nouri/papers/nips08mre.pdf , which also combines a kind of counter idea with function approximation to improve exploration. <sep> ""initializing E-values to 1"": I like this idea. I wonder if one could prove bounds similar to the delayed Q-learning algorithm with this approach. It is reminiscent of https://arxiv.org/pdf/1205.2606.pdf , which also drives exploration by beginning with an overly optimistic estimate and letting the data (in a function approximation setting) decay the influence of this initialization. <sep> ""So after visited n times"" -> ""So after being visited n times"". <sep> ""figure 1a"" -> ""Figure 1a"". (And, in other places.) <sep> ""An important property of E-values is that it decreases over repetition"" -> ""An important property of E-values is that they decrease over repetition"". <sep> ""t utilize counters, can"" -> ""t utilize counters can"". <sep> "" hence we were interested a convergence measure"": Multiple problems in this sentence, please fix. <sep> Figure 2: How many states are in this environment? Some description is needed. <sep> Figure 3: The labels in this figure (and all the figures) are absurdly small and, hence, unreadable. <sep> ""now turn to show that by using counters,"" -> ""now turn to showing that, by using counters,"". <sep> Theorem 3.1: I'm not quite getting why we want to take a stochastic rule and make it deterministic. Note that standard PAC-MDP algorithms choose deterministically. It's not clear why we'd want to start from a stochastic rule. <sep> "" models(Bellemare"" -> "" models (Bellemare"". <sep> ""Efficient memory-based learning for robot control"": This reference is incomplete. (I'm skeptical that it represents the first use of this problem, but I can't check it.) <sep> ""Softmax exploration fail"" -> ""Softmax exploration fails"". <sep> ""whom also analyzed"" -> ""who also analyzed"". <sep> ""non-Markovity"" -> ""non-Markovianness""?","This is a very interesting paper that also seems a little underdeveloped. As noted by the reviewers, it would have been nice to see the idea applied to domains requiring function approximation to confirm that it can scale -- the late addition of Freeway results is nice, but Freeway is also by far the simplest exploration problem in the Atari suite. There also seems to be a confusion between methods such as UCB, which explore/exploit, and purely exploitative methods. The case gamma_E > 0 is also less than obvious. Given the theoretical leanings of the paper, I would strongly encourage the authors to focus on deriving an RMax-style bound for their approach."
"After reading the other reviews and the authors' responses, I am satisfied that this paper is above the accept threshold.  I think there are many areas of further discussion that the authors can flesh out (as mentioned below and in other reviews), but overall the contribution seems solid.   I also appreciate the reviewers' efforts to run more experiments and flesh out the discussion in the revised version of the submission. <sep> Final concluding thoughts: <sep> -- Perhaps pi^ref was somehow better for the structured prediction problems than RL problems? <sep> -- Can one show regret bound for multi-deviation if one doesn't have to learn x (i.e., we were given a good x a priori)? <sep> --------------------------------------------- <sep> ORIGINAL REVIEW <sep> First off, I think this paper is potentially above the accept threshold.  The ideas presented are interesting and the results are potentially interesting as well.   However, I have some reservations, a significant portion of which stem from not understanding aspects of the proposed approach and theoretical results, as outlined below. <sep> The algorithm design and theoretical results in the appendix could be made substantially more rigorous. Specifically: <sep> --  basic notations such as regret (in Theorem 1), the total reward (J), Q-value (Q), and value function (V) are not defined.  While these concepts are fairly standard, it would be highly beneficial to define them formally. <sep> -- I'm not convinced that the ""terms in the parentheses"" (Eq. 7) are ""exactly the contextual bandit cost"".  I would like to see a more rigorous derivation of the connection.  For instance, one could imagine that the policy disadvantage should be the difference between the residual costs of the bandit algorithm and the reference policy, rather than just the residual cost of the bandit algorithm. <sep> -- I'm a little unclear in the proof of Theorem 1 where Q(s,pi_n) from Eq 7 fits into Eq 8. <sep> -- The residual cost used for CB.update depends on estimated costs at other time steps h!=h_dev.  Presumably, these estimated costs will change as learning progresses.  How does one reconcile that?  I imagine that it could somehow work out using a bandit algorithm with adversarial guarantees, but I can also imagine it not working out.  I would like to see a rigorous treatment of this issue. <sep> -- It would be nice to see an end-to-end result that instantiates Theorem 1 (and/or Theorem 2) with a contextual bandit algorithm to see a fully instantiated guarantee. <sep> With regards to the algorithm design itself, I have some confusions: <sep> -- How does one create x in practice? I believe this is described in Appendix H, but it's not obvious. <sep> -- What happens if we don't have a good way to generate x and it must be learned as well?  I'd imagine one would need larger RNNs in that case. <sep> -- If x is actually learned on-the-fly, how does that impact the theoretical results? <sep> -- I find it curious that there's no notion of future reward learning in the learning algorithm.  For instance, in Q learning, one directly models the the long-term (discounted) rewards during learning.  In fact, the theoretical analysis talks about advantage functions as well.  It would be nice to comment on this aspect at an intuitive level. <sep> With regards to the experiments: <sep> -- I find it very curious that the results are so negative for using only 1-dev compared to multi-dev (Figure 9 in Appendix).  Given that much of the paper is devoted to 1-dev, it's a bit disappointing that this issue is not analyzed in more detail, and furthermore the results are mostly hidden in the appendix. <sep> -- It's not clear if a reference policy was used in the experiments and what value of beta was used. <sep> -- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings?  My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but I would like to hear the authors' thoughts. <sep> Finally, the overall presentation of this paper could be substantially improved.  In addition to the above uncertainties, some more points are described below.  I don't view these points as ""deal breakers"" for determining accept/reject. <sep> -- This paper uses too many examples, from part-of-speech tagging to credit assignment in determining paths.  I recommend sticking to one running example, which substantially reduces context switching for the reader.  In every such example, there are extraneous details are not relevant to making the point, and the reader needs to spend considerable effort figuring that out for each example used. <sep> -- Inconsistent language. For instance, x is sometimes referred to as the ""input example"", ""context"" and ""features"". <sep> -- At the end of page 4, ""Internally, ReslopePolicy takes a standard learning to search step.""  Two issues: 1) ReslopePolicy is not defined or referred to anywhere else.  2) is the remainder of that paragraph a description of a ""standard learning to search step""? <sep> -- As mentioned before, Regret is not defined in Theorem 1 & 2. <sep> -- The discussion of the high-level algorithmic concepts is a bit diffuse or lacking.  For instance, one key idea in the algorithmic development is that it's sufficient to make a uniformly random deviation.  Is this idea from the learning to search literature?  If so, it would be nice to highlight this in Section 2.2.","The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log-scale x-axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information."
"Revised Review: <sep> The authors have addressed most of my concerns with the revised manuscript. I now think the paper does just enough to warrant acceptance, although I remain a bit concerned that since the benefits are only achievable with customized hardware, the relevance/applicability of the work is somewhat limited. <sep> Original Review: <sep> The paper proposes a technique for quantizing the weights of a neural network, with bit-depth/precision varying on a per-parameter basis. The main idea is to minimize the number of bits used in the quantization while constraining the loss to remain below a specified upper bound. This is achieved by formulating an upper bound on the number of bits used via a set of ""tolerances""; this upper bound is then minimized while estimating any increase in loss using a first order Taylor approximation. <sep> I have a number of questions and concerns about the proposed approach. First, at a high level, there are many details that aren't clear from the text. Quantization has some bookkeeping associated with it: In a per-parameter quantization setup it will be necessary to store not just the quantized parameter, but also the number of bits used in the quantization (takes e.g. 4-5 extra bits), and there will be some metadata necessary to encode how the quantized value should be converted back to floating point (e.g., for 8-bit quantization of a layer of weights, usually the min and max are stored). From Algorithm 1 it appears the quantization assumes parameters in the range [0, 1]. Don't negative values require another bit? What happens to values larger than 1? How are even bit depths and associated asymmetries w.r.t. 0 handled (e.g., three bits can represent -1, 0, and 1, but 4 must choose to either not represent 0 or drop e.g. -1)? None of these details are clearly discussed in the paper, and it's not at all clear that the estimates of compression are correct if these bookkeeping matters aren't taken into account properly. <sep> Additionally the paper implies that this style of quantization has benefits for compute in addition to memory savings. This is highly dubious, since the method will require converting all parameters to a standard bit-depth on the fly (probably back to floating point, since some parameters may have been quantized with bit depth up to 32). Alternatively custom GEMM/conv routines would be required which are impossible to make efficient for weights with varying bit depths. So there are likely not runtime compute or memory savings from such an approach. <sep> I have a few other specific questions: Are the gradients used to compute \\mu computed on the whole dataset or minibatches? How would this scale to larger datasets? I am confused by the equality in Equation 8: What happens for values shared by many different quantization bit depths (e.g., representing 0 presumably requires 1 bit, but may be associated with a much finer tolerance)? Should ""minimization in equation 4"" refer to equation 3? <sep> In the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources.","Given the changes to the paper, the reviewers agree that the paper meets the bar for publication at *CONF*. There are some concerns regarding the practical impact on CPUs and GPUs. I ask the authors to clearly discuss the impact on different hardware. One can argue if adaptive quantization techniques are helpful, then there is a chance that future hardware will support them. All of the experiments are conducted on toy datasets. Please consider including some experiments on Imagenet as well."
"This paper studies the problem of learning a single convolutional filter using SGD. The main result is: if the ""patches"" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling). The convergence rate, and how ""sufficiently aligned"" depend on some quantities related to the underlying data distribution. A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high. <sep> Detailed comments: <sep> 1. It would be good to clarify what the angle requirement means on page 2. It says the angle between Z_i, Z_j is at most \\rho, is this for any i,j? From the later part it seems that each Z_i should be \\rho close to the average, which would imply pairwise closeness (with some constant factor). <sep> 2. The paper first proves result for a single neuron, which is a clean result. It would be interesting to see what are values of \\gamma(\\phi) and L(\\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions. <sep> 3. The convergence rate depends on \\gamma(\\phi_0), from the initialization, \\phi_0 is probably very close to \\pi/2 (the closeness depend on dimension), which is  also likely to make \\gamma(\\phi_0) depend on dimension (this is especially true of Gaussian). <sep> 4. More precisely, \\gamma(\\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data. Also \\gamma(\\phi_0) depends on \\gamma_{avg}(\\phi_0) and \\rho, when \\rho is reasonable (say a constant), \\gamma(\\phi_0) really needs to be a constant that is independent of dimension. On the other hand, in Theorem 3.4 we can see that the upperbound on \\alpha (the quality of initialization) depends on the dimension. <sep> 5. Even assuming \\rho is a constant strictly smaller than \\pi/2 seems a bit strong. It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average. Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average. <sep> Overall I feel the result is interesting but hard to interpret correctly. The details of the theorem do not really support the high level claims very strongly. The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees. The reviewer tried to do that for Gaussian and as I mentioned above (esp. 4) the result does not seem very impressive, maybe there are other distributions where this result works better? <sep> After reading the response, I feel the contribution for the single neuron case does not require too much assumptions and is itself a reasonable result. I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from ""general distribution"". Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted. I hope the revised version will clearly discuss the limitations of the approach and potential future directions as the response did.","Dear authors, <sep> The reviewers all appreciated your work and agree that this a very good first step in an interesting direction."
"[Reviewed on January 12th] <sep> This article applies the notion of ""conceptors"" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation.  After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation. Follows a section of experiments on variants of MNIST commonly used for continual learning. <sep> Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea. The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.  The numeric examples, although quite toy, provide a clear illustration. <sep> A few things are still missing to back the strong claims of this paper: <sep> * Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer.  This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures. <sep> * It could also be welcome to use a more grounded vocabulary, e.g. on p.2 ""Figure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources"" could be much more simply said as ""Figure 1 shows the ellipses corresponding to three sets of R^3 points"". Being less grandiose would make the value of this article nicely on its own. <sep> * Some examples beyond the contrived MNIST toy examples would be welcome. For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation. I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning. Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST. The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further. If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication. <sep> The later point would normally make me attribute a score of ""6: Marginally above acceptance threshold"" by current DL community standards, but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.","This paper is a timely application of linear algebra to propose a method for reducing catastrophic interference by training a new task in a subspace of the parameter space using conceptors. The conceptors are deployed in the backprop, making this a valuable alternative to recent continual learning methods such as EWC. The paper is clearly written and the results give a clear validation of the method. The reviewers agree as to the merits of the paper."
"The paper deals with the increasingly popular GAN approach to constructing generative models.  Following the first formulation of GANs in 2014, it was soon realized that the training dynamics was highly unstable, leading to significant difficulties in achieving stable results. The paper by Arjovsky et al (2017) provided a framework based on the Wasserstein distance, a distance measure between probability distributions belonging to the class of so-called Integral Probability Metrics (IPMs). This approach solved the stability issues of GANs and demonstrated improved empirical results. Several other works were then developed to deal with these stability issues, specifically the Fisher IPM. Both these methods relied on discriminating between distributions P and Q based on computing a function f, belonging to an appropriate function class {\\cal F}, that maximizes the deviation E_{x~P}f(x)-E_{x~Q}f(x). The main issue relates to the choice of the class {\\cal F}. For the Wasserstein distance this was the class of L_1 Lipschitz functions, while for the Fisher distance it was the class of square integrable functions. The present paper introduces a new notion of distance, where {\\cal F} is the defined through the Sobolev norm, based on the L_2 norm of the gradient of f(x), with respect to a measure \\mu(x), where the latter can be freely chosen under certain assumptions. <sep> The authors prove a theorem related to the properties of the Sobolev norm, and express it in terms of the component-wise conditional distributions. Moreover, they show that the optimal critic f is obtained by solving a PDE subject to zero boundary conditions. They then use their suggested metric in order to develop a GAN algorithm, and present experimental results demonstrating its utility. The Sobolev IPM has two nice features. First, it is based on the component-wise conditional distribution of the CDFs, and, second, its relation to the Laplacian regularizer from manifold learning. Its 1D version also relates to the well-known von Mises Cramer statistics used in hypothesis testing. <sep> The paper belongs to a class of recent papers attempting to suggest improvements to the original GAN algorithm, relying on the KL divergence. It is well conceived and articulated, and provides an interesting and potentially powerful new direction to improve GANs in practice. However, it is somewhat difficult to follow the paper, and would urge the authors to improve and augment their presentation of the following issues. <sep> 1) One often poses regularization schemes based on optimality criteria. Is there any optimality principle under which the Sobolev IPM is a desired choice? <sep> 2) The authors argue that their approach is especially well suited for discrete sequential data. This issue was not clear to me, and it would be good if the authors could expand on this issue and provide a clearer explanation. <sep> 3) How would the Sobolev norm behave under a change of coordinates or a homeomorphism of the space? Would it make sense to require some invariance in this respect? <sep> 4) The Lagrangian in eq. (9) contains both a Lagrange constraint on the Sobolev norm and a penalty term. Why are both needed? Why do the updates of  \\lambda and p in Algorithm 1 used different schemes (SGD and ADAM, respectively). <sep> 5) Table 2, p. 13 – it would be nice to see a comparison to the recently introduced gradient penalty approach, Gulrajani et al., Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017. <sep> 6) The integral defining F_p(x) on p. 3 has x as an argument on the LHS and as an integrand of the RHS. Please correct this. Also specify that x=(x_1,\\ldots,x_d).",The paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semi-supervised cases.
"This paper presents a neural network-based approach to generate topic-specific questions with the motivation that topical questions are more meaningful in practical applications like real-world conversations. Experiments and evaluation have been conducted on the AQAD corpus to show the effectiveness of the approach. <sep> Although the main contributions are clear, the paper contains numerous typos, grammatical errors, incomplete sentences, and a lot of discrepancies between text, notations, and figures making it ambiguous and difficult to follow. <sep> Authors claim to generate topic-specific questions, however, the dataset choice, experiments, and examples show that the generated questions are essentially keyword/key phrase-based. This is also apparent in Section 4.1 where authors present some observation without any supporting proof or empirical evidence. Moreover, the example in Figure 1 shows a conversation, but, typically, in an ongoing multi-round conversation people do not tend to repeat the keywords or key phrases or named entities, and topic shifts might occur at any time. <sep> Overall, a misconception about topic vs. keywords might have led the authors to claim that their work is the first to generate topic-specific questions whereas this has been studied before by Chali & Hasan (2015) in a non-neural setting. ""Topic"" in general has a broader meaning, I would suggest authors to see this to get an idea about what topic entails to in a conversational setting: https://developer.amazon.com/alexaprize/contest-rules . I think the proposed work is mostly related to: 1) ""Towards Natural Question-Guided Search"" by Kotov and Zhai (2010), and 2) ""K2Q: Generating Natural Language Questions from Keywords with User Refinements"" by Zheng et al. (2011), and other recent factoid question generation papers where questions are generated from a given fact (e.g. ""Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus"" by Serban et al. (2016)). <sep> It is not clear how the question types are extracted from the given sentences. Please provide details. Which keywords are employed to accomplish this? Also, please explain the absence of the ""why"" type question. <sep> Figure 3 and the associated descriptions are very hard to follow. Please draw the figure by matching it with the descriptions.  Where are the bi-LSTMs in the figure? What are ac_t and em_t? <sep> My major concern is with the experiments and evaluation. The dataset essentially contains questions about product reviews and does not match authors motivation/observation about real-world conversations. Moreover, evaluation has been conducted on a very small test set (just about 1% of the selected corpus), making the results unconvincing. More details are necessary about how exactly Kim's and Liu's models are used to get question types and topics. <sep> Human evaluation results per category would have been more useful. How did you combine the scores of the human evaluation categories? Also, automatic evaluation and human evaluation results do not correlate well. Please explain.","The pros and cons of the paper under consideration can be summarized below: <sep> Pros: <sep> * Reviewers thought the underlying model is interesting and intuitive <sep> * Main contributions are clear <sep> Cons: <sep> * There is confusion between keywords and topics, which is leading to a somewhat confused explanation and lack of clear comparison with previous work. Because of this, it is hard to tell whether the proposed approach is clearly better than the state of the art. <sep> * Typos and grammatical errors are numerous <sep> As the authors noted, the concerns about the small dataset are not necessarily warranted, but I would encourage the authors to measure the statistical significance of differences in results, which would help alleviate these concerns. <sep> An additional comment: it might be worth noting the connections to query-based or aspect-based summarization, which also have a similar goal of performing generation based on specific aspects of the content. <sep> Overall, the quality of the paper as-is seems to be somewhat below the standards of *CONF* (although perhaps on the borderline), but the idea itself is novel and results are good. I am not recommending it for acceptance to the main conference, but it may be an appropriate contribution for the workshop track."
"In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network. Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods. In general I found this paper clearly written and technically sound. I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures. <sep> Contribution: <sep> As discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units. The analysis becomes considerably more complicated, and the contribution seems to be novel and significant. I am not sure why did the authors mentioned the work on over-parameterization though. It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small). <sep> Comments on the Assumptions: <sep> - Please explain the motivation behind the standard Gaussian assumption of the input vector x. <sep> - Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6). <sep> Without extra justifications, it seems that the theoretical result only holds for an artificial problem setting. While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results. <sep> General Comment: <sep> The technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs. From the analysis in the main paper, I believe the theoretical contribution is correct and sound. While I appreciate the technical contributions, in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples). Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector, and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems.","This submission is a continuation of a line of theoretical work that seeks to characterize optimization landscapes of neural networks by the presence or absence of spurious local minima. As the number of critical points grows combinatorially for larger networks, it is very challenging to show such results. The present submission extends slightly previous work by considering two hidden units and their proof technique goes beyond that of Brutzkus and Globerson, 2017, potentially leading to more interesting results if they can be extended to more complex networks. <sep> The setting of two hidden units is quite limited - far from any practical setting. If this were the stepping stone to proving optimality of certain optimization strategies for more complex networks, this may be of some interest, but it seems doubtful. One indication is given in Sec. 7 / Fig. 1 in which it is shown that for even quite small numbers of hidden units, spurious local optima do occur and are reached 40% of the time for random initializations even with only 11 nodes."
"## Summary <sep> This paper aims to tackle the question: ""why does standard SGD based algorithms on neural network converge to 'good' solutions?"" <sep> Pros: <sep> Authors ask the question of convergence of optimization (ignoring generalization error): how ""likely"" is that an over-parameterized (d1d0 > N) single hidden layer binary classifier ""find"" a good (possibly over-fitted) local minimum. They make a set of assumptions (A1-A3) which are weaker (d1 > N^{1/2}) than the ones used earlier works. Previous works needed a wide hidden layer (d1 > N). <sep> Assumptions (d0=input dim, d1=hidden dim, N=n of datapoints, X=datapoints matrix): <sep> A1. Datapoints X come from a Gaussian distribution <sep> A2. N^{1/2} < d0 =< N <sep> A3. N polylog(N) < d0d1 (approximate n of. parameters)  and d1 =< N <sep> This paper proves that total ""angular volume"" of ""regions"" (defined with respect to the piecewise linear regions of neuron activations) with differentiable bad-local minima are exponentially small when compared with to the total ""angular volume"" of ""regions"" containing only differentiable global-minimal. The proof boils down to counting arguments and concentration inequality. <sep> Cons: <sep> Non-differentiable stationary points are left as a challenging future work on this paper. Non-differentiability aside, authors show a possible way by which shallow neural networks might be over-fitting the data. But this is only half the story and does not completely answer the question. First, exponentially vanishing (in N) volume of the ""regions"" containing bad-local minima doesn't mean that the number of bad local minima are exponentially small when compared to number global minima. <sep> Secondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction. <sep> Lastly, appropriate comparisons with the existing literature is lacking. It is hinted that this paper is more general as the assumptions are more realistic. However, it comes at a cost of losing sharpness in the theoretical results. It is not well motivated why one should study the angular volume of the global and local minima. <sep> ## Questions and comments <sep> 1. How critical is Gaussian-datapoints assumption (A1)? Which part of the proof fails to generalize? <sep> 2. Can the proof be extended to scalar regression?  It seems hard to generalize to vector output neural networks. What about deep neural networks? <sep> 3. Can you relate the results to other more recent works like: https://arxiv.org/pdf/1707.04926.pdf. <sep> 4. Piecewise linear and positively homogeneous (https://arxiv.org/pdf/1506.07540.pdf) activation seem to be important assumption of the paper. It should probably be mentioned explicitly. <sep> 5. In the experiments section, it is mentioned that ""...inputs to the hidden neurons converge to a distinctly non-zero value. This indicates we converged to DLMs."" How can you guarantee that it is a local minimum and not a saddle point?","The paper analyzes neural network with hidden layer of piecewise linear units, a single output, and a quadratic loss. The reviewers find the results incremental and not ""surprising"", and also complained about comparison with previous work. I think the topic is very pertinent, and definitely more relevant compared to studying multi-layer linear networks. Hence, I recommend the paper be presented in the workshop track."
"Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc. The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks. <sep> As the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y). <sep> Originality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work. In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results. Thus, I would say original. <sep> Importance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points. While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference. <sep> Clarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version. <sep> Comments: <sep> 1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5). <sep> 2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector? <sep> 3. In section 5, the authors make a connection with the work of Ge et al. 2016. They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016. However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation). It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work. If that is the case, could the authors describe this a bit further?","The paper nicely unifies previous results and develops the property of local openness. While interesting, I find the application to multi-layer linear networks extremely limiting. There appears to be a sub-field in theory now focusing on solely multi-layer linear networks which is meaningless in practice. I can appreciate that this could give rise to useful proof techniques and hence, I am recommending it to the workshop track with the hope that it can foster more discussions and help researchers move away from studying multi-layer linear networks."
"This paper aims to translate source code from one programming language to another using a neural network architecture that maps trees to trees. The encoder uses an upward pass of a Tree LSTM to compute embeddings for each subtree of the input, and then the decoder constructs a tree top-down. As nodes are created in the decoder, a hidden state is passed from parents to children via an LSTM (one for left children, one for right children), and an attention mechanism allows nodes in the decoder to attend to subtrees in the encoder. <sep> Experimentally, the model is applied to two synthetic datasets, where programs in the source domain are sampled from a PCFG and then translated to the target domain with a hand-coded translator. The model is then trained on these pairs. Results show that the nproposed approach outperforms sequence representations or serialized tree representations of inputs and outputs. <sep> Pros: <sep> - Nice model which seems to perform well. <sep> - Reasonably clear explanation. <sep> A couple questions about the model: <sep> - the encoder uses only bottom-up information to determine embeddings of subtrees. I wonder if top-down information would create embeddings with more useful information for the attention in the decoder to pick up on. <sep> - I would be interested to know more details about how the hand-coded translator works. Does it work in a context-free, bottom-up fashion? That is, recursively translate two children nodes and then compute the translation of the parent as a function of the parent node and translations of the two children? If so, I wonder what is missing from the proposed model that makes it unable to perfectly solve the first task? <sep> Cons: <sep> - Only evaluated on synthetic programs, and PCFGs are known to generate unrealistic programs, <sep> so we can only draw limited conclusions from the results. <sep> - The paper overstates its novelty and doesn't properly deal with related work (see below) <sep> The paper overstates its novelty and has done a poor job researching related work. <sep> Statements like ""We are the first to consider employing neural network approaches towards tackling the problem [of translating between programming languages]"" are obviously not true (surely many people have *considered* it), and they're particularly grating when the treatment of related  work is poor, as it is in this paper. For example, <sep> there are several papers that frame the code migration problem as one of statistical machine translation (see Sec 4.4 of [1] for a review and citations), but this paper makes no reference to them. Further, [2] uses distributed representations for the purpose of code migration, which I would call a ""neural network approach,"" so there's not any sense that I can see in which this statement is true. The paper further says, ""To the best of our knowledge, this is the first tree-to-tree neural network architecture in the literature."" This is worded better, but it's definitely not the first tree-to-tree neural network. See, e.g., [3, 4, 5], one of which is cited, so I'm confused about this claim. <sep> In total, the model seems clean and somewhat novel, but it has only been tested on unrealistic synthetic data, the framing with respect to related work is poor, and the contributions are overstated. <sep> [1] https://arxiv.org/abs/1709.06182 <sep> [2]  Trong Duc Nguyen, Anh Tuan Nguyen, and Tien N Nguyen. 2016b. Mapping API elements for code migration with vector representations. In Proceedings of the International Conference on Software Engineering (ICSE). <sep> [3] Socher, Richard, et al. ""Semi-supervised recursive autoencoders for predicting sentiment distributions."" Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2011. <sep> [4] https://arxiv.org/abs/1703.01925 <sep> [5] Parisotto, Emilio, et al. ""Neuro-symbolic program synthesis."" arXiv preprint arXiv:1611.01855 (2016).","the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option."
"Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators. The best one found is termed Swish unit; x * sigmoid(b*x). <sep> The properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU, PLReLU etc. As pointed out by the authors themselves for b=1 Swish is equivalent to SiL proposed in Elfwing et. al. (2017). <sep> In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Again, the authors do state that ""our results may not be directly comparable to the results in the corresponding works due to differences in our training steps."" <sep> Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. More explanation is required on why is it important and how does it help optimization. Distribution of learned b in Swish for different layers of a network can interesting to observe.","The author's propose to use swish and show that it performs significantly better than Relus on sota vision models. Reviewers and anonymous ones counter that PRelus should be doing quite well too. Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community. As a results, I'm going to recommend publishing to a workshop for now."
"The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms. The proposed games are interesting and clearly challenging, but I am not sure what they tell us about the algorithms chosen to test them. There are some clarity issues with the justification and evaluation which undermine the message the authors are trying to make. <sep> In particular, I have the following concerns: <sep> • these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. What does this tell us about the relative merits of each approach? The authors could do more to formally motivate these games as ""difficult"" for any deep learning architecture if possible. <sep> • the authors compare linear models with non-linear models at some point for attacker policies, but it is unclear whether these linear models are able to express the optimal policy. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). <sep> • As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work. There is possibly too much focus on the proofs of these theorems. <sep> • There are a number of ambiguities and errors which places difficulties on the interpretation (and potential replication) of the experiments. As this is an empirical study, this is the yardstick by which the paper should be judged. In particular, this relates to: <sep> ◦ The architecture of each of the tested Deep RL methods. <sep> ◦ What is done to select appropriate tuning parameters of the tested Deep RL methods, if anything. <sep> ◦ It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 <sep> ◦ Fig 4. right looks like a reward signal, but is labelled Proportion correct. The text is not clear enough to be sure which it is. <sep> ◦ Fig 4. left and right has 4 methods: rl rewards, rl correct actions, sup rewards, and sup correct actions. The specifics of how these methods are constructed is unclear from the paper. <sep> ◦ What parts of the evaluation explores how well these methods are able to represent the states (feature/representation learning) and what parts are evaluating the propagation of sparse rewards (the reinforcment learning core)? The authors could be clearer and more targetted with respect to this question. <sep> There is value in this work, but in its current state I do not think it is ready for publicaiton. <sep> # Detailed notes <sep> [p4, end of sec 3] The authors say that the difficulty of the games can be varied with ""continuous changes in potential"", but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). <sep> [p4, sec 4.1] <sep> ""strategy unevenly partitions the occupied levels...with the proportional difference between the two sets being sampled randomly"" <sep> What is meant by this? The proportional difference between the two sets is discussed as if it is a continuous property, but must be chosen from the discrete set of all available partitions. If one partition one is chosen uniformly randomly from all possibly sets A, B (and the potential proportion calculated) then I don't know why it would be written in this way. That suggests that proportions that are closer to 1:1 are chosen more often than ""extreme"" partitions, but how? This feels a little under-justified. <sep> ""very different states A, B (uneven potential, disjoint occupied levels)"" <sep> Are these states really ""very different"", or at least for the reasons indicated. Later on (Theorem 3) we see how an optimal partition is generated. This chooses a partition where one part contains all pieces in layer (l+1) and above and one part with all pieces in layer (l-1) and below, with layer l being distributed between the two parts. The first part will typically have a slightly lower potential than the other and all layers other than layer l will be disjoint. <sep> [p6, Fig 4] The right plot y-limits vary between -1 and 1 so it cannot represent a proportion of correct actions. Also, in the text the authors say: <sep> >> The results, shown in Figure 4 are surprising. Reinforcement learning <sep> >> is better at playing the game, but does worse at predicting optimal moves. <sep> I am not sure which plot shows the playing of the game. Is this the right hand plot? In which case are we looking at rewards? In fact, I am a little confused as to what is being shown here. Is ""sup rewards"" a supervised learning method trained on rewards, or evaluated on rewards, or both? And how is this done. The text is just not clear enough. <sep> [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. This relates to the ""surprising"" fact that ""Reinforcement learning is better at playing the game, but does worse at predicting optimal moves."". I think an important point here is how many training/test examples there are in each bin. If there are more in the range 3-7 moves from the end of the game, than there are outside this range, then the supervised learner will <sep> [p8 proof of theorem 3] <sep> ""φ(A l+1 ) < 0.5 and φ(A l ) > 0.5."" <sep> Is it true that both these inequalities are strict? <sep> ""Since A l only contains pieces from levels K to l + 1"" <sep> In fact this should read from levels K to l. <sep> ""we can move k < m − n pieces from A l+1 to A l"" <sep> Do the authors mean that we can define a partition A, B where A = A_{l+1} plus some (but not all) elements in level l (A_{l}\\setminus A_{l+1})? <sep> ""...such that the potential of the new set equals 0.5"" <sep> It will equal exactly 0.5 as suggested, but the authors could make it more precise as to why (there is a value n+k < l (maybe <=l) such that (n+k)*2^{-(K-l+1)}=0.5 (guaranteed). They should also indicate why this then justifies their proof (namely that phi(S0)-0.5 >= 0.5). <sep> [p8 paramterising action space] A comment: this doesn't give as much control as the authors suggest. Perhaps the agent should also chose the proportion of elements in layer l to set A. For instance, if there are a large number of elements in l, and or phi(A_{l+1}) is very close to 0.5 (or phi(A_l) is very close to 0.5) then this doesn't give the attacker the opportunity to fine tune the policy to select very good partitions.  It is unclear expected level of control that agents have under various conditions (K and starting states). <sep> [p9 Fig 8] As the defender's score is functionally determined by the attackers score, it doesn't help to include this on the plot. It just distracts from the signal.","The paper introduces an interesting family of two-player zero-sum games with tunable complexity, called Erdos-Selfridge-Spencer games, as a new domain for RL. The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single-agent vs. multi-agent training. The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read. <sep> A drawback of the paper is that it does not make a *significant* contribution to the field. In combing through the reviewer comments, none of them identify a significant contribution. Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution. As the paper is still interesting, the committee would like to recommend this for the workshop track. <sep> Pros: <sep> Interesting domain with tunable complexity <sep> High-quality extensive empirical results <sep> Writing is clear <sep> Cons: <sep> Lacks a significant contribution <sep> Appears to overlook self-play, the dominant RL training paradigm for decades (multiagent training appears to be related but different) <sep> Per Reviewer3, ""I remain unconvinced that these games are good general tests for Deep RL"
"Overall: <sep> I really enjoyed reading this paper and think the question is super important. I have some reservations about the execution of the experiments as well as some of the conclusions drawn. For this reason I am currently a weak reject (weak because I believe the question is very interesting). However, I believe that many of my criticisms can be assuaged during the rebuttal period. <sep> Paper Summary: <sep> For RL to play video games, it has to play many many many many times. In fact, many more times than a human where prior knowledge lets us learn quite fast in new (but related) environments. The authors study, using experiments, what aspects of human priors are the important parts. <sep> The authors' Main Claim appears to be: ""While common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical."" <sep> Overall, I find this interesting. However, I am not completely convinced by some of the experimental demonstrations. <sep> Issue 0: The experiments seem underpowered / not that well analyzed. <sep> There are only 30 participants per condition and so it's hard to tell whether the large differences in conditions are due to noise and what a stable ranking of conditions actually looks like. I would recommend that the authors triple the sample size and be more clear about reporting the outcomes in each of the conditions. <sep> It's not clear what the error bars in figure 1 represent, are they standard deviations of the mean? Are they standard deviations of the data? Are they confidence intervals for the mean effect? <sep> Did you collect any extra data about participants? One potentially helpful example is asking how familiar participants are with platformer video games. This would give at least some proxy to study the importance of priors about ""how video games are generally constructed"" rather than priors like ""objects are special"". <sep> Issue 1: What do you mean by ""objects""? <sep> The authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about ""objects are special"" are very important. However, an alternative explanation is that people explore things which look ""different"" (ie. Orange when everything else is black). <sep> The problem here comes from an unclear definition of what the authors mean by an ""object"" so in revision I would like authors to clarify what precisely they mean by a prior about ""the world is composed of objects"" and how this particular experiment differentiates ""object"" from a more general prior about ""video games have clearly defined goals, there are 4 clearly defined boxes here, let me try touching them."" <sep> This is important because a clear definition will give us an idea for how to actually build this prior into AI systems. <sep> Issue 2: Are the results here really about ""high level"" priors? <sep> There are two ways to interpret the authors' main claim: the strong version would maintain that semantic priors aren't important at all. <sep> There is no real evidence here for the strong version of the claim. A real test would be to reverse some of the expected game semantics and see if people perform just as well as in the ""masked semantics"" condition. <sep> For example, suppose we had exactly the same game and N different types of objects in various places of the game where N-1 of them caused death but 1 of them opened the door (but it wasn't the object that looked like a key). My hypothesis would be that performance would fall drastically as semantic priors would quickly lead people in that direction. <sep> Thus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game. This is much more supported by the data, but still I think very particular to this situation. Imagine a slight twist on the game: <sep> There is a sword (with a lock on it), a key, a slime and the door (and maybe some spikes). The player must do things in exactly this order: first the player must get the key, then they must touch the sword, then they must kill the slime, then they go to the door. Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well). <sep> Thus, I think the authors' claim needs to be qualified quite a bit. It's also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here (the authors do this when they discuss versions of the game with different physics).","This paper turned out to be quite difficult to call. My take on the pros/cons is: <sep> 1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants. <sep> 2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors. The study is well conceived and well executed. I consider the study to be a contribution by itself. <sep> 3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used. <sep> 4. However, the study is not definitive, as astutely argued by AnonReviewer2. Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior. So it is possible that some factor other than human prior may account for the behavior seen in the human experiments. <sep> 5. It would indeed be better, as argued by AnonReviewer2, to use some information-theoretic measure to distinguish the normal game from the modified games. <sep> 6. The paper has been substantially improved and cleaned up from the original version. <sep> 7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper. <sep> Bottom line: Given the procs and cons of the paper, the committee recommends this for workshop."
"Summary: <sep> In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. To this, they add channels based on low base quality, low mapping quality. The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance. <sep> Pros: <sep> The paper tackles what seems to be both an important and challenging problem. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. <sep> Cons: <sep> While we liked both the challenge posed and the idea to solve it we found several major issues with the work. <sep> First, the writing is far from clear. There are typos and errors all over at an unacceptable level. Many terms are not defined or defined after being introduced (e.g. CIGAR, MF, BQMQ). A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 ""Results"" p. 3 is not really results but part of the methods. The ""pipeline"" is never well defined, only implicitly in p.7 top, and then it is hard to relate the various figures/tables to bottom line results (having the labels wrong does not help that). <sep> The filters by themselves seem trivial and as such do not offer much novelty. Moreover, the authors filter the ""normal"" samples using those (p.7 top), which makes the entire exercise a possible circular argument. <sep> If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. <sep> The entire dataset is based on 4 patients. It is not clear what is the source of the other cancer control case. The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it? How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)? Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients. As such, this may have very little relevance for the actual problem of cfDNA. <sep> Finally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited. <sep> Albeit the above caveats, we iterate the paper offers a nice construction for an important problem. We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.","Authors present a method for representing DNA sequence reads as one-hot encoded vectors, with genomic context (expected original human sequence), read sequence, and CIGAR string (match operation encoding) concatenated as a single input into the framework. Method is developed on 5 lung cancer patients and 4 melanoma patients. <sep> Pros: <sep> - The approach to feature encoding and network construction for task seems new. <sep> - The target task is important and may carry significant benefit for healthcare and disease screening. <sep> Cons: <sep> - The number of patients involved in the study is exceedingly small. Though many samples were drawn from these patients, pattern discovery may not be generalizable across larger populations. Though the difficulty in acquiring this type of data is noted. <sep> - (Significant) Reviewer asked for use of public benchmark dataset, for which authors have declined to use since the benchmark was not targeted toward task of ultra-low VAFs. However, perhaps authors could have sourced genetic data from these recommended public repositories to create synthetic scenarios, which would enable the broader research community to directly compare against the methods presented here. The use of only private datasets is concerning regarding the future impact of this work. <sep> - (Significant) The concatenation of the rows is slightly confusing. It is unclear why these were concatenated along the column dimension, rather than being input as multiple channels. This question doesn't seem to be addressed in the paper. <sep> Given the pros and cons, the commitee recommends this interesting paper for workshop."
"## Review summary <sep> Overall, the paper makes an interesting effort to tightly integrate expectation-maximization (EM) training algorithms with evolutionary algorithms <sep> (EA). However, I found the technical description lacking key details and the experimental comparisons inadequate. There were no comparisons to non- <sep> evolutionary EM algorithms, even though they exist for the models in question. <sep> Furthermore, the suggested approach lacks a principled way to select and tune key hyperparameters. I think the broad idea of using EA as a substep within a monotonically improving free energy algorithm could be interesting, <sep> but needs far more experimental justification. <sep> ## Pros / Stengths <sep> + effort to study more than one model family <sep> + maintaining monotonic improvement in free energy <sep> ## Cons / Limitations <sep> - poor technical description and justification of the fitness function <sep> - lack of comparisons to other, non-EA algorithms <sep> - lack of study of hyperparameter sensitivity <sep> ## Paper summary <sep> The paper suggests a variant of the EM algorithm for binary hidden variable models, where the M-step proceeds as usual but the E-step is different in two ways. First, following work by J. Lucke et al on Truncated Posteriors, the true posterior over the much larger space of all possible bit vectors is approximated by a more tractable small population of well-chosen bit vectors, <sep> each with some posterior weight. Second, this set of bit vectors is updated using an evolutionary/genetic algorithm. This EA is the core contribution, <sep> since the work on Trucated Posteriors has appeared before in the literature. <sep> The overall EM algorithm still maintains monotonic improvement of a free energy objective. <sep> Two well-known generative models are considered: Noisy-Or models for discrete datasets and Binary Sparse Coding for continuous datasets. Each has a previously known, closed-form M-step (given in supplement). The focus is on the E-step: how to select the H-dimensional bit vector for each data point. <sep> Experiments on artificial bars data and natural image patch datasets compare several variants of the proposed method, while varying a few EA method substeps such as selecting parents by fitness or randomly, including crossover or not, or using generic or specialized mutation rates. <sep> ## Significance <sep> Combining evolutionary algorithms (EA) within EM has been done previously, as in Martinez and Vitria (Pattern Recog. Letters, 2000) or Pernkopf and <sep> Bouchaffra (IEEE TPAMI, 2005) for mixture models. However, these efforts seem to use EA in an ""outer loop"" to refine different runs of EM, while the present approach uses EA in a substep of a single run of EM. I guess this is technically different, but it is already well known that any E-step method which monotonically improves the free energy is a valid algorithm. Thus, the paper's significance hinges on demonstrating that the particular E step chosen is better than alternatives. I don't think the paper succeeded very well at this: there were no comparisons to non-EA algorithms, or to approaches that use EA in the ""outer loop"" as above. <sep> ## Clarity of Technical Approach <sep> What is \\tilde{log P} in Eq. 7? This seems a fundamental expression. Its plain-text definition is: ""the logarithm of the joint probability where summands that do not depend on the state s have been elided"". To me, this definition is not precise enough for me to reproduce confidently... is it just log p(s_n, y_n | theta)? I suggest revisions include a clear mathematical definition. This omission inhibits understanding of this paper's core contributions. <sep> Why does the fitness expression F defined in Eq. 7 satisfy the necessary condition for fitness functions in Eq. 6? This choice of fitness function does not seem intuitive to me. I think revisions are needed to *prove* this fitness function obeys the comparison property in Eq. 6. <sep> How can we compute the minimization substep in Eq. 7 (min_s \\tilde{logP})? Is this just done by exhaustive search over bit vectors? I think this needs clarification. <sep> ## Quality of Experiments <sep> The experiments are missing a crucial baseline: non-EA algorithms. Currently only several varieties of EA are compared, so it is impossible to tell if the suggested EA strategies even improve over non-EA baselines. As a specific example, previous work already cited in this paper -- Henniges et al (2000) -- <sep> has developed a non-EA EM algorithm for Binary Sparse Coding, which already uses the truncated posterior formulation. Why not compare to this? <sep> The proposed algorithm has many hyperparameters, including number of generations, number of parents, size of the latent space H, size of the truncation, etc. The current paper offers little advice about selecting these values intelligently, but presumably performance is quite sensitive to these values. I'd like to see some more discussion of this and (ideally) more experiments to help practitioners know which parameters matter most, <sep> especially in the EA substep. <sep> Runtime analysis is missing as well: Is runtime dominated by the EA step? How does it compare to non-EA approaches? How big of datasets can the proposed method scale to? <sep> The reader walks away from the current toy bars experiment somewhat confused. <sep> The Noisy-Or experiment did not favor crossover and and favored specialized mutations, while the BSC experiment reached the opposite conclusions. How does one design an EA for a new dataset, given this knowledge? Do we need to exhaustively try all different EA substeps, or are there smarter lessons to learn? <sep> ## Detailed comments <sep> Bottom of page 1: I wouldn't say that ""variational EM"" is an approximation to <sep> EM. Sometimes moving from EM to variational EM can mean we estimate posteriors <sep> (not point estimates) for both local (example-specific) and global parameters. <sep> Instead, the *approximation* comes simply from restricting the solution space to gain tractability. <sep> Sec. 2: Make clear earlier that hidden var ""s"" is assumed to be discrete, not continuous. <sep> After Mutation section: Remind readers that ""N_g"" is number of generations","This method makes a connection between evolutionary and variational methods in a particular model. This is a good contribution, but there has been little effort to position it in comparison to standard methods that do the same thing, showing relative strengths and weaknesses. <sep> Also, please shorten the abstract."
"Thank you for the feedback, and I have read it. <sep> The authors claimed that they used techniques in [6] in which I am not an expert for this. However I cannot find the comparison that the authors mentioned in the feedback, so I am not sure if the claim is true. <sep> I still recommend rejection for the paper, and as I said in the first review, the paper is not mature enough. <sep> ==== original review === <sep> The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss. Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following: <sep> 1. write down MMD as an integral probability metric (IPM) <sep> 2. say the test function, which originally should be in an RKHS, will be approximated using random feature approximations. <sep> Although the authors explained the intuition a bit and showed some empirical results, I still don't see why this method should work better than directly minimising MMD. Also it is not preferred to look at the generated images and claim diversity, instead it's better to have some kind of quantitative metric such as the inception score. <sep> Finally, given the fact that we have too many GAN related papers now, I don't think the innovation contained in the paper (which is using random features) is good enough to be published at *CONF*. Also the paper is not clearly written, and I would suggest better not to copy-past paragraphs in the abstract and intro. <sep> That said, I would welcome for the authors feedback and see if I have misunderstood something.","The reviewers all outlined concerns regarding novelty and the maturity of this work. It would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps. Finally, the approach should be tried on more difficult image datasets."
"This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation. This paper has some interesting insights and a few ideas of how to validate an evaluation method. The topic is an important one and a very difficult one. However, the work has some problems in rigor and justification and the conclusions are overstated in my view. <sep> Pros <sep> -Several interesting ideas for evaluating evaluation metrics are proposed <sep> -The authors tackle a very challenging subject <sep> Cons <sep> -It is not clear why GANs are the only generative model considered <sep> -Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper. <sep> -The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification <sep> -The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for ""discriminativeness"" and seems like something that can be gamed. <sep> - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. <sep> Several references I suggest: <sep> https://arxiv.org/abs/1706.08500 (FID score) <sep> https://arxiv.org/abs/1511.04581 (MMD as evaluation)","The problem addressed here is an important one: What is a good evaluation metric for generative models? A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs. Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test. This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns. <sep> From a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction. One must read more than a few pages to get to the answer of why the metrics that are advocated were picked. It need not read like a mystery. <sep> R4: ""The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification"" <sep> R2: ""First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."" - the first point of which is also related to a concern of R4. <sep> Given the overall high selectivity of *CONF*, the present submission falls short."
"This paper proposes a graph classification method by integrating three techniques, community detection, graph kernels, and CNNs. <sep> * This paper is clearly written and easy to follow. Thus the clarity is high. <sep> * The originality is not high as the application of neural networks for graph classification has already been studied elsewhere and the proposed method is a direct combination of three existing methods, community detection, graph kernels, and CNNs. <sep> * The quality and the significance of this paper it not high due to the following reasons: <sep> - The motivation is misleading in two folds. <sep> First, the authors say that the graph kernel + SVM approach has a drawback due to two independent processes of graph representation and learning. <sep> However, the parameters included in respective graph kernel is usually optimized via the SVM classification, hence they are not independent with each other. <sep> Second, the authors say that the proposed method addresses the above issue of independence between graph representation and learning. <sep> However, it also uses the two-step procedure as it first obtain the kernel matrix K via graph kernels and then apply CNN for classification, which is fundamentally the same as the existing approach. <sep> Although community detection is used before graph kernels, such subgraph extraction process is already implicitly employed in various graph kernels. <sep> I recommend to revise and clarify this point. <sep> - In experimental evaluation, why several kernels including SP, RW, and WL are not used in the latter five datasets? <sep> This missing experiment significantly deteriorate the quality of empirical evaluation and I strongly recommend to add results for such kernels. <sep> - It is mentioned that the parameter h is fixed to 5 in the WL kernel. However, it is known that the performance of the WL kernel depends on the parameter and it should be tuned by cross-validation. <sep> In contrast, parameters (number of epochs and the learning rate) are tuned in the proposed method. Thus the current comparison is not fair. <sep> - In addition to the above point, how are parameters for GR and RW? <sep> - Runtime is shown in Table 4 but there is no comparison with other methods. Although it is mentioned in the main text that the proposed method is faster than Graph CNN and Depp Graph Kernels, there is no concrete values and this statement is questionable (Runtime will easily vary due to the hardware configuration). <sep> * Additional comment: <sep> - Why is the community detection step needed? What will happen if K is directly constructed from given N graphs and what is the advantage of using not the original graphs but extracted subgraphs? <sep> - In the first step of finding characteristic subgraphs, frequent subgraph mining can be used instead community detection. <sep> Frequent subgraph mining is extensively used in various methods for classification of graph-structured data, for example: <sep> * Tsuda, K., Entire regularization paths for graph data, ICML 2007. <sep> * Thoma, M. et al., Discriminative frequent subgraph mining with optimality guarantees, Statistical Analysis and Data Mining, 2010 <sep> * Takigawa, I., Mamitsuka, H., Generalized Sparse Learning of Linear Models Over the Complete Subgraph Feature Set, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017 <sep> What is the advantage of using the community detection compared to frequent subgraph mining or other subgraph enumeration methods?","The reviewers were unanimous in their assessment that the paper was not ready for publication in *CONF*. Their concerns included: <sep> - lack of novelty over Niepert, Ahmed, Kutzkov, ICML 2016 <sep> - The approach learns combinations of graph kernels and its expressive capacity is thus limited <sep> - The results are close to the state of the art and it is not clear whether any improvement is statistically significant. <sep> The authors have not provided a response to these concerns."
"The paper proposes a modified approach to RL, where an additional ""episodic memory"" is kept by the agent. What this means is that the agent has a reservoir of n ""states"" in which states encountered in the past can be stored. There are then of course two main questions to address (i) which states should be stored and how (ii) how to make use of the episodic memory when deciding what action to take. <sep> For the latter question, the authors propose using a ""query network"" that based on the current state, pulls out one state from the memory according to certain probability distribution. This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory. Intuitively, one can see why this may be advantageous as one gets some information from the past. (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.) <sep> The first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights. The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose. However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past). There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable. <sep> There is also a toy example created to show that this approach works well compared to the RNN based approaches. <sep> Positives: <sep> - An interesting new idea that has potential to be useful in RL <sep> - An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks) <sep> Negatives: <sep> - The math is fudged around quite a bit with approximations that are not always justified <sep> - While overall the writing is clear, in some places I feel it could be improved. I had a very hard time understanding the set-up of the problem in Figure 2. [In general, I also recommend against using figure captions to describe the setup.] <sep> - The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.","This paper presents a memory architecture for RL based on reservoir sampling, and is meant to be an alternative to RNNs. The reviewers consider the idea to be potentially interesting and useful, but have concerns about the mathematical justification. They also point out limitations in the experiments: in particular, use of artificial toy problems, and a lack of strong baselines. I don't think the paper is ready for *CONF* publication in its current form."
"This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero. As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. The DReLU is supposed to remedy the problem of covariate shift better. <sep> The presentation of the paper is clear. The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed). Statistical tests are performed for many of the experimental results, which is solid. <sep> However, I have some concerns. <sep> 1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? If it is done by hyperparameter tuning with cross-validation, the training cost may be too high. <sep> 2) I believe the control experiments are encouraging, but I do not agree that other techniques like Dropouts are not useful. Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important. The arguments for skipping this experiments are respectful, but not convincing enough. <sep> 3) Batch normalization is popular, especially for the convolutional neural networks. However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway.","meta score: 4 <sep> This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. <sep> Pros <sep> - good set of experiments using CIFAR, with good results <sep> - attempt to explain the approach using expectations <sep> Cons <sep> - theoretical explanations are not so convincing <sep> - limited novelty <sep> - CIFAR is relatively limited set of experiments <sep> - does not compare with using bn after relu, which is now well-studied and seems to address the motivation of this paper (and thus questions the conclusions)"
"The paper adresses a very interesting question about the handling of the dynamics of a recommender systems at scale (here for linking to some articles). <sep> The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise. This is equivalent to separate a local estimation error from the noise. <sep> The idea is interesting but maybe not pushed far enough in the paper: <sep> *At fixed context x, assuming that the error is a function of the average reward u and of the number of displays r of the context could be a constant could be a little bit more supported (this is a variance explanation that could be tested statistically, or the shape of this 2D function f(u,r) could be plot to exhibit its regularity). <sep> * None of the experiments is done on public data which lead to an impossible to reproduce paper <sep> * The proposed baselines are not really the state of the art (Factorization Machines, GBDT features,...) and the used loss is MSE which is strange in the context of CTR prediction (logistic loss would be a more natural choice) <sep> * I'm not confident with the proposed surrogate metrics. In the paper, the work of Lihong Li &al on offline evaluation on contextual bandits is mentioned and considered as infeasible here because of the renewal of the set of recommendation. Actually this work can be adapted to handle theses situations (possibly requiring to bootstrap if the set is actually regenerating too fast). Also note that Yahoo Research R6a - R6b  datasets where used in ICML'12 Exploration and Exploitation 3 challenge where about pushing some news in a given context and could be reused to support the proposed approach. An other option would be to use some counterfactual estimates (See Leon Bottou &all and Thorsten Joachims &all) <sep> * If the claim is about a better exploration,  I'd like to have an idea of the influence of the tuning parameters and possibly a discussion/comparison over alternatives strategies (including an epsilon-n greedy algorithm) <sep> Besides theses core concerns, the papers suffers of some imprecisions on the notations which should be clarified. <sep> * As an example using O(1000) and O(1M) in the figure one. Everyone understands what is meant but O notation are made to eliminate constant terms and O(1) = O(1000). <sep> * For eqn (1) it would be better to refer to and ""optimistic strategy"" rather to UCB because the name is already taken by an algorithm which is not this one. Moreover the given strategy would achieve a linear regret if used as described in the paper which is not desirable for bandits algorithms (smallest counter example with two arms following a Bernouilli with different parameters if the best arms generates two zero in a row at the beginning, it is now stuck with a zero mean and zero variance estimate). This is why bandits bounds include a term which increase with the total number of plays. I agree that in practice this effect can be mitigated at that the strategy can be correct in the contextual case (but then I'd like to the dependancies on x to be clear) <sep> * The papers never mentions whats is a scalar, a vector or a matrix. This creates confusion: as an example eqn (3) can have several different meaning depending if the values are scalars, scalars depending on x or having a diagonal \\sigma matrix <sep> * In the paragraph above (2) I unsure of what is a ""binomial noise error distribution"" for epsilon, but a few lines later epsilon becomes a gaussian why not just mention that you assume the presence of a gaussian noise on the parameters of a Bernouilli distribution ?","Meta score: 4 <sep> The paper concerns the development of a density network for estimating uncertainty in recommender systems. The submitted paper is not very clear and it is hard to completely understand the proposed method from the way it is presented. This makes assessing the contribution of the paper difficult. <sep> Pros: <sep> - addresses an interesting and important problem <sep> - possible novel contribution <sep> Cons: <sep> - poorly written, hard to understand precisely what is done <sep> - difficult to compare with the state-of-the-art, not helped by disorganised literature review <sep> - experimentation could be improved <sep> The paper needs more work before being ready for publication."
"This paper proves a new separation results from 3-layer neural networks to 2-layer neural networks. The core of the analysis is a proof that any 2-layer neural networks can be well approximated by a polynomial function with reasonably low degrees. Then the authors constructs a highly non-smooth function can be represented by a 3-layer network, but impossible to approximate by any polynomial-degree polynomial function. <sep> Similar results about polynomial approximation can be found in [1] (Theorem 4). To me, the result proved in [1] is spiritually very similar to propositions 3-4. The authors need to justify the difference. <sep> The main strength of the new separation result is that it holds for a larger class of input distributions. Comparing to Daniely (2017) which requires the input distribution to be spherically uniform, the new result only needs the distribution to be lower bounded by 1/poly(d) in a small ball of radius 1/poly(d). Conceptually I don't think this is a much weaker condition. For a ""truly"" non-uniform distribution, one should allow its density function to be very close to zero at certain regions of the ball. Nevertheless, the result is a step forward from Daniely (2017) and the paper is well written. <sep> I am still in doubt of the practical value of such kind of separation results. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. This function has a super large Lipschitz constant, which we don't expect to see in practice. Consider the function f(x)=cos(Nx). When N is chosen large enough, the function f can not be well approximated by any 2-layer network with polynomial size. Does it imply that the family of cosine functions is rich enough so that it is a better family to learn than 2-layer neural networks? I guess the answer would be negative. In addition, the paper doesn't show that any 2-layer network can be well approximated by a 3-layer network, which is a missing piece in justifying the richness of 3-layer nets. <sep> Finally, the constructed ""hard"" function has order d^5 Lipschitz constant, but Theorem 7 assumes that the 2-layer networks' weight must be bounded by O(d^2). This assumption is crucial to the proof but not well justified (especially considering the d^5 factor in the function definition). <sep> [1] On the Computational Efficiency of Training Neural Networks, Livni et al., NIPS'14",The reviewers point out that most of the results are already known and are not novel. There are also issues with the presentation. Studying only depth 2 and depth 3 networks is very limiting.
"The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning. The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals. There are a few ideas the paper discusses: <sep> (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not ""worth it"" until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree. <sep> (2) that therefore, block diagonal layers lead to more efficient networks. This point is murkier, because the paper doesn't discuss possible increases in *training time* (due to increased number of iterations) in much detail. At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn't seem to be that useful (maybe it is for real-time predictions? Please cite some work in that case) <sep> (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train) <sep> [as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix] <sep> (4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (Martingale or Markov chain theory) <sep> This is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above). The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption. The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job. <sep> The paper could have gone farther experimentally (or theoretically) in my opinion. For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated. I was also wondering about when 2 or more layers are block sparse, do these blocks overlap? i.e., are they randomly permuted between layers so that the blocks mix? And even with a single block, does it matter what permutation you use? (or perhaps does it not matter due to the convolutional structure?) <sep> The section on the variance of the weights is rather unclear mathematically, starting with the abstract and even continuing into the paper. We are talking about sample variance? What does DeltaVar mean in eq (2)? The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX' has a negative eigenvalue. <sep> I agree this relationship with random matrices could be interesting, but it seems too vague right now. Is there some central limit theorem explanation? Are you sure that you've run enough iterations to fully converge? (Fig 4 was still trending up for b1=64). Was it due to the convolutional net structure (you could test this)? Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable). Would this affect the distributions? <sep> Furthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn't see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost). <sep> Minor comment: last paragraph of 4.1, comparing with Sindhwani et al., was confusing to me. Why was this mentioned? And it doesn't seem to be comparable. I have no idea what ""Toeplitz (3)"" is.","The authors propose a technique for weight pruning that leaves block diagonal weights, instead of unstructured sparse weights, leading to faster inference. However, the experiments demonstrating the quality of the pruned models are insufficient. The authors also discuss connections to random matrix theory; but these connections are not worked out in detail."
"Summary <sep> This paper presents differentiable Neural Computational Machines (∂NCM), an abstraction of existing neural abstract machines such as Neural Turing Machines (NTMs) and Neural Random Access Machines (NRAMs). Using this abstraction, the paper proposes loss terms for incorporating supervision on execution traces. Adding supervision on execution traces in ∂NCM improves performance over NTM and NRAM which are trained end-to-end from input/output examples only. The observation that adding additional forms of supervision through execution traces improves generalization may be unsurprising, but from what I understand the main contribution of this paper lies in the abstraction of existing neural abstract machines to ∂NCM. However, this abstraction does not seem to be particularly useful for defining additional losses based on trace information. Despite the generic subtrace loss (Eq 8), there is no shared interface between ∂NCM versions of NTM and NRAM that would allow one to reuse the same subtrace loss in both cases. The different subtrace losses used for NTM and NRAM (Eq 9-11) require detailed knowledge of the underlying components of NTM and NRAM (write vector, tape, register etc.), which questions the value of ∂NCM as an abstraction. <sep> Weaknesses <sep> As explained in the summary, it is not clear to me why the abstraction to NCM is useful if one still needs to define specific subtrace losses for different neural abstract machines. <sep> The approach seems to be very susceptible to the weight of the subtrace loss λ, at least when training NTMs. In my understanding each of the trace supervision information (hints, e.g. the ones listed in Appendix F) provides a sensible inductive bias we would the NTM to incorporate. Are there instances where these biases are noisy, and if not, could we incorporate all of them at the same time despite the susceptibility w.r.t λ? <sep> NTMs and other recent neural abstract machines are often tested on rather toyish algorithmic tasks. I have the impression providing extra supervision in form of execution traces makes these tasks even more toyish. For instance, when providing input-output examples as well as the auxiliary loss in Eq6, what exactly is left to learn? What I like about Neural-Programmer Interpreters and Neural Programmer [1] is that they are tested on less toyish tasks (a computer vision and a question answering task respectively), and I believe the presented method would be more convincing for a more realistic downstream task where hints are noisy (as mentioned on page 5). <sep> Minor Comments p1: Why is Grefenstette et al. (2015) an extension of NTMs or NRAMs? While they took inspiration from NTMs, their Neural Stack has not much resemblance with this architecture. <sep> p2: What is B exactly? It would be good to give a concrete example at this point. I have the feeling it might even be better to explain NCMs in terms of the communication between κ, π and M first, so starting with what I, O, C, B, Q are before explaining what κ and π are (this is done well for NTM as ∂NCM in the table on page 4). In addition, I think it might be better to explain the Controller before the Processor. Furthermore, Figure 2a should be referenced in the text here. <sep> p4 Eq3: There are two things confusing in these equations. First, w is used as the write vector here, whereas on page 3 this is a weight of the neural network. Secondly, π and κ are defined on page 2 as having an element from W as first argument, which are suddenly omitted on page 4. <sep> p4: The table for NRAM as ∂NCM needs a bit more explanation. Where does {1}=I come from? This is not obvious from Appendix B either. <sep> p3 Fig2/p4 Eq4: Related to the concern regarding the usefulness of the ∂NCM abstraction: While I see how NTMs fit into the NCM abstraction, this is not obvious at all for NRAMs, particularly since in Fig 2c modules are introduced that do not follow the color scheme of κ and π in Fig 2a (ct, at, bt and the registers). <sep> p5: There is related work for incorporating trace supervision into a neural abstract machine that is otherwise trained end-to-end from input-output examples [2]. <sep> p5: ""loss on example of difficulties"" -> ""loss on examples of the same difficulty"" <sep> p5: Do you have an example for a task and hints from a noisy source? <sep> Citation style: sometimes citation should be in brackets, for example ""(Graves et al. 2016)"" instead of ""Graves et al. (2016)"" in the first paragraph of the introduction. <sep> [1] Neelakantan et al. Neural programmer: Inducing latent programs with gradient descent. *CONF*. 2015. <sep> [2] Bosnjak et al. Programming with a Differentiable Forth Interpreter. ICML. 2017.","While the reviewers considered the basic idea of adding supervision intermediate to differentiable programming style architectures to be interesting and worthy of effort, they were unsure if <sep> 1: the proposed abstractions for discussing ntm and nram are well motivated/more generally applicable <sep> 2: the methods used in this work to give intermediate supervision are more generally applicable"
"This paper presents an analysis of an agent trained to follow linguistic commands in a 3D environment.  The behaviour of the agent is analyzed by means of a set of ""psycholinguistic"" experiments probing what it learned, and by inspection of its visual component through an attentional mechanism. <sep> On the positive side, it is nice to read a paper that focuses on understanding what an agent is learning. On the negative side, I did not get many new insights from the analyses presented in the study. <sep> 3 A situated language learning agent <sep> I can't make up the chair from the refrigerator in the figure. <sep> 4.1 Word learning biases <sep> This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. Conversely, when it is exposed to colors only, it will have a color bias. When the training set is balanced, the agent shows a mild bias for the simpler color property. How is this interesting or surprising? The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. This, however, is not addressed in the paper. <sep> Minor comments about this section: <sep> - Was there noise also in shape generation, or were all object instances identical? <sep> - propensity to select o_2: rather o_1? <sep> - I did not follow the paragraph starting with ""This effect provides"". <sep> 4.2 The problem of learning negation <sep> I found this experiment very interesting. <sep> Perhaps, the authors could be more explicit about the usage of negation here. The meaning of commands containing negation are, I think, conjunctions of the form ""pick something and do not pick X"" (as opposed to the more natural ""do not pick X""). <sep> modifiation: modification <sep> 4.3 Curriculum learning <sep> Perhaps the difference in curriculum effectiveness in language modeling vs grounded language learning simulations is due to the fact that the former operates on large amounts of natural data, where it's hard to define the curriculum, while the latter are typically grounded in toy worlds with a controlled language, where it's easier to construct the curriculum. <sep> 4.4 Processing and representation differences <sep> There is virtually no discussion of what makes the naturalistic setup naturalistic, and thus it's not clear which conclusions we should derive from the corresponding experiments. Also, I don't see what we should learn from Figure 5 (besides the fact that in the controlled condition shapes are easier than categories). For the naturalistic condition, the current figure is misleading, since different classes contain different numbers of instances. It would be better to report proportions. <sep> Concerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. What is novel here? <sep> Also, since introducing attention changes the architecture, shouldn't the paper report the learning behaviour of the attention-augmented network? <sep> The explanation of the attention mechanism is dense, and perhaps could be aided by a diagram (in the supplementary materials?). I think the description uses ""length"" when ""dimensional(ity)"" is meant. <sep> 6. Supplementary material <sep> It would be good to have an explicit description of the architecture, including number of layers of the various components, structure of the CNN, non-linearities, dimensionality of the layers, etc. (some of this information is inconsistently provided in the paper). <sep> It's interesting that the encoder is actually a BOW model. This should be discussed in the paper, as it raises concerns about the linguistic interest of the controlled language that was used. <sep> Table 3: indicates is: indicates if","This paper resulted in significant discussion -- both between R2 and the authors, and between the AC, PCs, and other solicited experts. <sep> The problem of language grounding (and instruction following) in virtual environments is clearly important, this work was one of the first in the recent resurgence, and the goal of understand what the agents have learned is clearly noble and important. In terms of raw recommendations, the majority reviewer recommendation is negative, but since concerns raised by R2 seemed subjective (which in principle is not a problem), out of abundance of caution, we solicited additional input. Unfortunately, we received feedback consistent with the concerns raised here: <sep> -- The lack of generality of the behavior found. Even if we ignore the difficult question of why the agent prefers what it does, it's unclear how the conclusions here generalize much farther than the model and environment used; the manuscript does not provide any novel or transferable principals of the form ""this kind of bias in the environment leads to this kind of bias in models with these properties"". <sep> -- We realize even providing that concrete a statement might be hard, but also missing are thorough comparisons to other kinds of models (e.g. non-deep, as asked by R1) to establish that this is a general phenomenon. <sep> Ultimately, there is a sense that this is too narrow an analysis, too soon. If there was one architecture for learning embodied agents in 3d environments that was clearly successful and useful, then studying its properties might be interesting (even crucial). But the dust in this space isn't settled. Our current agents are fairly poor, and so the impact of understanding the biases of a specific model trained in a specific environment seems fairly low. <sep> Finally -- this not taken into consideration in making the decision -- it is not okay to list personal homepage domains (that may reveal author identity to ACs) as conflict domains; those are meant for institutional conflicts/domains."
"Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results. It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction. However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place, by 2) reproducing the results in the same way as in the original work, 3) by avoiding introducing false claims based on a misunderstanding of terminology and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks. <sep> This paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, ""Learning to Navigate in Complex Environments"") and one of the architectures (NavA3C+D1D2L) from that paper. It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method. For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique. The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call ""static maze""), and in fixed mazes with changing goal environments (what they call ""environments with dynamic elements"" or ""random goal mazes""). <sep> This submission claims that: <sep> [a] ""[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms"", <sep> [b] ""following training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal"", <sep> [c] ""when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning"", <sep> [d] ""this state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results"", <sep> [e] ""Do DRL-based navigation algorithms really 'learn to navigate'? Our results answer this question negatively."" <sep> [f] ""we are the first to evaluate any DRL-based navigation method on maps with unseen structures"" <sep> The paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions). While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes. The use of attention heat maps is interesting. <sep> The main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results. <sep> Regarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a]. More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, "" Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation""). This widely accepted definition of navigation does not preclude being limited to known environments only. <sep> Regarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns. The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their ""I-maze"". Moreover, claim [d] about repeatability is also invalidated by the fact that the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable. <sep> Additionally, some statements made by the authors are demonstrably untrue. First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016). <sep> Second, the title of the submission, ""Do Deep Reinforcement Learning Algorithms Really Learn to Navigate"" makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper). Why did the authors not cite and consider (Parisotto et al, 2017, ""Neural Map: Structured Memory for Deep Reinforcement Learning""), which explicitly claims that their method is ""capable of generalizing to environments that were not seen during training""? It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*.","This paper received divergent ratings (7, 3, 3). While there is value in thorough evaluation papers, this manuscript has significant presentation issues. As all three reviewers point out, the way it is currently written, it misrepresents the claims made by Mirowski et al 2016 and over-reaches in its findings. Unfortunately, we cannot make a decision on what the manuscript may look like in future once these issues are fixed, and must reject."
"Summary: <sep> This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR. <sep> The authors conclude that in this experimental setting: <sep> - AT seems to defend models against shared dx's. <sep> - This is visible on universal perturbations, which become less effective as more AT is applied. <sep> - AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change. <sep> - Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT. <sep> Pro: <sep> - Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking. <sep> - The visualizations of universal perturbations as they change during AT are nice. <sep> - The basic observation wrt the behavior of AT is clearly communicated. <sep> Con: <sep> - The experiments performed are interesting directions, although unfocused and rather limited in scope. For instance, does the same phenomenon happen for different datasets? Different models? <sep> - What happens when we use adversarial attacks different from FGSM? Do we get similar results? <sep> - The papers lacks a more in-depth theoretical analysis. Is there a principled reason AT+FGSM defends against universal perturbations? <sep> Overall: <sep> - As is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of AT. A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations. <sep> Detailed: <sep> -","This paper studies to what extent adversarial training affects the properties of adversarial examples in object classification. <sep> Reviewers found the work going in the right direction, but agreed that it needs further evidence/focus in order to constitute a significant contribution to the *CONF* community. In particular, the AC encourages authors to relate their work to the growing body of (mostly concurrent) work on robust optimization and adversarial learning. For the above reasons, the AC recommends rejection at this time."
"The method proposes a new architecture for solving image super-resolution task. They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems. <sep> The writing of the paper needs improvement. I was not able to understand the proposed connection, as notation is inconsistent and it is difficult to figure out what the authors are stating. I am willing to reconsider my evaluation if the authors provide clarifications. <sep> The paper does not refer to recent advances in the problem, which are (as far as I know), the state of the art in the problem in terms of quality of the solutions. This references should be added and the authors should put their work into context. <sep> 1) Arguably, the state of the art in super resolution are techniques that go beyond L2 fitting. Specifically, methods using perceptual losses such as: <sep> Johnson, J. et al ""Perceptual losses for real-time style transfer and super-resolution."" European Conference on Computer Vision. Springer International Publishing, 2016. <sep> Ledig, Christian, et al. ""Photo-realistic single image super-resolution using a generative adversarial network."" arXiv preprint arXiv:1609.04802 (2016). <sep> PSNR is known to not be directly related to image quality, as it favors blurred solutions. This should be discussed. <sep> 2) The overall notation of the paper should be improved. For instance, in (1), g represents the observation (the LR image), whereas later in the text, g is the HR image. <sep> 3) The description of Section 2.1 is quite confusing in my view. In equation (1), y is the signal to be recovered and K is just the downsampling plus blurring. So assuming an L1 regularization in this equation assumes that the signal itself is sparse. Equation (2) changes notation referring y as f. <sep> 4) Equation (2) seems wrong. The term multiplying K^T is not the norm (should be parenthesis). <sep> 5) The first statement of Section 2.2. seems wrong. DL methods do state the super resolution problem as an inverse problem. Instead of using a pre-defined basis function they learn an over-complete dictionary from the data, assuming that natural images can be sparsely represented. Also, this section does not explain how DL is used for super resolution. The cited work by Yang et al learns a two coupled dictionaries (one for LR and HL), such that for a given patch, the same sparse coefficients can reconstruct both HR and LR patches. The authors just state the sparse coding problem. <sep> 6) Equation (10) should not contain the \\leq \\epsilon. <sep> 7) In the second paragraph of Section 3, the authors mention that the LR image has to be larger than the HR image to prevent border effects. This makes sense. However, with the size of the network (20 layers), the change in size seems to be quite large. Could you please provide the sizes? When measuring PSNR, is this taken into account? <sep> 8) It would be very helpful to include an image explaining the procedure described in the second paragraph of Section 3. <sep> 9) I find the description in Section 3 quite confusing. The authors relate the training of a single filter (or neuron) to equation (7), but they define D, that is not used in all of Section 2.1. And K does not show in any of the analysis given in the last paragraph of page 4. However, D and K seem two different things (it is not just one for the other), see bellow. <sep> 10) I cannot understand the derivation that the authors do in the last paragraph of page 4 (and beginning of page 5). What is phi_l here? K in equation (7) seems to match to D here, but D here is a collection of patches and in (7) is a blurring and downsampling operator. I cannot review this section. I will wait for the author's response clarifications. <sep> 11) The authors describe a change in roles between the representations and atoms in the training and testing phase respectively. I do not understand this. If I understand correctly, the final algorithm, the authors train a CNN mapping LR to HR images. The network is used in the same way at training and testing. <sep> 12) It would be useful to provide more details about the training of the network. Please describe the training set used by Kim et al. Are the two networks trained independently? One could think of fine-tuning them jointly (including the aggregation). <sep> 13) The authors show the advantage of separating networks on a single image, Barbara. It would be good to quantify this better (maybe in terms of PSNR?). This observation might be true only because the training loss, say than the works cited above. Please comment on this. <sep> 14) In figures 3 and 4, the learned filters are those on the top (above the yellow arrow). It is not obvious to me that the reflect the predominant structure in the data. (maybe due to the low resolution). <sep> 15) This work is related to (though clearly different)  that of LISTA (Learned ISTA) type of networks, proposed in: <sep> Gregor, K., & LeCun, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML) <sep> Which connect the network architecture with the optimization algorithm used for solving the sparse coding problem. Follow up works have used these ideas for solving inverse problems as well.","This paper addresses the question of how to solve image super-resolution, building on a connection between sparse regularization and neural networks. <sep> Reviewers agreed that this paper needs to be rewritten, taking into account recent work in the area and significantly improving the grammar. The AC thus recommends rejection at this time."
"The paper presents a new CNN architecture: CrescendoNet. It does not have skip connections yet performs quite well. <sep> Overall, I think the contributions of this paper are too marginal for acceptance in a top tier conference. <sep> The architecture is competitive on SVHN and CIFAR 10 but not on CIFAR 100. The performance is not strong enough to warrant acceptance by itself. <sep> FractalNets amd DiracNets (https://arxiv.org/pdf/1706.00388.pdf) have demonstrated that it is possible to train deep networks without skip connections and achieve high performance. While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet. Hence, CrescendoNet does not have the best performance among skip connection free networks. <sep> You claim that FractalNet shows no ensemble behavior. This is clearly not true because FractalNet has ensembling directly built in, i.e. different paths in the network are explicitly averaged. If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet. While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble. Besides, as Veit showed, ResNet also shows ensemble behavior. Hence, using ensembling in deep networks is not a significant contribution. <sep> The authors claim that ""Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance"". I don't think the experiments show that ensemble behavior leads to high performance. Just because a network performs averaging of different paths and individual paths perform worse than sets of paths doesn't imply that ensembling as a mechanism is in fact the cause of the performance of the entire architecture. Similary, you say ""On the other hand, the ensemble model can explain the performance improvement easily."" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet. <sep> Path-wise training is not original enough or indeed different enough from drop-path to count as a major contribution. <sep> You claim that the number of layers ""increase exponentially"" in FractalNet. This is misleading. The number of layers increases exponentially in the number of paths, but not in the depth of the network. In fact, the number of layers is linear in the depth of the network. Since depth is the meaningful quantity here, CrescendoNet does not have an advantage over FractalNet in terms of layer number. Also, it is always possible to simply add more paths to FractalNet if desired without increasing depth. Instead of using 1 long paths, one can simply use 2, 3, 4 etc. While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers. CrescendoNets do not extend beyond this design principle. <sep> You say that ""First, path-wise training procedure significantly reduces the memory requirements for convolutional layers, which constitutes the major memory cost for training CNNs. For example, the higher bound of the memory required can be reduced to about 40% for a Crescendo block with 4 paths where interval = 1."" This is misleading, as you need to store the weights of all convolutional layers to compute the forward pass and the majority of the weights of all convolutional layers to compute the backward pass, no matter how many weights you intend to update. In a response to a question I posed, you mentioned that we you meant was ""we use about 40% memory for the gradient computation and storage"". Fair enough, but ""gradient computation and storage"" is not mentioned in the paper. Also, the reduction to 40% does not apply e.g. to vanilla SGD because the computed gradient can be immediately added to the weights and does not need to be stored or combined with e.g. a stored momentum term. <sep> Finally, nowhere in the paper do you mention which nonlinearities you used or if you used any at all. In future revisions, this should be rectified. <sep> While I can definitely imagine that your network architecture is well-designed and a good choice for image classification tasks, there is a very saturated market of papers proposing various architectures for CIFAR-10 and related datasets. To be accepted to *CONF*, either outstanding performance or truly novel design principles are required.","The paper proposes a new convolutional network architecture, called CrescendoNet. Whilst achieving competitive performance on CIFAR-10 and SVHN, the accuracy of the proposed model on CIFAR-100 is substantially lower than that of state-of-the-art models with fewer parameters; the paper presents no experimental results on ImageNet. The proposed architecture does not provide clear new insights or successful new design principles. This makes it unlikely the current manuscript will have a lot of impact."
"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. <sep> While the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions: <sep> - AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end. <sep> - While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup. <sep> - Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners. <sep> - Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations. <sep> Other comments: <sep> - Paper would benefit from writing improvements to make it read better. <sep> - ""simply use the weighted error function"": I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted. <sep> -  ""to replace the softmax error function (used in deep learning)"": I don't think we have softmax error function","The paper presents a boosting method and uses it to train an ensemble of convnets for image classification. The paper lacks conceptual and empirical comparisons with alternative boosting and ensembling methods. In fact, it is not even clear from the experimental results whether or not the proposed method outperforms a simple baseline model that averages the predictions of T independently trained convolutional networks."
"Summary: <sep> The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. This identity connection acts as a ""surrogate memory"" component, preserving hidden activations over time steps. <sep> The experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames. It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI. <sep> The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs. <sep> Review: <sep> I very much like the paper. The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures! <sep> I have a few comments and questions: <sep> 1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise? If bit-wise, can you elaborate why? I might have missed something. <sep> 2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c? Also some curves in the appendix stop abruptly without visible explosions. Were these experiments run until completion? If so, would it be possible to plot the complete curves? <sep> 3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task. Optimal hyperparameters are usually model-specific. Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript. <sep> Le et al. (2015) for instance perform a coarse grid search for each model. <sep> 4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up. There is no (multiplicative) gating as in Highway Networks. <sep> 5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating. The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions. <sep> 6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful. Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations. Would RINs readily learn to reset parts of the hidden state? <sep> 7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences. <sep> Overall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). After a revision I would consider to increase the score. <sep> References: <sep> Henaff, Mikael, Arthur Szlam, and Yann LeCun. ""Recurrent orthogonal networks and long-memory tasks."" In International Conference on Machine Learning, pp. 2034-2042. 2016. <sep> Le, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. ""A simple way to initialize recurrent networks of rectified linear units."" arXiv preprint arXiv:1504.00941 (2015).","The authors propose to use identity + some weights in the recurrent connections to prevent vanishing gradients. The reviewers found the experiments to have weak baselines, weakening the claims of the paper."
"Summary: <sep> This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of ""learnability"" --- the learnability of a model (N1) is defined as the ""expected agreement"" between the output of N1, and the output of another model N2 which has been trained to match N1 (on a dataset of size n).  The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is --- furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model. <sep> The paper presents a number of interesting results: <sep> 1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler). <sep> 2) Networks trained with random data are significantly less learnable than networks trained on real data. <sep> 3) Networks trained on small mini-batches (larger variance SGD updates) are more learnable than those trained on large minibatches. <sep> These results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; these results at first seem to run counter to the ideas from statistical learning theory that models with high capacity (VC dimension, radamacher complexity, etc.) have much weaker generalization guarantees than lower capacity models.  These results suggest that models that have high capacity (by one definition) are also capable of being simple (by another definition).  These results nicely complement the work which studies the ""sharpness/curvature"" of the local minima found by neural networks, which argue that the minima which generalize better are those with lower curvature. <sep> Review: <sep> Quality:  I found this to be high quality work. The paper presents many results across a variety of network architectures.  One area for improvement is presenting results on larger datasets (currently all experiments are on CIFAR-10), and/or on non-convolutional architectures.  Additionally, a discussion of why learnabiblity might imply low generalization error would have been interesting (the more formal, the better), though it is unclear how difficult this would be. <sep> Clarity:  The paper is written clearly.  A small point: Step 2 in section 3.1 should specify that argmax of N1(D2) is used to generate labels for the training of the second network.  Also, what dataset D_i is used for tables 3-6? Please specify. <sep> Originality: The specific questions tackled in this paper are original (learnability on random vs. real data, large vs. small networks, and large vs. small mini-batch training).  But it is unclear to me exactly how original this use of ""learnability"" is in evaluating how simple a model is.  It seems to me that this particular use of ""learnability"" is original, even though PAC learnability was defined a while ago. <sep> Significance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize.  I believe it is important to find new ways of formally defining the ""simplicity/capacity"" of a model, such that ""simpler"" models can be proven to have smaller generalization gap (between train and test error) relative to more ""complicated"" models. It is clear that VC dimension and radamacher complexity alone are not enough to explain the generalization performance of neural networks, and that neural networks with high capacity by these definitions are likely ""simple"" by other definitions (as we have seen in this paper).  This paper makes an important contribution to this conversation, and could perhaps provide a starting point for theoreticians to better explain why deep networks generalize well. <sep> Pros <sep> - nice experiments, with very interesting results. <sep> - Helps explain one way in which large networks are in fact ""simple"" <sep> Cons <sep> - The paper does not attempt to relate the notion of learnability to that of generalization performance.  All it says is that these two metrics appear to be well correlated.","+ The paper proposes an interesting empirical measure of ""learnability"" of a trained network: how well the predictive function it represents can be learned by another network. And shows it empirically seems to correlate with better generalization. <sep> - The work is purely empirical: it features no theory relating this learnability to generalization <sep> - Learnability measure is somewhat ad-hoc with moving parts left to be specified (learning network, data splits, ...) <sep> - as pointed out by a reviewer, learnability doesn't really provide any answers for now. <sep> - the work would be much stronger if it went beyond a mere correlation study, and if learnability considerations allowed to derive a new approach/regularization scheme that was convincingly shown to improve generalization."
"1. Summary <sep> This paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change. <sep> 2. High level paper <sep> - I believe the writing is a bit sloppy. For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below). This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models. <sep> 3. High level technical <sep> - I'm confused at the first and second lines of equation (19). In the first line, shouldn't the first term not contain \\Delta W ? In the second line, shouldn't the first term be \\tilde{\\mathcal{L}}(W_0 + \\Delta W) ? <sep> - For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data. My worry is if you're compressing these networks with your method are the weights not treated as binary anymore? Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright. But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply. <sep> - Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354). What is your compression ratio for 0 accuracy loss? I think this is a key experiment that should be run as this result would be much easier to compare with the other methods. <sep> - Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers? <sep> - The first paper to propose weight sharing was not Han et al., 2015, it was actually: <sep> Chen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. ""Compressing Neural Networks with the Hashing Trick"" ICML 2015 <sep> Although they did not learn the weight sharing function, but use random hash functions. <sep> 4. Low level technical <sep> - The end of Section 2 has an extra 'p' character <sep> - Section 3.1: ""Here, X and y define a set of samples and ideal output distributions we use for training"" this sentence is a bit confusing. Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3. <sep> - Section 3.1: ""W is the learnt model...\\hat{W} is the final, trained model"" This is unclear: W and \\hat{W} seem to describe the same thing. I would just remove ""is the learnt model and"" <sep> 5. Review summary <sep> While the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read. This combined with some key experimental questions I have make me think this paper still needs work before being accepted to *CONF*.","Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state-of-the-art. <sep> Paper presentation quality also needs to be improved."
"In this paper, an number of very strong (even extraordinary) claims are made: <sep> * The abstract promises ""a framework to understand the unprecedented performance and robustness of deep neural networks using field theory."" <sep> * Page 8 states that this is ""This is a first attempt to describe a neural network with a scalar quantum field theory."" <sep> * Page 2 promises the use of the ""Goldstone theorem"" (no less) to understand phase transition in deep learning <sep> * It also claim that many ""seemingly different experimental results can be explained by the presence of these zero eigenvalue weights."" <sep> * Three important results are stated as ""theorem"", with a statement like ""Deep feedforward networks learn by breaking symmetries"" proven in 5 lines, with no formal mathematics. <sep> These are extraordinary claims, but  when reaching page 5, one sees that the basis of these claims seems to be the Lagrangian of a simple phi-4 theory, and Fig. 1 shows the standard behaviour of the so-called mexican hat in physics, the basis of the second-order transition. Given physicists have been working on neural network for more than three or four decades, I am surprise that this would enough to solve all these problems! <sep> I tried to understand these many results, but I am afraid I cannot really understand or see them. In many case, the explanation seems to be a vague analogy. These are not without interest, and maybe there is indeed something deep in this paper, but it is so far hidden by the hype. Still, I fail to see how the fact that phase transitions and negative direction in the landscape is a new phenomena, and how it explains all the stated phenomenology. Beside, there are quite a lot of things known about the landscape of these problems <sep> Maybe I am indeed missing something, but i clearly suspect the authors are simply overselling physics results. <sep> I have been wrong many times, but I beleive that the authors should probably precise their claim, and clarify the relation between their results and both the physics AND statistics litterature, or better, with the theoretical physics litterature applied to learning, which is ---astonishing-- absent in the paper. <sep> About the content: <sep> The main problem for me is that the whole construction using field theory seems to be used to advocate for the appearence of a phase transition in neural nets and in learning. This rises three comments: <sep> (1) So we really need to use quantum field theory for this? I do not see what should be quantum here (despite the very vague remarks page 12 ""WHY QUANTUM FIELD THEORY?"") <sep> (2) This is not new. Phase transitions in learning in neural nets are being discussed since aboutn 40 years, see for instance all the pionnering work of Sompolinky et al. one can see for instance the nice review in https://arxiv.org/abs/1710.09553 In non aprticular order, phase transition and symmetry breaking are discussed in <sep> * ""Statistical mechanics of learning from examples"", Phys. Rev. A 45, 6056 – Published 1 April 1992 <sep> * ""The statistical mechanics of learning a rule"", Rev. Mod. Phys. 65, 499 – Published 1 April 1993 <sep> * Phase transitions in the generalization behaviour of multilayer neural networks http://iopscience.iop.org/article/10.1088/0305-4470/28/16/010/meta <sep> * Note that some of these results are now rigourous, as shown in ""Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models"", https://arxiv.org/abs/1708.03395 <sep> * The landscape of these problems has been studied quite extensivly, see for instance ""Identifying and attacking the saddle point problem in high-dimensional non-convex optimization"", https://arxiv.org/abs/1406.2572 <sep> (3) There is nothing particular about deep neural net and neural nets about this. Negative direction in the Hessian in learning problems appears in matrix and tensor factorizaion, where phase transition are well understood (even rigorously, see for instance, https://arxiv.org/abs/1711.05424 ) or in problems such as unsupervised learning, as e.g.: <sep> https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.86.2174 <sep> https://journals.aps.org/pre/pdf/10.1103/PhysRevE.50.1766 <sep> Here are additional comments: <sep> PAGE 1: <sep> * ""It has been discovered that the training process ceases when it goes through an information bottleneck (ShwartzZiv & Tishby, 2017)"". <sep> While this paper indeed make a nice suggestion, I would not call it a discovery yet as this has never been shown on a large network. Beside, another paper in the conference is claiming exacly the opposite, see : ""On the Information Bottleneck Theory of Deep Learning"". This is still subject of discussion. <sep> * ""In statistical terms, a quantum theory describes errors from the mean of random variables. "" <sep> Last time I studied quantum theory, it was a theory that aim to explain the physical behaviours at the molecular, atomic and sub-atomic levels, usinge either on the wave function (Schrodinger) or the Matrix operatir formalism (Hesienbger) (or if you want, the path integral formalism of Feynman). <sep> It is certainly NOT a theory that describes errors from the mean of random variables. This is, i beleive, the field of ""statistics"" or ""probability"" for correlated variables. It is certianly used in physics, and heavily both in statistical physics and in quantum thoery, but this is not what the theory is about in the first place. <sep> Beside, there is little quantum in this paper, I think most of what the authors say apply to a statistical field theory ( https://en.wikipedia.org/wiki/Statistical_field_theory ) <sep> * ""In the limit of a continuous sample space, the quantum theory becomes a quantum field theory."" <sep> Again, what is quantum about all this? This true for a field theory, as well for continous theories of, say, mechanics, fracture, etc... <sep> PAGE 2: <sep> * ""Using a scalar field theory we show that a phase transition must exist towards the end of training based on empirical results."" <sep> So it is a scalar classical field theory after all. This sounds a little bit less impressive that a quantum field theory. Note that the fact that phase transition arises in learning, and in a statistical theory applied to any learning process, is an old topic, with a classical litterature. The authors might be interested by the review ""The statistical mechanics of learning a rule"", Rev. Mod. Phys. 65, 499 – Published 1 April 1993 <sep> PAGE 8: <sep> * ""In this work we solved one of the most puzzling mysteries of deep learning by showing that deep neural networks undergo spontaneous symmetry breaking."" <sep> I am afraid I fail to see what is so mysterious about this nor what the authors showed about it. In any case, gradient descent break symmetry spontaneously in many systems, including phi-4, the Ising model or (in learning problems) the community detection problem (see eg https://journals.aps.org/prx/abstract/10.1103/PhysRevX.4.011047). I am afraid I miss what is new there... <sep> * ""This is a first attempt to describe a neural network with a scalar quantum field theory."" <sep> Given there seems to be little quantum in the paper, I fail to see the relevance of the statement. Secondly, I beleive that field theory has been used, many times and in greater lenght, both for statistical and dynamical problems in neural nets, see eg. <sep> * http://iopscience.iop.org/article/10.1088/0305-4470/27/6/016/meta <sep> * https://arxiv.org/pdf/q-bio/0701042.pdf <sep> * http://www.lps.ens.fr/~derrida/PAPIERS/1987/gardner-zippelius-87.pdf <sep> * http://iopscience.iop.org/article/10.1088/0305-4470/21/1/030/meta <sep> * https://arxiv.org/pdf/cond-mat/9805073.pdf","The paper makes overly strong claims, too weakly supported by a hard to follow and insufficiently rigorous mathematical argument. Connections with a large body of relevant prior literature are missing."
"This paper proposed some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score, which are motivated and borrowed from the image quality assessment techniques. These energy component in the objective function allows learning of different set of features and determination on whether the features are adequately represented. experiments on the using different hyper-parameters of the energy function, as well as visual inspections on the quality of the learned images, are presented. <sep> It appears to me that the novelty of the paper is limited, in that the main approach is built on the existing BEGAN framework with certain modifications. For example, the new energy function in equation (4) larges achieves similar goal as the original energy (1) proposed by Zhao et. al (2016), except that the margin loss in (1) is changed to a re-weighted linear loss, where the dynamic weighting scheme of k_t is borrowed  from the work of Berthelot et. al (2017). It is not very clear why making such changes in the energy would supposedly make the results better, and no further discussions are provided.  On the other hand, the several energy component introduced are simply choices of the similarity measures as motivated from the image quality assessment, and there are probably a lot more in the literature whose application can not be deemed as a significant contribution to either theories or algorithm designs in GAN. <sep> Many results from the experimental section rely on visual evaluations, such as in Figure~4 or 5; from these figures, it is difficult to clearly pick out the winning images. In Figure~5, for a fair  evaluation on the performance of model interploations, the same human model should be used for competing methods, instead of applying different human models and different interpolation tasks in different methods.","The paper received borderline-negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper. Although R3 was marginally positive, they pointed out that the experiments are ""extremely weak"". The AC look at the paper and agrees with R3 on this point. Therefore the paper cannot be accepted in its current form. The experiments and clarity need work before resubmission to another venue."
"1. Summary <sep> The authors of the paper compare the learning of representations in DNNs with Shannons channel coding theory, which deals with reliably sending information through channels. In channel coding theory the statistical properties of the coding of the information can be designed to fit the task at hand. With DNNs the representations cannot be designed in the same way. But the representations, learned by DNNs, can be affected indirectly by applying regularization. Regularizers can be designed to affect statistical properties of the representations, such as sparsity, variance, or covariance. The paper extends the regularizers to perform per-class regularization. This makes sense, because, for example, forcing the variance of a representation to go towards zero is undesirable as it would state that the unit always has the same output no matter the input. On the other hand having zero variance for a class is desirable as it means that the unit has a consistent activation for all samples of the same class. The paper compares different regularization techniques regarding their error performance. They find that applying representation regularization outperforms classical approaches such as L1 and L2 weight regularization. They also find, that performing representation regularization on the last layer achieves the best performance. Class-wise methods generally outperform methods that apply regularization on all classes. <sep> 2. Remarks <sep> Shannons channel coding theory was used by the authors to derive regularizers, that manipulate certain statistical properties of representations learned by DNNs. In the reviewers opinion, there is no theoretical connection between DNNs and channel theory. For one, DNNs are no channels in the sense that they transmit information. DNNs are rather pipes that transform information from one domain to another, where representations are learned as an intermediate model as the information is being transformed. Noise introduced in the process is not due to a faulty channel but due to the quality of the learned representations themselves. The paper falls short in explaining how DNNs and Shannons channel coding theory fit together theoretically and how they used it to derive the proposed regularizers. Despite the theoretical gap between the two was not properly bridged by the authors, channel coding theory is still a good metaphor for what they were trying to achieve. <sep> The authors recognize that there is similar research being done independently by Belharbi et al. (2017). The similarities and differences between the proposed work and Belharbi et al. should be discussed in more detail. <sep> The authors conclude that it is unclear which statistical properties of representations are generally helpful when being strengthened. It would be nice if they had derived at least a set of rules of thumb. Especially because none of the regularizers described in the paper only target one specific statistical property but multiple. One good example that was provided, is that L1-rep consistently failed to train on CIFAR-100, because too much sparsity can hurt performance, when having many different classes (100 in this case). These kinds of conclusions will make it easier to transfer the presented theory into practice. <sep> 3. Conclusion <sep> The comparison between DNNs and Shannons channel coding theory stands on shaky ground. The proposed regularizes are rather simple, but perform well in the experiments. The effect of each regularizer on the statistical properties of the representation and the relations to previous work (especially Belharbi et al. (2017)) should be discussed in more detail.","The paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold. The main issue, raised by both R2 and R3 was that connection between representation learning in deep nets and coding theory was not fully justified/made. With no reviewer advocating acceptance, it is not possible to accept the paper unfortunately."
"This paper examines sparse connection patterns in upper layers of convolutional image classification networks.  Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks.  Heuristics for distributing connections among windows/groups and a measure called ""scatter"" are introduced to construct the connectivity masks, and evaluated experimentally on CIFAR-10 and -100, MNIST and Morse code symbols. <sep> While it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions.  I've described many of the points I was confused by in more detailed comments below. <sep> Detailed comments and questions: <sep> The distribution of connections in ""windows"" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image.  But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure.  So are do the ""windows"" correspond to spatial windows, and if so, how?  Or are they different (maybe arbitrary) groupings over the feature maps? <sep> Also a bit confusing is the notation ""conv2"", ""conv3"", etc.  These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example).  But here it seems just to indicate the number of ""CL"" layers: 2.  And p.1 says that the ""CL"" layers are those often referred to as ""FC"" layers, not ""conv"" (though they may be convolutionally applied with spatial 1x1 kernels). <sep> The heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets.  For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop.  So some regions are more important than others, and the top half may be more important than an equally spaced global view.  So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the ""scatter"" metric described. <sep> Another broader question I have is in the distinction between lower and upper layers (those referred to as ""feature extracting"" and ""classification"" in this paper).  It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret).  So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the ""classification"" function is pushed down to lower layers, as the upper layers are reduced in size.  How would they respond to similar reductions? <sep> I'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?  I'm also unsure whether the windows are over spatial extent only, or over features.","The paper received weak scores: 4,4,5. R2 complained about clarity. R3's point about the lack of fully connected layers in current SOA deepnets is very valid and the authors response far from convincing. Unfortunately the major revision provided by the authors was not commented on by the reviewers, but many of the major shortcomings of the work still remain. <sep> Generally, the paper is below the acceptance threshold, so cannot be accepted."
"The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. They find weak but statistically significant correlations for a subset of the evaluated metrics, an improvement over the situation that has been observed in open-domain dialog generation. <sep> Other than the necessarily condensed model section (describing a model explained at greater length in a different work) the paper is quite clear and well-written throughout, and the authors' explication of metrics like BLEU and greedy matching is straightforward and readable. But the novel work in the paper is limited to the human evaluations collected and the correlation studies run, and the authors' efforts to analyze and extend these results fall short of what I'd like to see in a conference paper. <sep> Some other points: <sep> 1. Where does the paper's framework for response generation (i.e., dialog act vectors and delexicalized/lexicalized slot-value pairs) fit into the landscape of task-oriented dialog agent research? Is it the dominant or state-of-the-art approach? <sep> 2. The sentence ""This model is a variant of the ""ld-sc-LSTM"" model proposed by Sharma et al. (2017) which is based on an encoder-decoder framework"" is ambiguous; what is apparently meant is that Sharma et al. (2017) introduced the hld-scLSTM, not simply the ld-scLSTM. <sep> 3. What happens to the correlation coefficients when exact reference matches (a significant component of the highly-rated upper right clusters) are removed? <sep> 4. The paper's conclusion naturally suggests the question of whether these results extend to more difficult dialog generation datasets. Can the authors explain why the datasets used here were chosen over e.g. El Asri et al. (2017) and Novikova et al. (2016)?","This paper tackles a very important problem: evaluating natural language generation. The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores. This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions. This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature."
"I like the idea of coupling the language and the conversation model. This is in line with the latest trends of constructing end-to-end NN models that deal with the conversation in a holistic manner. The idea of enforcing information isolation is brilliant. Creating hidden information and allowing the two-party model to learn through self-play is a very interesting approach and the results seem promising. <sep> Having said that, I feel important references are missing and specific statements of the paper, like that ""Their success is however limited to conversations with very few turns and without goals"" can be argued. There are papers that are goal oriented and have many turns. I will just provide one example, to avoid being overwhelming, although more can be found in the literature. That would be the paper of T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. Rojas-Barahona, P.-H. Su, S. Ultes and S. Young (2017). ""A Network-based End-to-End Trainable Task-oriented Dialogue System."" EACL 2017, Valencia, Spain. In fact in this paper even more dialogue modules are coupled. So, the ""fresh challenge"" of the paper can be argued. <sep> It is not clear to me how you did the supervised part of the training. To my experience, although supervised learning can be used, reinforcement learning seems to be the most popular choice. Also, I had to read most of the paper to understand that the system is based on a simulator. Additionally, it is not clear how you got the ground-truth for the training. How are the action and the dialogue generated by the simulator guaranteed to follow the optimal policy? <sep> I also disagree with the statement that ""based on those... to estimate rewards"". If ruled-based systems were sufficient, there would not be a need for statistical dialogue managers. However, the latter is a very active research area. <sep> Figure 1 is missing information (for my likings), like not defined symbols. In addition, it's not self-contained. Also, I would prefer a longer, descriptive and informative label to make the figure as self-explained as possible. I believe it would add to the clarity of the paper. <sep> Also, fundamental information, according to my opinion is missing. For example, what are the restrictions R and how is the database K formed? What is the size of the database? How many actions do you define? Some of them are defined in the action state decoder, but it is not clear if it is all of them. <sep> GRU -> abbreviation not defined <sep> I would really appreciate a figure to better explain the subsection ""Encoding External Knowledge"". In the current form I am struggling to understand what the authors mean. <sep> How is the embedding matrix E created? <sep> Have you tried different unit sizes d? Have you tried different unit sizes for the customer and the service? <sep> ""we use 2 transformation matrixes"" -> if you could please provide more details <sep> How is equation 2 related to figure 1? <sep> Typo: ""name of he person"" <sep> ""During the supervised learning... and the action states"". I am not sure I get what you mean. May you make this statement more clearly by adding an equation for example? <sep> What happens if you use random rather than supervised learning weight initialisation? <sep> Equation 7: What does T stand for? <sep> I cannot find Table 1, 2 and 5 referred to in-text. Moreover I am not sure about quite some items. For example, what is number db? What is the inference set? <sep> 500k of data is quite some. A figure on convergence would be nice. <sep> Setting generator: You mention the percentage of book and flight not found. What about the rest of the cases? <sep> Typo: ""table 3 and table 3"" <sep> The set of the final states of the dialogue is not the same as the ones presented at Fig. 2. <sep> Sub section reward generation is poorly described. After all, reward seems to play a very important role for the proposed system. Statements like  ""things such as"" (instead the exhaustive list of rules for example) or ""the error against the optimal distance"" with no note what should be considered the optimal distance make the paper clarity decreased and the results not possible to be reproduced. Personally I would prefer to see some equations or a flow chart. By the way, have you tried and alternative reward function? <sep> Table 4 is not easy for me to understand. For example, what do you mean when you say eval reward? <sep> Implementation details. I fail to understand how the supervised learning is used (as said already). Also you make a note for the value network, but not for the policy network. <sep> There are some minor issues with the references such as pomdp or lstm not being capitalised <sep> In general, I believe that the paper has a great potential and is a noticeable work. However, <sep> the paper could be better organised. Personally, I struggled with the clarity of some text portions. <sep> For me, the main drawback of the paper is that it was't tested with human users. The actual success of the system when evaluated by humans can be surprisingly different from the one that comes from simulation.","While using self-play for training a goal-oriented dialogue system makes sense, the contribution of this paper compared to previous work (that the paper itself cites) seems too minor, and the limitations of using toy synthetic data further weaken the work."
"My main concern for this paper is that the description of the Visual Concepts is completely unclear for me. At some point I thought I did understand it, but then the next equation didnt make sense anymore... If I understand correctly, f_p is a representation of *all images* of a specific layer *k* at/around pixel ""p"", (According to last line of page 3). That would make sense, given that then the dimensions of the vector f_p is a scalar (activation value) per image for that image, in layer k, around pixel p. Then f_v is one of the centroids (named VCs). However, this doesnt seem to be the case, given that it is impossible to construct VC activations for specific images from this definition. So, it should be something else, but it does not become clear, what this f_p is. This is crucial in order to follow / judge the rest of the paper. Still I give it a try. <sep> Section 4.1 is the second most important section of the paper, where properties of VCs are discussed. It has a few shortcomings. First, iIt is unclear why coverage should be >=0.8 and firerate ~ 1, according to the motivation firerate should equal to coverage: that is each pixel f_p is assigned to a single VC centroid. Second, ""VCs tent to occur for a specific class"", that seems rather a bold statement from a 6 class, 3 VCs experiment, where the class sensitivity is in the order 40-77%. Also the second experiment, which shows the spatial clustering for the ""car wheel"" VC, is unclear, how is the name ""car wheel"" assigned to the VC? That has have to be named after the EM process, given that EM is unsupervised. Finally the cost effectiveness training (3c), how come that the same ""car wheel"" (as in 3b) is discovered by the EM clustering? Is that coincidence? Or is there some form of supervision involved? <sep> Minor remarks <sep> - Table 1: the reported results of the Matching Network are different from the results in the paper of Vinyals (2016). <sep> - It is unclear what the influence of the smoothing is, and how the smoothing parameter is estimated / set. <sep> - The VCs are introduced for few-shot classification, unclear how this is different from ""previous few-shot methods"" (sect 5). <sep> - 36x36 patches have a plausible size within a 84x84 image, this is rather large, do semantic parts really cover 20% of the image? <sep> - How are the networks trained, with what objective, how validated, which training images? What is the influence of the layer on the performance? <sep> - Influence of the clustering method on VCs, eg k-means, gaussian, von-mises (the last one is proposed)? <sep> On a personal note, I've difficulties with part of the writing. For example, the introduction is written rather ""arrogant"" (not completely the right word, sorry for that), with a sentence, like ""we have only limited insights into why CNNs are effective"" seems overkill for the main research body. The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere). Finally, the introduction paragraph of Section 5 is rather bold, ""resembles the learning process of human beings""? Not so sure that is true, and it is not supported by a reference (or an experiment). <sep> In conclusion: <sep> This paper presents a method for creating features from a (pre-trained) ConvNet. <sep> It clusters features from a specific pooling layer, and then creates a binary assignment between per image extracted feature vectors and the cluster centroids. These are used in a 1-NN classifier and a (smoothed) Naive Bayes classifier. The results show promising results, yet lack exploration of the model, at least to draw conclusions like ""we address the challenge of understanding the internal visual cues of CNNs"". I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc). Moreover, the introduction should be rewritten, and the the background section of VCs (Sect 3) should be clarified. Therefore, I rate the current manuscript as a reject. <sep> After rebuttal: <sep> The writing of the paper greatly improved, still missing insights (see comments below). Therefore I've upgraded my rating, and due to better understanding now, als my confidence.","The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few-shot learning setting for novel classes. <sep> The work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning. This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing. The paper falls short of the acceptance threshold in its current form. <sep> PS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id=BJ_QxP1AZ&noteId=BJaIDpBEM"
"This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. It is fair to say that this paper contains almost no novelty. <sep> This paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. Prior work along this line includes [3, 4, 5, 6, 7]. <sep> Using MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8]. Having a separate subsection (2.1) discussing this seems unnecessary. <sep> Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. It is unclear why discriminative training, such as MMI, sMBR, and lattice-free MMI, is mentioned in section 2.3. Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations. <sep> The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters. In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3). There is no reason to believe that ASG can be faster than CTC in both training and decoding. <sep> The connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. Otherwise, the objective won't be a proper probability distribution. <sep> The citation style in section 2.4 seems off. Also see [4] for a great description of how beam search is done in CTC. <sep> Details about training, such as the optimizer, step size, and batch size, are missing. Does no batching (in section 3.2) means a batch size of one utterance? <sep> In the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set? Something is wrong unless the authors are using different beam widths in the two settings. <sep> The paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet. It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet. Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj. None of the comparison in table 4 is really meaningful, because none of the other systems have parameters as many as 19 layers of convolution. Why does CTC fail when trained without the blanks? Is there a way to fix it besides using ASG? It is also unclear why speaker-adaptive training is not needed. At which layer do the features become speaker invariant? Can the system improve further if speaker-adaptive features are used instead of log mels? This paper would be much stronger if the authors can include these experiments and analyses. <sep> [1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016 <sep> [2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017 <sep> [3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014 <sep> [4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015 <sep> [5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015 <sep> [6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016 <sep> [7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015 <sep> [8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013 <sep> [9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017","Pros <sep> -- Competitive results on LibriSpeech. <sep> Cons <sep> -- Limited novelty, and lacks enough comparisons. <sep> -- Comparison with other end-to-end approaches, and on other commonly used datasets, like WSJ, missing. <sep> -- Gated convnets have already been proposed. <sep> -- Letter based systems have been shown to be competitive to phone based systems. <sep> -- Optimization criterion is quite similar to lattice-free MMI proposed by Povey et al., but with a letter based LM and a slightly different HMM topology. <sep> Given the cons pointed out by reviews, the AC is recommending that the paper be rejected."
"The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10. <sep> Overall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume. <sep> It's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either. <sep> I would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve. <sep> Overall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance.","Pros: <sep> + The idea of end-to-end training that simultaneously learns the weights and appropriate precision for those weights is very appealing. <sep> Cons: <sep> - Experimental results are far from the state-of-the-art, which makes the empirical evaluation unconvincing. <sep> - More justification is needed for the update of the number of bits using the sign of the gradient."
"The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation. <sep> I think it's a great idea to investigate the effects of data augmentation more thoroughly. While it is a technique that is often used in literature, there hasn't really been any work that provides rigorous comparisons with alternative approaches and insights into its inner workings. Unfortunately I feel that this paper falls short of achieving this. <sep> Experiments are conducted on two fairly similar tasks (image classification on CIFAR-10 and CIFAR-100), with two different network architectures. This is a bit meager to be able to draw general conclusions about the properties of data augmentation. Given that this work tries to provide insight into an existing common practice, I think it is fair to expect a much stronger experimental section. In section 2.1.1 it is stated that this was a conscious choice because simplicity would lead to clearer conclusions, but I think the conclusions would be much more valuable if variety was the objective instead of simplicity, and if larger-scale tasks were also considered. <sep> Another concern is that the narrative of the paper pits augmentation against all other regularisation techniques, whereas more typically these will be used in conjunction. It is however very interesting that some of the results show that augmentation alone can sometimes be enough. <sep> I think extending the analysis to larger datasets such as ImageNet, as is suggested at the end of section 3, and probably also to different problems than image classification, is going to be essential to ensure that the conclusions drawn hold weight. <sep> Comments: <sep> - The distinction between ""explicit"" and ""implicit"" regularisation is never clearly enunciated. A bunch of examples are given for both, but I found it tricky to understand the difference from those. Initially I thought it reflected the intention behind the use of a given technique; i.e. weight decay is explicit because clearly regularisation is its primary purpose -- whereas batch normalisation is implicit because its regularisation properties are actually a side effect. However, the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques, so I guess this is not the intended meaning. Please clarify this, as the terms crop up quite often throughout the paper. I suspect that the distinction is somewhat arbitrary and not that meaningful. <sep> - In the abstract, it is already implied that data augmentation is superior to certain other regularisation techniques because it doesn't actually reduce the capacity of the model. But this ignores the fact that some of the model's excess capacity will be used to model out-of-distribution data (w.r.t. the original training distribution) instead. Data augmentation always modifies the distribution of the training data. I don't think it makes sense to imply that this is always preferable over reducing model capacity explicitly. This claim is referred to a few times throughout the work. <sep> - It could be more clearly stated that the reason for the regularising effect of batch normalisation is the noise in the batch estimates for mean and variance. <sep> - Some parts of the introduction could be removed because they are obvious, at least to an *CONF* audience (like ""the model would not be regularised if alpha (the regularisation parameter) equals 0""). <sep> - The experiments with smaller dataset sizes would be more interesting if smaller percentages were used. 50% / 80% / 100% are all on the same order of magnitude and this setting is not very realistic. In practice, when a dataset is ""too small"" to be able to train a network that solves a problem reliably, it will generally be one or more orders of magnitude too small, not 2x too small. <sep> - The choices of hyperparameters for ""light"" and ""heavy"" motivation seem somewhat arbitrary and are not well motivated. Some parameters which are sampled uniformly at random should be probably be sampled log-uniformly instead, because they represent scale factors. It should also be noted that much more extreme augmentation strategies have been used for this particular task in literature, in combination with padding (for example by Graham). It would be interesting to include this setting in the experiments as well. <sep> - On page 7 it is stated that ""when combined with explicit regularization, the results are much worse than without it"", but these results are omitted from the table. This is unfortunate because it is a very interesting observation, that runs counter to the common practice of combining all these regularisation techniques together (e.g. L2 + dropout + data augmentation is a common combination). Delving deeper into this could make the paper a lot stronger. <sep> - It is not entirely true that augmentation parameters depend only on the training data and not the architecture (last paragraph of section 2.4). Clearly more elaborate architectures benefit more from data augmentation, and might need heavier augmentation to perform optimally because they are more prone to overfitting (this is in fact stated earlier on in the paper as well). It is of course true that these hyperparameters tend to be much more robust to architecture changes than those of other regularisation techniques such as dropout and weight decay. This increased robustness is definitely useful and I think this is also adequately demonstrated in the experiments. <sep> - Phrases like ""implicit regularization operates more effectively at capturing reality"" are too vague to be meaningful. <sep> - Note that weight decay has also been found to have side effects related to optimization (e.g. in ""Imagenet classification with deep convolutional neural networks"", Krizhevsky et al.) <sep> REVISION: I applaud the effort the authors have put in to address many of my and the other reviewers' comments. I think they have done so adequately for the most part, so I've decided to raise the rating from 3 to 5, for what it's worth. <sep> The reason I have decided not to raise it beyond that, is that I still feel that for a paper like this, which studies an existing technique in detail, the experimental side needs to be significantly stronger. While ImageNet experiments may be a lot of work, some other (smaller) additional datasets would also have provided more interesting evidence. CIFAR-10 and CIFAR-100 are so similar that they may as well be considered variants of the same dataset, at least in the setting where they are used here. <sep> I do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but I think for real-world relevance, variety in problem settings (i.e. datasets) is simply much more important. I think it would be fine if additional experiments on other datasets were not varied along all these other axes, to cut down on the amount of work this would involve. But not including them at all unfortunately makes the results much less impactful.","The reviewers agree that the authors have made an interesting contribution studying the effect of data augmentation, but they also agree that the claims made by the paper require a broader empirical study beyond the limited number of tasks surveyed in the current revision. I urge the authors to follow this advice and see what they find."
"This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like ""red"" and also compositional operators like ""not"". <sep> I think this model is elegant, beautiful and timely. The authors do a good job of explaining it clearly. I like the modules of composition that seem to make a very intuitive sense for the ""algebra"" that is required and the parsing algorithm is clean. <sep> However, I think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation. <sep> I have 2.5 major issues with the paper and a few minor comments: <sep> Parsing: <sep> * The authors don't really say what is the base case for \\Psi that scores tokens (unless I missed it and if indeed it is missing it really needs to be added) and only provide the recursive case. From that I understand that the only features that they use are whether a certain word makes sense in a certain position of the rule application in the context of the question. While these features are based on Durrett et al.'s neural syntactic parser it seems like a pretty weak signal to learn from. This makes me wonder, how does the parser learn whether one parse is better than the other? Only based on this signal? It makes me suspicious that the distribution of language is not very ambiguous and that as long as you can construct a tree in some context you can do it in almost any other context. This is probably due to the fact that the CLEVR dataset was generated mostly using templates and is not really natural utterances produced by people. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced. <sep> * Related to that is that the decision for a score of a certain type t for a span (i,j) is the sum for all possible rule applications, rather than a max, which again means that there is no competition between different parse trees that result with the same type of a single span. Can the authors say something about what the parser learns? Does it learn to extract from the noise clear parse trees? What is the distribution of rules in those sums? is there some rule that is more preferred than others usually? It seems like there is loss of information in the sum and it is unclear what is the effect of that in the paper. <sep> Evaluation: <sep> * Related to that is indeed the fact that they use CLEVR only. There  is now the Cornell NLVR dataset that is more challenging from a language perspective and it would be great to have an evaluation there as well. Also the authors only compare to 3 baselines where 2 don't even see the entire KB, so the only ""real"" baseline is relation net. The authors indeed state that it is state-of-the-art on clevr. <sep> * It is worth noting that relation net is reported to get 95.5 accuracy while the authors have 89.4. They use a subset so this might be the reason, but I am not sure how they compared to relation net exactly. Did they re-tune parameters once you have the new dataset? This could make a difference in the final accuracy and cause an unfair advantage. <sep> * I would really appreciate more analysis on the trees that one gets. Are sub-trees interpretable? Can one trace the process of composition? This could have been really nice if one could do that. The authors have a figure of a purported tree, but where does this tree come from? From the mode? Form the authors? <sep> Scalability: <sep> * How much of a problem would it be to scale this? Will this work in larger domains? It seems they compute an attention score over every entity and also over a matrix that is squared in the number of entities. So it seems if the number of entities is large that could be very problematic. Once one moves to larger KBs it might become hard to maintain full differentiability which is one of the main selling points of the paper. <sep> Minor comments: <sep> * I think the phrase ""attention"" is a bit confusing - I thought of a distribution over entities at first. <sep> * The feature function is not super clearly written I think - perhaps clarify in text a bit more what it does. <sep> * I did not get how the denotation that is based on a specific rule applycation t_1 + t_2 --> t works. Is it by looking at the grounding that is the result of that rule application? <sep> * Authors say that the neural enquirer and neural symbolic machines produce flat programs - that is not really true, the programs are just a linearized form of a tree, so there is nothing very flat about it in my opinion. <sep> Overall, I really enjoyed reading the paper, but I was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the KB is large.","This paper presents a neural compositional model for visual question answering. The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1: the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base. It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.), and then compare with state of the art on those."
"This paper designs a deep learning architecture that mimics the structure of the well-known MCTS algorithm. From gold standard state-action pairs, it learns each component of this architecture in order to predict similar actions. <sep> I enjoyed reading this paper. The presentation is very clear, the design of the architecture is beautiful, and I was especially impressed with the related work discussion that went back to identify other game search and RL work that attempts to learn parts of the search algorithm. Nice job overall. <sep> The main flaw of the paper is in its experiments. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). That makes for a staggering 15 billion rollouts of prior data that goes into the MCTSNet model. This is compared to 25 rollouts of MCTS that make the decision for the baseline. I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. I'm sure I'm misinterpreting some detail here, but how is this a fair comparison? <sep> Would it be fair to have a baseline that learns the MCTS coefficient on the training data? Or one that uses the value function that was learned with classic search? I find it difficult to understand the details of the experimental setup, and maybe some of these experiments are reported. Please clarify. Also: colors are not distinguishable in grey print. <sep> How would the technique scale with more MCTS iterations? I suspect that the O(N^2) complexity is very prohibitive and will not allow this to scale up? <sep> I'm a bit worried about the idea of learning to trade off exploration and exploitation. In the end you'll just allow for the minimal amount of exploration to solve the games you've already seen. This seems risky, and I suspect that UCB and more statistically principled approaches would be more robust in this regard? <sep> Are these Sokoban puzzles easy for classical AI techniques? I know that many of them can be solved by A* search with a decent heuristic. It would be fair to discuss this. <sep> The last sentence of conclusions is too far reaching; there is really no evidence for that claim.","All reviewers agree that the contribution of this paper, a new way of training neural nets to execute Monte-Carlo Tree Search, is an appealing idea. For the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality. Two of the reviewers point out flaws in implementing in a single domain, 10x10 Sokoban with four boxes and four targets. Since their training methodology uses supervised training on approximate ground-truth trajectories derived from extensive plain MCTS trials, it seems unlikely that the trained DNN will be able to generalize to other geometries (beyond 10x10x4) that were not seen during training. Sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios. <sep> Pros: Good technical quality, interesting novel idea, exposition is mostly clear. Good empirical results in one very limited domain. <sep> Cons: Single 10x10x4 Sokoban domain is too limited to derive any general conclusions. <sep> Point for improvement: The paper compares performance of MCTSnet trials vs. plain MCTS trials based on the number of trials performed. This is not an appropriate comparison, because the NN trials will be much more heavyweight in terms of CPU time, and there is usually a time limit to cut off MCTS trials and execute an action. It will be much better to plot performance of MCTSnet and plain MCTS vs. CPU time used."
"The paper compares some recently proposed method for validation of properties of piece-wise linear neural networks and claims to propose a novel method for the same. Unfortunately, the proposed ""branch and bound method"" does not explain how to implement the ""bound"" part (""compute lower bound"") -- and has been used several times in the same application, incl.: <sep> Ruediger Ehlers. Planet. https://github.com/progirep/planet, <sep> Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis <sep> Alessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351 <sep> Specifically, the authors say: ""In our experiments, we use the result of minimising the variable corresponding to the output of the network, subject to the constraints of the linear approximation introduced by Ehlers (2017a)"" <sep> which sounds a bit like using linear programming relaxations, which is what the approaches using branch and bound cited above use. If that is the case, <sep> the paper does not have any original contribution. If that is not the case, <sep> the authors may have some contribution to make, but have not made it in this paper, as it does not explain the lower bound computation other than the one based on LPs. <sep> Generally, I find a jarring mis-fit between the motivation (deep learning for driving, presumably involving millions or billions of parameters) and the actual reach of the methods proposed (hundreds of parameters). <sep> This reach is NOT inherent in integer programming, per se. Modern solvers routinely solve instances with tens of millions of non-zeros in the constraint matrix, but require a strong relaxation. The authors may hence consider improving the LP relaxation, noting that the big-M constraint are notorious for producing weak relaxations.","All three reviewers are in agreement that this paper is not ready for *CONF* in its current state. Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form."
"SUMMARY. <sep> The paper presents an extension of word2vec for structured features. <sep> The authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features. <sep> The learned representation of features is tested on a recommendation-like task. <sep> ---------- <sep> OVERALL JUDGMENT <sep> The paper is not clear and thus I am not sure what I can learn from it. <sep> From what is written on the paper I have trouble to understand the definition of the model the authors propose and also an actual NLP task where the representation induced by the model can be useful. <sep> For this reason, I would suggest the authors make clear with a more formal notation, and the use of examples, what the model is supposed to achieve. <sep> ---------- <sep> DETAILED COMMENTS <sep> When the authors refer to word2vec is not clear if they are referring to skipgram or cbow algorithm, please make it clear. <sep> Bottom of page one: ""a positive example is 'semantic'"", please, use another expression to describe observable examples, 'semantic' does not make sense in this context. <sep> Levi and Goldberg (2014)  do not say anything about factorization machines, could the authors clarify this point? <sep> Equation (4), what do i and j stand for? what does \\beta represent? is it the embedding vector? How is this formula related to skipgram or cbow? <sep> The introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model. <sep> The experimental section is rather poor, first, the authors only compare themselves with word2ve (cbow), it is not clear what the reader should learn from the results the authors got. <sep> Finally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic.","The paper presents an approach for learning continuous-valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings. The revised paper is a merger of the original submission and another *CONF* submission. This meta-review takes into account all of the comments on both submissions and revisions. <sep> The merged paper is an improvement over the two separate ones. However, the contribution over previous work is still a bit unclear. It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups. <sep> The experiments are also quite limited. The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non-standard."
"This paper deals with early stopping but the contributions are limited. This work would fit better a workshop as a preliminary result, furthermore it is too short. Following a short review section per section. <sep> Intro: The name SFC is misleading as the method consists in stopping early the training with an optimized learning schedule scheme. Furthermore, the work is not compared to the appropriate baselines. <sep> Proposal: The first motivation is not clear. The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks. Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking. <sep> The second motivation, w.r.t. IB seems interesting but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done. <sep> The section 3 is quite long and could be compressed to improve the relevance of this experimental section. All the accuracies(unsup dict, unsup, etc) on CIFAR10/CIFAR100 are reported from the paper (Oyallon & Mallat, 2015), ignoring 2-3 years of research that leads to new numerical results. Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example. <sep> Finally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here. <sep> I believe re-thinking new learning rate schedules is interesting, however I recommend the rejection of this paper.","The paper addresses the training time of CNNs, in the common setting where a CNN is trained on one domain and then used to extract features for another domain. The paper proposes to speed up the CNN training step via a particular proposed training schedule with a reduced number of epochs. Training time of the pre-trained CNN is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study. However, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance. The experimental validation is also thin, and the writing needs improvement."
"The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data. <sep> According to the paper, the experiments are done using a data set which is obtained from measurements of an industrial production process. Figure 2 indicates that reconstructing fewer dimensions of this dataset leads to lower MSE scores. I don't see how this is showing anything besides the obvious fact that reconstructing fewer dimensions is an easier task than reconstructing all of them.  The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. Since the paper doesn't describe much about the privately owned data at all, there is no possibility to replicate the work. The paper doesn't frame the work in prior research at all and the six papers it cites are only referred to in the context of describing the architecture. <sep> I found it very hard to distil what the main contribution of this work was according to the paper. There were also not many details about the precise architecture used. It is implied that GRU networks and were used but the text doesn't actually state this explicitly. By saying so little about the data that was used, it was also not clear what the temporal correlations of the context vectors are supposed to tell us. <sep> The paper describes how existing methods are applied to a specific data set. The benefit of only reconstructing a subset of the input dimensions seems very data specific to me and I find it hard to consider this a novel idea by itself. Presenting sequential data in a windowed format is a standard procedure and not a new idea either. All in all I don't think that the paper presents any new ideas or interesting results. <sep> Pros: <sep> * The visualizations look nice. <sep> Cons: <sep> * It is not clear what the main contribution is. <sep> * Very little information about the data. <sep> * No clear experiments from which conclusions can be drawn. <sep> * No new ideas. <sep> * Not well rooted in prior work.","This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis. The application is very narrow and the data set is proprietary. The approach is not clearly described, but seems very straightforward and is not placed in context of prior work. It is therefore not clear how to evaluate the contribution of the method. The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the *CONF* community."
"This paper presents an interesting approach to identify substructural features of molecular graphs contributing to the target task (e.g. predicting toxicity). The algorithm first builds two conv nets for molecular graphs, one is for searching relevant substructures (policy improvement), and another for evaluating the contribution of selected substructures to the target task (policy evaluation). These two phases are iterated in a reinforcement learning manner as policy iterations. Both parts are based on conv nets for molecular graphs, and this framework is a kind of 'self-supervised' scheme compared to the standard situations that the environment provides rewards. The experimental validations demonstrate that this model can learn a competitive-performed conv nets only dependent on the highlighted substructures, as well as reporting some case study on the inhibition assay for hERG proteins. <sep> Technically speaking, the proposed self-supervised scheme with two conv nets is very interesting. This demonstrates how we can perform progressive substructure selections over molecular graphs to highlight relevant substructures as well as maximizing the prediction performance. Given that conv nets for molecular graphs are not trivially interpretable, this would provides a useful approach to use conv nets for more explicit interpretations of how the task can be performed by neural nets. <sep> However, at the same time, I had one big question about the purpose and usage of this approach. As the paper states in Introduction, the target problem is 'hard selection' of substructures, rather than 'soft selection' that neural nets (with attention, for example) or neural-net fingerprints usually provide. Then, the problem would become a combinatorial search problem, which has been long studied in the data mining and machine learning community. There would exist many exact methods such as LEAP, CORK, and graphSig under the name of 'contrast/emerging/discriminative' pattern mining exactly developed for this task. Also, it is widely known that we can even perform a wrapper approach for supervised learning from graphs simultaneously with searching all relevant subgraphs as seen in Kudo+ NIPS 2004, Tsuda ICML 2007, Saigo+ Machine Learning 2009, etc. It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods. <sep> In addition to the above point, several technical points below would also be unclear. <sep> - A simple heuristic by adding 'selected or not' variables to the atom features works as intended? Because this is fed to the conv net, it seems we can ignore this elements of features by tweaking the weight parameters accordingly. If the conv net performs the best when we use the entire structure, then learning might be forced to ignore the selection. Can we guarantee in some sense this would not happen? <sep> - Zeroing out the atom features also sounds quite simple and a bit groundless. Confusingly, the P network also has an attention mechanism, and it is a bit unclear to me what was actually worked. <sep> - In the experiments, the baseline is based on LR, but this would not be fair because usually we cannot expect any linear relationship for molecular fingerprints. It's highly correlated due to the inclusion relationships between subgraphs. At least, any nonlinear baseline (e.g. Random forest or something?) should be presented for discussing the results. <sep> Pros: <sep> - interesting self-supervised framework provided for highlighting relevant substructures for a given prediction task <sep> - the hard selection setting is encoded in input graph featurization <sep> Cons: <sep> - it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks). At least one of the typical ones should be compared or discussed. <sep> - I'm still not quite sure whether or not some heuristic parts work as intended.","Pro: <sep> - Interesting approach to tie together reinforcement Q-learning with CNN for prediction and reward function learning in predicting downstream effects of chemical structures, while providing relevant areas for decision-making. <sep> Con: <sep> - Datasets are small, generalizability not clear. <sep> - Performance is not high (although performance wasn't the goal necessarily) <sep> - Sometimes test performance is higher than training performance, making results questionable. <sep> - Should include comparison to other wrapper-based combinatorial approaches. <sep> - Too targeted an appeal/audience (better for chemical journal)"
"This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. <sep> Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables. <sep> The idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. <sep> Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. <sep> The proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …). <sep> Overall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)? <sep> Other remarks. <sep> - In section 2.2 and 4 there is some confusion between iteration indices and samples indices ""i"". <sep> - Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables. <sep> -  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. <sep> - The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. <sep> - The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting. <sep> - The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing?","Authors present a method for modeling neurodegenerative diseases using a multitask learning framework that considers ""censored regression"" problems (to model where the outputs have discrete values and ranges). Given the pros/cons, the committee feels this paper is not ready for acceptance in its current state. <sep> Pro: <sep> - This approach to modeling discrete regression problems is interesting and may hold potential, but the evaluation is not in a state where strong meaningful conclusions can be made. <sep> Con: <sep> - Reviewers raise multiple concerns regarding evaluation and comparison standards for tasks. While authors have added some model comparisons in response, in other areas comparisons don't appear complete. For example, when using MRI data, networks compared all use features derived from images, rather than systems that may learn from images themselves. Authors claim dataset is too small to learn directly from pixels in this data (in comments), but transfer learning and data augmentation have been successfully applied to learn from datasets of this size. In addition, new multitask techniques in the imaging domain have also been presented that dynamically learn the network structure, rather than relying on a hand-crafted neural network design. How this approach would compare is not addressed."
"The authors tackle the problem of estimating risk in a survival analysis setting with competing risks. They propose directly optimizing the time-dependent discrimination index using a siamese survival network. Experiments on several real-world dataset reveal modest gains in comparison with the state of the art. <sep> - The authors should clearly highlight what is their main technical contribution. For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier. However, this is unclear from the writing. <sep> - One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks. It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines. In other words, the authors oversell their own work, specially in comparison with the state of the art. <sep> - The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there. The application/setting may be novel, but not the architecture of choice. <sep> - From Eq. 4 to Eq. 5, the authors argue that the denominator does not depend on the model parameters and can be ignored. However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values. This could be problematic if the risks are unbalanced. <sep> - The competitive gain of the authors method in comparison with other competing methods is minor. <sep> - The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t. Is the latter a proxy for the former? How are they related?","Reviewers unanimous in assessment that manuscript has merits, but does not satisfy criteria for publication. <sep> Pros: <sep> - Potentially novel application of neural networks to survival analysis with competing risks, where only one terminal event from one risk category may be observed. <sep> Cons: <sep> - Incomplete coverage of other literature. <sep> - Architecture novelty may not be significant. <sep> - Small performance gains (though statistically significant)"
"The authors review and evaluate several empirical methods to create faster versions of big neural nets for vision without sacrificing accuracy. They show using the ResNet architecture that combining distillation, pruning, and cascades are complementary and can yield pretty nice speedups. <sep> This is a great idea and could be a strong paper, but it's really hard to glean useful recommendations from this for several reasons: <sep> - The writing of the paper makes it hard to understand exactly what's being compared and evaluated. For a paper like this it's really crucial to be precise. When the authors say ""specialization"" or ""specialized model"", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades. The distinction of ""task-aware"" also seems arbitrary to me and obfuscates the contribution of the paper as well. As far as I can tell, the technique is exactly the same, all that's changing is a slight modification. It's not like any of the intuitions or objectives are changing, so adding this new terminology just complicates things. For example, just say ""We distill a parent model to a child model with a subset of the labels."" <sep> - In terms of substance, the experiments don't really add much value in terms of general lessons. For example, the Cat/Dog from ImageNet distillation only works if the target labels are exactly a subset of the original. Obviously if the parent model was overcomplete before, it is certainly overcomplete now. The proposed cascade method is also fairly trivial -- a cheap distilled model backs off to the reference model. Why not train the whole cascade end-to-end? What about multiple levels of cascades? The only useful conclusion I can draw from the experiments is that (1) distillation still works (2) cascades also still work (3) pruning doesn't seem that useful in comparison. Training a cascade also involves a bunch of non-trivial design choices which are largely ignored -- how to set pass through, how to train the model, etc. etc. <sep> - Nit: where are the blue squares in Figure 4? (Distill only) shouldn't those be the fastest methods (aside from pruning)? <sep> An ideal story would for a paper like this would be: here are some complementary ideas that we can combine in non-obvious ways for superlinear benefits, e.g. it turns out that by distilling into a cascade in some end-to-end fashion, you can get much better accuracy vs. speed trade-offs. Instead this paper is a grab-back of tricks. Such a paper can also provide value, but to do that right, the tricks need to be obvious *in retrospect only* and/or the experiments need to show a lot of precise practical lessons. All in all this paper reads like a tech report but not a conference publication.","This paper does not meet the bar for *CONF* - neither in terms of the quality of the write-up, nor in experimental design. The two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all. The rebuttal does not change this assessment."
"This paper describes a setting in which a system learns collections of inverse-mapping functions that transform altered inputs to their unaltered ""canonical"" counterparts, while only needing unassociated and separate sets of examples of each at training time.  Each inverse map is an ""expert"" E akin to a MoE expert, but instead of using a feed-forward gating on the input, an expert is selected (for training or inference) based on the value of a distribution-modeling function c applied to the output of all experts:  The expert with maximum value c(E(x)) is selected.  When c is an adversarially trained discriminator network, the experts learn to model the different transformations that map altered images back to unaltered ones.  This is demonstrated using MNIST with a small set of synthetic translations and noise. <sep> The fact that these different inverse maps arise under these conditions is interesting --- and Figure 5 is quite convincing in showing how each expert generalizes.  However, I think the experimental conditions are very limited:  Only one collection of transformations is studied, and on MNIST digits only.  In particular, I found the fact that only one of ten transformations can be applied at a time (as opposed to a series of multiple transforms) to be restrictive.  This is touched on in the conclusion, but to me it seems fundamental, as any real-world new example will undergo significantly more complex processes with many different variables all applied at once. <sep> Another direction I think would be interesting, is how few examples are needed in the canonical distribution?  For example, in MNIST, could the canonical distribution P be limited to just one example per digit (or just one example per mode / style of digit, e.g. ""2"" with loop, and without loop)?  The different handwriters of the digits, and sampling and scanning process, may themselves constitute in-the-wild transformations that might be inverted to single (or few) canonical examples --- Is this possible with this mechanism? <sep> Overall, it is nice to see the different inverse maps arise naturally in this setting.  But I find the single setting limiting, and think the investigation could be pushed further into less restricted settings, a couple of which I mention above. <sep> Other comments: <sep> - c is first described to be any distribution model, e.g. the autoencoder described on p.5.  But it seems that using such a fixed, predefined c like the autoencoder may lead to collapse:  What is preventing an expert from learning a single constant mode that has high c value?  The adversarially trained c doesn't suffer from this, because presumably the discriminator will be able to learn the difference between a single constant mode output and the distribution P.  But if this is the case, it seems a critical part of the system, not a simple implementation choice as the text seems to say. <sep> - The single-net baseline is good, but I'd like to get a clearer picture of its results.  p.8 says this didn't manage to ""learn more than one inverse mechanism"" --- Does that mean it learns to invert a single mechanism (that is always translates up, for example, when presented an image)?  Or that it learned some mix of transforms that didn't seem to generalize as well?  Or does it have some other behavior?  Also, I'm not entirely clear on how it was trained wrt c --- is argmax(c(E(x)) always just the single expert?  Is c also trained adversarially?  And if so, is the approximate identity initialization used?","PROS: <sep> 1. All the reviewers thought that the work was interesting and showed promise <sep> 2. The paper is relatively well written <sep> CONS: <sep> 1. Limited experimental evaluation (just MNIST) <sep> The reviewers were all really on the fence about this but in the end felt that while the idea was a good one and the authors were responsive in their rebuttal, the experimental evaluation needed more work."
"This paper attempts to extend analytical results pertaining to the loss surface of linear networks to a nonlinear network with a single hidden ReLU layer.  Unfortunately though, at this point I feel that the theoretical results, which constitute the majority of the paper, are of limited novelty and/or significance.  However, I still remain very open to counterarguments to this opinion and the points raised below. <sep> First, I don't believe that Lemma 2.2 is precisely true, at least as currently stated.  In particular, it would appear that L_f could have a differentiable local minima that is only a saddle point in L_gA.  For example, if there is a differentiable valley in L_f that terminates on the boundary of an activation region, then this phenomena could occur, since a local-minima-creating boundary in L_f might just lead to a saddle point in L_gA.  Regardless, the basic premise of this result is quite straightforward anyway. <sep> Turning to Lemma 2.3 and 2.4, I don't understand the relevance of these results.  Where are they needed later or applied?  Additionally, Theorem 2.5 is very related to results already proven for linear networks in earlier work (Kawaguchi, 2016), so there is little novelty here. <sep> There also seem to be issues with Corollary 2.7, which as an aggregation result can be viewed as the main contribution of the paper.  Part (1) of this corollary is obvious.  Part (2) depends on Lemma 2.2, which as stated previously may be problematic.  Most seriously though, Part (3) only considers critical points (i.e., derivative equal to zero), not local minima occurring at non-differentiable locations.  To me this greatly mutes the value of this result, and the contribution of the paper overall, because local minimum are *very* likely to occur on the boundary between activation regions at non-differentiable points (e.g. as in Figure 2).  I therefore don't understand the utility of only considering the differentiable local minima. <sep> Overall though, the main point that within areas of fixed activation the network behaves much like a linear network (with all local minima also global minima when constrained within each region), is not especially noteworthy, because it provides no pathway for comparing minima from different activation regions, which is the main problem to begin with. <sep> Beyond this, the paper makes a few less-technical observations regarding bad local minima.  For example, in Section 3.1 the argument is made that the linear region created when all activations are equal to one, will have a local minimum, and this minimum might be suboptimal.  However, these arguments pertain to the surrogate function L_gA, and if the minima to L_gA occurs on the boundary to another activation region, then this solution might not be a local minima to L_f, the real objective we care about.  Am I missing something here? <sep> As for Section 4.2, the paper needs to do a better job of explaining exactly what is been shown in Table 2.  I can maybe guess, but it is not at all clear what the accuracy percentage is referring to, nor precisely how rich and random minima are computed.  Also, the assumption that P(a = 1) = P(a = 0) = 0.5 is not very realistic, although admittedly this type of simplification is sometimes adopted in the literature. <sep> Minor comment: <sep> * Near the beginning in the introduction, it is claimed that ""the vanishing gradient problem has been solved by using rectified linear units.""  This is not actually true, and portends problematic claims later in the paper.",The reviewers are unanimous in their opinion that the theoretical results in this paper are of limited novelty and significance. Several parts of the paper are not presented clearly enough. As such the paper is not ready for *CONF*-2018 acceptance.
"Summary: <sep> This paper presents a new network architecture for learning a regression of probability distributions. <sep> The distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes. The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins. By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level. <sep> Under these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution. These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution. <sep> The approach is evaluated on three tasks, two synthetic and one real world. The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles. On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance. On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines. However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing. <sep> Notes to authors: <sep> I'm not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying. How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only? Do the multiple input distributions actually help? <sep> You use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs? <sep> Could you also run experiments on the real-world datasets used by the 3BE paper? <sep> What is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)? The width of the network is bounded by the two input distributions, so is this network just incredibly deep? Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints. <sep> It would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix.","The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method."
"Summary <sep> This paper proposes a hybrid model (C+VAE)---a variational autoencoder (VAE) composed with a differentiable decision tree (DDT)---and an accompanying training scheme.  Firstly, the prior is specified as a mixture distribution with one component per class (SVAE).  During training, the ELBO's KL term uses the component that corresponds to the known label.  Secondly, the DDT's leaves are parametrized with the encoder distribution q(z|x), and thus gradient information flows back through the DDT into the posterior approximations in order to make them more discriminative.  Lastly, the VAE and DDT are trained together by alternating optimization of each component (plus a ridge penalty on the decoder means).  Experiments are performed on MNIST, demonstrating tree classification performance, (supervised) neg. log likelihood performance, and latent space interpretability via the DDT. <sep> Evaluation <sep> Pros:  Giving the VAE discriminative capabilities is an interesting line of research, and this paper provides another take on tree-based VAEs, which are challenging to define given the discrete nature of the former and continuous nature of the latter.  Thus, I applaud the authors for combining the two in a way that admits efficient training.  Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit's class.  I can see this being used for dataset augmentation or adversarial example generation, for instance. <sep> Cons:  An indefensible flaw in the work is that the model is evaluated on only MNIST.  As there is no strong theory in the paper, this limited experimental evaluation is reason enough for rejection.  Yet, moreover, the negative log likelihood comparison (Table 2) is not an informative comparison, as it speaks only to the power of adding supervision.  Lastly, I do not think the interpretability provided by the decision tree is as great as the authors seem to claim.  Decision trees provide rich and interpretable structure only when each input feature has clear semantics.  However, in this case, the latent space is being used as input to the tree.  As the decision tree, then, is merely learning hard, class-based partitioning rules for the latent space, I do not see how the tree is representing anything especially revealing.  Taking Figure 2 as an example (which I do like the end result of), I could generate similar results with a black-box classifier by using gradients to perturb the latent '4' mean into a latent '7' mean (a la DeepDream).  I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.  Maybe there is another use case in which a decision tree is superior; I'm just saying Section 4.3 doesn't convince me to the extent that was promised earlier in the paper (and by the title). <sep> Comment:  It's easier to make a latent variable model interpretable when the latent variables are given clear semantics in the model definition, in my opinion.  Otherwise, the semantics of the latent space become too entangled.  Could you, somehow, force the tree to encode an identifiable attribute at each node, which would then force that attribute to be encoded in a certain dimension of latent space?",The paper proposes a new model called differential decision tree which captures the benefits of decision trees and VAEs. They evaluate the method only on the MNIST dataset. The reviewers thus rightly complain that the evaluation is thus insufficient and one also questions its technical novelty.
"Summary: <sep> The paper presents an information theoretic regularizer for deep learning algorithms. The regularizer aims to enforce compression of the learned representation while conditioning upon the class label so preventing the learned code from being constant across classes. The presentation of the Z <sep> latent variable used to simplify the calculation of the entropy H(Y|C) is confusing and needs revision, but otherwise the paper is interesting. <sep> Major Comments: <sep> - The statement that I(X;Y) = I(C;Y) + H(Y|C) relies upon several properties of Y which are not apparent in the text (namely that Y is a function of X, <sep> so I(X;Y) should be maximal, and Y is a smaller code space than X so it should be H(Y)). If Y is a larger code space than X then it should still be true, but the logic is more complicated. <sep> - The latent code for Z is unclear. Given the use of ReLUs it seems like Y <sep> will be O or +ve, and Z will be 0 when Y is 0 and 1 otherwise, so I'm unclear as to when the value H(Y|Z) will be non-zero. The data is then partitioned within a batch based on this Z value, and monte carlo sampling is used to estimate the variance of Y conditioned on Z, but it's really unclear as to how this behaves as a regularizer, how the z is sampled for each monte carlo run, and how this influences the gradient. The discussion in Appendix C <sep> doesn't mention how the Z values are generated. <sep> - The discussion on how this method differs from the information bottleneck is odd, as the bottleneck is usually minimising the encoding mutual information <sep> I(X;Y) minus the decoding mutual information I(Y;C). So directly minimising <sep> H(Y|C) is similar to the IB, and also minimising H(Y|C) will affect I(C;Y) as <sep> I(C;Y) = H(Y) - H(Y|C). <sep> - The fine tuning experiments (Section 4.2) contain no details on the parameters of that tuning (e.g. gradient optimiser, number of epochs, <sep> batch size, learning rates etc). <sep> - Section 4.4 is obvious, and I'd consider it a bug if regularising with label information performed worse than regularising without label information. <sep> Essentially it's still adding supervision after you've removed the classification loss, so it's natural that it would perform better. This experiment could be moved to the appendix without hurting the paper. <sep> - In appendix A an upper bound is given for the reconstruction error in terms of the conditional entropy. This bound should be related to one of the many upper bounds (e.g. Hellman & Raviv) for the Bayes rate of a predictor, as there is a fairly wide literature in this area. <sep> Minor Comments: <sep> - The authors do not state what kind of input variations they are trying to make the model invariant to, and as it applies to CNNs there are multiple different kinds, many of which are not amenable to a regularization based system for inducing invariance. <sep> - The authors should remind the reader once that I(X;Y) = H(Y) - H(Y|X) = H(X) - <sep> H(X|Y), as this fact is used multiple times throughout the paper, and it may not necessarily be known by readers in the deep learning community. <sep> - Computing H(Y|C) does not necessarily require computing c separate entropies, there are multiple different approaches for computing this entropy. <sep> - The exposition in section 3 could be improved by saying that H(X|Y) measures how much the representation compresses the input, with high values meaning large amounts of compression, as much of X is thrown away when generating Y. <sep> - The figures are difficult to read when printed in grayscale, the graphs should be made more readable when printed this way (e.g. different symbols, <sep> dashed lines etc). <sep> - There are several typos (e.g. pg 5 ""staking"" -> ""stacking"").","The proposed conditional variance regularizer looks interesting and the results show some promise. However, as the reviewers pointed out, the connection between the information-theoretic argument provided and the final form of the regularizer is too tenuous in its current form. Since this argument is central to the paper, the authors are urged to either provide a more rigorous derivation or motivate the regularizer more directly and place more emphasis on its empirical evaluation."
"This paper proposes to modify how noise factors are treated when developing VAE models.  For example, the original VAE work from (Kingma and Welling, 2013) applies a deep network to learn a diagonal approximation to the covariance on the decoder side.  Subsequent follow-up papers have often simplified this covariance to sigma^2*I, where sigma^2 is assumed to be known or manually tuned.  In contrast, this submission suggests either treating sigma^2 as a trainable parameter, or else introducing a more flexible zero-mean mixture-of-Gaussians (MoG) model for the decoder noise.  These modeling adaptations are then analyzed using various performance indicators and empirical studies. <sep> The primary issues I have with this work are threefold:  (i) The paper is not suitably organized/condensed for an *CONF* submission, (ii) the presentation quality is quite low, to the extent that clarity and proper understanding are jeopardized, and (iii) the novelty is limited.  Consequently my overall impression is that this work is not yet ready for acceptance to *CONF*. <sep> First, regarding the organization, this submission is 19 pages long (*excluding* references and appendices), despite the clear suggestion in the call for papers to limit the length to 8 pages: ""There is no strict limit on paper length. However, we strongly recommend keeping the paper at 8 pages, plus 1 page for the references and as many pages as needed in an appendix section (all in a single pdf). The appropriateness of using additional pages over the recommended length will be judged by reviewers.""  In the present submission, the first 8+ pages contain minimal new material, just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques.  There is almost no novelty here.  In my mind, this type of well-known content is in no way appropriate justification for such a long paper submission, and it is unreasonable to expect reviewers to wade through it all during a short review cycle. <sep> Secondly, the presentation quality is simply too low for acceptance at a top-tier international conference (e.g., it is full of strange sentences like ""Such amelioration facilitates the VAE capable of always reducing the artificial intervention due to more proper guiding of noise learning.""  While I am sympathetic to the difficulties of technical writing, and realize that at times sufficiently good ideas can transcend local grammatical hiccups, my feeling is that, at least for now, another serious pass of editing is seriously needed.  This is especially true given that it can be challenging to digest so many pages of text if the presentation is not relatively smooth. <sep> Third and finally, I do not feel that there is sufficient novelty to overcome the issues already raised above.  Simply adapting the VAE decoder noise factors via either a trainable noise parameter or an MoG model represents an incremental contribution as similar techniques are exceedingly common.  Of course, the paper also invents some new evaluation metrics and then applies them on benchmark datasets, but this content only appears much later in the paper (well after the soft 8 page limit) and I admittedly did not read it all carefully.  But on a superficial level, I do not believe these contributions are sufficient to salvage the paper (although I remain open to hearing arguments to the contrary).","The reviewers agreed that the paper was too long (more than twice the recommended page limit not counting the appendix) and difficult to follow. They also pointed out that its central idea of learning the noise distribution in a VAE was not novel. While the shortened version uploaded by the authors looks like a step in the right direction, it was not sufficient to convince the reviewers."
"Summary: <sep> This paper presents a derivation which links a DNN to recursive application of maximum entropy model fitting. The mathematical notation is unclear, and in one cases the lemmas are circular (i.e. two lemmas each assume the other is correct for their proof). Additionally the main theorem requires complete independence, but the second theorem provides pairwise independence, and the two are not the same. <sep> Major comments: <sep> - The second condition of the maximum entropy equivalence theorem requires that all T are conditionally independent of Y. This statement is unclear, as it could mean pairwise independence, or it could mean jointly independent <sep> (i.e. for all pairs of non-overlapping subsets A & B of T I(T_A;T_B|Y) = 0). <sep> This is the same as saying the mapping X->T is making each dimension of T <sep> orthogonal, as otherwise it would introduce correlations. The proof of the theorem assumes that pairwise independence induces joint independence and this is not correct. <sep> - Section 4.1 makes an analogy to EM, but gradient descent is not like this process as all the parameters are updated at once, and only optimised by a single (noisy) step. The optimisation with respect to a single layer is conditional on all the other layers remaining fixed, but the gradient information is stale (as it knows about the previous step of the parameters in the layer above). This means that gradient descent does all 1..L steps in parallel, and this is different to the definition given. <sep> - The proofs in Appendix C which are used for the statement I(T_i;T_j) >= <sep> I(T_i;T_j|Y) are incomplete, and in generate this statement is not true, so requires proof. <sep> - Lemma 1 appears to assume Lemma 2, and Lemma 2 appears to assume Lemma 1. <sep> Either these lemmas are circular or the derivations of both of them are unclear. <sep> - In Lemma 3 what is the minimum taken over for the left hand side? Elsewhere the minimum is taken over T, but T does not appear on the left hand side. <sep> Explicit minimums help the reader to follow the logic, and implicit ones should only be used when it is obvious what the minimum is over. <sep> - In Lemma 5, what does ""T is only related to X"" mean? The proof states that <sep> Y -> T -> X forms a Markov chain, but this implies that T is a function of <sep> Y, not X. <sep> Minor comments: <sep> - I assume that the E_{P(X,Y)} notation is the expectation of that probability distribution, but this notation is uncommon, and should be replaced with a more explicit one. <sep> - Markov is usually romanized with a ""k"" not a ""c"". <sep> - The paper is missing numerous prepositions and articles, and contains multiple spelling mistakes & typos.","The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments."
"This paper studies the problem of learning to generate graphs using deep learning methods. The main challenges of generating graphs as opposed to text or images are said to be the following: <sep> (a) Graphs are discrete structures, and incrementally constructing them would lead to non-differentiability (I don't agree with this; see below) <sep> (b) It's not clear how to linearize the construction of graphs due to their symmetries. Based on this motivation, the paper decides to generate a graph in ""one shot"", directly  outputting node and edge existence probabilities, and node attribute vectors. <sep> A graph is represented by a soft adjacency matrix A (entries are probability of existence of an edge), an edge attribute tensor E (entries are probability of each edge being one of d_e discrete types), and a node attribute matrix F, which has a node vector for each  potential node. A cross entropy loss is developed to measure the loss between generated A, E, and F and corresponding targets. <sep> The main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. A downside to the algorithm is that it has complexity O(k^4) for graphs with k nodes, but the authors argue that this is not a problem when generating small graphs. Once the best correspondence is found, it is treated as constant and gradients are propagated appropriately. <sep> Experimentally, generative models of chemical graphs are trained on two datasets. Qualitative results and ELBO values are reported as the dimensionality of the embeddings is varied. No baseline results are presented. A further small set of experiments evaluates the quality of the matching algorithm on a synthetic setup. <sep> Strengths: <sep> - Generating graphs is an interesting problem, and the proposed approach seems like an easy-to-implement, mostly reasonable way of approaching the problem. <sep> - The exposition is clear (although a bit more detail on MPM matching would be appreciated) <sep> However, there are some significant weaknesses. First, the motivation for one-shot graph construction is not very strong: <sep> - I don't understand why the non-differentiability argued in (a) above is an issue. If training uses a maximum likelihood objective, then we should be able to decompose the generation of a graph into a sequence of decisions and maximize the sum of the logprobs of the conditionals. People do this all the time with sequence data and non-differentiability is not an issue. <sep> - I also don't agree that the one shot graph construction sidesteps the issue of how to linearize the construction of a graph. Even after doing so, the authors need to solve a matching problem to resolve the alignment issue. I see this as equivalent to choosing an order in which to linearize the order of nodes and edges in the graph. <sep> Second, the experiments are quite weak. No baselines are presented to back up the claims motivating the formulation. I don't know how to interpret whether the results are good or bad. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). If one is willing to spend O(k^4) computation to solve the alignment problem, then there seem like many possibilities that could be easily applied to the autoregressive formulation. The authors might also be interested in a concurrent *CONF* submission that approaches the problem from an autoregressive angle (https://openreview.net/pdf?id=Hy1d-ebAb). <sep> Finally, I would have expected to see a discussion and comparison to ""Learning Graphical State Transitions"" (Johnson, 2017). Please also don't make statements like ""To the best of our knowledge, we are the first to address graph generation using deep learning."" This is very clearly not true. Even disregarding Johnson (2017), which the authors claim to be unaware of, I would consider approaches that generate SMILES format (like Gomez-Bombarelli et al) to be doing graph generation using deep learning. <sep> Overall, the paper is about an interesting subject, but in my opinion the execution isn't strong enough to warrant publication at this point.","The authors present GraphVAE, a method for fitting a generative deep model, a variational autoencoder, to small graphs. Fitting deep learning models to graphs remains challenging (although there is relevant literature as brought up by the reviewers and anonymous comments) and this paper is a strong start. <sep> In weighing the various reviews, AnonReviewer3 is weighed more highly than AnonReviewer1 and AnonReviewer2 since that review is far more thorough and the reviewer is more expert on this subject. Unfortunately, the review from AnonReviewer1 is extremely short and of very low confidence. As such, this paper sits just below the borderline for acceptance. In general, the main criticisms of the paper are that some claims are too strong (e.g. non-differentiability of discrete structures), treatment of related work (missing references, etc.) and weak experiments and baselines. The consensus among the reviews (even AnonReviewer2) is that the paper is preliminary. The paper is close, however, and addressing these concerns will make the paper much stronger. <sep> Pros: <sep> - Proposes a method to build a generative deep model of graphs <sep> - Addresses a timely and interesting topic in deep learning <sep> - Exposition is clear <sep> Cons: <sep> - Treatment of related literature should be improved <sep> - Experiments and baselines are somewhat weak <sep> - ""Preliminary"" <sep> - Only works on rather small graphs (i.e. O(k^4) for graphs with k nodes)"
"This paper considers a dichitomy between ML and RL based methods for sequence generation. It is argued that the ML approach has some ""discrepancy"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. An alpha-divergence formulation is considered to combine both methods. <sep> Unfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. I therefore have no option but to vote for reject of this paper, based on my educated guess. <sep> Below are the points that I'm particularly confused about: <sep> 1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me. For example, <sep> 1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators. From the context, I guess the authors mean ""empirical training distribution""? <sep> 1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a ""discrepancy"" to me. The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. <sep> In addition, I don't see at all why this discrepancy is a discrepancy between training and testing data. As long as both of them are identically distributed, then no discrepancy exists. <sep> 1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong. First, the model is *not* trained on the true distribution which is unknown. The model is trained on an empirical distribution whose points are sampled from the true distribution. I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution. <sep> 2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a ""reward"" function, but I don't know what it means and the authors should perhaps explain further. I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.","The reviewers agreed that this paper is not quite ready for publication at *CONF*. One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite. One of the main criticisms was issues with the composition. The paper seems to lack a clear formal explanation of the problem and the proposed methodology. The reviewers in general weren't convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn't seem to significantly help in the experiment presented. <sep> Pros: <sep> - The proposed idea is interesting <sep> - The problem is timely and of interest to the community <sep> - Addresses multiple important problems at the intersection of ML and RL in sequence generation <sep> Cons: <sep> - Novel but somewhat incremental <sep> - The experiments are not compelling (i.e. the results are not strong) <sep> - A necessary baseline is missing <sep> - Significant issues with the writing - both in terms of clarity and correctness."
"This paper introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. The authors then introduce a greedy algorithm that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively. <sep> The application of this algorithm is shown to lead to architectures that differ substantially from hand-designed models with the same number of layers: most of the parameters end up in intermediate layers, with fewer parameters in earlier and later layers. This indicates that common heuristics to divide capacity over the layers of a network are suboptimal, as they tend to put most parameters in later layers. It's also nice that simpler tasks yield smaller models (e.g. MNIST vs. CIFAR in figure 3). <sep> The experimental section is comprehensive and the results are convincing. I especially appreciate the detailed analysis of the results (figure 3 is great). Although most experiments were conducted on the classic benchmark datasets of MNIST, CIFAR-10 and CIFAR-100, the paper also includes some promising preliminary results on ImageNet, which nicely demonstrates that the technique scales to more practical problems as well. That said, it would be nice to demonstrate that the algorithm also works for other tasks than image classification. <sep> I also like the alternative perspective compared to pruning approaches, which most research seems to have been focused on in the past. The observation that the cross-correlation of a weight vector with its initial values is a good measure for effective filter use seems obvious in retrospect, but hindsight is 20/20 and the fact is that apparently this hasn't been tried before. It is definitely surprising that a simple method like this ends up working this well. <sep> The fact that all parameters are reinitialised whenever any layer width changes seems odd at first, but I think it is sufficiently justified. It would be nice to see some comparison experiments as well though, as the intuitive thing to do would be to just keep the existing weights as they are. <sep> Other remarks: <sep> Formula (2) seems needlessly complicated because of all the additional indices. Maybe removing some of those would make things easier to parse. It would also help to mention that it is basically just a normalised cross-correlation. This is mentioned two paragraphs down, but should probably be mentioned right before the formula is given instead. <sep> page 6, section 3.1: ""it requires convergent training of a huge architecture with lots of regularization before complexity can be introduced"", I guess this should be ""reduced"" instead of ""introduced"".","Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous."
"In this paper the authors studied the problem of off-policy learning, in the bandit setting when a batch log of data generated by the baseline policy is given. Here they first summarize the surrogate objective functions derived by existing approaches such as importance sampling and variance regularization (Swaminathan et. al). Then they extend the results in Theorem 2 of the paper by Cortes et. al (which also uses the empirical Bernstein inequality by Maurer and Pontil), and derive a new surrogate objective function that involves the chi-square divergence. Furthermore, the authors also show that the lower bound of this objective function can be iteratively approximated by variational f-GAN techniques, which could potentially be more numerically stable and empirically has lower variance. <sep> In general, I think the problem studied in this paper is very interesting, and the topic of counterfactual learning, especially policy optimization with the use of offline and off-policy log data, is important. However, I think the theoretical contribution in this paper on off-policy learning is quite incremental. Also the parts that involve f-GAN is still questionable to me. <sep> Detailed comments: <sep> In these variance regularization formulations (for example the one proposed in this paper, or the one derived in Swaminathan's paper), \\lambda can be seen as a regularization parameter that trades-off bias and variance of the off-policy value estimator R(h) (for example the RHS of equation 6). To exactly calculate \\lambda either requires the size of the policy class (when the policy class is finite), or the complexity constants (which exists in C_1 and C_2 in equation 7, but it is not clearly defined in this paper). Then the main question is on how to choose \\lambda such that the surrogate objective function is reasonable. For example in the safety setting (off-policy policy learning with baseline performance guarantees, for example see the problem setting in the paper by P. Thomas 2015: High Confidence off-policy improvement), one always needs the upper-bound in 6) to hold. This makes the choice of \\lambda crucial and challenging. Unfortunately I don't see much discussions in this paper about choosing \\lambda, even in the context of bias-variance trade-offs. This makes me uncomfortable in believing that the results in experiments hold for other (reasonable) choices of \\lambda. <sep> The contribution of this paper is of two-fold: 1) the authors extend the results from Cortes's paper to derive a new surrogate objective function, and 2) they show how this objective can be approximated by f-GAN techniques. The first contribution is rather incremental as it's just a direct application of Theorem 2 in Cortes's paper. Regarding the second contribution, I am a bit concerned about the derivations of Equation 9, especially the first inequality and the second equality. I see that the first inequality is potentially an application of the conjugate function inequality, but more details are needed (f^* is not even defined). For the second equality, it's unclear to me how one can swap the sup and the E_x operators. More explanations are definitely needed to show their mathematical correctness, especially when this part is a main contribution.  Even if the derivations are right, the f-GAN surrogate objective is a lower bound of the surrogate objective function, while the surrogate function is an upper bound of the true objective function (which is inaccessible). How does one guarantees that the f-GAN surrogate objective is a reasonable one? <sep> Numerical comparisons between the proposed approach, and the approach from Swaminathan's paper are required to demonstrate the superiority of the proposed approach. Are there comparisons in performance between the approach from the original chi-square surrogate function and the one from the f-GAN objective (in order to showcase the need of using f-GAN) as well? <sep> Minor comments: <sep> In experimental section, method POEM is not defined. <sep> The paper is in an okay status. But there are several minor typos, for example \\hat{R}_{(} in page 3, and several typos in Algorithm 1 and Algorithm 2. <sep> In general, I think this paper is studying an interesting topic, but the aforementioned issues make me feel that the paper's current status is still unsuitable for publication.","The reviewers agree that the paper studies and interesting problem with an interesting approach. The reviewers raised some concerns regarding the theoretical and empirical results. The authors have made changes to the paper, but given the theoretical nature of the paper and the extent of changes, another review is needed before publication."
"Paper Summary: <sep> This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions. These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. This paper improves on the upper bound given by [2] and the lower bound given by [1]. They also provide a tight bound for the one dimensional input case. Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST <sep> Main Comments: <sep> The paper is very well written and clearly states and explains the contributions. However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds, with no other novel interpretations or insights into deep architectures. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful. <sep> Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. <sep> [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio <sep> [2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein","Dear authors, <sep> The reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. That said, the improvement over existing work (especially Montufar, 2017) is minor. This, combined with the limited attraction of such work, means that the paper will not be accepted. <sep> I acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re-review a significantly updated version."
"This paper is well constructed and written. It consists of a number of broad ideas regarding density estimation using transformations of autoregressive networks. Specifically, the authors examine models involving linear maps from past states (LAM) and recurrence relationships (RAM). <sep> The critical insight is that the hidden states in the LAM are not coupled allowing considerable flexibility between consecutive conditional distributions. This is at the expense of an increased number of parameters and a lack of information sharing. In contrast, the RAM transfers information between conditional densities via the coupled hidden states allowing for more constrained smooth transitions. <sep> The authors then explored a variety of transformations designed to increase the expressiveness of LAM and RAM. The authors importantly note that one important restriction on the class of transformations is the ability to evaluate the Jacobian of the transformation efficiently. A composite of transformations coupled with the LAM/RAM networks provides a highly expressive model for modelling arbitrary joint densities but retaining interpretable conditional structure. <sep> There is a rich variety of synthetic and real data studies which demonstrate that LAM and RAM consistently rank amongst the top models demonstrating potential utility for this class of models. <sep> Whilst the paper provides no definitive solutions, this is not the point of the work which seeks to provide a description of a general class of potentially useful models.","This paper looks at building new density estimation methods and new methods for tranformations and autoregressive models. The request from reviewers for comparison improves the paper. These models have seen a wide range of applications and have been highly successful, needing the added benefits shown and their potential impact to be expanded further."
"There could be an interesting idea here, but the limitations and applicability of the proposed approach are not clear yet. More analysis should be done to clarify its potential. Besides, the paper seriously needs to be reworked. The text in general, but also the notation, should be improved. <sep> In my opinion, the authors should explain how to apply their algorithm to more general network architectures, and test it, in particular to convnets. An experiment on a modern dataset beyond MNIST would also be a welcome addition. <sep> Some comments: <sep> - The method is present as a fully-connected network training procedure. But the resulting network is not really fully-connected, but modular. This is clear in Fig. 1 and in the explanation in Sect. 3.1. The newly added hidden neurons at every iteration do not project to the previous pool of hidden neurons. It should be stressed that the networks end up with this non-conventional ""tiled"" architecture. Are there studies where the capacity of such networks is investigated, when all the weights are trained concurrently. <sep> - It wasn't clear to me whether the memory reallocation could be easily implemented in hardware. A few references or remarks on this issue would be welcome. <sep> - The work ""Efficient supervised learning in networks with binary synapses"" by Baldassi et al. (PNAS 2007) should be cited. Although usually ignored by the deep learning community, it actually was a pioneering study on the use of low resolution weights during inference while allowing for auxiliary variables during learning. <sep> - Coming back my main point above, I didn't really get the discussion on Sect. 5.3. Why didn't the authors test their algorithm on a convnet? Are there any obstacles in doing so? It seems quite important to understand this point, as the paper appeals to technical applications and convolution seems hard to sidestep currently. <sep> - Fig. 3: xx-axis: define storage efficiency and storage requirement. <sep> - Fig. 4: What's an RSBL? Acronyms should be defined. <sep> - Overall, language and notation should really be refined. I had a hard time reading Algorithm 1, as the notation is not even defined anywhere. And this problem extends throughout the paper. <sep> For example, just looking at Sect. 4.1, ""training and testing data x is normalized…"", if x is not properly defined, it's best to omit it;  ""… 2-dimentonal…"", at least major typos should be scanned and corrected.","This is an interesting paper and addresses an important problem of neural networks with memory constrains. New experiments have been added that add to the paper, but the full impact of the paper is not yet realised, needing further exploration of models of current practice, wider set of experiments and analysis, and additional clarifying discussion."
"This paper addresses multiple issues arising from the fact that commonly reported best model performance numbers are a single sample from a performance distribution. These problems are very real, and they deserve significant attention from the ML community.  However, I feel that the proposed solution may actually compound the issues highlighted. <sep> Firstly, the proposed metric requires calculation of multiple test set experiments for every evaluation. In the paper up to 100 experiments were used. This may be reasonable in scenarios where the test set is hidden, and individual test numbers are never revealed. It also may be reasonable if we cynically assume that researchers are already running many test-set evaluations. But I am very opposed to any suggestion that we should relax the maxim that the test set should be used only once, or as close to once as is possible. Even the idea of researchers knowing their test set variance makes me very uneasy. <sep> Secondly, this paper tries to account for variation in results due to different degrees of hyper-parameter tuning. This is certainly an admirable aim, since different research groups have access to very different types of resources. However, the suggested approach relies on randomly picking hyper-parameters from ""a range that we previously found to work reasonably well"". This randomization does not account for the many experiments that were required to find this range. And the randomization is also not extended to parameters controlling the model architecture (I suspect that a number of experiments went into picking the 32 layers in the ResNet used by this paper). Without a solid and consistent basis for these hyper-parameter perturbations, I worry that this approach will fail to normalize the effect of experiment numbers while also giving researchers an excuse to avoid reporting their experimental process. <sep> I think this is a nice idea and the metric does merge the stability and low variance of mean score with the aspirations of best score. The metric may be very useful at development time in helping researchers build a reasonable expectation of test time performance in cases where the dev and test sets are strongly correlated. However, for the reasons outlined above, I don't think the proposed approach solves the problems that it addresses. Ultimately, the decision about this paper is a subjective one. Are we willing to increase the risk of inadvertent hyper-parameter tuning on the test set for the sake of a more stable metric?","The subject of model evaluation will always be a contentious one, and the reviewers were not yet fully-convinced by the discussion. The points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control."
"This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives. The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction. <sep> The paper is well-written, with clear explanations of the desired properties of the model and a concise set of experiments that are easy to follow. The strongest result is that on unbalanced WordNet, while the Flickr and MovieLens results are a little less clear but do show that this technique does not cause any loss in performance. <sep> A few points of feedback: <sep> - Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings. They also report very high numbers on WordNet, though I'm not sure they are directly comparable. <sep> - The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \\sigma. It's not clear if this is also implicit in the softplus derivation (by analogy with Eq. (4), should we assume that it approximates the \\sigma = 1 case?). What effect does this have on the embedding space? Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data. <sep> - The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare? <sep> - Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? (Why would we expect the smoothed box model to handle unseen captions better?) <sep> - There's a strong emphasis on how smoothing makes training easier. Do you have any metrics to directly support this, such as variance under random restarts? <sep> - In the abstract and introduction, it's easy to gloss over ""inspired by"" and assume that the actual model is a Gaussian convolution. Could be more direct here that it's a softplus approximation.","The manuscript presents a promising new algorithm for learning geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures. The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions. This is shown to be particularly effective for non-overlapping boxes, where the previous method fail. <sep> The primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non-domain expert. This can be improved by including an additional general introduction. Otherwise, the manuscript was well written. <sep> Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. In our opinion, this paper is a clear accept."
"(Score raised from 8 to 9 after rebuttal) <sep> The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such ""winning-tickets"" on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets. <sep> The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing ""feature"" of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version. <sep> 1) The main ""weakness"" of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively ""small"" and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author's decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds ""in general"". The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies. <sep> 2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably ""break"" the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, …? On page 2, second paragraph, the paper states: ""When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch"". I don't fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough ""juice"" and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers). <sep> 3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow ""shipping"" each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer). <sep> 4) Some things that would be interesting to try: <sep> 4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all ""behind"" some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights' ""trajectories"" during training to see if there is a correlation between the ""distance weights traveled"" and whether or not they end up in the winning ticket. <sep> 4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning? <sep> 5) Typo (should be through): ""we find winning tickets though a principled search process"" <sep> 6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well)","The authors posit and investigate a hypothesis -- the ""lottery ticket hypothesis"" -- which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts. Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of ""winning tickets"". <sep> This paper received very favorable reviews, though there were some notable points of concern. The reviewers and the AC appreciated the detailed and careful experimentation and analysis. However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large-scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously. <sep> Overall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work."
"Post rebuttal update/comment: <sep> I thank the authors for the revision and have updated the score (twice!) <sep> One genuinely perplexing result to me is that the method behaves better than random pruning, yet after selecting the salient neurons the weights can be reinitialized, as per the rebuttal: <sep> > # Initialization procedure <sep> - It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. Given (variance scaled) initial weights, SNIP finds the architecturally important parameters in the network, then the pruned network is established and trained in the standard way. <sep> First, there is work which states quite the opposite (e.g. https://arxiv.org/abs/1803.03635). Please relate to it. <sep> Fundamentally, if you decouple weight pruning from initialization it also means that: <sep> - the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization <sep> - the second and higher layers will be pruned somewhat randomly - even if the connections pruned were meaningful with the original weights, after the reinitialization the functions computed by the neurons in lower layers will be different, and have no relation to pruned weights. Thus the pruning will be essentially random (though possibly from a very specific random distribution). In other words - then neurons in a fully connected layer can be freely swapped, each neuron in the next layer behaves on al of them anyway we are thinking here about the uninitialized neurons, with each of them having a distribution over weights and not a particular set of sampled weights, this is valid because we will reinitialize the neurons). Because of that, I wouldn't call any particular weight/connection architecturally important and find it strange that such weights are found. <sep> I find this behavior really perplexing, but I trust that your experiments are correct. however, please, if you have the time, verify it. <sep> Original review: <sep> The paper presents an intriguing result in which a salient, small subset of weights can be selected even in untrained networks given sensible initialization defaults are used. This result is surprising - the usual network pruning procedure assumed that a network is pretrained, and only then important connections are removed. <sep> The contributions of the paper are two-fold: <sep> 1) it reintroduces a multiplicative sensitivity measure similar to the Breiman garotte <sep> 2) and shows which other design choices are needed to make it work on untrained networks, which is surprising. <sep> While the main idea of the paper is clear and easy to intuitively understand, the details are not. My main concern is that paper differentiates between weights and connections (both terms are introduced on page iv to differentiate from earlier work). However, it is not clear what are the authors referring to: <sep> - a conv layer has many repeated applications of the same weight. Am I correct to assume that a conv layer has many more connections, than weights? Furthermore, are the dramatic sparsities demonstrated over connections counted in this manner? This is important - on MNIST each digit has a constant zero border, all connections to the border are not needed and can be trivially removed (one can crop the images to remove them for similar results). Thus we can trivially remove connections, without removing weights. <sep> - in paragraph 5.5 different weight initialization schemes are used for the purpose of saliency estimation, but the paragraph then says ""Note that for training VS-X initialization is used in all the cases."" Does it mean that first a set of random weights is sampled, then the sensitivities are computed, then a salient set of connections is established and the weights are REINITIALIZED from a distribution possibly different than the one used to compute the sensitivity? The fact that it works is very surprising and again suggests that the method identifies constant background pixels rather than important weights. <sep> - on the other hand, if there is a one-to-one correspondence between connections and weights, then the differentiation from Karnin (1990) at the bottom of p. iv is misleading. <sep> I would also be cautious about extrapolating results from MNIST to other vision datasets. MNIST has dark backgrounds. Let f(w,c) = 0*w*c. Trivially, df/dw = df/dc = 0. Thus the proposed sensitivity measure picks non-background pixels, which is also demonstrated in figure 2. However, this is a property of the dataset (which encodes background with 0) and not of the method! This should be further investigated - a quick check is to invert MNIST (make the images black-on-white, not white-on-black) and see if the method still works. Fashion MNIST behaves in a similar way. Thus the only non-trvial experiments are the ones on CIFAR10 (Table 2), but the majority of the analysis is conducted on white-on-black MNIST and Fashion-MNIST. <sep> Finally, no experiment shows the benefit of introducing the variables ""c"", rather than using the gradient with respect to the weights. let f be the function computed by the network. Then: <sep> - df/d(cw) is the gradient passed to the weights if the ""c"" variables were not introduced <sep> - df/dw = df/d(cw) d(cw)/dw = df/d(cw) * c = df/d(cw) <sep> - df/dc = df/d(cw) d(cw)/dc = df/d(cw) * w <sep> Thus the proposed change seems to favor a combination of weight magnitude and the regular df/dw magnitude. I'd like to see how using the regular df/dw criterion would fare in single-shot pruning. In particular, I expect using the plain gradient to lead to similar selections to those in Figure 2, because for constant  pixels 0 = df/d(cw) = df/dc = df/dw. <sep> Suggested corrections: <sep> In related work (sec. 2) it is pointed that Hessian-based methods are unpractical due to the size od the Hessian. In fact OBD uses a diagonal approximation to the hessian, which is computed with complexity similar to the gradient, although it is typically not supported by deep learning toolkits. Please correct. <sep> The description of weight initialization schemes should also be corrected (sec. 4.2). The sentence ""Note that initializing neural networks is a random process, typically done using normal distribution with zero mean and a fixed variance."" is wrong and artificially inflates the paper's contribution.  Variance normalizing schemes had been known since the nineties (see efficient backprop) and are the default in many toolkits, e.g. Pytorch uses the Kaiming rule which sets the standard deviation according to the fan-in: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L56. <sep> Please enumerate the datasets (MNIST, Fashion-MNIST, CIFAR10) in the abstract, rather than saying ""vision datasets"", because MNIST in particular is not representative of vision datasets due to the constant zero padding, as explained before. <sep> Missing references: <sep> - Efficient Backprop http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf discusses variance scaling initialization, and approximations to the hessian. Since both are mentioned in the text this should be cited as well. <sep> - the Breiman non-negative garotte (https://www.jstor.org/stable/1269730) is a similar well-known technique in statistics <sep> Finally, I liked the paper and wanted to give it a higher score, but reduced it because of the occurrence of many broad claims made in the paper, such as: 1) method works on MNIST => abstract claims it generally works on vision datasets 2) paper states ""typically used is fixed variance init"", but the popular toolkits (pytorch, keras) actually use the variance scaling one by default 3) the badly explained distinction between connection and weight and the relation that it implies to prior work. I will revise the score if these claims are corrected.","This method proposes a criterion (SNIP) to prune neural networks before training. The pro is that SNIP can find the architecturally important parameters in the network without full training. The con is that SNIP only evaluated on small datasets (mnist, cifar, tiny-imagenet) and it's uncertain if the same heuristic works on large-scale dataset. Small datasets can always achieve high pruning ratio, so evaluation on ImageNet is quite important for pruning work. The reviewers have consensus on accept. The authors are recommended to compare with previous work [1][2] to make the paper more convincing. <sep> [1] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015. <sep> [2] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016."
"The paper provides a number of novel interesting theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called ""2 stage VAEs"" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. <sep> Main theoretical contributions: <sep> 1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1). <sep> In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered. <sep> 2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5). <sep> In case when r < d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r ""informative"" dimensions to produce the outputs perfectly landing on the true data manifold. <sep> Main algorithmic contributions: <sep> (0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. <sep> Review summary: <sep> I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. <sep> *************** <sep> *** Couple of comments and typos: <sep> *************** <sep> (0) Is the code / checkpoints going to be available anytime soon? <sep> (1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning. <sep> (2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. <sep> (3) It would be nice to specify the dimensionality of the Sz matrix in definition 1. <sep> (4) Line ater Eq. 3: I think it should be ∫pgt(x)log⁡pθ(x)dx ? <sep> (5) Eq 4: p_\\theta(x|x) <sep> (6) Page 4: ""... mass to most all measurable..."". <sep> (7) Eq 34. Is it sqrt(\\gamma_t) or just \\gamma_t? <sep> (8) Line after Eq 40. Why exactly D(u^*) is finite? <sep> I only checked proofs of Theorems 1 and 2 in details and those looked correct. <sep> [1] Lucic et al., 2018. <sep> [2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html <sep> [3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642",The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper. The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs. The paper will be a valuable addition to the *CONF* program.
"# Summary <sep> The article proposes a deep learning-based approach aimed at matching face images to voice recordings belonging to the same person. <sep> To this end, the authors use independently parametrized neural networks to map face images and audio recordings -- represented as spectrograms -- to embeddings of fixed and equal dimensionality. Key to the proposed approach, unlike related prior work, these modules are not directly trained on some particular form of the cross-modal matching task. Instead, the resulting embeddings are fed to a modality-agnostic, multiclass logistic regression classifier that aims to predict simple covariates such as gender, nationality or identity. The whole system is trained jointly to maximise the performance of these classifiers. Given that (face image, voice recording) pairs belonging to the same person must share equal for these covariates, the neural networks embedding face images and audio recordings are thus indirectly encouraged to map face images and voice recordings belonging to the same person to similar embeddings. <sep> The article concludes with an exhaustive set of experiments using the VGGFace and VoxCeleb datasets that demonstrates improvements over prior work on the same set of tasks. <sep> # Originality and significance <sep> The article follows-up on recent work [1, 2], building on their original application, experimental setup and model architecture. The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. While the fact that these covariates are strongly associated to face images and audio recordings had already been discussed in [1, 2], the idea of actually using them to drive the learning process is novel in this particular task. <sep> While the article does not present substantial, general-purpose methodological innovations in machine learning, I believe it constitutes a solid application of existing techniques. Empirically, the proposed covariate-driven architecture is demonstrated to lead to better performance in the (VGGFace, VoxCeleb) dataset in a comprehensive set of experiments. As a result, I believe the article might be of interest to practitioners interested in solving related cross-modal matching tasks. <sep> # Clarity <sep> The descriptions of the approach, related work and the different experiments carried out are written clearly and precisely. Overall, the paper is rather easy to read and is presented using a logical, easy-to-follow structure. <sep> In my opinion, perhaps the only exception to that claim lies in Section 3.4. If possible, I believe the Seen-Heard and Unseen-Unheard scenarios should be introduced in order to make the article self-contained. <sep> # Quality <sep> The experimental section is rather exhaustive. Despite essentially consisting of a single dataset, it builds on [1, 2] and presents a solid study that rigorously accounts for many factors, such as potential confounding due to gender and/or nationality driving prediction performance in the test set. <sep> Multiple variations of the cross-modal matching task are studied. While, in absolute terms, no approach seems to have satisfactory performance yet, the experimental results seem to indicate that the proposed approach outperforms prior work. <sep> Given that the authors claimed to have run 5 repetitions of the experiment, I believe reporting some form of uncertainty estimates around the reported performance values would strengthen the results. <sep> However, I believe that the success of the experimental results, more precisely, of the variants trained to predict the ""covariate"" identity, call into question the very premise of the article. Unlike gender or nationality, I believe that identity is not a ""covariate"" per se. In fact, as argued in Section 3.1, the prediction task for this covariate is not well-defined, as the set of identities in the training, validation and test sets are disjoint. In my opinion, this calls into question the hypothesis that what drives the improved performance is the fact that these models are trained to predict the covariates. Rather, I wonder if the advantages are instead a ""fortunate"" byproduct of the more efficient usage of the data during the training process, thanks to not requiring (face image, audio recording) pairs as input. <sep> # Typos <sep> Section 2.4 <sep> 1) ""... image.mGiven ..."" <sep> 2) Cosine similarity written using absolute value |f| rather than L2-norm ||f||_{2} <sep> 3) ""Here we are give a probe input ..."" <sep> # References <sep> [1] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. ""Learnable PINs: Cross-Modal Embeddings for Person Identity."" arXiv preprint arXiv:1805.00833 (2018). <sep> [2] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. ""Seeing voices and hearing faces: Cross-modal biometric matching."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.",All reviewers agree that the proposed method interesting and well presented. The authors' rebuttal addressed all outstanding raised issues. Two reviewers recommend clear accept and the third recommends borderline accept. I agree with this recommendation and believe that the paper will be of interest to the audience attending *CONF*. I recommend accepting this work for a poster presentation at *CONF*.
"This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. <sep> First, it is discussed how weight decay affects the learning dynamics in networks with batch normalization when trained with SGD. The dominant generalization benefit due to weight decay comes from increasing the effective learning rate of parameters on which batch normalization is applied. The authors therefore hypothesize that a larger learning rate has a regularization effect. <sep> Second, the role of weight decay is discussed when training with second order methods without batch normalization. Under the approximation of not differentiating the curvature matrix used in second order method, it is shown that using weight decay is equivalent to adding to the loss an L2 regularization in the metric space of the curvature matrix considered. It is then shown that if the curvature matrix is the Gauss-Newton matrix, this L2 regularization (and hence the weight decay) is equivalent to the Frobenius norm of the input-output Jacobian when the input has a spherical Gaussian distribution. Similar arguments are made about KFAC with Gauss-Newton norm. The generalization benefit due to weight decay in this case is claimed based on the recent paper by Novak et al 2018 which empirically shows a strong correlation between input-output Jacobian norm and generalization error. <sep> Finally, the role of weight decay is discussed for second order methods when using batch normalization. In this case it is discussed for Gauss-Newton KFAC that the benefit mostly comes from the application of weight decay on the softmax layer and the effect of weight decay on other weights cancel out due to batch normalization. A comparison between Gauss-Newton KFAC and Fischer KFAC is also made. Thus the generalization benefit is presumably attributed to the second order properties of KFAC and a smaller norm of softmax layer weights. <sep> Comments: <sep> The paper is technically correct and proofs look good. <sep> I have mixed comments about this paper. I find the analysis in section 4.2 and 4.3 which discuss about the role of weight decay for second order methods (with and without batch-norm) to be novel and insightful (described above). <sep> But on the other hand, I feel section 4.1 is more of a discussion on existing work rather than novel contribution. Most of what is said, both analytically and experimentally, is a repetition of van Laarhoven 2017, except for a few details. It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017. But instead the authors brush it under the carpet by saying they did not find the gamma and beta parameters to have significant impact on performance, and fixed them during training.  I also find the claim of section 4.1 to be a bit mis-leading because it is claimed that weight decay applied with SGD and batch normalization only has benefits due to batch-norm dynamics, and not due to complexity control even though in Fig 2 and 4, there is a noticeable difference between training without weight decay, and training with weight decay only on last layer. Furthermore, when hypothesizing the regularization effect of large learning rate in section 4.1, a large body of literature that has studied this effect has not been cited. Examples are [1], [2], [3]. <sep> I have other concerns which mainly stem from lack of clarity in writing: <sep> 1. In the line right above remark 1, it is not clear what ""assumption"" refer to. I am guessing the distribution of the input being spherical Gaussian? <sep> 2. In remark 1, regarding the claim about the equivalence of L2 norm of theta under Gauss-Newton metric and the Frobenius norm of input-output Jacobian, why does f_theta need to be a linear function without any non-linearity? I think the linearity part is only needed for the KFAC result. <sep> 3. In remark 1, what does it mean by ""Furthermore, if G is approximated by KFAC""? For linear f_theta, given lemma 1 and theorem 1, the claimed equivalence always holds true, no? <sep> 4. In the 1st line of last paragraph of page 6, what are the general conditions under which the connection between Gauss-Newton norm and Jacobian norm does not hold true? <sep> 5. In figure 5, how are the different points in the plots achieved? By varying hyper-parameters? <sep> A minor suggestion: in theorem 1 (and lemma 1), instead of assuming network has no bias, it can be said that the L2 regularization term does not have bias terms. This is more reasonable because bias terms have no effect on complexity and so it is reasonable to not apply weight decay on bias. <sep> Overall I think the paper is good *if* section 4.1 is sorted out and writing (especially in section 4.2) is improved. For these reasons, I am currently giving a score of 6, but I will increase it if my concerns are addressed. <sep> [1] a bayesian perspective on generalization and stochastic gradient descent <sep> [2] Train longer, generalize better: closing the generalization gap in large batch training of neural networks <sep> [3] Three Factors Influencing Minima in SGD",Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.
"The paper presents a learning-based method for learning the latent context codes from demonstrations along with a GAIL model. <sep> This amounts to learning the option segments and the policies simultaneously. <sep> The main contribution is the model the problem as a time-dependent context and then use a directed information flow loss instead of the mutual information loss. <sep> 1. What is the effect of models of the underlying distribution of latent codes. <sep> Can it be categorical only, or can it be continuous? <sep> Could we also model it as multidimensional? <sep> The current results only provide single dimensional categorial distribution as latent codes. <sep> 2. The paper missed an important line of work which solves nearly the same problem -- option discovery and policy learning. <sep> Krishnan -- Discovery of Deep Option(1703.08294). This work was used by authors in continuous options and then again for program generation (https://openreview.net/pdf?id=rJl63fZRb). <sep> They explicitly infer the option parameters, along with termination conditions with the Expectation Propagation method. <sep> The results are in very similar domains hence comments, if not a comparison, would be useful. <sep> 3. The authors state that the main problem with an InfoGail style method is dependence on the full trajectory as in eq 1. Hence the directed info flow is required to solve the problem. However in the actual model, the authors make a sequence of variational approximations -- (a) reduction of eq2 to eq1 with a variation lower bound on posterior p(c|c,\\tau) and then replace the prior p(c) with q(c|c,\\tau) in eq 5. But looking at the model diagram in fig 2. the VAE actually makes the Markovian assumption -- i.e. c only depends on c_{t-1} and s_{t}. If that is true then how would this be very different from InfoGAIL mutual information loss. <sep> It appears that to capture the authors' mathematical intuition the VAE should have a recurrent generator which should have a hidden state factor passing in to capture dependence on history until the current time. <sep> 3a. In fact the first term in eq 6 looks closer to the actually used model. If that is not true then the authors should clarify. <sep> 4. Experiments do capture the notion discovery of options. But the simplicity of data leaves much to be desired. <sep> One of the main difference of this work in comparison to unsupervised segmentation models GMM or BP-AR-HMM is the fact that the options learned are composable. But the authors only show this composability on the circle domain -- which is arguably a toy-domain. <sep> A reasonable confirmation that the model indeed learns composition is to generate a trajectory for a sequence of latent code not seen in data. -- like walking -- normal -- left-right-left can be converted to limping gait -- left-left-right-right. This is only a suggestive example. <sep> 5. In appendix eq 8 how is the reduction from line 3 to line 4 of the equation made -- what is the implicit assumption. <sep> joint distribution p(c, \\tau) is written out as p (\\tau|c) p(c) without an integral.","This paper proposes an approach for imitation learning from unsegmented demonstrations. The paper addresses an important problem and is well-motivated. Many of the concerns about the experiments have been addressed with follow-up comments. We strongly encourage the authors to integrate the new results and additional literature to the final version. With these changes, the reviewers agree that the paper exceeds the bar for acceptance. Thus, I recommend acceptance."
"This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian, which would facilitate the computation and learning of spectral features in graph convnets. It further proposes an extension with back propagation through the Lanczos algorithm, in order to train end to end models. <sep> Overall, the idea of using Lanczos algorithm to bypass the computation of the eigendecomposition, and thus simplify filtering operations in graph signal processing is not new [e.g., 35]. However, using this algorithm in the framework of graph convents is new, and certainly interesting. The authors seem to claim that their method permits to learn spectral filters, what other methods could not do - this is not completely true and should probably be rephrased more clearly: many graph convnets, actually learn features. <sep> The general construction and presentation of the algorithms are generally clear, and pretty complete. A few things that could be clarified are the following: <sep> - in the spectral filters of Eq (4), what gets fundamentally different from polynomial filters proposed in other graph convnets architectures? <sep> - what happens when the graph change? Do the learned features make sense on different graphs? And if yes, why? If not, the authors should be more explicit in their presentation <sep> - what is the complexity of the proposed methods? that should be minimally discussed (at least), as it is part of the key motivations for the proposed algorithms <sep> - how is the learning done in 3.2? If there is any learning at all? (btw, S below Eq (6) is a poor notation choice, as S is used earlier for something else) <sep> - the results are not very impressive - they are good, but not stellar, and could benefit from showing an explicit tradeoff in terms of complexity too? <sep> The discussion in the related work, and the analogy with manifold learning are interesting. However, that brings probably to one of the main issues with the papers - the authors are obviously very knowledgeable in graph convnets, graph signal processing, and optimisation. However, there are really too many things in this paper, which leads to numerous shortcuts, and some time confusion. Given the page limits, not everything can be treated with the level of details that it would deserve. It might be good to consider trimming down the paper to its main and core aspects for the next version.","The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph-structured inputs. They commented particularly on the quality of the research idea, and its depth of development. The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results. <sep> A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus. Overall, the paper is a clear advance, and I recommend it for acceptance."
"This paper proposes an approach for automatic robot design based on Neural graph evolution. <sep> The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice. <sep> My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below). <sep> The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature. <sep> What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches? <sep> I would like to see additional experiments to answer this questions. <sep> In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair. <sep> You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES. <sep> If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison. <sep> Detailed comments: <sep> - in the abstract you say that ""NGE is the first algorithm that can automatically discover complex robotic graph structures"". This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover? <sep> - in the introduction you mention that automatic robot design had limited success. This is rather subject, and I would tend to disagree.  Moreover, the same limitations that apply to other algorithms to make them successful, in my opinion, apply to your proposed algorithm (e.g., difficulty to move from simulated to real-world). <sep> - The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction. What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design. <sep> - The stated contributions number 3 and 5 are not truly contributions. #3 is so generic that a large part of the previous literature on the topic fall under this category -- not new. #5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach. <sep> - Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider? <sep> - Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections? <sep> - First line page 4 you mention AF, without introducing the acronym ever before. <sep> - Sec 3.1: the statements about MB and MF algorithms are inaccurate. Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114) <sep> - ""to speed up and trade off between evaluating fitness and evolving new species"" Unclear sentence. speed up what? why is this a trade-off? <sep> - Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying. <sep> - Sec 4.1:  would argue that computational cost is rarely a concern among evolutionary algorithms. The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations. <sep> - Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?","Lean in favor <sep> Strengths: The paper tackles the difficult problem of automatic robot design. The approach uses graph neural <sep> networks to parameterize the control policies, which allows for weight sharing / transfer to new policies even <sep> as the topology changes. Understanding how to efficiently explore through non-differentiable changes to the body <sep> is an important problem (AC). The authors will release the code and environments, which will be useful in an area where there are <sep> currently no good baselines (AC). <sep> Weaknesses: There are concerns (particularly R2, R1) over the lack of a strong baseline, and with the results <sep> being demonstrated on a limited number of environments (R1) (fish, 2D walker). In response, the authors clarified the nomenclature and <sep> description of a number of the baselines, and added others. AC: there is no submitted video (searches for ""video"" on the PDF text <sep> produces no hits); this is seen by the AC as being a real limitation from the perspective of evaluation. <sep> AC agrees with some of the reviewer remarks that some of the original stated claims are too strong. <sep> AC: the simplified fluid model of Mujoco (http://mujoco.org/book/computation.html#gePassive) is <sep> unable to model the fluid state, in particular the induced fluid vortices that are responsible for a <sep> good portion of fish locomotion, i.e., ""Passive and active flow control by swimming fishes and <sep> mammals"" and other papers. Acknowledging this kind of limitation will make the paper stronger, not weaker; <sep> the ML community can learn from much existing work at the interface of biology and fluid mechancis. <sep> There remain points of contention, i.e., the sufficiency of the baselines. However, the reviewers R2 and R3 have <sep> not responded to the detailed replies from the authors, including additional baselines (totaling 5 at present) <sep> and pointing out that baselines such as CMA-ES (R2) in a continuous space and therefore do not translate in any obvious way <sep> to the given problem at hand. <sep> On balance, with the additional baselines and related clarifications, the AC feels that this paper makes a <sep> useful and valid contribution to the field, and will help establish a benchmark in an important area. <sep> The authors are strongly encouraged to further state caveats and limitations, and to emphasize why some <sep> candidate baseline methods are not readily applicable."
"There has been a lot of work on limited precision training and inference for deep learning hardware, but in most of this work, the accumulators for the multiply-and-add (FMA) operations that occur for inner products are chosen conservatively or treated as having unlimited precision. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. They propose an information theoretic approach to argue that by using fewer bits of mantissa in the accumulator than necessary, the variance of the resulting sum is less than what it would have been if sufficient bits of mantissa were used. This is surprising to me, as quantization is usually modeled as _adding_ noise, leading to an _increase_ in variance (Mc Kinstry et al. 2018), so this is a nice counterexample to that intuition. Unfortunately the result is presented in a way that implies the variance reduction is what causes the degradation in performance, while obviously (?) it's just a symptom of a deeper problem. E.g., adding noise or multiplying by a constant to get the variance to where it should be, will not help the network converge. The variance is just a proxy for lost information. The authors should make this more clear. <sep> Loss of variance is regarded as a proxy to the error induced/loss of information due to reduced mantissa prevision. The authors present their metric called Variance Retention Ratio (VRR) as a function of the mantissa length of product terms, partial sum (accumulator) terms, and the length of the accumulation. Thereafter, the mantissa precision of the accumulator is predicted to maintain the error of accumulation within bounds by keeping the VRR as close to 1 as possible. The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. For tightness analysis they present convergence results while perturbing the mantissa bits to less than those predicted by their formula, and show that it leads to more than 0.5% loss in the final test error of the network. <sep> Some questions that the manuscript leaves open in it's current form: <sep> 0. Does this analysis only apply to ReLu networks where all the accumulated terms are positive? Would a tanh nonlinearity, e.g. in an RNN, result in a different kind of swamping behavior? I don't expect the authors to add a full analysis for the RNN case if it's indeed different, but it would be nice to comment on it. <sep> 1. Do the authors assume that the gradients and deltas will always be within the exponent range of representation? I do not find a mention of this in the paper. In other words, are techniques like loss scaling, etc. needed in addition? Other studies in literature analyzing IEEE fp16 seem to suggest so. <sep> 2. The authors do not provide details on how they actually performed the experiments when running convergence experiments. It is not straightforward to change the bit width of the accumulator mantissa in CPU or GPU kernel libraries such as CUDNN or Intel MKL. So how do they model this? <sep> 3. On page 7, the authors point out that they provide a theoretical justification of why the chunk size should neither be too small or too large - but I do not see such a justification in the paper. More detailed explanation is needed. <sep> There are a few minor typos at a few places, e.g. <sep> 1. Page 4: ""… , there is a an accumulation length…."" <sep> 2. Page 6: ""…floaintg-point format…"" <sep> Some figures, notably 2 and 5, use text that is unreadably small in the captions. I know this is becoming somewhat common practice in conference submissions with strict pages limits, but I implore the authors to consider shaving off space somewhere else. Some of us still read on paper, or don't have the best eyes!","The authors present a theoretical and practical study on low-precision training of neural networks. They introduce the notion of variance retention ratio (VRR) that determines the accumulation bit-width for <sep> precise tailoring of computation hardware. Empirically, the authors show that their theoretical result extends to practical implementation in three standard benchmarks. <sep> A criticism of the paper has been certain hyperparameters that a reviewer found to be chosen rather arbitrarily, but I think the reviewers do a reasonable job in rebutting it. <sep> Overall, there is consensus that the paper presents an interesting framework and does both practical and empirical analysis, and it should be accepted."
"General: <sep> In general, this is a well-written paper. This work focuses on the robustness of conditional GAN(RoC-GAN) when facing the noise. The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. The main contribution of the paper is to introduce a two-pathway model, where one of them is used to perform regression as ordinary GAN while the other one helps the whole model span the target domain. <sep> Strength: <sep> 1. The idea is simple and straightforward. The authors provide necessary theoretical analysis and empirical validation for their model. <sep> 2. The proposed method seems technically correct to me. i.e. Although I am not very sure how well it works in practice, the idea is fine. <sep> Possible Improvements: <sep> 1. I agree adding another auto-encoder as a helper may give better generation results by spanning the whole target space, but I don't think this constraint is strong enough in practice. <sep> 2. In section 3.3, the time complexity of computing 'L_deconv' seems extremely large. From the perspective of numerical optimization, optimizing such a matrix will cause trouble if the dimension of weight matrices are large. i.e. optimizing the high-dimensional covariance matrix seems a problem to me. <sep> 3. The experiments looks good. The experiments could be more convincing if using more complex data sets(e.g. CIFAR10, ImageNet) besides CelebA. My concern for using such data sets(the resolution of images is low and the distribution is simple)  is that: although the noise seems to corrupt most of the image, the distribution of the image is not complex, so the generative model can recover it easily. Since this is a more empirical paper, the experiments should be more convincing. <sep> Conclusion: <sep> The author(s) are thoughtful and they put lots of work on this paper. The proposed method is simple. For novelty and significance, I think the idea is not very fancy to me. I am not very convinced by the method proposed in the paper. Although the paper demonstrates the robustness of their model with different experiments, most of them were not performed on deep neural networks and complicated data sets. As a conclusion, I vote for weak rejection. <sep> Minor suggestion: <sep> Increase the resolution of the figures. <sep> ------------------------------- After Rebuttal --------------------------------- <sep> I am very satisfied with the authors' response, so I will change my vote from rejection to acceptance.","The proposed method suggests a way to do robust conditional image generation with GANs. The premise is to make the image to image translation model resilient to noise by leveraging structure in the output space, with an unsupervised ""pathway"". <sep> In general, the qualitative results seem reasonable on a a number of datasets, including those suggested by reviewers. The method appears simple, novel and easy to try. The main concerns seem to be that the idea is maybe too simple, but I'm not particularly bothered by that. The authors showed it working well on a variety of tasks (synthetic and natural), provide SSIM numbers that look compelling (despite SSIM's short-comings) and otherwise give compelling arguments for the technical soundness of the approach. <sep> Thus, I recommend acceptance."
"Let me start by apologizing for the delayed review - in fact I was asked today to replace an earlier assigned reviewer. Hopefully the clarifications I request won't be too time consuming to meet the deadline coming up. <sep> ### <sep> First of all, the problem which the authors are attempting to answer is quite important: the effect of over-parametrization is not well understood on a theoretical level. As the paper illustrate, 2-layer networks are already capable of generalizing while being over-parameterized, therefore justifying their setting. <sep> Next this paper motivates the study of complexity quantities that tend to decrease with the number of parameters, in particular figure 3 motivates the conjecture that the complexity measure in Theorem 2 can control generalization error. The paper also does a great job comparing related work, motivating their results. <sep> ### <sep> At this point, I would like to request a couple of clarifications in the proofs. Perhaps it's due to the fact that I only spent a day reading, but at least I think we could improve on its readability. Regardless, I currently do not yet trust a couple of the proofs, and I believe the acceptance of this paper should be conditioned on confirming the correctness of these proofs. <sep> (1) Let's start with Lemma 10. In the middle equation block, we obtain a bound <sep> \\| alpha^prime \\|_p^p <= beta^p ( 1 + D/K ) <sep> and the proof concludes alpha^prime is in Q. However this cannot be the case for all alpha^prime. <sep> Consider x=0 which is in S_{p, beta}^D, then we have alpha^prime = 0 as well. In the definition of Q, we require all the j's to sum up to K+D, which is not met here. <sep> At the same time, the next claim <sep> \\| alpha \\|_2 <= D^{1/2 - 1/p} \\| alpha^prime \\|_p does not seem to follow from the above calculations. In particular, alpha^prime seems to be defined with respect to an x in S_{p, beta}, however in this case we did not specify such an x. Perhaps did you mean there exist such an alpha^prime? <sep> (2) In the proof of Theorem 3, there is an important inequality needed to complete the proof max{ <s, f_i> , <s, -f_i> } >= 1/2 * ( <s, [f_i]_+> + <s, [-f_i]_+> ) <sep> Perhaps I am missing something obvious, but I believe this inequality fails when we choose s as a constant vector, and f_i to have the same number of positive and negative signs (which is possible in a Hadamard matrix). In this case, the left hand side should be equal to zero, where as the right hand side will be positive. <sep> ### <sep> To summarize, if these proofs can be confirmed, I believe this paper would have made significant contribution to the problem of over-parametrization in deep learning, and of course should be accepted. <sep> ### <sep> I corrected several typos and found minor issues as I read, perhaps this will be useful to improve readability as well. <sep> Page 13, proof of Lemma 8 <sep> - after the V_0 term is separated, there is a sup over \\|V_0\\|_F <= r in the expectation, which should be \\|V-V_0\\|_F <= r instead. <sep> Page 14, Lemma 9 <sep> - the lemma did not define rho_{ij} in the statement <sep> Page 15, proof of Lemma 9 <sep> - in equation (12), there is an x_y vector that should x_t <sep> Page 15, proof of Theorem 1 <sep> - while I eventually figured it out, it's unclear how Lemma 8 is applied here. Perhaps one more step identifying the exact matrices in the statement of Lemma 8 will be helpful to future readers, and maybe explain where the sqrt(2) factor come from as well. <sep> Page 16, proof of Lemma 10 <sep> - in the beginning of the proof, to stay consistent with the notation, we should replace S_{p, beta} with S_{p, beta}^D <sep> - I believe the cardinality of Q should be (K + D - 1) choose (D - 1), as we need to choose positive j's to sum up to (K+D) in the definition of Q. This reduces down to the problem of choosing natural numbers j's summing K, which is (K+D-1) choose (D-1). Consider the stack exchange post here: <sep> https://math.stackexchange.com/questions/919676/the-number-of-integer-solutions-of-equations <sep> Page 16, proof and statement of Lemma 11 <sep> - I believe in the first term, the factor should be m instead of sqrt(m). I think the mistake happened when applying the union bound, as it should only affect the term containing delta <sep> Page 17, Lemma 12 <sep> - same as Lemma 11, we should have m instead of sqrt(m) <sep> Page 18, proof of Theorem 3 <sep> - at the bottom the statement ""F is orthogonal"" does not imply the norm is less than 1, but rather we should say ""F is orthonormal"" <sep> Page 19, proof of Theorem 3 <sep> - at the top, ""we will omit the index epsilon"" should be ""xi"" instead <sep> - in the final equation block, we have the Rademacher complexity of F_{W_2}, instead it should be F_{W^prime}","I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn't quite close the problem. <sep> p.s.: It seems that centering the weight matrices at initialization is a key idea. The authors note that Dziugaite and Roy used bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size. Looking back at that work, they look at networks where the size increases by a very large factor (going from e.g. 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor. The type of increase also seems much less severe than those pictured in Figures 3/5. Since Dzugate and Roy's bounds involved optimization, perhaps the increase there is merely apparent."
"This paper tries to analyze the impact of the staleness on machine learning models in different settings, including model complexity, optimization methods or the number of workers. In this work, they study the convergence behaviors of a wide array of ML models and algorithms under delayed updates, and propose a new convergence analysis of asynchronous SGD method for non-convex optimization. <sep> The following are my concerns: <sep> 1. ""For CNNs and DNNs, the staleness slows down deeper models much more than shallower counterparts."" I think it is straightforward. I want to see the theoretical analysis of the relation between model complexity and staleness. <sep> 2. ""Different algorithms respond to staleness very differently"".  This finding is quite interesting. Is there any theoretical analysis of this phenomenon? <sep> 3. The ""gradient coherence""  in the paper is not new. I am certain that ""gradient coherence"" is very similar to the ""sufficient direction"" in [1]. <sep> 4. What is the architecture of the network? in the paper, each worker p can communicate with other workers p'. Does it mean that it is a grid network? or it is just a start network. <sep> 5. in the top of page 3, why the average delay under the model is 1/2s +1, isn't it (s-1)/2? <sep> 6.  on page 5, ""This is perhaps not surprising, given the fact that deeper models pose more optimization challenges even under the sequential settings."" why it is obvious opposite to your experimental results in figure 1(a)? Could you explain why shallower CNN requires more iterations to get the same accuracy? it is a little counter-intuitive. <sep> 7. I don't understand what does ""note that s = 0 execution treats each worker's update as separate updates instead of one large batch in other synchronous systems"" mean in the footnote of page 5. <sep> Above all, this paper empirically analyzes the effect of the staleness on the model and optimization methods. It would be better if there is some theoretical analysis to support these findings. <sep> [1] Training Neural Networks Using Features Replay  https://arxiv.org/pdf/1807.04511.pdf <sep> ===after rebuttal=== <sep> All my concerns are addressed. I will upgrade the score.",The reviewers that provided extensive and technically well-justified reviews agreed that the paper is of high quality. The authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.
"This is a hybrid paper, making contributions on two related fronts: <sep> 1. the paper proposes a performance metric for sequence labeling, capturing salient qualities missed by other metrics, and <sep> 2. the paper also proposes a new sequence labeling method based on inference in a hierarchical Bayesian model, focused on simultaneously labeling multiple sequences that have the same underlying procedure but with varying segment lengths. <sep> This paper is not a great topic fit for *CONF*: it's primarily about a hand-designed performance metric for sequence labeling and a hierarchical Bayesian model with Gaussian observations and fit with Gibbs sampling in a full-batch setting. The *CONF* 2019 reviewer guidelines suggest ""Ask yourself: will a substantial fraction of *CONF* attendees be interested in reading this paper?"" and based on my understanding of the *CONF* audience I suspect not. Based on looking at past *CONF* proceedings, this paper's topic and collection of techniques is not in the *CONF* mainstream (though it's not totally unrelated). The authors could convince me that I'm mistaken by pointing out closely related *CONF* papers (e.g. with a similar mix of techniques in their methods, or similarly proposing a hand-designed performance metric); as far as I can tell, none of the papers cited in the references are from *CONF*, but rather from e.g. NIPS, AISTATS, and IEEE TPAMI, which I believe would be better fits for this kind of work. <sep> One way to make this work more relevant to the *CONF* audience would be to add feature learning (especially based on neural network architectures). That might also entail additional technical contributions, like how to fit models like these in the minibatch setting (where the current Gibbs sampling method might not apply). <sep> On the proposed performance metric, the discussion of existing metrics as they apply to the example in Fig 3 was really helpful. (I assume, but didn't check, that the authors' characterization of the published performance metrics is accurate, e.g. ""no traditional clustering criteria can distinguish C_2 from C_3"".) The proposed metric seems to help. <sep> But it's a bit complicated, with several free design decisions involved (e.g. choosing the scoring function \\mathcal{H} in Sec 3.1, the choice of conditional entropy H in Sec 3.2, the choice of \\beta in Sec 3.3, the choice of the specific algebraic forms of RSS, LASS, SSS, and TSS). Certainly the proposed metrics incorporate the kind of information that the authors argue can be important, but the design details of how that information is summarized into a single number aren't really explored or weighed against alternative designs choices. <sep> If a primary aim of this paper is to propose a new performance metric, and presumably to have it catch on with the rest of the field, then the contribution would be much greater if the design space was clearly articulated, alternatives were considered, and multiple proposals were validated. Validation could be done with human labelers ranking the intuitive 'goodness' of labeling results (and then compared to rankings derived from the proposed performance metrics), and with comparing how the metrics correlate with performance on various downstream tasks. <sep> Another idea is to take advantage of a better segmentation performance metric and use it to automatically tune the hyperparameters of the sequence labeling methods considered in the experiments section. (IIUC hyperparameters were set by hand in the experiments.). That would make for more interesting experiments that give a more comprehensive summary of how these techniques can compare. <sep> However, as it stands, while the performance metric itself may have merit, in this paper it is not sufficiently well validated or compared to alternatives. <sep> On the hierarchical Bayesian model, the current model design andinference algorithm are okay but don't constitute major technical contributions. I was surprised by some model details: for example, in ""Modeling the procedure"" of Sec 4.1, it would be much more satisfying to generate the (p_1, ..., p_s) sequence from an HMM instead of sampling the elements of the sequence independently, dropping any chance to learn transition structure as part of the Bayesian inference procedure. More importantly, it wasn't made clear if 'self-transitions' where p_s = p_{s+1} were ruled out, though such transitions might confuse the model's semantics. As another example, in ""Modeling the realizations in each time-series"" of Sec 4.1, the procedure based on iid sampling and sorting seems unnatural, and might make inference more complex. Why not just sample the durations directly (rather than indirectly defining them via sorting independently-generated indices)? If there's a good reason, it should probably be discussed (e.g. maybe parameterizing the durations directly would make it easier to express prior distributions over *absolute* segment lengths, but harder to express distributions over *relative* segment lengths?). Finally, the restriction to conditionally iid Gaussian observations was disappointing. <sep> The experimental results were solid on the task for which the model's extra assumptions paid off, but that's a niche comparison. <sep> One suggestion on the baseline front: you can tie multiple HMMs to have the same procedure (i.e. the same state sequences not counting repeats) by fixing the number of states to be s (the length of the procedure sequence) and fixing the transition matrices to have an upper-bidiagonal support structure. A similar construction can be used for HSMMs. I think a natural Gibbs sampling procedure would emerge. This approach is probably written down in the HMM literature (it seems every conceivable HMM variant has been studied!) but I don't have a reference for it. <sep> Overall, this paper needs more work. <sep> Minor suggestions: <sep> - maybe refer to ""segment structure"" (e.g. in Sec 3), as ""changepoint structure"" (and consider looking into changepoint performance metrics if you haven't already) <sep> - if you used code from other authors in your baselines, it would be good to cite that code (e.g. GitHub links)","While the reviews of this paper were somewhat mixed (7,6,4), I ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined. <sep> The reviewer with a score of 4, argues that this work is not a good fit for *CONF*, but, although tailoring new metrics may not be a common area that is explored, I don't believe that it's outside the range of *CONF*'s interest, and therefore also more unique."
"The authors consider the use of tensor approximations to more accurately capture syntactical aspects of compositionality for word embeddings. Given two words a and b, when your goal is to find a word whose meaning is roughly that of the phrase (a,b), a standard approach to to find the word whose embedding is close to the sum of the embeddings, a + b. The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. They test this idea specifically on the use-case where (a,b) is an adjective,noun pair, and show that their form of compositionality outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgements. In their model, the word embeddings are learned separately, then the tensor T is learned by minimizing an objective whose goal is to minimize the error in predicting observed trigram statistics. The specific objective comes from a nontrivial tensorial extension of the original matricial RAND-WALK model for learning word embeddings. <sep> The topic is fitting with *CONF*, and some attendees will find the results interesting. As in the original RAND-WALK paper, the theory is interesting, but not the main attraction, as it relies on strong generative modeling assumptions that essentially bake in the desired results. The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to *CONF* attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown). <sep> Pros: <sep> - theoretical justification is given for their assumption that the higher-order interactions can be modeled by a tensor <sep> - the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement <sep> Cons: <sep> - no downstream applications are given which show that these higher order interactions can be useful for downstream tasks. <sep> - the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b? <sep> - comparison should be made to the linear composition method in the Arora, Liang, Ma *CONF* 2017 paper <sep> Some additional citations: <sep> - the above-mentioned *CONF* paper provides a performant alternative to unweighted linear composition <sep> - the 2017 Gittens, Achlioptas, Drineas ACL paper provides theory on the linear composition of some word embeddings","AR1 is concerned about lack of downstream applications which show that higher-order interactions are useful and asks why not to model higher-order interactions for all (a,b) pairs. AR2 notes that this submission is a further development of Arora et al. and is satisfied with the paper. AR3 is the most critical regarding lack of explanations, e.g. why linear addition of two word embeddings is bad and why the corrective term proposed here is a good idea. The authors suggest that linear addition is insufficient when final meaning differs from the individual meanings and show tome quantitative results to back up their corrective term. <sep> On balance, all reviewers find the theoretical contributions sufficient which warrants an accept. The authors are asked to honestly reflect all uncertain aspects of their work in the final draft to reflect legitimate concerns of reviewers."
"In this interesting study, the authors show that incorporating rotation-equivariant filters  (i.e. enforcing weight sharing across filters with different orientations) in a CNN model of the visual system is a useful prior to predict responses in V1. After fitting this model to data, they find that the RFs of model V1 cells do not resemble the simple Gabor filters of textbooks, and they present other quantitative results about V1 receptive fields. The article is clearly written and the claims are supported by their analyses. It is the first time to my knowledge that a rotation-equivariant CNN is used to model V1 cells. <sep> The article would benefit from the following clarifications: <sep> 1. The first paragraph of the introduction discusses functional cell types in V1, but the article does not seem to reach any new conclusion about the existence of well-defined clusters of functional cell types in V1. If this last statement is correct, I believe it is misleading to begin the article with considerations about functional cell types in V1. Please clarify. <sep> 2. For clarity, it would help the reader to mention in the abstract, introduction and/or methods that the CNN is trained on reproducing V1 neuron activations, not on an image classification task as in many other studies (Yamins 2014, etc). <sep> 3. ""As a first step, we simply assume that each of the 16 features corresponds to one functional cell type and classify all neurons into one of these types based on their strongest feature weight."" and ""The resulting preferred stimuli of each functional type are shown in Fig. 6."" <sep> Again, I think these statements are misleading because they suggest that V1 cells indeed cluster in distinct functional cell types rather than form a continuum. However, from the data shown, it is unclear whether the V1 cells recorded form a continuum or distinct clusters. Unless this question is clarified and the authors show the existence of functionally distinct clusters in their data, they should preferably not mention ""cell types"" in the text. <sep> Suggestions for improvement and questions (may not necessarily be addressed in this paper): <sep> 4. ""we apply batch normalization"" <sep> What is the importance of batch normalization for successfully training the model? Do you think that a sort of batch normalization is implemented by the visual system? <sep> 5. ""The second interesting aspect is that many of the resulting preferred stimuli do not look typical standard textbook V1 neurons which are Gabor filters. "" <sep> OK but the analysis consists of iteratively ascending the gradient of activation of the neuron from an initial image. This cannot be compared directly to the linear approximation of the V1 filter that is computed experimentally from doing a spike-triggered average (STA) from white noise. A better comparison would be to do a single-step gradient ascent from a blank image. In this case, do the filters look like Gabors? <sep> 6. Did you find any evidence that individual V1 neurons are themselves invariant to a rotation? <sep> 7. The article could be more self-contained. There are a lot of references to Klindt et al. (2017) on which this work is based, but it would be nice to make the article understandable without having to read this other article. <sep> Typo: Number of fearture maps in last layer <sep> Conclusion: <sep> I believe this work is significant and of interest for the rest of the community studying the visual system with deep networks, in particular because it finds an interesting prior for modeling V1 neurons, that can probably be extended to the rest of the visual system. However, it would benefit from the clarifications mentioned above.","The overall consensus after an extended discussion of the paper is that this work should be accepted to *CONF*. The back-and-forth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification (trending positive) of the reviewer scores."
"Summary: This paper is about models for solving basic math problems. The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type. The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down. <sep> Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models. There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these. The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good. <sep> Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). It would have been useful to compare the general models here with some specific math problem-focused ones as well. Some details weren't clear to me. More in the comments below. <sep> Verdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion. <sep> Comments: <sep> - One area that could stand to be improved is prior work. I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems. Since this is the core contribution, this should also be the main comparison. For example, EMLNP 2017 paper ""Deep Neural Solver for Math Word Problems"" mentions a size 60K problem dataset. A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse. <sep> - The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets? <sep> - The authors divide dataset construction into crowdsourcing and synthetic. This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students. These are solved, and only require very limited validation. They are also categorized by difficulty and area. Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc? <sep> - How do are the difficulty levels synthetically determined? <sep> - When generating the questions, the authors ""first sample the answer"". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected. <sep> - The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology. <sep> - I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3). This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive. But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful. In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this). On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited. <sep> - Really would be good to do real-world tests in a more extensive way. A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions? <sep> - For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here? <sep> - The 1+1+...+1 example is pretty intriguing, and could be a nice ""default"" question! <sep> - Minor typo: in the abstract: ""test spits"" should be ""test splits""","Pros: <sep> - A useful and well-structured dataset which will be of use to the community <sep> - Well-written and clear (though see Reviewer 2's comment concerning the clarity of the model description section) <sep> - Good methodology <sep> Cons: <sep> - There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn't be harvested from school texts directly. Presumably it would've been a lot more work but please address the issue in your rebuttal. <sep> - Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics-specific models (a couple of which are mentioned in the text). On the other hand, Reviewer 2 is concerned that the specific choices (e.g. ""thinking steps"") made for the general models are non-standard in seq-2-seq models. I haven't heard about the thinking step approach but perhaps it's out there somewhere. It would be helpful generally to have more discussion about the reasoning involved in these decisions. <sep> I think this is a useful contribution to the community, well written and thoughtfully constructed. I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section. Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen. Also consider adding additional experiments to allay the concerns of the reviewers."
"PRIOR COMMENT:   This paper should be rejected based on the experimental work. <sep> Experiments need to be reported for larger datasets.  Note the MGAN <sep> paper reports results on STL-10 and ImageNet as well. <sep> NOTE:  this was addressed by the 27/11 revision, which included good results for these other data sets, thus I now withdraw the comment <sep> Note, your results on CIFAR-10 are quite different to those in the <sep> MGAN paper.  Your inceptions scores are worse and FIDs are better!!  I <sep> expect you have different configurations to their paper, but it would be good for this to be explained.  NOTE:   explained in response! <sep> NOTE:  this was addressed by the 27/11 revision <sep> I thought the related work section was fabulous, and as an extension to BGAN, the paper is a very nice idea.  So I benefited a lot from reading the paper. <sep> I have some comments on Bayesian treatment.  In Bayesian theory, the true distribution pdata cannot appear in any evaluated formulas, <sep> as you have it there in Eqn (1) which is subsequently used in your likelihood Eqn (2).  Likelihoods are models and cannot involve ""truth"". <sep> Lemma 1:  Very nice observation!!  I was trying to work that out, <sep> once I got to Eqn (3), and you thought of it. <sep> Also, you do need to explain 3.2 better.  The BGAN paper, actually, is a bit confusing from a strict Bayesian perspective, though for different reasons.  The problem you are looking at is not a time-series problem, so it is a bit confusing to be defining it as such.  You talk about an iterative Bayesian model with priors and likelihoods.  Well, maybe that can be *defined* as a probabilistic model, but it is not in any sense a Bayesian model for the estimation of pmodel. <sep> NOTE:  anonreviewer2 expands more on this <sep> What you do with Equation (3) is define a distribution on qg(θg) and qd(θd) (which, confusingly, involves the <sep> ""true"" data distribution ... impossible for a Bayesian formulation). <sep> You are doing a natural extension of the BGAN papers formulation in their Eqs (1) and (2).  This, as is alluded to in Lemma 1.  Your formulation is in terms of two conditional distributions, so conditions should be given that their is an underlying joint distribution that agrees with these.  Lemma 1 gives a negative result. <sep> You have defined it as a time series problem, and apparantly one wants this to converge, as in Gibbs sampling style.  Like BGAN, you have just arbitrarily defined a ""likelihood"". <sep> To me, this isn't a Bayesian model of the unsupervised learning task, <sep> its a probabilistic style optimisation for it, in the sense that you are defining a probability distribution (over qg(θg) and qd(θd)) and sampling from it, but its not really a ""likelihood"" in the formal sense.  A <sep> likelhood defines how data is generated.  Your ""likelihood"" is over model parameters, and you seem to have ignored the data likelihood, <sep> which you define in sct 3.1 as pmodel(). <sep> Anyway, I'm happy to go with this sort of formulation, but I think you need to call it what it is, and it is not Bayesian in the standard sense.  The theoretical treatment needs a lot of cleaning up.  What you have defined is a probabilistic time-series on qg(θg) and qd(θd). <sep> Fair enough, thats OK.  But you need to show that it actually works in the estimation of pmodel.  Because one never has pdata, all your Theorem 1 does is show that asymptotically, your method works. <sep> Unfortunately, I can say the same for many crude algorithms, and most of the existing published work.  Thus, we're left with requiring a substantial empirical validation to demonstrate the method is useful. <sep> Now my apologies to you: I could make somewhat related statements about the theory of the BGAN paper, and they got to publish theirs at <sep> *CONF*!  But they did do more experimentation. <sep> Oh, and some smaller but noticable grammar/word usage issues. <sep> NOTE:  thanks for your good explanation of the Bayesian aspects of the model ... <sep> yes I agree, you have a good Bayesian model of the GAN computation , but it is still not a Bayesian model of the unsupervised inference task.  This is a somewhat minor point, and should not in anyway influence worth of the paper ... but clarification in paper would be nice.","The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version."
"Pros: <sep> - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. <sep> - The empirical and theoretical analyses are clear, seem thorough, and make sense. <sep> - Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian). <sep> Cons: <sep> - The premises of the analyses are not very convincing, limiting the significance of the paper. <sep> - In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. <sep> - It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper. <sep> - Some parts of the paper feel long-winded and aimless. <sep> [Quality] <sep> See above pros and cons. <sep> A few less important disagreement I have with the paper: <sep> - I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large. <sep> - Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar. <sep> [Clarity] <sep> In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded. <sep> Section 2 background takes too much space. <sep> Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment. <sep> Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment. <sep> A few editorial issues: <sep> - On page 4 footnote 2, as far as I know the paper did not define BPD. <sep> - There are two lines of text between Fig. 4 and Fig. 5, which is confusing. <sep> [Originality] <sep> I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel. <sep> However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: <sep> Vít Škvára et al. Are generative deep models for novelty detection truly better? <sep> ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried. <sep> A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images). <sep> [Significance] <sep> The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly. <sep> However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test? <sep> Section 5 is based on a 2nd order expansion on the logp(x) given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.","This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there's consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication."
"The paper considers the problem of dictionary learning. Here the model that we are given samples y, where we know that y = Ax where A is a dictionary matrix, and x is a random sparse vector. The goal is typically to recover the dictionary A, from which one can also recover the x under suitable conditions on A. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias). <sep> The main comparison with prior work is with [1]. Both give algorithms of this type for the same problem, with similar assumptions (although there is some difference; see below). In [1], the authors give two algorithms: one with a better sample complexity than the algorithm presented here, but which has some systematic, somewhat large, error floor which it cannot exceed, and another which can obtain similar rates of convergence to the exact solution, but which requires polynomial sample complexity (the explicit bound is not stated in the paper). The algorithm here seems to build off of the former algorithm; essentially replacing a single hard thresholding step with an IHT-like step. This update rule is able to remove the error floor and achieve exact recovery. However, this makes the analysis substantially more difficult. <sep> I am not an expert in this area, but this seems like a nice and non-trivial result. The proofs are quite dense and I was unable to verify them carefully. <sep> Comments: <sep> - The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates. The authors claim that some amount of noise can be tolerated, but do not quantify how much. <sep> - A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0. <sep> [1] Arora, S. Ge, R., Ma, T. and Moitra, A. Simple, Efficient, and Neural Algorithms for Sparse Coding. COLT 2015.","Alternating minimization is surprisingly effective for low-rank matrix factorization and dictionary learning problems. Better theoretical characterization of these methods is well motivated. This paper fills up a gap by providing simultaneous guarantees for support recovery as well as coefficient estimates for linearly convergence to the true factors, in the online learning setting. The reviewers are largely in agreement that the paper is well written and makes a valuable contribution. The authors are advised to address some of the review comments around relationship to prior work highlighting novelties."
"This paper presents a class of neural networks that does not have bad local valleys. The ""no bad local valleys"" implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn't increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output. <sep> The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that <sep> * adding skip connections doesn't harm the generalization. <sep> * adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance. <sep> * comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting. <sep> However, from a theoretical point of view, I would say the contribution of this work doesn't seem to be very significant, for the following reasons: <sep> * In the first place, figuring out ""why existing models work"" would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones. <sep> * The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally ""equivalent"" to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17') it is easy to attain global minima. <sep> * I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive. <sep> Below, I'll list specific comments/questions about the paper. <sep> * Assumption 3.1.2 doesn't make sense. Assumption 3.1.2 says ""there exists N neurons satisfying…"" and then the first bullet point says ""for all j = 1, …, M"". Also, the statement ""one of the following conditions"" is unclear. Does it mean that we must have either ""N satisfying the first bullet"" or ""N satisfying the second bullet"", or does it mean we can have N/2 satisfying the first and N/2 satisfying the second? <sep> * The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions. <sep> * Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses. <sep> * Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it's implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes. <sep> * For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn't necessarily satisfy the assumptions? <sep> * Can you show the ""improvement"" of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys. <sep> Minor points <sep> * In the Assumption 3.1.3, the N in r≠s∈N means [N]? <sep> * In the introduction, there is a sentence ""potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),"" which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18' and Yun et al. 18'). <sep> * Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as ""for example, in the fully connected network case, this means that all data points are distinct.","This paper introduces a class of deep neural nets that provably have no bad local valleys. By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima. Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks. The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results. After revisions the reviewers are satisfied that their comments have been addressed. This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the *CONF* community."
"Revision: <sep> The authors have taken my advice and addressed my concerns wholeheartedly. It is clear to me that they have taken efforts to make notable progress during the rebuttal period. Summary of their improvements: <sep> - They have extended their methodology to handle multiple strokes <sep> - The model has been converted to a latent-space generative model (similar to Sketch-RNN, where the latent space is from a seq2seq VAE, and SPIRAL where the latent space is used by an adversarial framework) <sep> - They have ran addition experiments on a diverse set of datasets (now includes Kanji and QuickDraw), in addition to omniglot and mnist. <sep> - Newer version is better written, and I like how they are also honest to admit limitations of their model rather than hide them. <sep> I think this work is a great companion to existing work such as Sketch-RNN and SPIRAL. As mentioned in my original review, the main advantage of this is the ability to train with very limited compute resources, due to the model-based learning inspired by model-based RL work (they cited some work on world models). Taking important concepts from various different (sub) research areas and synthesizing them into this nice work should be an inspiration to the broader community. The release of their code to reproduce results of all the experiments will also facilitate future research into this exciting topic of vector-drawing models. <sep> I have revised my score to 8, since I believe this to be at least in the better half of accepted papers at *CONF* based on my experience of publishing and attending the conference in the past few years. I hope the other reviewers can have some time to reevaluate the revision. <sep> Original review: <sep> Summary: they propose a differentiable learning algorithm that can output a brush stroke that can approximate a pixel image input, such as MNIST or Omniglot. Unlike sketch-pix2seq[3] (which is a pixel input -> sketch output model based on sketch-rnn[2]), their method trains in an unsupervised manner and does not require paired image/stroke data. They do this via training a ""world model"" to approximate brush painting software and emulate it. Since this emulation model is differentiable, they can easily train an algorithm to output a stroke to approximate the drawing via back propagation, and avoid using RL and costly compute such in earlier works such as [1]. <sep> The main strength of this paper is the original thought that went into it. From reading the paper, my guess is the authors came from a background that is not pure ML research (for instance, they are experts in Javascript, WebGL, and their writing style is easy to read), and it's great to see new ideas into our field. While research from big labs [1] have the advantage of having access to massive compute so that they can run large scale RL experiments to train an agent to ""sketch"" something that looks like MNIST or Omniglot, the authors probably had limited resources, and had to be more creative to come up with a solution to do the same thing that trains in a couple of hours using a single P40 GPU. Unlike [1] that used an actual software rendering package that is controlled by a stroke-drawing agent, their creative approach here is to train a generator network to learn to approximate a painting package they had built, and then freeze the weights of this generator to efficiently train an agent to draw. The results for MNIST and Omniglot look comparable to [1] but achieved with much fewer resources. I find this work refreshing, and I think it can be potentially much more impactful than [1] since people can actually use it with limited compute resources, and without using RL. <sep> That being said, things are not all rosy, and I feel there are things that need to be done for this work to be ready for publication in a good venue like *CONF*. Below are a few of my suggestions that I hope will help the authors improve their work, for either this conference, or if it gets rejected, I encourage the authors to try the next conference with these improvements: <sep> 1) multiple strokes, longe strokes. I don't think having a model that can output only a single stroke is scalable to other (simple) datasets such pixel versions of KangiVG [4] or QuickDraw [5]. The authors mentioned the need for an RNN, but couldn't the encoder just output the stroke in a format that contains the pen-down / pen-up event, like the stroke format suggested in [2]? Maybe, maybe not, but in either case, for this work to matter, multiple stroke generation is needed. Most datasets are also longer than 16 points, so you will need to show that your method works for say 80-120 points for this method to be comparable to existing work. If you can't scale up 16 points, would like to see a detailed discussion as to why. <sep> 2) While I like this method and approach, to play devil's advocate, what if I simply use an off the shelf bmp-to-svg converter that is fast and efficient (like [6]), and just build a set of stroke data from a dataset of pixel data, and train a sketch-rnn type model described in [3] to convert from pixel to stroke? What does this method offer that my description fails to offer? Would like to see some discussion there. <sep> 3) I'll give a hint for as to what I think for (2). I think the value in this method is that it can be converted to a full generative model with latent variables (like a VAE, GAN, sketch-rnn) where you can feed in a random vector (gaussian or uniform), and get a sketch as an output, and do things like interpolate between two sketches. Correct me if I'm wrong, but I don't think the encoder here in the first figure outputs an embedding that has a Gaussian prior (like a VAE), so it fails to be a generative model (check out [1], even that is a latent variable model). I think the model can be easily converted to one though to address this issue, and I strongly encourage the authors to try enforcing a Gaussian prior to an embedding space (that can fit right between the 16x16x128 average pooling op to the fully connected 1024 sized layer), and show results where we can interpolate between two latent variables and see how the vector sketches are interpolated. This has also been done in [2]. If the authors need space, I suggest putting the loss diagrams near the end into the appendix, since those are not too interesting to look at. <sep> 4) As mentioned earlier, I would love to see experimental results on [4] KangiVG and [5] QuickDraw datasets, even subsets of them. An interesting result would be to compare the stroke order of this algorithm with the natural stroke order for human doodles / Chinese characters. <sep> Minor points: <sep> a) The figures look like they are bitmap, pixel images, but for a paper advocating stroke/vector images, I recommend exporting the diagrams in SVG format and convert them to PDF so they like crisp in the paper. <sep> b) Write style: There are some terms like ""huge"" dataset that is subjective and relative. While I'm happy about the writing style of this paper, maybe some reviewers who are more academic types might not like it and have a negative bias against this work. If things don't work out this time, I recommend the authors asking some friends who have published (successfully) at good ML conferences to proof read this paper for content and style. <sep> c) It's great to see that the implementation is open sourced, and put it on github. Next time, I recommend uploading it to an anonymous github profile/repo, although personally (and for the record, in case area chairs are looking), I don't mind at all in this case, and I don't think the author's github address revealed any real identity (I haven't tried digging deeper). Some other reviewers / area chairs might not like to see a github link that is not anonymized though. <sep> So in the end, even though I really like this paper, I can only give a score of 6 (edit: this has since been revised upward to 8). If the authors are able to address points 1-4, please do what you can in the next few weeks and give it your best shot. I'll look at the paper again and will revise the score upwards by a point or two if I think the improvements are there. If not, and this work ends up getting rejected, please consider improving the work later on and submitting to the next venue. Good luck! <sep> [1] SPIRAL https://arxiv.org/abs/1804.01118 <sep> [2] sketch-rnn https://arxiv.org/abs/1704.03477 <sep> [3] sketch-pix2seq https://arxiv.org/abs/1709.04121 <sep> [4] http://kanjivg.tagaini.net/ <sep> [5] https://quickdraw.withgoogle.com/data <sep> [6] https://vectormagic.com/","The paper proposes a novel differential way to output brush strokes, taking a few ideas from model-based learning. The method is efficient in that one can train it in an unsupervised manner and does not require paired data. The strengths of the paper are the qualitative results that demonstrate nice interpolations among other things, on a number of datasets (esp. post-rebuttal). <sep> The weaknesses of the paper are the writing (which I think is relatively easy to improve if the authors make an honest effort) and some of the quantitative evaluation. I would encourage the authors to get in touch with the SPIRAL paper authors in order to get access to the SPIRAL generated MNIST test data and then perhaps the classification metric could be updated. <sep> In summary, from the discussion, the major points of contention were the somewhat lacking initial evaluation (which was fixed to a large extent) and the quality of writing (which could be fixed more). I believe the submission is genuinely novel, interesting (esp. the usage of world model-like techniques) and valuable for the *CONF* audience so I recommend acceptance."
"Edit and a further question: Reading again Section 7, I'm wondering whether the  the high generalization is possible due to the fact that at test time only one of the two candidates is unseen, and the other is seen. Having *both* candidates to be unseen makes the problem significantly harder since the only way for the listener to get it right is to associate the message with the right candidate, rather than relying in some other strategy like whether the message is novel (thus it's the seen candidate) or new (thus it's the unseen candidate). As such, I don't think I can fully trust your conclusions due to this potential confounder. <sep> -------------------------------------------------------------- <sep> The authors propose a measure of compositionality in representations. Given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation  z of the input x. The authors find that this measure correlates with the mutual information between the input x and z, approximates the human judges of compositionality on a language dataset and finally presents a study on the relation between the proposed measure and generalizalization performance, concluding that their measure correlates with generalization error as well as absolute test accuracy. <sep> This in an interesting study and attacks a very fundamental question; tracking compositionality in representations could pave the way towards representations that facilitate transfer learning and better generalization. While the paper is very clear with respects to results, I found the presentation of the proposed measure overly confusing (and somewhat more exaggerated that what is really going on). <sep> The authors start with a very clean example, that can potentially facilitate clarifying in a visual way the process of obtaining the measure. However, I feel that clarity is being traded-off for formality. It needs several reads to really distill the idea that essentially the authors are simply learning vectors of primitives that when added should resemble the representation of the input. Moreover, the name of the measure is a bit misleading and not justified by the experiments and the data. The authors do not deal with trees in any of the examples, but rather with a set of primitives (apparent in the use of addition as a composition function which being commutative does not allow for word-order and the like deep syntactic properties). <sep> Now, onto the measure. I like the idea of learning basis vectors from the representations and constraining to follow the primitive semantics. Of course, this constraints quite a bit the form of compositionality that the authors are searching for. <sep> The idea of additive semantics has been explored in NLP, however it's mostly applicable for primitives with intersective semantics (e.g., a white towel is something that is both white and a towel). Do the authors think that this restricts their experiments (especially the natural languages ones)? What about other composition techniques found in the literature of compositional semantics (e.g., by Baroni and Zamparelli, 2010). <sep> This is good to be clarified.  Moreover, given the simplicity of the datasets in the current study, wouldn't a reasonable baseline be to obtain the basis vector of blue by averaging all the latent representations of blue?  Similarly, how sensitive are conclusions with respect to different composition functions? <sep> Section 4 is potentially very interesting, but I don't seem to understand why it's good news that TRE correlates with I(x;\\theta). Low TRE indicates high-degree of compositionality. I suspect that low MI means that input and latent representation are somewhat independent but I don't see the connection to compositional components. Can the authors clarify? <sep> Section 5 is a nice addition. The authors mention that they learn word and phrase representations. Where are the word representations used? My understanding is that you derive basis word representations by using SGD and the phrase vectors and compute TRE with these. If this is the case, an interesting experiment would be to report how similar the induced basis vectors are (either some first-order or second-order similarity) to the pre-trained ones. <sep> Section 8 presents results on discrete representations. Since this is the experiment most similar to the recent work that uses topographic similarity (and since the authors already prime the reader at section 7 about relation between the 2 measures), it would be interesting to see the empirical relation between TRE and topographic and its relation to generalization and absolute performance. <sep> Baroni and Zamparelli (2010) Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space","This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed. All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic. However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far. I'm recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it. <sep> I'm also somewhat concerned at R2's mention of a potential confound in one experiment. The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I'm presuming that this issue has been resolved. <sep> I also ask the authors to release code shortly upon de-anonymization, as promised."
"The paper proposes a new way to construct adversarial examples: do not change the intensity of the input image directly, but deform the image plane (i.e. compose the image with Id + tau where tau is a small amplitude vector field). <sep> The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to *CONF* 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]). <sep> Pros: <sep> - the paper is well written, very easy to read, well explained (and better formalized than [Xiao and al.]); <sep> - the idea of deforming images is new (if we forget about [Xiao and al.]) and simple; <sep> - experiments show what such a technique can achieve on MNIST and ImageNet. Interestingly, one can see on MNIST the parts of the numbers that the adversarial attack is trying to delete/create. <sep> Cons: <sep> - the paper is a bit weak, in that it is not very dense, and in that there is not much more content than the initial idea; <sep> - for instance, more discussions about the results obtained could have been appreciated (such as my remark above about MNIST); <sep> - for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?); <sep> - for instance, what about generating adversarial examples for which the network would be fully (wrongly) confident? (instead of just borderline unsure); etc. <sep> - The interpolation scheme (how is defined the intensity I(x,y) for a non-integer location (x,y) within the image I) is rather important (linear interpolation, etc.) and should be at least mentioned in the main paper, and at best studied (it might impact the gradient descent path and the results); <sep> - question: does the algorithm converge? could there be a proof of this? This is not obvious, as the objective potentially changes with time (selection of the current m best indices k of |F_k - F_l|). Also, the final overshoot factor (1+eta) is not very elegant, and not guaranteed to perform well if tau* starts being not small compared to the second derivative (i.e. g''.tau^2 not small) while I guess that for image intensities, spatial derivatives can be very high if no intensity smoothing scheme is used. <sep> - note: the approximation tau* = sum_i tau_i (section 2.3) does not stand in the case of non-small deformations. <sep> - still in section 2.3, I do not understand the statement ""given that \\nabla f is moderate"": where does this property come from? or is ""given"" meant to be understood as ""provided..."" (i.e. under the assumption that...)? <sep> - computational times could have been given (though I guess they are reasonable). <sep> Other remarks: <sep> - suggestion: I find the ""slight abuse of notation"" (of confusing the derivative with the gradient) a bit annoying and suggest to use a different symbol, such as \\nabla g. This could be useful in particular in the following perspective: <sep> - Mathematical side note: the ""gradient"" of a functional is not a uniquely-defined object in that it depends on the metric chosen in the tangent space. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). The choice of the metric can then be seen as a prior over desired gradient descent paths. In the paper, the deformation fields get smoothed by a Gaussian filter at some point (eq. 7), in order to be smoother: this can be interpreted as a prior (gradient descent paths should be made of smooth deformations) and as an associated inner product change (there do exist a metric M such that the gradient for that metric is \\nabla_M g = S \\nabla_L2 g). It is possible to favor other kind of deformations (not just smooth ones, but for instance rigid ones, etc. [and by the way this could make the link with ""Manitest: Are classifiers really invariant?"" by Fawzi and Frossard, BMVC 2015, who observe that a rigid motion can affect the classifier output]). If interested, you can check ""Generalized Gradients: Priors on Minimization Flows"" by Charpiat et al. for general inner products on deformations (in particular favoring rigid motion), and ""Sobolev active contours"" by Sundaramoorthi et al. for inner products more dedicated to smoothing (such as with the H1 norm). <sep> - Note: about the remark in section 3.2: deformation-induced transformations are a subset of all possible transformations of the image (which are all representable with intensity changes), so it is expected that a training against attacks on the intensity performs better than a training against attacks on spatial deformations.","The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations. Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference. <sep> On the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks. The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack. The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al. (see below). Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation. <sep> There were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1). R2 would have appreciated more analysis on how to defend against the attack. <sep> A controversial point is the relation / novelty with respect to Xiao et al., *CONF* 2018. As e.g. pointed out by R1: ""The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to *CONF* 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.])."" <sep> On the balance, all three reviewers recommended acceptance of the paper. Regarding novelty over Xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks."
"Overall: <sep> This paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE. <sep> The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising. <sep> Clarity: <sep> The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand. <sep> Significance: <sep> I think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score. <sep> The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed. <sep> Question and minor comments: <sep> In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians. <sep> In this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL. <sep> Faced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method? <sep> To clarify the usefulness of this method, I think the additional experimental comparisons are needed. <sep> About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased ""experimentally"". <sep> Followings are minor comments. <sep> In experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance. <sep> I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator. <sep> However, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper. <sep> In Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure? <sep> (Same for Figure 4)","The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi-Monte-Carlo-sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below. <sep> The proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score-function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake-sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance. <sep> The AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems."
"This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. The tracker receives, from its own perspective, partially observed visual information o_t^{alpha} about the target (e.g., an image that may show the target) and the target receives both observations from its own perspective o_t^{beta} and a copy of the information from the tracker's perspective. Both agents are standard convnet + LSTM neural architectures trained using A3C and are evaluated in 2D and 3D environments. The reward function is not completely zero-sum, as the tracked agent's reward vanishes when it gets too far from a reference point in the maze. <sep> The work is very incremental over Luo et al (2018) ""End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning"", as the only two additions are extra observations o_t^{alpha} for the target, and a reward function that has a fudge factor when the target gets too far away. Citing Sun Tzu's ""Art of War"" (please use the correct citation format) is not convincing enough for adding the tracker's observations as inputs for the target agent. Should not the asymmetrical relationship work the other way round, with the tracker knowing more about the target? <sep> Experiments are conducted using two baselines for the target agent, one a random walk and another an agent that navigates to a target according to a shortest path planning algorithm. The ablation study shows that the tracker-aware observations and a target's reward structure that penalizes when it gets too far do help the tracker's performance, and that training the target agent helps the tracker agent achieve higher scores. The improvement is however quite small and the task is ad-hoc. <sep> The paper would have benefitted from a proper analysis of the trajectories taken by the adversarial target as opposed to the heuristic ones, and from comparison with non-RL state-of-the-art on tracking tasks. Further multi-agent tasks could also have been considered, such as capture the flag tasks as in ""Human-level performance in first-person multiplayer games with population-based deep reinforcement learning"".","The paper presents an adversarial learning framework for active visual tracking, a tracking setup where the tracker has camera control in order to follow a target object. The paper builds upon Luo et al. 2018 and proposes jointly learning tracker and target policies (as opposed to tracker policy alone). This automatically creates a curriculum of target trajectory difficulty, as opposed to the engineer designing the target trajectories. The paper further proposes a method for preventing the target to fast outperform the tracker and thus cause his policy to plateau. Experiments presented justify the problem formulation and design choices, and outperform Luo et al. . The task considered is very important, active surveillance with drones is just one sue case. <sep> A downside of the paper is that certain sentences have English mistakes, such as this one: ""The authors learn a policy that maps raw-pixel observation to control signal straightly with a Conv-LSTM network. Not only can it save <sep> the effort in tuning an extra camera controller, but also does it outperform the..."" However, overall the manuscript is well written, well structured, and easy to follow. The authors are encouraged to correct any remaining English mistakes in the manuscript."
"The authors perform a careful study of mixed integer linear programming approaches for verifying robustness of neural networks to adversarial perturbations. They propose three enhancements to MILP formulations of neural network verification: Asymmetric bounds, restricted domain and progressive bound tightening, which lead to significantly more scalable verification algorithms vis-a-vis prior work. They study the effectiveness of MILP solvers both in terms of verifying robustness (compared to other complete/incomplete verifiers) and generating adversarial attacks (compared to PGD attacks) and show that their approach compares favorable across a number of architectures on MNIST and CIFAR-10. They perform careful ablation studies to validate the importance of the <sep> Quality: The paper is very well written and organized. The problem is certainly of great interest to the deep learning community, given the difficulty of properly evaluating (and then improving) defenses against adversarial attacks. The experiments are done carefully with convincing ablation studies. <sep> Clarity: The authors explain the relevant concepts carefully and all the experimental results are clearly written and explained. <sep> Originality: The authors propose conceptually simple but practically significant enhancements to MILP formulations of neural network verification. However, the novelty wrt https://arxiv.org/pdf/1711.00455.pdf is not discussed carefully in my view (the  asymmetric bounds were already studied in this paper, as well as a novel branch and bound strategy). The progressive bound tightening is a novel idea as far as I can see - however, the ablation experiments show that this idea is not significant in terms of performance improvement. In terms of experiments, the authors indeed obtain strong results on verified adversarial error rates and generate attacks that PGD is unable to - however, again the results do not outperform latest results (in terms of the  best achievable upper bounds on verified error rates) available well before the *CONF* deadline - https://arxiv.org/pdf/1805.12514.pdf . It would be great if the authors addressed these issues in a revised version of the paper. <sep> Significance: The work does establish a strong algorithm for complete verification of neural networks along with several ideas that are critical to obtain strong performance with this approach. <sep> Question: <sep> 1. I am unclear on the ""restricted domain"" contribution claimed in the paper - is this just exploiting the fact that the inputs to the classifier are normalized to a given range, in addition to being no more than eps away from the nominal input? <sep> Cons <sep> 1. The authors do not compare their approach to that of https://arxiv.org/pdf/1711.00455.pdf , both in terms of conceptual novelty and in terms of experimental results. In particular, it is not clear to me whether the authors' approach remains superior on domains where tight bounds on the neural networks inputs are not available, like the problems studied in the ACAS system in the ReLuPlex paper. <sep> 2. The authors' MILP solution approach relies on having access to the state of the art commercial MILP solver Gurobi. While Gurobi is free for academic research use, for large scale neural network verification applications, this does restrict use of the approach (particularly due to limited licenses being available). It would be interesting to see a comparison that uses a freely available MILP solver (like scip.zib.de) to see how critical the approach's scalability depends on the quality of the MILP solver. <sep> 3. The authors do not outperform the latest SOA numbers in terms of verified adversarial error rates on MNIST and CIFAR classifers. It would be good to see a comparison on results from https://arxiv.org/pdf/1711.00455.pdf  (I believe the training code and trained networks are available online).","The paper investigates mixed-integer linear programming methods for neural net robustness verification in presence of adversarial attckas. The paper addresses and important problem, is well-written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field."
"This paper describes a novel method for solving inverse problems in imaging. <sep> The basic idea of this approach is use the following steps: <sep> 1. initialize with nonnegative least squares solution to inverse problem (x0) <sep> 2. compute m different projections of x0 <sep> 3. estimate x from the m different projections by solving ""reformuated"" inverse problem using TV regularization. <sep> The learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a ""good"" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be. <sep> More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. <sep> Empirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem. <sep> The core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the ""inverse-ness"" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned. <sep> At the heart of this paper is the idea that for an L-Lipschitz function f : R^k → R the sample complexity is O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided. <sep> This isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we really get the benefits. <sep> The authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares. <sep> Practically I'm quite concerned about their method requiring training 130 separate convolutional neural nets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is it possible that any network at all would be okay? Can we just reconstruct the image from regression on 130 randomly-initialized convolutional networks? <sep> The proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding.","This paper proposes a novel method of solving inverse problems that avoids direct inversion by first reconstructing various piecewise-constant projections of the unknown image (using a different CNN to learn each) and then combining them via optimization to solve the final inversion. <sep> Two of the reviewers requested more intuitions into why this two stage process would fight the inherent ambiguity. <sep> At the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper. <sep> The authors also have significantly improved the clarity of the manuscript throughout the discussion period. <sep> It would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. ""Deep Component Analysis via Alternating Direction Neural Networks <sep> "" of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning a feedforward mapping."
"In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning. <sep> The main idea is that two states should be distinguished *functionally* in terms of the actions that are needed to reach them, <sep> in contrast with generative methods which try to capture all aspects of the state dynamics, even those which are not relevant for the task at hand. <sep> The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize. <sep> The experimental study is based on 6 simulated environments and outlines various properties of the framework. <sep> Overall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like *CONF*. <sep> The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning ""downstream tasks"" in a second step. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further. <sep> A second point is about unsufficiently clear thoughts about the way to intuitively advocate for the approach. The authors first claim that two states are functionally different if they are reached from different actions. Thinking further about what ""functionally"" means, I would rather have said that two states are functionally different if different goals can be reached from them. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). But thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would end-up with a different intuitive presentation of their framework. Shouldn't finally D_{act} be a distance between goals rather than between states? <sep> Section 4 lists the properties that can be expected from the framework. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as ""state abstraction (or clustering?) from actionable representation"". And the corresponding properties should come with their own questions and subsection in the experimental study (more about this below). <sep> About the related work, a few remarks: <sep> - The authors do not refer to papers about using auxiliary tasks. Though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones. <sep> - The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. To me, they must read it. <sep> - The authors should also read Laversanne-Finot et al. (2018, CoRL) who learn goal space representations and show an ability to extract independently controllable features from that. <sep> A positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. Also, the results described in Fig. 5 are interesting. A side note is that the authors address in this Figure a problem pointed in Penedones et al (2018) about ""The Leakage Propagation problem"" and that their solution seems more convincing than in the original paper, maybe they should have a look. <sep> But there are also several weaknesses: <sep> - for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. This definitely hampers reproducibility of the work. A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest. <sep> - most importantly, in Section 6.4, 6.5 and 6.6, much too few details are given. Particularly in 6.6, the task is hardly described with a few words. The message a reader can get from this section is not much more than ""we are doing something that works, believe us!"". So the authors should choose between two options: <sep> * either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than ""with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either"". <sep> * or add a huge appendix with all the missing details. <sep> I'm clearly in favor of the first option. <sep> Some more detailed points or questions about the experimental section: <sep> - not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used. <sep> - in Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible? <sep> - in Section 6.3, about the pushing experiment, I would like to argue against the fact that the block position is the important factor and the end-effector position is secundary. Indeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint? <sep> - Globally, although it is interesting, Fig.6 only conveys a quite indirect message about the quality of the learned representation. <sep> - Still in Fig. 6, what is described as ""blue"" appears as violet in the figures and pink in the caption, this does not help when reading for the first time. <sep> - In Section 6.4, Fig.7 a, ARC happens to do better than the oracle. The authors should describe the oracle in more details and discuss why it does not provide a ""perfect"" representation. <sep> - Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. Why insist on this one? And a deeper discussion of the performance of each method would be much more valuable than just showing these curves. <sep> - Section 6.5 is so short that I do not find it useful at all. <sep> - Section 6.6 should be split into the HRL question and the clustering question, as mentioned above. But this only makes sense if the experiments are properly described, as is it is not useful. <sep> Finally, the discussion is rather empty, and would be much more interesting if the experiments had been analyzed in more details. <sep> typos: <sep> p1: that can knows => know p7: euclidean => Euclidean","To borrow the succinct summary from R1, ""the paper suggests a method for generating representations that are linked to goals in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the <sep> policies leading to them are similar."" The reviewers and AC agree that this is a novel and worthy idea. <sep> Concerns about the paper are primarily about the following. <sep> (i) the method already requires good solutions as input, i.e., in the form of goal-conditioned policies, (GCPs) <sep> and the paper claims that these are easy to learn in any case. <sep> As R3 notes, this then begs the question as to why the actionable representations are needed. <sep> (ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and <sep> additional detail. <sep> After much discussion, there is now a fair degree of consensus. While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper. <sep> The AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained <sep> the extent to which training for auxiliary tasks implicitly solve this problem in any case. <sep> The AC also suggests nominating R3 for a best-reviewer award."
"This is an application paper on dense volumetric synthesis of liquids and smoke. Given densely registered 4D implicit surfaces (volumes over time) for a structured scene, a neural-network based model is used to interpolate simulations for novel scene conditions (e.g. position and size of dropped water ball). The interpolation model composes two components -- given these conditions, it first regresses weights combining a set of precomputed deformation fields, and then a second model regresses dense volumetric deformation corrections -- these are helpful as some events are not easily modeled with a set of basis deformations. <sep> I found the paper hard to read at first, since the paper is heavy on terminology, only really understood what is going on when I went through the examples in the appendix, which are helpful and then on a second read the content was clear and appears technically correct. I would advise considering defining in more detail early the problem setup (e.g. Fig 13 was helpful), explain some of the variables in context. <sep> This is primarily an application paper on simulating liquids in controlled scenes using nets and appears novel in that narrow domain. The specific way deformations are composed -- using v_inv to backwards correct basis deformations, following up the mixing of those with a correction model -- is intuitive and is also something I see for the first time. <sep> The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. This was done for Fig 6, but would be nice to also see it numerically in ablation in Fig. 4. Another useful experiment would be to vary the number of bases and/or the resolution of the deformation correction network and see the effects. <sep> More importantly, it would be very helpful is to try this approach for modeling deforming object and body shapes for which there are many datasets (e.g. Shapenet). Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than *CONF*. <sep> ---- Post author feedback comment ---- <sep> I raised my rating to 7 as the paper itself is solid, main concern as another reviewer points out is it may be a bit too specialist for *CONF*. If the AC decides to reject based on this fact I am ok with that as well. <sep> I think it would be helpful to add more ablation (deformation-only results for all cases) and experiments with different numbers of bases in the final version. If that's added it will strengthen the paper.","This paper presents a novel method for synthesizing fluid simulations, constrained to a set of parameterized variations, <sep> such as the size and position of a water ball that is dropped. The results are solid; there is little related <sep> work to compare to, in terms of methods that can ""compute""/recall simulations at that speed. <sep> The method is 2000x faster than the orginal simulations. This comes with the caveats that: <sep> (a) the results are specific to the given set of parameterized environments; the method is learning a <sep> compressed version of the original animations; (b) there is a loss of accuracy, and therefore <sep> also a loss of visual plausibility. <sep> The AC notes that the paper should use the *CONF* format for citations, i.e., ""(foo et al.)"" rather than ""(19)"". <sep> The AC also suggests that limitations should also be clearly documented, i.e., as seen from the <sep> perspective of those working in the fluid simulation domain. <sep> The principle (and only?) contentious issue relates to the suitability of the paper for the *CONF* audience, <sep> given its focus on the specific domain of fluid simulations. The AC is of two minds on this: <sep> (i) the fluid simulation domain has different characteristics to other domains, and thu <sep> understanding the *CONF* audience can benefit from the specific nature of the predictive problems that <sep> come the fluid simulation domain; new problems can drive new methods. There is a loose connection <sep> between the given work and residual nets, and of course res-nets have also been recently reconceptualized as PDEs. <sep> (ii) it's not clear how much the *CONF* audience will get out of the specific solutions being described; <sep> it requires understanding spatial transformer networks and a number of other domain-specific issues. <sep> A problem with this type of paper in terms of graphics/SIGGRAPH is that it can also be seen as ""falling short"" <sep> there, simply because it is not yet competitive in terms of visual quality or the generality of <sep> fluid simulators; it really fulfills a different niche than classical fluid simulators. <sep> The AC leans slightly in favor of acceptance, but is otherwise on the fence."
"Summary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see. The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one. My main concerns are highlighted in points (3) and (4) in the detailed comments below. <sep> Clarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side. Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion. <sep> Originality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments. Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables. The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising. The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update. <sep> Disclaimer: I have not checked the proofs in the appendix. <sep> Detailed comments: <sep> 1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice. Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task. Can you comment on why something like that was not used at least as baseline? <sep> 2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it's harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle. I think it's meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C. It is also not clear in this example how the comparison was done. For instance, are the value functions used, input-dependent? Is the policy input aware? <sep> 3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP. I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting. This might still be information available in hindsight and used in training, but won't be available 'online' -- similar to slack variable, or privileged information at training time.  And in this case it's not clear to me if this would still result in a variance reduction in the policy update. Case 2 has some of that flavour, but restricts z_t to an iid process. Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work. <sep> 4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden. From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables. Thus we are training a policy that should be able to deal with a distribution of inputs z. How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline? Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z? <sep> References: <sep> [1] Schaul, T., Horgan, D., Gregor, K. and Silver, D., 2015, June. Universal value function approximators. In International Conference on Machine Learning (pp. 1312-1320). <sep> [POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly.","This paper proposes an input-dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer's concerns. I recommend acceptance."
"Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. There are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion. What problem is being solved here? Are we describing using latent variables (AE approach) for BMI?  Are we discussing domain adaptation, i.e. handling the nonstationarity that so plagues BMI and array data?  Clearly the issue of stability is being addressed but how?  A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days.  Are we instead attempting to show that a single BMI can be used across multiple days? <sep> This paper is extremely interesting but suffers from lack of focus, rigor, and clarity. <sep> Focus : <sep> AE to RNN to EMG is that the idea to compare vs. Domain adaptation via CCA/KLDM/ADAM. <sep> Of course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate. <sep> Rigor: <sep> What are meaningful comparisons for all for the AE and DA portions? The AE part is strongly related to either to Kao 2017 or Pandarinath 2018 but nothing like that is compared.  The domain adaptation part evokes data augmentation strategies of Sussillo 2016 but that is not compared. <sep> If I were reviewing this manuscript for a biological journal a rigorous standard would be online BMI results in two animals.  Is there a reason why this isn't the standard for *CONF*? Is the idea that non-biological journals / conferences are adequate to vet new ideas before really putting them to the test in a biological journal?  The manuscript is concerned with the vexing problem of BMI stability of time, which seems to be a problem where online testing in two animals would be critical. (I appreciate this is a broader topic relevant to the BMI field beyond just this paper, but it would be helpful to get some thinking on this in the rebuttal). <sep> Clarity : <sep> This paper needs to be pretty seriously clarified.  The mathematical notation is not adequate to the job, nor is the motivation for the varied methodology. I cannot tell if the subscript is for time or for day. Also, what is the difference between z_0 vs. Z_0? I do not know what exactly is going into the AE or the ADAN. <sep> The neural networks are not described to a point where one could reproduce this work. The notation for handling time is inadequate.   E.g. despite repeated readings I cannot tell how time is handled in the auto-encoder, e.g. nxt is vectorized vs feeding n-sized vector one time step at a time? <sep> Questions <sep> What is the point of the latent representation in the AE if it is just fed to an LSTM? Is it to compare to not using it? <sep> Page 3, how precisely is time handled in the AE?  If time is just vectorized, how can one get real-time readouts? In general there is not enough detail to understand what is implemented in the AE. If only one time slice is entered into AE, then it seems clear AE won't be very good because one desires latent representation of the dynamics, not single time slices. <sep> How big is the LSTM used to generate the EMG? <sep> It seems like a the most relevant baseline is to compare to the data perturbation strategies in Sussillo 2016.  If you have an LSTM already up and running to predict EMG, this seems very doable. <sep> Page 4, ""We then use an ADAN to align either the distribution of latent variables or the distributions of the residuals of the reconstructed neural data, the latter a proxy for the alignment of the neural latent variables.""  This sentence is not adequate to explain the concepts of the various distributions, the residuals of reconstructed neural data (where do the residuals come from?), and why is one a proxy for the other.  Please expand this sentence into a few sentences, if necessary to define these concepts for the naive reader. <sep> Page 5, What parameters are minimized in equation (2)? Please expand the top sentence of page 5. <sep> Page 6, top - ""In contrast, when the EMG predictor is  trained simultaneously with the AE…"" Do you mean there is again a loss function defined by both EMG prediction and AE and summed, and then backprop is used to train both in an end-to-end fashion?  Please clarify. <sep> Page 8, How do the AE results and architecture fit into the EMG reconstruction ""BMI"" results? Is that all decoding results are first put through the AE -> LSTM -> EMG pipeline? I.e. your BMI is neural data -> AE -> LSTM -> EMG?  If so, then how does the ADAN / CCA and KLDM fit in?  You first run those three DA algorithms and then pipe it through the BMI? <sep> Page 8, How can you say that the BMI improvement of 6% is meaningful to the BMI user if you did not test the BMI online?","BMIs need per-patient and per-session calibration, and this paper seeks to amend that. Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten-year old approach, but do so using a novel adversarial approach that seems to work. <sep> The reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended. Clinical evaluation is an important next step."
"Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. Periodically compare the validation BLEU score of the multilingual model with that of each individual model, and turn off distillation for language pairs where the multilingual model is better. On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. Supplementary experiments justify the strategy of selectively turning off distillation, and quantify the effect using only the top 8 vocabulary items in distillation. <sep> The main idea makes sense, and the results are very convincing, especially since it appears that hyper-parameters were not tuned extensively (eg, weight of 0.5 on the distillation loss, for all language pairs). Implementation should be very straightforward, especially with the trick of pre-computing top-k probabilities from the teacher model at each corpus position. One small barrier to practical application that the authors fail to acknowledge is the requirement to train individual models, which will at least double training time compared to a single multilingual model. <sep> The main missing experiment is higher-capacity multilingual models, which Johnson et al show to be beneficial in settings with a large number of language pairs. Using a multilingual model of the same (relatively small) size as the individual models as is done here is likely to be suboptimal, especially for the 44-language pair TED setting. A related point is that the corpora used seem to be quite small (eg 4.5M and 1M sentences for WMT Czech and German, respectively, while the available training corpora are closer to 15M and 4.5M). Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful. <sep> Also missing are experiments on out-of-English translation, which would establish the viability of the proposed technique for many-to-many translation via bridging. Out-of-English is a more difficult problem than into-English. I can't see any reason the proposed technique wouldn't also work in this setting, but this remains to be shown. <sep> Although it's great that the technique is shown to work without embellishments, there are a few obvious strategies it would have been interesting to explore, such as making the weight on the distillation loss dependent on the difference in performance between the multilingual and individual models; and allowing for the distillation loss to be turned back on if the performance of the multilingual model starts to drift back down for a particular language pair. I also wondered about the effect of the gradient accumulation strategy in algorithm 1, where individual batches from each language pair are effectively grouped into one giant batch for the purpose of parameter updates. I can see that this could stabilize training, but it would be good to know whether it's crucial for success, especially when the number of language pairs is large. <sep> Further details: <sep> As aforementioned -> As mentioned <sep> (1) 2nd line: Doesn't make sense as written. You need to distinguish the gold y_t from hypothesized ones in the 1() function. <sep> Above (2): is served as -> serves as <sep> 3.2 First paragraph. Since D presumably consists of D^l for all languages l, <sep> L_ALL(D,...) should be a function of teacher parameters theta^l for all languages l rather than just one as written. <sep> In top-K distillation, is the teacher distribution renormalized or simply truncated? <sep> Generalization analysis, pg 8: presumably you are sampling from N(0, sigma^2) - <sep> this should be described as such. <sep> Reference: <sep> Johnson et al, ""Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"" TACL, 2017.",This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.
"This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. It is thorough in its evaluation of both methodological considerations and different datasets. <sep> The main advantage would seem to be a large speedup over MCMC-based methods (Figure 4), which could be of significant value to the phylogenetics community. This point would benefit from more discussion. How do the number of iterations (reported in Figures 3&4, which was done carefully) correspond to wallclock time? Can this new method scale to numbers of sites and sequences that were previously unfeasible? <sep> The main technical contribution is the use of SBNs as variational approximations over tree-space, but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper. Additionally, the issue of estimating the support of the subsplit CPTs needs more discussion. As the authors acknowledge, complete parameterizations of these models scale in a combinatorial way with ""all possible parent-child subsplit pairs"", and they deal with this by shrinking the support up front with various heuristics. It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak. Since VB is often concerned with the limited-data regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is. <sep> Overall, this work is an interesting extension of variational Bayes to a tree-structured inference problem and is thorough in its evaluation. While it is a bit focused on classical inference for *CONF*, it could be interesting both for the VI community and as a significant application advancement. <sep> Other notes: <sep> In table 1, is the point that all methods are basically the same with different variance? This is not clear from the text. What about the variational bounds?","The reviewers lean to accept, and the authors clearly put a significant amount of time into their response. I will also lean to accept. However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down."
"This paper focuses on the problem of convergence in multi-objective optimisation with differentiable losses. This topic is timely and relevant, given the increasing amount of recent work on multi-objective architectures, e.g. GANs, adversarial learning, multi-agent reinforcement learning. The authors focus on stable fixed points (SFP), rather than Nash equilibria, as the solution concept in the entirety of their analysis. Casting the recently proposed LOLA gradient adjustment into a general matrix form, they diagnose an example where the shaping term in LOLA prevents convergence to SFP. They also find that discarding the shaping term leads to an earlier method (which they name ''LA'') with convergence guarantees in two-player two-action games. However, this also loses the opponent shaping ability of LOLA. To address these limitation, the authors propose SOS, which interpolates between LA and LOLA, and dynamically chooses the interpolation coefficient p so that their adjusted gradient preserves LOLA's shaping ability only to the extent allowed by the constraint of moving in LA's direction. The main goal of the paper is to show that SOS converges locally to SFP, and to fixed points only, while avoiding strict saddles. Experiments on synthetic games show that SOS preserves the benefit of LOLA while avoiding its theoretically-predicted issues, and a more complex Gaussian mixture GAN experiment shows SOS is empirically competitive with other gradient adjustment methods. <sep> The main conceptual novelty consists of the dynamic interpolation term to combine advantages of LOLA and LA while avoiding pitfalls of both. The major strength of the paper lies in the clear justification for this interpolation approach. The paper contains strong theoretical results for general differentiable games, and deserves the notice of the *CONF* community if valid. However, I have major concerns with the proof of Theorem 2 (i.e. Theorem D.4 in the appendix), which affects the validity of Corollary 3 and Theorem 4. <sep> In the proof of Theorem D.4: <sep> 1. How does the expression uTM−1GMu have conformable dimensions, when G∈Rd×d while u∈Rd−1? Was any assumption made about the matrix M=(I+αHd)1/2? <sep> 2. In the middle of page 14, a unit vector u∈Sm is defined, but it is not clear what vector space is meant by Sm. <sep> 3. In the second-to-last line of page 14, a quantity S is used but not defined clearly in any preceding part of the proof. Remark D.5 refers to S as the symmetric part of G, and asserts that S is not positive definite. If the quantity S used in the proof is the same non-PD quantity, then S does not have a Cholesky factorisation. So how is Cholesky decomposition conducted at end of page 14? <sep> 4. In the first line of page 15, a quantity A is used but not defined anywhere else in the entire paper. <sep> 5. From the subsequent line, it appears to be the anti-symmetric part of H. Is it correct assumption? If so, H2 is not (ST−AT)(S+A). If you replace it with correct form, whole quantity does not compute to be positive or becomes meaningless. <sep> As Theorem 2 is the crux for all the theoretical advancement presented in the paper, clarifications on above correctness questions is very important for clear acceptance of this work. <sep> While Definition 1 precisely defines differentiable games to have *twice* differentiable losses, why do the authors assume *thrice* differentiable losses at the start of Section 4? <sep> In Section 2.2, the authors make a broad statement that ''Nash equilibria cannot be the right solution concept for multi-agent learning.'' They provide one example where Nash is undesirable (L^1 = L^2 = xy). However, since this example can be viewed as a fully cooperative game with joint loss L = 2xy, it does not support the broader statement that Nash is undesirable in all games. Because this statement directly motivates the authors to focus on stable fixed points, rather than Nash, as the solution concept in their subsequent analysis, it is very important to provide better justification for the claim. <sep> Minor comments: <sep> 1. Under Proposition 1, the authors suddenly speak of ''...the policy being optimal''. Since the author's work pertains to general multi-objective settings, not solely multi-agent reinforcement learning, the word ''policy'' sounds strange in context. <sep> 2. The statement of Proposition B.1, and the concluding line of the derivation, left out a coefficient α that is present in Proposition 1 in the main text. <sep> 3. While the authors claim and prove independence of theoretical results from choice of a and b, are there any practical implications in terms of performance or convergence?","This paper provides interesting results on convergence and stability in general differentiable games. The theory appears to be correct, and the paper reasonably well written. The main concern is in connections to an area of related work that has been omitted, with overly strong statements in the paper that there has been little work for general game dynamics. This is a serious omission, since it calls into question some of the novelty of the results because they have not been adequately placed relative to this work. The authors should incorporate a thorough discussion on relations to this work, and adjust claims about novelty (and potentially even results) based on that literature."
"This work adds to a growing literature on biologically plausible (BP) learning algorithms. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. This seemingly runs counter to the conclusions of Bartunov et al.; while the authors state that their results are ""complementary"", they also state that the findings ""directly conflict"" with the results of Bartunov, concluding that BP algorithms remain viable options for both learning in artificial networks and the brain. <sep> To reach these conclusions the authors report results on a number of experiments. First, they show successful training of a ResNet-18 architecture on ImageNet using sign-symmetry, with their model performing nearly as well as one trained with backpropagation. Next, they demonstrate decent performance on MS COCO object detection using RetinaNet. Finally, they end with a discussion that seeks to explain the differences in their approach and the approach of Batunov et al, and with a potential biological implementation of sign symmetry. <sep> Overall the clarity of the writing is sufficient. The algorithm is properly explained, and there are sufficient citations to reference prior work. The results are generally clear (though there is an incomplete experiment, I agree with the authors that it is unlikely for the preliminary results to change). I believe that there is enough detail for this work to be reproducible. The work is also sufficiently novel in that experiments using sign-symmetry on difficult datasets have not been undertaken, to my knowledge. <sep> Unfortunately, the clarity and rigor of the *scientific argument* is insufficient for a number of reasons. These will be enumerated below. <sep> First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author's conclusions highlight precisely these two points). Unfortunately, the experiments and underlying experimental logic push towards addressing the first question, and use this as evidence towards a conclusion to the second question. More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn't seem to be an important detail with sign-symmetry, for reasons explained below). This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry's merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a ""direct conflict"". To this end, though the authors claim that the conditions on which Bartunov et al tested are ""somewhat restrictive"", this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry's usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments. <sep> Second, the work does not sufficiently weigh the ""degree"" of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. Of course, one doesn't want to go down the road of declaring that ""algorithm A is more plausible than algorithm B!"", but the nuances should at least be seriously discussed if the algorithms are to be properly compared. In backpropagation the feedback connections must be similar in sign and magnitude. Sign-symmetry eliminates the requirement that the connections be similar in magnitude. However, this factor is arguably the least important of the two (the direction of the gradient is more important than the magnitudes), and we are still left with feedback weights that somehow have to tie their sign to their feedforward counterparts, which is not an issue in target propagation or feedback alignment. The authors try to explain away this difficulty with an appeal to molecular biology, which leads into my third point. <sep> Third, the appeal to molecular mechanisms to explain how sign-symmetry can arise is not rigorous. There is a plethora of molecular mechanisms at play in our cells; indeed, there are enough mechanisms to hand-craft *any* sort of circuit one likes. Thus, it is somewhat vacuous to conclude that a particular circuit can be ""easily implemented"" in the brain simply by appealing to a hand-crafted circuit. For this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons X, Y, Z. Unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal. But perhaps most problematic, the argument leaves the problem of sign-switching in the feedforward network to ""future work"". This is perhaps *the most* important problem at play here, and until it is answered, these arguments don't have sufficient impact. <sep> Altogether the scientific argument of this work needs tightening. The tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms. The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms.","This heavily disputed paper discusses a biologically motivated alternative to back-propagation learning. In particular, methods focussing on sign-symmetry rather than weight-symmetry are investigated and, importantly, scaled to large problems. The paper demonstrates the viability of the approach. If nothing else, it instigates a wonderful platform for debate. <sep> The results are convincing and the paper is well-presented. But the biological plausibility of the methods needed for these algorithms can be disputed. In my opinion, these are best tackled in a poster session, following the good practice at neuroscience meetings. <sep> On an aside note, the use of the approach to ResNet should be questioned. The skip-connections in ResNet may be all but biologically relevant."
"The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of ""missing"" (to the generative model) features. <sep> Novelty: <sep> Generative models have a long history of being used to impute missing data. e.g. fritz/absps/ranzato cvpr.pdf,"" target=""_blank"" rel=""nofollow"">http://www.cs.toronto.edu/ fritz/absps/ranzato cvpr.pdf, https://arxiv.org/pdf/1610.04167.pdf, <sep> https://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F] <sep> It is a little difficult to guage what the novelty of this work is. <sep> Clarity <sep> This is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use ""p""). <sep> Experimental Results <sep> The model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot? <sep> Beyond that: <sep> (a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)? <sep> (b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess? <sep> The qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned). <sep> Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed? <sep> Priors on b: <sep> What kind of priors on b did you experiment with?","This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works. <sep> R1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3. <sep> Based on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to *CONF* (https://openreview.net/forum?id=ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre-trained VAE model. <sep> There are still two weaknesses pointed out by R3 that would help improve the paper by addressing them: <sep> 1. The paper does not handle different kinds of missingness beyond missing at random. <sep> 2. VAE model makes the trade-off between computational complexity and accuracy. <sep> Point 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section."
"========\\ <sep> Summary\\ <sep> ========\\ <sep> The paper deals with hyper-parameter optimization of neural networks. The authors formulate the problem as a bilevel optimization problem: minimizing the validation loss over the hyperparameters, subject to the parameters being at the minimum of the training loss. The authors propose an approximation of the so-called best-response function, that maps the hyperparameters to the corresponding optimal parameters (w.r.t the minimization of the training loss), allowing a formulate as a single-level optimization problem and the use gradient descent algorithm. The proposed approximation is based on shifting and scaling the weights and biases of the network. There are no guarantee on its quality except in some very simple cases. The approach assumes a distribution on the hyperparameters, governed by a parameter, which is adapted during the course of the training to achieve a compromise between the flexibility of the best-response function and the quality of its local approximation around the current hyperparameters. The authors show that their approach beats grid-search, random search and Bayesian optimization on the CIFAR-10 and PTB datasets. They point out that the dynamic update of the hyperparameters during the training allows to reach a better performance than any fixed hyperparameter. \\ <sep> ======================\\ <sep> Comments and questions\\ <sep> ======================\\ <sep> Can cross-validation be adapted to this approach? \\ <sep> Can this be used to optimize the learning rate? Which is of course a crucial hyperparameter and that needs an update schedule during the training. \\ <sep> Section 3.2:\\ <sep> ""If the entries are too large, then θ̂ φ will not be flexible enough to capture the best- response over the sampled neighborhood. However, its entries must remain sufficiently large so that θ̂ φ captures the local shape around the current hyperparameter values."" Not clear why -- more explanations would be helpful. \\ <sep> ""minimizing the first term eventually moves all probability mass towards an optimum λ∗ ,resulting in σ = 0"". I can't see how minimizing the first term w.r.t \\phi (as in section ""2.2.Local approximation"") would alter \\sigma. \\ <sep> ""τ must be set carefully to ensure..."". The authors still do not explain how to set \\tau. \\ <sep> Section 3.3: \\ <sep> If the hyperparameter is discrete and falls in Case 2, then REINFORCE gradient estimator is used. What about the quality of this gradient? \\ <sep> Section 5, paragraph Gradient-Based HO: ""differentiating gradient descent"" needs reformulation -- an algorithm cannot be differentiated. \\ <sep> Pros \\ <sep> - The paper is pretty clear \\ <sep> - Generalizes a previous idea and makes it handle discrete hyperparameters and scale better. \\ <sep> - I like the idea of hyperparameters changing dynamically during the training which allows to explore a much larger space than one value \\ <sep> - Although limited, the experimental results are convincing \\ <sep> Cons \\ <sep> - The method itself depends on some parameters and it is not clear how to choose them. Therefore it might be tricky to make it work in practice. I feel like there is a lot of literature around HO but very often people still use the very simple grid/random search, because the alternative methods are often quite complex to implement and make really work. So the fact that the method depends on ""crucial"" parameters but that are not transparently managed may be a big drawback to its applicability. \\ <sep> - No theoretical guarantee on the quality of the used approximation for neural networks \\ <sep> - Does not handle the learning rate which is a crucial hyperparameter (but maybe it could) \\","The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results. Reviewer's concerns seem to be addressed well in rebuttals and extended version of the paper."
"Summary: <sep> This paper introduces equi-normalization (ENorm): a normalization technique that relies on the scaling invariance properties of the ReLU, similarly to Path-SGD. Their method explicitly use this property to balance the weights of the network, without changing the function computed by the network. The main difference with Path-SGD is that the network is explicitly balanced, while Path-SGD uses a regularizer to implicitly balance the network. Since it doesn't rely on mini-batch to normalize the network, Equi-normalization could be a good alternative to BN in small mini-batch regime. The method is validated on 3 tasks (MLP on CIFAR10, CNN on CIFAR10, Reidual Network on ImageNet). <sep> Clarity: <sep> The paper is quite clear, although a bit long (10 pages). The related work section is particularly nice. I really appreciated the ""positioning"" paragraph, which really explains how the method differs from others. <sep> Novelty: <sep> The paper is quite incremental, due to its similarities with Path-SGD. It seems quite close to what Weight Normalization is doing as well (see detailed comments). <sep> Pros and Cons: <sep> + Clearly written <sep> + Clearly motivated <sep> + Nice review of literature <sep> - Quite incremental (close to Path-SGD / Weight Normalization), and missing actual comparison with Weight Normalization, which seems to be the direct competitor of ENorm (see detailed comments) <sep> - Some flaws in the experimental setup (see detailed comments), particularly in the fully-connected experiment. <sep> - Doesn't scale (yet) to deeper architectures, which is precisely where small batch sizes become a problem for BN and thus where ENorm would be needed. <sep> Detailed Comments: <sep> 1. Differences with Weight Normalization: <sep> I have trouble seeing the difference between the proposed method and Weight Normalization (WN), or the more advanced Normalization Propagation (NP). It seems that both methods are performing quite similar normalization: WN reparameterize the network such that ||W[:,j]||_2 = 1, so it seems that you wouldn't need to re-balance the network when using WN. Could the authors elaborate on this? Moreover WN is simple to implement, fast, and also works in the small batches settings as well. Finally, since those methods are quite similar, the authors should compare their method against WN in their experimental setup. <sep> 2. Initialization of the Weights: <sep> The Xavier initialization you are using in the CIFAR10 experiments is designed to work with Tanh activations functions. For ReLUs, one should use the Kaiming initialization, which has been proven to work way better for ReLUs (He et al., 2015a).  This certainly explains the poor performances of your baseline when you increase the number of layers in Figure 4, and probably explains why you need to add a BN layer at the end of your network to help with the training of the baselines. I suggest the authors to re-run the baselines using proper initialization for ReLUs. <sep> 3. Fully-Connected Layers Benchmark: <sep> I think the fully-connected benchmark you used is quite poor. A baseline with 1 layer only reaches ~54 % test accuracy and your method needs a 11 layer model to increase this baseline performance of about ~0.5 % only. The deep autoencoder on MNIST (see e.g. Desjardins et al., Natural neural networks, NIPS 2015), would probably be a better benchmark for fully-connected layers (of course using ReLUs in place of Sigmoids).  It would also reinforce your empirical results by adding a 3rd dataset. <sep> 4. ImageNet Experiment: <sep> Do you use Ghost Batch Normalization in this experiments (i.e. calculating the BN statistics on each GPU separately)? It would certainly explain the poor performances of BN with tiny batch size (32 examples on 8 GPUs means only 8 examples per GPU). <sep> 5. Computation Time performances: <sep> It is stated in the conclusion that ""using ENorm during training adds a much smaller computational overhead than BN or GN …"". I can see that Table 1 gives an overview of the number of elements accessed during normalization, but I do think that a proper plot showing the accuracy versus the wall-clock time would be a better way of showing how your method compares in practice with BN or GN. Moreover, as stated previously, comparison with WN (and / or NP) need to be performed as well, especially because WN and NP are also way faster to compute than BN and GN. <sep> 6. Shortening the paper: <sep> The recommended paper size for *CONF* is 8 pages. Here would be a few pointers that could help you reduce a bit the length of the paper: Introduction and Related Work takes up 3 pages already and I think there is quite some overlap between the 2 (about BN in particular), so there is probably quite a lot of space to gain here if the authors were to reduce a bit the introduction or make the related work a sub-section of the introduction. The ENorm presentation is 4 pages long (which is quite a lot). Section 3.6 might be totally discarded since it is vanilla application of the chain rule. The extension to convolution layers and max pooling could be transferred to the appendix. <sep> Minor Comments: <sep> You should add ""Optimizing neural networks with kronecker-factored approximate curvature"" in the literature review about optimization landscape, as it is an important research direction on natural gradient. <sep> Conclusion: <sep> All in all, I find that this work is a bit too incremental, missing some important comparisons with other techniques and its experimental setup could definitely be improved. Also, the speedup claims should also be supported with empirical experimentation. <sep> Revision: <sep> I thank the authors for the all the extra experiments they performed. The paper looks good to me, and increased my evaluation accordingly.","The proposed ENorm procedure is a normalization scheme for neural nets whereby the weights are rescaled in a way that minimizes the sum of L_p norms while maintaining functional equivalence. An algorithm is given which provably converges to the globally optimal solution. Experiments show it is complementary to, and perhaps slightly better than, other normalization schemes. <sep> Normalization issues are important for DNN training, and normalization schemes like batch norm, weight norm, etc. have the unsatisfying property that they entangle multiple issues such as normalization, stochastic regularization, and effective learning rates. ENorm is a conceptually cleaner (if more algorithmically complicated) approach. It's a nice addition to the set of normalization schemes, and possibly complementary to the existing ones. <sep> After a revision which included various new experiments, the reviewers are generally happy with the paper. While there's still some controversy over whether it's really better than things like batch norm, I think the paper would be worth publishing even if the results came out negative, since it is a very natural idea which took some algorithmic insight in order to actually execute."
"This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve. VIP have been very successful in solving min-max style problems. Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. The authors look at a simple GAN setup where both the generator and the discriminator are linear models. In this case two kinds of gradient updates can be derived. First are simultaneous updates, and the other is alternated updates. The authors show that simultaneous updates are not even bounded and diverge to infinity, whereas alternated updates are more stable and stay bounded, but need not necessarily converge. However, I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case. The second key idea is the use of extra-gradient updates. Extra-gradient updates perform an ""extra"" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the ""extra step"".  This extra-gradient method is a close approximation to Euler's method, though far more computationally efficient.  However, the extragradient step requires one to calculate gradient twice, which can be expensive in large models. For this reason, the authors suggest using gradients from past as the ""extragradient"" in the extragradient method. <sep> For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence.  Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation. Authors also show how one can use these ideas using other first order methods such as ADAM instead of SGD. Experiments are shown on the DCGAN architecture. <sep> On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance","The paper presents a variational inequality perspective on the optimization problem arising in GANs. Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions. In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms. Experiments on CIFAR10 with WGAN etc. show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices. <sep> General convergence results in the context of general non-monotone VIPs is still an open problem for future exploration. The questions raised by the reviewers are well answered. The reviewers unanimously accept the paper for *CONF* publication."
"This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels). <sep> Namely: <sep> (1) the ""Information Bottleneck curve"" cannot be computed with the Information Bottleneck Lagrangian approach (because of optimization landscape issues: optimization of such a piecewise-linear function with a linear penalty will always yield the same optimum whatever the slope of the penalty is [same story as L1 vs. L0]); <sep> (2) there are many solutions to the optimization of the IB Lagrangian for any given compression/performance ratio (i.e. for any given beta in the IB Lagrangian method: I(Y,T)/I(X,T)) and some of them are provably trivial; thus optimizing just the IB Lagrangian does not imply that the solution will be interesting, and better (or complementary) criteria are needed. <sep> Another point discussed also is about the successive layers of perfect classifiers (neural networks), in which I(Y,T) remains constant while I(X,T) decreases. <sep> Pros: <sep> - the paper is well written, mostly self-contained, and easy to read (for someone familiar with information theory); <sep> - all mathematical points are detailed and well explained, with sufficient introduction; <sep> - the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;) <sep> - information bottleneck is a topic of prime interest in the community these days; <sep> - the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning; <sep> - the solution brought to the IB Lagrangian issues is simplistic though efficient (squaring I(X,T) so that it's not linear in I(X,T) anymore). <sep> Cons: <sep> - not much. <sep> Remarks: <sep> - there exist recent papers tackling the information bottleneck concept for neural networks from a variational perspective, which enables them to compute exactly the mutual informations (such as ""Compressing Neural Networks using the Variational Information Bottleneck"" by Dai & al., ICML 2018); I have not seen these papers cited in the article, nor discussed (nor used); I feel it would be appropriate, either in the general literature section, either for discussing how to compute in practice the mutual informations (exact values vs. estimates or lower bounds as here). <sep> - at first reading, I had found the tone of the beginning of the paper (first section) a bit aggressive, though this feeling disappeared later. Maybe rephrase some expressions that might be wrongly perceived? <sep> - About multilabel classification (end of section 2): multilabel classification can still be seen as with deterministic expected outputs, if considered as a task from X to P(Y) (power set of Y, i.e. set of all possible subsets of labels). <sep> - As in practice T is constrained to belong to a particular space of functions (neural network layer with predefined architecture): how does this impact the study? For instance the T_alpha in equation (5) are not reachable anymore; the optimization space for the IB Lagrangian is different; etc. Which properties/conclusions can be kept, and which ones cannot? <sep> - What about sampling on the other part of the IB curve, the horizontal one (same I(Y,T) for various I(X,T))? Would it bring any insight, and how to do it? <sep> - A side remark about applying IB to neural networks: What about neural networks that are not a ""linear"" chain of layers (i.e. most networks now)? i.e. Inception, ResNet, U-nets, etc., where computational flows are parallel, sometimes keeping full information till the end. For instance in a U-net, meant for image processing, features computed at the beginning at a full pixelic resolution are communicated to the last layer. This is not an image classification task though, as predictions are made for each pixel; still, given an input image X, there is only one correct output Y, so, still in the deterministic supervised classification problem.","This paper considers the information bottleneck Lagrangian as a tool for studying deep networks in the common case of supervised learning (predicting label Y from features X) with a deterministic model, and identifies a number of troublesome issues. (1) The information bottleneck curve cannot be recovered by optimizing the Lagrangian for different values of β because in the deterministic case, the IB curve is piecewise linear, not strictly concave. (2) Uninteresting representations can lie on the IB curve, so information bottleneck optimality does not imply that a representation is useful. (3) In a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more. Experiments on MNIST illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic. There was a substantial degree of disagreement between the reviewers of this paper. One reviewer (R3) suggested that all the conclusions of the paper are the consequence of P(X,Y) being degenerate. The authors responded to this criticism in their response and revision quite effectively, in the opinion of the AC. Because R3 failed to participate in the discussion, this review has been discounted in the final decision. The other two reviewers were considerably more positive about the paper, with one (R1) having basically no criticisms and the other (R2) expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios. Following the revision and discussion, R2 expressed general satisfaction with the paper, so the AC is recommending acceptance. The AC thinks that the final paper would be clearer if the authors were to carefully distinguish between ground-truth labels used in training and the labels estimated by the model for a given input. At the moment, the symbol Y appears to be overloaded, standing for both. Perhaps the authors should place a hat over Y when it is standing for estimated labels?"
"This paper proposes training multiple generative models that share a common latent variable, which is learned in a weakly supervised fashion, to achieve high level coordination between multiple agents. Each agent has a separate VRNN model which is conditioned on the agent's own trajectory history as well as the shared latent variable. The model is trained to maximize the ELBO objective and log-likelihood over macro-intent labels. Experimental results are conducted over a basketball gameplay dataset (to model the trajectories of the offensive team members) and a synthetic dataset. The results show that the proposed model is on-par with the baseline models in terms of ELBO while showing that it can model multi-modality better and is preferred more by humans. <sep> In general, the paper is well written and the overall framework captures the essence of the problem that the authors are trying to solve. <sep> Furthermore, incorporating an auxiliary latent variable to model the coordination between multiple agents is interesting. <sep> I have several comments related to the strength of the baselines and contribution of individual components in the proposed model. <sep> Major Comments <sep> - It seems that VRNN-single and VRNN-indep are two models on the far two ends of a spectrum. To understand the contribution of the shared macro-intent, how would an intermediate baseline model where a set of parameters are shared between agents and each agent also has an independent set of parameters perform? This could be accomplished by sharing the parameters of the first layer of GRU networks and learning the second layer parameters independently. <sep> - How is the threshold for macro-intent generation selected? How does this parameter affect the overall performance? Since the smoothness of the segments between two macro-intents depend on this parameter, I am wondering its effect on the learned posterior distribution. <sep> - Rather than using the prediction of the macro-intent RNN as a single global vector (\\hat{g}_t), could using separate vectors for each agent (corresponding blocks of \\hat{g}_t) as inputs to VRNN give the same results? Since the macro-intent RNN is already aware of all the macro-intents, it would be interesting to see if individual macro-intents are sufficient for VRNN to generate corresponding trajectories. <sep> Minor Comments <sep> - Do results in Table (1) come from sampling or using mode of the distributions? How peaked are the learned posterior distributions? <sep> - What is the performance of the macro-intent RNN model? <sep> - In Eq (2), ""<=T"" should be ""<=t"" (as in Eq (11) in Chung 2015). <sep> - In Page 6, bullet point 4: it should be ""except we maximize the mutual information…""","The paper presents generative models to produce multi-agent trajectories. The approach of using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines. <sep> In response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent-specific parameters and further clarifications were made for other main comments (i.e., baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?)."
"This work addresses the problem of learning latent embeddings of high-dimensional time series data. The paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data. To this scope, the study proposes to cluster the data in a latent space estimated through an auto-encoder. The clustering is obtained by leveraging on the idea of self-organising maps (SOM). Within this setting, the data is mapped into a 2D lattice where each coordinate point represents the center of an inner cluster. <sep> This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. <sep> This definition of the problem allows an heuristic for circumventing the non-differentiability of the discrete mapping. The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings. <sep> The experiments are carried out with respect to synthetic 2D time-series, chaotic time-series from dynamical systems, and clinical data. In each case the proposed method shows promising results with respect to the proposed benchmark. <sep> The study presents some interesting methodological and technical ideas. On the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology. Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability). The impact of these modeling choices would deserve more investigation and discussion. <sep> Detailed comments: <sep> - As also stated by the authors, the use of a 2D latent representation is completely arbitrary. It may be true that a 2D embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …). Therefore the feeling is that the proposed structure may be quite ad-hoc, and one may wonder whether the algorithm would still generalise to more complex latent representations. <sep> - Related to the previous comment, the number of latent points seems to be crucial to the performance of the method. However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters. <sep> - The method relies on several cost terms plugged together. While each of them takes care of specific consistency aspects of the model, their mutual relation and balance may be very critical. This is governed by a series of trade-off parameters whose effect is not discussed  nor explored throughout the study. I guess that the optimisation stability may be also quite sensitive to this trade-off, and it would be important to provide more details about this aspect. <sep> - Surprisingly, k-means seems to perform quite well in spite of its simplicity. Also, there is no mention about initialisation and choice of the parameter ""k"". The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. <sep> - Still related to the comparison with respect to the state-of-art, interpretability in time series analysis can be achieved with much lesser assumptions and parameters by using standard approaches such as independent component analysis. I would expect this sort of comparison, especially in case of long-term data such as the one provided in the Lorenz system. <sep> - Clustering of short-term time series, such as the clinical ones, is a challenging task. The feeling is that a highly parametrised model, such as the proposed one,  may still not be superior with respect to classical methods, such as the mixture of linear regressions. This sort of comparison would be quite informative to appreciate the real value of the proposed methodology.","This paper combines probabilistic models, VAEs, and self-organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time-series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization. <sep> The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper-parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results. <sep> The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated. <sep> Thus, the reviewers felt the paper should be accepted."
"This manuscript describes a deep convolutional neural network for assigning proteins to subcellular compartments on the basis of microscopy images. <sep> Positive points: <sep> - This is an important, well-studied problem. <sep> - The results appear to improve significantly on the state of the art. <sep> - The experimental comparison is quite extensive, including reimplementations of four, competing state-of-the-art methods, and lots of details about how the comparisons were carried out. <sep> - The manuscript also includes a human-computer competition, which the computer soundly wins. <sep> - The manuscript is written very clearly. <sep> Concerns: <sep> There is not much here in the way of new machine learning methods. <sep> The authors describe a particular neural network architecture <sep> (""GapNet-PL"") and show empirical evidence that it performs well on a particular dataset.  No claims are made about the generalizability of the particular model architecture used here to other datasets or other tasks. <sep> A significant concern is one that is common to much of the deep learning literature these days, namely, that the manuscript fails to separate model development from model validation. We are told only about the final model that the authors propose here, with no discussion of how the model was arrived at.  The concern here is that, <sep> in all likelihood, the authors had to try various model topologies, <sep> training strategies, etc., before settling on this particular setup. <sep> If all of this was done on the same train/validation/test split, then there is a risk of overfitting. <sep> The dataset used here is not new; it was the basis for a competition carried out previously.  It is therefore somewhat strange that the authors chose to report only the results from their reimplementations of competing methods.  There is a risk that the authors' <sep> reimplementations involve some suboptimal choices, relative to the methods used by the originators of those methods. <sep> Another concern is the potential circularity of the labels.  At one point, we are told that ""Most importantly, these labels have not been derived from the given microscopy images, but from other biotechnologies such as microarrays or from literature.""  However, <sep> earlier we are told that the labels come from ""a large battery of biotechnologies and approaches, such as microarrays, confocal microscopy, knowledge from literature, bioinformatics predictions and additional experimental evidence, such as western blots, or small interfering RNA knockdowns.""  The concern is that, to the extent that the labels are due to bioinformatics predictions, then we may simply be learning to re-create some other image processing tool. <sep> The manuscript contains a fair amount of biology jargon (western blots, small interfering RNA knockdowns, antibodies, Hoechst staining, <sep> etc.) that will not be understandable to a typical *CONF* reader. <sep> At the end, I think it would be instructive to show some examples where the human expert and the network disagreed. <sep> Minor: <sep> p. 2: ""automatic detection of malaria"" -- from images of what? <sep> p. 2: Put a semicolon before ""however"" and a comma after. <sep> p. 2: Change ""Linear Discriminant"" to ""linear discriminant."" Also, remove the abbreviations (SVM and LDA), since they are never used again in this manuscript. <sep> p. 5: Delete comma in ""assumption, that."" <sep> p. 8: ""nearly perfect"" -> ""nearly perfectly"" <sep> The confusion matrices in Figure 5 should not be row normalized -- <sep> just report raw counts.  Also, it would be better to order the classes so that confusable ones are nearby in the list.","The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the *CONF* call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well-written and executed, I would recommend it for acceptance."
"Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively. This time-varying operator replaces the traditional max operator. The authors show empirical performance gains of DBS+Q-learning over Q-learning in a gridworld and DBS+DQN over DQN on Atari games. <sep> Novelty: (1) The error bound of value iteration with the Boltzmann softmax operator and convergence & convergence rate results in this setting seem novel. (2) The novelty of the dynamic Boltzmann operator is somewhat thin, as (Singh et al. 2000) show that a dynamic weighting of the Boltzmann operator achieves convergence to the optimal value function in SARSA(0). In that work, the weighting is state-dependent, so the main algorithmic novelty in this paper is removing the dependence on state visitation for the beta parameter by making it solely dependent on time. A question for the authors: How does the proof in this work relate to / differ from the convergence proofs in (Singh et al. 2000)? <sep> Clarity: In the DBS Q-learning algorithm, it is unclear under which policy actions are selected, e.g. using epsilon-greedy/epsilon-Boltzmann versus using the Boltzmann distribution applied to the Q(s, a) values. If the Boltzmann distribution is used then the algorithm that is presented is in fact expected SARSA and not Q-learning. The paper would benefit from making this clear. <sep> Soundness: (1) The proof of Theorem 4 implicitly assumes that all states are visited infinitely often, which is not necessarily true with the given algorithm (if the policy used to select actions is the Boltzmann policy). (2) The proof of Theorem 1 uses the fact that |L(Q) - max(Q)| <= log(|A|) / beta, which is not immediately clear from the result cited in McKay (2003). (3) The paper claims in the introduction that ""the non-expansive property is vital to guarantee … the convergence of the learning algorithm."" This is not necessarily the case -- see Bellemare et al., Increasing the Action Gap: New Operators for Reinforcement Learning, 2016. <sep> Quality: (1) I appreciate that the authors evaluated their method on the suite of 49 Atari games. This said, the increase in median performance is relatively small, the delta being about half that of the increase due to double DQN. The improvement in mean score in great part stems from a large improvement occurs on Atlantis. <sep> There are also a number of experimental details that are missing. Is the only change from DQN the change in update rule, while keeping the epsilon-greedy rule? In this case, I find a disconnect between the stated goal (to trade off exploration and exploitation) and the results. Why would we expect the Boltzmann softmax to work better when combined to epsilon-greedy? If not, can you give more details e.g. how beta was annealed over time, etc.? <sep> Finally, can you briefly compare your algorithm to the temperature scheduling method described in Fox et al., Taming the Noise in Reinforcement Learning via Soft Updates, 2016? <sep> Additional Comments: <sep> (1) It would be helpful to have Atari results provided in raw game scores in addition to the human-normalized scores (Figure 5). (2) The human normalized scores listed in Figure 5 for DQN are different than the ones listed in the Double DQN paper (Van Hasselt et al, 2016). (3) For the DBS-DQN algorithm, the authors set beta_t = ct^2 - how is the value of c determined? (4) Text in legends and axes of Figure 1 and Figure 2 plots is very small. (5) Typo: citation for MacKay - Information Theory, Inference and Learning Algorithms - author name listed twice. <sep> Similarly, if the main contribution is DBS, it would be interesting to have a more in-depth empirical analysis of the method -- how does performance (in Atari or otherwise) vary with the temperature schedule, how exploration is affected, etc.? <sep> After reading the other reviews and responses, I still think the paper needs further improvement before it can published.","Pros: <sep> - a method that obtains convergence results using a using time-dependent (not fixed or state-dependent) softmax temperature. <sep> Cons: <sep> - theoretical contribution is not very novel <sep> - some theoretical results are dubious <sep> - mismatch of Boltzmann updates and epsilon-greedy exploration <sep> - the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf -- and consequently the reviewers did not change their scores. <sep> The reviewers agree that the paper should be rejected in the submitted form."
"[Summary] <sep> This paper proposes a Graph-Sequence-to-Sequence (GraphSeq2Seq) model to fuse the dependency graph among words into the traditional Seq2Seq framework. <sep> [clarity] <sep> This paper is basically well written though there are several grammatical errors (I guess the authors can fix them). <sep> Motivation and goal are clear. <sep> [originality] <sep> Several previous methods have already tackled to integrate graph structures into seq2seq models. <sep> Therefore, from this perspective, this study is incremental rather than innovative. <sep> However, the core idea of the proposed method, that is, combining the word representation, sub-graph state, incoming and outgoing representations seems to be novel. <sep> [significance] <sep> The experimental setting used in this paper is slightly out of the current main stream of NMT research. <sep> For example, the current top-line NMT systems uses subword unit for input and output sentences, but this paper doesn't. <sep> Moreover, the experiments were performed only on the very small datasets, IWSLT-2014 and 2015, which have at most 153K training parallel sentences. <sep> Therefore, it is unclear whether the proposed method has essential effectiveness to improve the performance on the top-line NMT baselines. <sep> Comparing on the small datasets, the proposed method seems to significantly improve the performance over current best results of NPMT+LM. <sep> Overall, I like the idea of utilizing sub-graphs for simplicity and saving the computational cost to encode a structural (grammatical or semantic) information. <sep> However, I really wonder if this type of technique really works well on the large training datasets...","This paper proposes a new method for graph representation in sequence-to-sequence models and validates its results on several tasks. The overall results are relatively strong. <sep> Overall, the reviewers thought this was a reasonable contribution if somewhat incremental. In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small. In addition, as far as I can tell all comparisons with other graph-based baselines actually aren't implemented in the same toolkit with the same hyperparameters, so it's a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences. <sep> I think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at *CONF* this year I am leaning in favor of the other very good papers in my area."
"The authors propose a new method for constructing adversarial examples called MarginAttack. The method is inspired by Rosen's algorithm, a classical algorithm in constrained optimization. At its core, Rosen's algorithm (instantiated for adversarial examples) alternates between moving towards the set of misclassified points and moving towards the original data point (while ensuring that we do not move too far away from the set of misclassified points). The authors provide theoretical guarantees (local convergence) and a broad set of experiments. The experiments show that MarginAttack finds adversarial examples with small distortion (as good as the baselines or slightly better), and that the algorithm runs faster than the Carlini-Wagner (CW) baseline (but slower than other methods). <sep> The authors make a distinction between ""fixed perturbation"" attacks and ""zero confidence"" attacks. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Method such as projected gradient descent fall into the ""fixed perturbation"" category, while MarginAttack and CW belong to the ""zero confidence"" category. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Indeed, their results show that MarginAttack is 3x - 5x faster than CW and sometimes achieves smaller perturbations. <sep> First of all, I would like to emphasize that the authors conducted a thorough experimental study on multiple datasets using multiple baseline algorithms. Unfortunately, the comparison to CW and PGD still leaves some questions in my opinion: <sep> - The authors state that CW does an internal binary search over the Lagrangian multiplier, and that this search goes for up to 10 steps. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters. To the best of my knowledge, the CW implementation in Cleverhans is specifically set up so that the user does not need to tune a large number of hyperparameters (the implementation accepts a running time overhead to achieve this). Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search. <sep> - The authors explicitly state that the step sizes for CW were tuned for best performance, but do not mention this for PGD. For a fair comparison, the step sizes used for PGD should also be (approximately) tuned. Moreover, it is not clear why PGD is only used for an l_inf comparison and not a l_2 comparison. <sep> - In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). <sep> I would be grateful if the authors could provide their view on these points. Until then, I will assign a rating of 5 since tuning the parameters of optimization algorithms is crucial for a fair comparison. <sep> Additional comments: <sep> - In the introduction, the authors equate white-box attacks with access to gradient information. But generally a white-box attack is understood as an attack that has arbitrary access to the target network. It may be helpful for the reader to clarify this. <sep> - In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. So it is not clear that there is a large gap in difficulty. Moreover, the authors state that fixed perturbation attacks often come with theoretical guarantees. But to the best of my knowledge, there is no comprehensive theory that describes when a fixed perturbation attack should be expected to succeed in attacking a commonly used neural network. <sep> - On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. Why is that? <sep> - The authors state that JSMA (Papernot et al., 2016) is one of the earliest works that use gradient information for constructing adversarial examples. However, L-BFGS as employed by Szegedy et al., 2013 also uses gradient information. Moreover, the authors may want to cite the work of Biggio et al. from 2013 (see the survey https://arxiv.org/abs/1712.03141). <sep> - Since all distances referred to by d(x, y) seem to be norms (and the paper relies on the existence of dual norms), it may be more clear for the reader to use the norm notation || . || from the beginning.","The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments. <sep> In the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side. <sep> The comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence. <sep> The referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold. <sep> Although the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time."
"The paper introduces a spectral regularization with the aim of obtaining representations that are easier to interpret. <sep> Some sentences are often confusing and, in general, clarity needs to be improved. <sep> The motivation of the work is not very strong in my opinion, in particular by adding such a prior the space of possible solutions greatly shrinks and I am afraid that interesting solutions will be lost. I think one should focus on properties rather than visual inspection. <sep> Also, isn't it that if we can clearly see the pattern, perhaps that pattern is linear and of easy discovery also by simpler models? <sep> More importantly, it seems that all experiments are performed on tasks where the underlying structure is known, however this is almost never the case in practice. <sep> Assuming one uses the proposed spectral regularization, how would one interpret it in such cases? <sep> In section 2 please clarify the paragraph on bounded Lp norm. <sep> I am sorry but why isn't there a relation, for convolutional nets, <sep> between neurons in different channels? Each element in the feature map represents the input surrounding that location in a k dimensional space. <sep> The authors state that the usual bottleneck for autoencoders is composed of 2/3 <sep> neurons, this is simply not true. There has been extensive work on overcomplete representations that shows that is better to have many more dimensions but only few degrees of freedom. <sep> The spectral bottleneck should cite VQVAE as the approach is very similar and the authors should compare to it. <sep> For the topological inference experiment it is assumed that one knows the structure, <sep> but how to address the more general problem? <sep> More practically, the regularization enforces smoothing (if few eigenfunctions are used, which is never explained in the paper) between connected nodes, did the authors try to have a simple L2 penalty instead? E.g. minimize the difference between activations in the group. <sep> Regarding the capsule network example, when you write that without regularization each digit responds differently to perturbation of the same dimension, isn't it possibly true only up to a, unknown, permutation of the neurons? <sep> To summarize, while the idea sounds interesting, I miss to find the easy interpretability of results and also the overall motivation sounds a bit weak. <sep> More importantly the selection of W, crucial for defining structure, is not discussed at all in the paper. <sep> Experiments are performed on toy examples only whereas here, given that we can possibly interpret the results I would have liked something more involved to better show that this kind of interpretability is needed. <sep> Missing cites: <sep> [1] van den Oord et al, Neural Discrete Representation Learning. <sep> [2] Koutnik et al, Evolving neural networks in compressed weight space.","The work presents a method of imposing harmonic structural regularizations to layers of a neural network. While the idea is interesting, the reviewers point out multiple issues. <sep> Pros: <sep> + Interesting method <sep> + Hidden layer coherence tends to improve <sep> Cons: <sep> - Deficient comparisons to baselines or context with other works. <sep> - Insufficient assessment of impact to model performance. <sep> - Lack of strategy to select regularizers <sep> - Lack of evaluation on more realistic datasets"
"This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance of the classifier output from the encoding of the correct class. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient. Hence I consider it below the *CONF* bar. <sep> I find the approach weakly motivated. The argument in Figure 1 is very hand-wavy with no clear experimental or theoretical support. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture. <sep> Moreover, the approach is not fundamentally different from standard networks with cross-entropy training. One can consider adding an extra layer (with number of neurons equal to the encoding vector dimensions) and keeping the weights of these neurons fixed (the output weights are essentially the encoding dictionary). Then training with cross-entropy is increasing the inner product with these vectors. This is qualitatively very similar to the proposed approach of this paper. Is there a benefit from explicitly considering the encoding vectors? <sep> Moreover, why is the length of the encoding vectors important from a conceptual point of view? As far as I can tell, this is simply encouraging the output of the network to be large in norm. This could be leading to gradient masking, similar to the phenomena observed for defensive distillation. <sep> I find the proposed approach to watermark evasion interesting. However I consider it orthogonal to the rest of the results so it is hard to consider it as a contribution to the main point of the paper. <sep> Figure 3 is missing white-box evaluation of RO classifiers. Is this on purpose? It is important to understand if the claimed improvement in robustness actually stems from RO rather than mostly from combining it with adversarial training. <sep> The authors report an increase in white-box adversarial robustness. However, I don't believe that the evaluation of their method is thorough. There are plenty of examples by now where PGD has not been sufficient to evaluate the ground-truth robustness of a model. This can distort the relative robustness of different approaches. Given that the increase from baselines in white-box robustness is relatively small (<10% for most datasets) a much more thorough evaluation is required to conclusively demonstrate the benefit of this method. For instance, applying the SPSA attack from Uesato et al. (2018, https://arxiv.org/abs/1802.05666) or a variant of the CW (https://arxiv.org/abs/1608.04644) attack adapted to the particular method used. <sep> As an additional point of concern emphasizing this issue, the authors present the results of Kannan et al. (2018) as state-of-the-art. So far, there is no conclusive evidence about ALP improving the robustness of neural networks beyond adversarial training. The original paper was found to be not as robust as claimed and retracted from NIPS. A similar paper reporting ALP to improve robustness in smaller datasets (CIFAR10) was submitted to *CONF* (https://openreview.net/forum?id=Bylj6oC5K7) but was withdrawn after the authors performed additional experiments. The fact that the authors find the approach of Kannan et al. (2018) to offer an increase over the robustness of Madry et al. (2017) thus raises concerns about the reliability of the evaluation. <sep> Other comments: <sep> -- When the authors perform PGD, what is exactly the loss it is applied on? Is it clear that this is the optimal loss to use when attacking RO classifiers?","This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme. The paper is well-written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns."
"This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. Overall, I found the writing very clear, the main idea sound, and paper generally well executed, but I have serious concerns about the significance of the contributions that lead me to recommend rejection. It would be very useful to me if the authors would provide a concise list of what they consider the main contributions to be and why they are significant. As I see it, the paper does three main things: <sep> 1. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions. The simplifications and gradient derivations are well known and appear in many places (e.g. http://jonathan-huang.org/research/dirichlet/dirichlet.pdf, https://arxiv.org/pdf/1405.0099.pdf) and should not be considered contributions in the age of automatic differentiation (see Justin Domke's blog post on autodiff). <sep> 2. In section 3, the authors consider the unique challenges of using the proposed networks. They propose targeted activation functions that will improve the stability of learning. I found this to be the most interesting portion of the paper and the most significant contribution. Unfortunately, it is short on details and empirical results are referenced that do not appear in the paper (i.e. the second to last paragraph on page 5). If I were to rewrite this paper, I would focus on answering the question ""What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?"", replacing section 2 with an expanded section 3. <sep> 3. In section 4, the authors evaluate the proposed networks on a collection of synthetic and real tasks. In the end, the results are mixed, with the Dirichlet network performing best on the XENON1T task and the standard softmax network performing best on the CIFAR-100 task. In general, I don't mind mixed results and I appreciate that the authors included both sets of experiments; however, it is important that there is a convincing argument for why one would prefer the proposed solution even when accuracy is the same (e.g. it is faster, it is interpretable, etc.). The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. This may be true, but they only perform evaluations on tasks where the primary goal is accuracy. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that. <sep> In summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or (2) present a convincing evaluation that strongly motivates the proposed model's use or that provides some novel insight into the model's behavior. I think that the authors are on their way to achieving (1), but do not achieve (2). I would suggest finding an application that requires uncertainty estimates for the distribution and centering the paper around that application. <sep> Minor comments: <sep> - Figure 2 (right) should include a y-axis label (e.g. ""parameter value""). <sep> - In Figure 3 (right), it is not obvious what the ""Sigmoid"" line corresponds to. <sep> - It is not clear what the authors are trying to show in section 4.1. The EL activation function is smooth and monotone and the likelihood is convex, so there should be no question that the distribution will concentrate around y. <sep> - Section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.","This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet-Multinomial likelihood. This paper is clearly written with a sound main idea. However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. All the reviewers therefore considered this paper to be of limited novelty. Reviewer 2 also had a concern about the mixed experimental results of the proposed method. <sep> Reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions."
"The paper 'Generative model based on minimizing exact empirical Wasserstein distance' proposes a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context. <sep> Comparisons with other variants of Wasserstein GAN is proposed on MNIST. <sep> I see little novelty in the paper. The derivation of the primal version of the problem is already given in <sep> Cuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693). <sep> Using optimal transport computed on batches rather the on the whole dataset is already used in (among others) <sep> Genevay, A., Peyré, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS <sep> Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV <sep> Also, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on batches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very well described in the paper (Section 3): <sep> https://openreview.net/pdf?id=S1m6h21Cb <sep> Computing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be proved and discussed. <sep> Finally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered). <sep> Typos: <sep> Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup","This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions. <sep> As the reviewers point out, the primal approach has been studied by other papers (which this submission doesn't cite, even in the revision), and suffers from a well-known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don't think this work is ready for publication in *CONF*."
"This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. The paper well describes the problem of the current LSTM, especially focusing on the recurrent connection matrix operations, which is a bottle neck in this scenario, and introduces variants of RNNs (e.g., QRNN). Also these variants may not yield enough performance compared with LSTM, but 1-D convolution and/or deep structure helps to avoid the degradation. One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. Therefore, the paper's claim on CTC is not along with the current application trends. (It may be changed near future, but still hybrid systems are dominant). For example, the WSJ WER performance listed in Table 3 is easily obtained by a simple feed-forward DNN in the hybrid system. The latest Lattice free MMI with TDNN can achieve better performance (~2.X% WER), and this decoding is quite fast compared with LSTM. The authors should consider this current situation of state-of-the-art speech recognition. Also, the techniques described in the paper are all based on existing techniques, and the paper lacks the technical novelty. <sep> Other comments: <sep> - in Abstract and the first part of Introduction: as I mentioned above, CTC based character-prediction modeling is not a major acoustic model. <sep> - The paper needs some discussions about TDNN, which is a major acoustic modeling (fast and accurate) in Kaldi <sep> - p.4 first line ""and  represents element-wise multiplication"": The element-wise multiplication operation was first appeared in Eq. (1), and it should be explained there. <sep> - Section 3.2: I actually don't fully understand the claims of this experiment based on TIMIT, as it is phoneme recognition, and not directly related to the real application, which is the main target of this paper I think. My suggestion is to place these TIMIT based experiments as a preliminary experiment to investigate the variants of RNN or gated CNN before the WSJ experiments. (I did not say that Section 3.2 is useless. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.)","In this work, the authors conduct experiments using variants of RNNs and Gated CNNs on a speech recognition task, motivated by the goal of reducing the computational requirements when deploying these models on mobile devices. <sep> While this is an important concern for practical deployment of ASR systems, the main concerns expressed by the reviewers is that the work lacks novelty. Further, the authors choice to investigate CTC based systems which predict characters. These models are not state-of-the-art for ASR, and as such it is hard to judge the impact of this work on a state-of-the-art embedded ASR system. Finally, it would be beneficial to replicate results on a much larger corpus such as Librispeech or Switchboard. Based on the unanimous decision from the reviewers, the AC agrees that the work, in the present form, should be rejected."
"The authors propose several techniques to speed up the previously proposed Neural Theorem Prover approach. The techniques are evaluated via empirical results on several benchmark datasets. <sep> Learning interpretable models is an important topic and the results here are interesting and valuable to the community. However, I feel that the paper in its current form is not yet ready for publication in *CONF*, for the following reasons: <sep> 1) The authors propose three improvements. The first is a speed-up through nearest neighbor search instead of a brute-force search. This is the most elaborated section out of the three, yet seems like the most trivial -- unless the authors can provide an analytical bound on the loss in ntp score w.r.t the neighborhood size. It is a standard and well-known technique to restrict the search to a neighborhood, widely used in any applications of word embedding (e.g. in Khot et el's Markov Logic Networks for Natural Language Question Answering). The attention mechanism (essentially reducing the model capacity) is also well-known but its effect in this particular framework is not properly elaborated. The same can be said for the use of mentions. <sep> 2) The section on experiment results seems a bit rushed -- the authors did mention some last-minute discovery that may affect some of the presented results. The section can be a little hard to parse. In particular, it would be useful for the authors to focus on providing more insights on how the proposed techniques improve the results, and in what ways. <sep> 3) Section 2 on the NTP framework is not very helpful for a reader that has not read the previous paper on NTP (in particular, the part on training and rule learning). For a reader that has done so, the section feels redundant.","This paper focuses on scaling up neural theorem provers, a link prediction system that combines backward chaining with neural embedding of facts, but does not scale to most real-world knowledge bases. The authors introduce a nearest-neighbor search-based method to reduce the time/space complexity, along with an attention mechanism that improves the training. With these extensions, they scale NTP to modern benchmarks for the task, including ones that combine text and knowledge bases, thus providing explanations for such models. <sep> The reviewers and the AC note the following as the primary concerns of the paper: (1) the novelty of the contributions is somewhat limited, as nearest neighbor search and attention are both well-known strategies, as is embedding text+facts jointly, (2) there are several issues in the evaluation, in particular around analysis of benefits of the proposed work on new datasets. There were a number of other potential weaknesses, such the performance on some benchmarks (Fb15k) and clarity and writing quality of a few sections. <sep> The authors provided significant revisions to the paper that addressed many of the clarity and evaluation concerns, along with providing sufficient comments to better contextualize some of the concerns. However, the concerns with novelty and analysis of the results still hold. Reviewer 3 mentions that it is still unclear in the discussion why the accuracy of the proposed approach matches/outperforms that of NTP, i.e. why is there not a tradeoff. Reviewer 4 also finds the analysis lacking, and feels that the differences between the proposed work and the single-link approaches, in terms of where each excels, are described in insufficient detail. Reviewer 4 focused more on the simplicity of the text encoding, which restricts the novelty as more sophisticated text embeddings approaches are commonplace. <sep> Overall, the reviewers raised different concerns, and although all of them appreciated the need for this work and the revisions provided by the authors, ultimately feel that the paper did not quite meet the bar."
"This paper aims for open-domain question answering with distant supervision. First, the authors proposed an aggregation-based openQA model with sentence discriminator and sentence reader. Second, they use a semantic labeler to handle distant supervision problem by utilizing other span supervision tasks, and propose two different denoising methods. They run experiments on 3 open-domain QA datasets and achieve SOTA. <sep> Strengths <sep> 1) Their semantic labeler and exploration of two different denoising methods are interesting and meaningful. <sep> 2) They conducted experiments on 3 widely-used open-domain datasets, and the performance gain is impressive. <sep> Weakness <sep> Although there is an impressive performance gain, the contribution of the paper seems to be marginal. <sep> 1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot. Also, the architecture of the models in this paper are almost identical to Chen et al (ACL 2017) and Lin et al (ACL 2018). <sep> 2) Thus, the contribution is more on semantic labeler and denoising method. However, this contribution is marginal as well since its role is almost the same as sentence discriminator plus pretraining methods which have widely used already. <sep> Questions <sep> 1) What exactly is the difference between semantic labeler and sentence discriminator? For me, it seems like both of them label each sentence `yes` or `no`. My thought is sentence discriminator is only trained on the target dataset (distant supervision dataset) while semantic labeler is also trained (either jointly or separately) trained on the source dataset (span supervision dataset). (If my thought is wrong, please let me know, I would like to update my score.) <sep> 2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. Similarly, Min et al (ACL 2018) have pretrained both QA model and sentence selector on SQuAD. While I think pretraining sentence selector on SQuAD is almost identical to sentence labeler with SSL method, could you give exact comparison of these different methods? For example, remove sentence labeler, and pretrain both sentence discriminator and reader on SQuAD, or jointly train them on SQuAD & target dataset. <sep> Marginal comments <sep> 1) At the beginning of Section 2.4.1, it says the semantic labeler is able to transfer knowledge from the span supervised data — however, the authors should be careful since people usually refers to `knowledge` as an external knowledge. This method is more like better learning of accurate sentence selection, not transferring knowledge. <sep> 2) Please mention the TriviaQA data you used is Wikipedia domain, since there are two different domains (Wikipedia and Web). <sep> 3) In References section, the conference venues in many papers are omitted. <sep> Overall comments <sep> The paper explored several different methods to deal with distant supervision via sentence labeling, and I really appreciate their efforts. While the result is impressive, the idea in the paper is similar to the methods that have widely used already.","This paper presents a model for question answering, where the idea is to have a collaborative model that aligns queries and sentences on a small supervised dataset and also uses semi-supervised information from a weakly supervised corpus to answer open domain questions resulting in short answer spans. <sep> The main criticism of the paper is regarding its novelty, and reviewers cite the similarities with prior work such as Chen et al. and Min et al. There is relative consensus between the reviewers that further work using the semi-supervised outlook with stronger results could strengthen the paper further."
"========= Summary ========= <sep> The authors propose ""Double Neural CFR"", which uses neural network function approximation in place of the tabular update in CFR. CFR is the leading method for finding equilibria in imperfect information games. However it is typically employed with a tabular policy, limiting its applicability large games. Typically, hand-crafted abstractions are employed for games that are too large for exact tabular solutions. Function approximation could remove the necessity for hand-crafted abstractions and allow CFR to scale to larger problems. <sep> The DN-CFR algorithm roughly consists of: <sep> - start with an arbitrary vmodel_0, smodel_0 <sep> for t = 0,1,2,3: <sep> - collect a ""batch"" of (infoset I, immediate counterfactual value v_I) samples by traversal against vmodel_t (as well as I, strategy samples) <sep> - train a new network vmodel_{t+1} on this ""batch"" with y=(v_I + vmodel_t(I)) and MSE loss <sep> - similarly for (I, strategy) <sep> - return smodel_t <sep> The authors also propose a novel MC samping strategy this is a mixture between outcome and external sampling. <sep> DN-CFR is evaluated on two games: a variant of Leduc hold-em with stack size 5, and one-card poker with 5 cards. If I understand correctly, these games have <10,000 and <100 infosets, respectively. The authors show that DN-CFR achieves similar convergence rates to tabular CFR, and outperform NFSP variants. <sep> ========== Comments ======== <sep> The authors are exploring an important problem that is of great interest to the IIG community. Their application of NN function approximation is reasonable and mostly theoretically well-grounded (but see below), I think it's on the right track. However, the games that are used for evaluation are very small, in fact I believe they have fewer states than the number of parameters in their network (the number of network parameters is not provided but I assume >1000). As a result, the NN is not providing any compression or generalization, and I would expect that the network can memorize the training set data exactly, i.e. predict the exact mean counterfactual value for each infoset over the data. If that's true, then DN-CFR is essentially exactly replicating tabular CFR (the approximation serves no purpose). <sep> As a result, in my opinion this work fails to address the important challenges for function approximation in CFR, namely: <sep> - Can function approximation allow for *generalization across infosets* in order to reduce sample complexity of CFR (i.e. an unsupervised abstraction)? Are specific network architectures required for good generalization? <sep> - The magnitude of counterfactual regrets in the support of the equilibrium decays to zero relative to dominated actions. Are NN models capable to estimating the regrets accurately enough to converge to a good strategy? <sep> - Are optimization methods able to deal with the high variance in large IIGs? <sep> - Since each successive approximation is being trained from the previous NN, does this cause errors to accumulate? <sep> - How do approximation errors accumulate across CFR iterations? <sep> - Is minimizing MSE loss sufficient to approximate the strategy well? (since the mapping from regrets -> strategy is non-linear) <sep> I believe there is also a theoretical problem with this algorithm. In Eq. 8, they minimize the loss of CF value predictions *over the distribution of infosets in the last CFR step (""batch"")*. However, this distribution may change between CFR iterations, for example if the strat_t folds 2-3 on the preflop then flop infosets with 2-3 hole cards will never be observed on iteration t+1. As a result, the NN loss will only be minimized over infosets observed in the last iteration - so the network will ""forget"" the regrets for all other infosets. I think this issue does not arise in these toy games because all infosets are observed at each iteration, but this is certainly not the case in real games. <sep> There are a number of ways that this issue could be addressed (e.g. train on historical infosets rather than the current batch, etc.) These would need to be explored. <sep> I would recommend that the authors evaluate on more complex games to answer the important questions stated above and resubmit. I think this would also make an excellent workshop submission in its current state as it contains many interesting ideas. <sep> Detailed comments: <sep> ""... the original CFR only works for discrete stand and action spaces..."": Are the authors implying that DN-CFR addresses this limitation? <sep> ""Moravk"" -> ""Moravcik"" <sep> ""...these methods do not explicitly take into account the hidden information in a game..."" Could you clarify? Is your point that these methods operate on the normal form rather than extensive form game? <sep> ""care algorithm design"" -> ""careful algorithm design"" <sep> The paragraph starting ""In standard RNN..."" should be reduced or moved to appendix. The exact NN architecture is not central to the ideas, and there are no experimental comparison with other architectures so we have no evidence that the architecture is relevant.","The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach. The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples). The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy. The paper should better discuss this more, even if it is not possible to provide theory. The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement)."
"Contribution: <sep> - Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) . <sep> - carried various classification approaches of the dataset (labelled) images in two steps: <sep> - Feature extraction (Scale Invariant Feature Transform with the Bag-of-Visual-Words approach as local feature extractor, and the Gray-level Co-occurrence Matrix and Local Binary Patterns as global feature extractor) then <sep> - Classification of the diffraction images is carried with three approaches. Two using images described by extracted features (from the previous feature extraction step) coupled with either random Forests or Support Vector Machines and a third consisting in a Convolution Neural Network (CNN) topology named DeepFreak. <sep> - The images are classified according to the diffraction patterns they encompass into one of 5 classes: blank, no-crystal, weak, good and strong. The last three describing presence of a crystalline structure. <sep> - A fine tuning step of the various algorithms was carried using AutoML optimization tools. <sep> All algorithms were off the shelf publicly available implementations and have previously been used for such domain applications (crystallography patterns). <sep> The approach and choices of classification algorithms is well articulated and results interesting. <sep> A few questions though: <sep> • In what way the diffraction images are 'synthetic'? Aren't they actual diffraction images but in a controlled known and controlled setting: set of parameters (beam, structure to analyze)? <sep> o More like a library of diffraction pattern images for various materials/structures. <sep> • How many structures were analyzed (Were there 25000 for the 25000 pattern images), one image each? <sep> o This is to understand  the representability of the samples (structures) analyzed regarding the possible structures (Hundreds of thousands as per paper's 2.1 ) . <sep> • What variations for each of the setting variabilities (X Ray beam(flux, beam size, divergence, and bandpass), crystal properties (unit cell, number of cells, and a structure factor table), and the experimental parameters (sources of background noise, detector point-spread, and shadows)) were used? <sep> o This is to assess the size of the pattern space. <sep> • Were any real-life setting obtained pattern samples classified using DiffraNet dataset patterns' fine-tuned classification algorithms? <sep> o This is to assess the generalization level of the DiffraNet dataset patterns' fine-tuned classification algorithms to real-life obtained patterns (relates to the previously stated representability of the samples). <sep> o If not, your statement "" … we plan to add new images and new classes that are common place in serial crystallography"" (in 6. Conclusions) would be an appreciated validation of general usability of your DiffraNet fine–tuned setting. <sep> • Were all the structures analyzed crystalline? <sep> o It's stated in Figure 2 and Table 6 that 2 classes are either blank or no-crystal but is that a known fact (purposely chosen) or no pattern images for crystalline structures due to inadequate experimental settings to uncover the crystalline nature of the analyzed structure? <sep> • Were the pattern images pre-processed in any manner before being classified? <sep> Nota: In table 6, use no-crystal class as in Figure 2 for consistency.","Reviewer ratings varied radically (from a 3 to an 8). However, the reviewer rating the paper as 8 provided extremely little justification for their rating. The reviewers providing lower ratings gave more detailed reviews, and also engaged in discussion with the authors. Ultimately neither decided to champion the paper, and therefore, I cannot recommend acceptance."
"This paper analyses the internal dynamics of an LSTM, focusing on the cell state as being the most important component, and analyses what directly influences the contents of the cell state using difference equations. The authors note that at any timestep, the output cell state is the sum of (previous cell state * forget) and (input gate * input). The former can only shrink or maintain the cell value, which the authors label 'catch' and the latter can increase the magnitude, labelled 'release'. <sep> The authors show that for a single neuron, with chosen values for the forget gate and inputs, consistent growth or consistent shrinking of the cell state can be observed. When the forget is large, say 0.9, the input gate can be anywhere in the range 0.5, 1.0] and still produce growth in the cell value. For forget gate = 0.25, an even larger range of input gate values all produce a shrinking cell value. <sep> Due to the forget gate seemingly having a stronger effect than the (input gate * input) component, the authors propose to hard wire the forget gate to produce a continuous and monotonic decrease, producing the DecayNet. The rate of this decay is controlled by a learned function of the input and previous hidden state, with some shifting in order to maintain a monotonic decrease. Each forget neuron will decrease at a different rate through the processing of a sequence, leading to sections of the cell state which will decay slowly and sections which will decay quickly. <sep> The authors perform two sets of experiments. The second is sequence classification with the standard 'pixel by pixel' permuted sequential MNIST, in which they show a new SOTA with using Recurrent Batch Norm combined with DecayNet. They also demonstrate a DecayNet with fewer parameters producing roughy the same median performance as a normal LSTM but with lower variance. <sep> The first experiment is described as ""image classification"", with MNIST and Fashion-MNIST. This section is unclear to me, I had initally assumed that the data would be fed in one pixel at a time, but due to the presence of the other experiments I presume this is not the case. It is not clear what the 'time' dimension is in how the RNNs are applied here, if not through some ordering of pixels. If the entire image is presented as a flattened input, and the time dimension is iterating through the dataset, then there is no reason to use an RNN here. More detail must be added here to make it clear exactly how these RNNs are being applied to images - the text says the softmax layer is produced from the final hidden state, but without the information about how the different hidden states are produced for a given training example this not meaningful. I can imagine that both tasks are pixel by pixel, and the only difference is whether to apply the permutation.. but that is my guesswork. <sep> In general I find this paper an interesting idea, reasonably well communicated but some parts are not clear. All the experiments (as far as I can tell) work on fixed length sequences. One advantage of an LSTM is that can run onnline on arbitrary length data, for example when used in a RL Agent. In those circumstances, does learning a fixed monotonic delay on the forget gate make sense? I would guess not, and therefore I think the paper could be more explicit in indicating when a DecayNet is a good idea. <sep> There are definitely tasks in which you want to have the forget gate drop to zero, to reset the state, and then go back up to 1 in subsequent timesteps to remember some new information. Presumably the monotonic delay would perform poorly. <sep> Is DecayNet appropriate only when you have fixed length sequences, where the distribution of 'when does relevant information appear in the input' is fixed? These questions make me doubt the generality of this approach, whereas ""this reformulation increases LSTM modelling power ... and also yields more consistent results"" from the abstract reads like this is a strictly better LSTM. A much wider variety of experiments would be required to justify this sentence. <sep> It would be interesting to see some diagrams of forget gate / cell state changes throughout a real task, ie a graph with `k` on the x axis. The presentation of the new forget gate in ""System 2"" is clear in terms of being able to implement it, but it's not intuitive to me what this actually looks like. The graphs I suggest might go a long way to providing intuition for readers. <sep> Overall while I like the spirit of trying to understand and manipulate LSTM learning dynamics I am recommending reject. I do not think the paper sufficiently motivates why a monotonic decay should be good, and while the new SOTA on permuted MNIST is great, I'm concerned that the first experiments are not reproducable, as detailed previously in this review. All hyperparameters appear to be present, so this paper would be reproducable, except for the NIST experiments. <sep> General recommendations for a future resubmission: <sep> * Clarify description of first MNIST experiments, and how they are different from permuted MNIST. <sep> * Experiments on a wider variety of canonical RNN tasks - Penn Treebank is an obvious contender. <sep> * Some mention of in what situations this is obviously not a good model to use (RL?) <sep> * More intuition / visualisations as to what the internal dynamics inside DecayNet look like, vs normal LSTM. <sep> * Devote less space to the initial dynamics analysis, or modify to be representative of a real task. This part was interesting on first read, but the only thing I think it really proves is 'when we artificially choose the input values things get bigger or smaller'. The important thing is, what actually happens when training on a task that we care about - if the same catch and release dynamics are observable, then that makes the idea more compelling. <sep> Notes and suggestions: <sep> I feel the notation would be clearer if instead of k = 1 .. D, this index was t = 1 ... T. This makes it cleare that s_k is not the k'th item in the array, but rather than whole activation array at a specific time. The notation \\sigma_t could then be replace with \\tanh. <sep> ""We replace \\sigma_t with sin for an ergodic delay over time"": as this is a new gate for the forget gate, should this be \\sigma_s? <sep> One DL rule of thumb heard relatively often is to simply initialise LSTM forget gate biases to 1, to ""remember more by default"". As this is a (much simpler) way of trying to influence the behaviour of the gate, and it anecdotally improves data efficiency, it is worth mentioning in the paper.","there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning-to-learn to experiment with modelling variable-length sequences (it's not like there's no other task that has this characteristics, e.g., language modelling, translation, ...)"
"This paper presents an instruction-following model consisting of two modules: a goal-prediction model that maps commands to goal representations, and an execution model that maps goal representations to policies. The second module is trained without command supervision via a goal exploration process, while the first module is trained supervisedly in a metric learning framework. <sep> This paper contains an important core insight---much of what's hard about instruction following is generic planning behavior that doesn't depend on the semantics of instructions, and pre-learning this behavior makes it possible to use natural language supervision more effectively. However, the paper also contains a number of serious evaluation and presentation issues. It is obviously not ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence, <sep> etc.) and should not have been submitted to *CONF* in its present form. <sep> SUPERVISION AND COMPARISONS <sep> I found comparisons between supervision conditions in this paper difficult to understand. It is claimed that the natural language instruction following approaches described in the first paragraph ""require a large amount of human supervision"" in the form of action sequences. This is not exactly true, as some approaches (e.g. Artzi 2013), can be trained with only task completion signals. <sep> More problematically, all these approaches are contrasted with reinforcement and imitation learning approaches, which are claimed to use ""little human supervision"". In fact, most of the approaches listed in this section use exactly the same supervision---either action sequences (imitation learning) or task completion signals (reinforcement learning). Indeed, the primary distinction is that the ""NLP-style"" approaches are typically evaluated on their ability to generalize to new instructions, while the ""RL-style"" approaches are evaluated on the (easier) problem of fitting the complete instruction distribution as quickly as possible. <sep> This confusion carries into the evaluation of the approach proposed in this paper, which is compared to RL and IL baselines. It's hard to tell from the text, but it appears that this is an ""RL-style"" evaluation setting, where we only care about rapid convergence rather than generalization. But the baselines are inadequately described, and it's not clear to me that they condition on the commands at all. More significantly, it's not clear what an evaluation based on <sep> ""timesteps"" means for a behavior-cloning approach---is this the number of distinct trajectories observed? The number of gradient steps taken? Without these explanations it is impossible to interpret the experimental results. <sep> GENERALITY OF PROPOSED APPROACH <sep> Despite the advantages of the high-level two-phase model proposed, the specific implementation in this paper has two significant shortcomings: <sep> - No evidence that it works with real language: despite numerous claims throughout the paper that the model is designed to interpret ""human instructions"", it is revealed on p7 that these instructions consist of one or two <sep> 5-way indicator features. This is an extremely impoverished instruction space, <sep> especially compared to the numerous papers cited in the introduction that make use of large datasets of complex natural-language strings generated by human annotators. The present experiments do not support the use of the word ""human"" <sep> anywhere in the paper. <sep> - No support for combinatorial action spaces. Even if we set aside the distinctions between human-generated instructions and synthetic command languages like used in Hermann Hill & al., the goal -> policy module is defined by a buffer of cached trajectories and goal representations. While this works for the simple environments considered in this paper, it cannot generalize to real-world instruction-following scenarios where the number of distinct goal configurations is too large to tractably enumerate. Again, this is a shortcoming that existing approaches do not suffer from (given appropriate assumptions about the structure of goal space), so the lack of comparisons is problematic. <sep> CLARITY <sep> The whole paper would benefit from copy-editing by an experienced English speaker, but a few sections are particularly problematic: <sep> - The first paragraph of 4.1.1 is extremely difficult to understand What does the fingertip do? What exactly is the action space? <sep> - The end of the second paragraph is also difficult to understand; after reading it I still don't know what the extra ""position"" targets do. <sep> - 4.1.4 is cut off mid-way through a sentence. <sep> - last sentence of 4.2 <sep> The figures are also impossible to interpret: three of the four are captioned <sep> ""overview of the proposed framework"", and none are titled.","The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions. A possibly nice idea, and possibly good for more efficient learning. <sep> But the technical realisation is less strong than the initial idea. The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication. <sep> It be noted that the authors refrained from using the rebuttal phase."
"The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks. <sep> The work is focused on experiments, and draws several conclusions that are interesting, mostly around the amount of gain one can expect and the fact that the choice of the dataset is task-dependent. <sep> One of the issue is that the authors if seems to believe that ELMO is the best contextual language model. The field is moving so quickly that the experiments might become invalid pretty soon (e.g. see BERT model referenced below). <sep> Finally, the analysis is mostly descriptive and there is few insight by the author about what should be the future work, apart from ""we need a better understanding"". <sep> Minor details: <sep> Page 1: ""can yield very strong performance on NLP tasks"" is a very busy way to express the fact that Sentence Encoders work well in practice. <sep> The field evolves quickly and ELMO has now a competitive models called BERT (arXiv.org > cs > arXiv:1810.04805). I understand that the results of the current papers would hove to be re-run on all these tasks, but I'm afraid the current paper will have a limited impact if it does not use the most effective method at the date of publication...","This paper presents an extensive empirical study to sentence-level pre-training. The paper compares pre-trained language models to other potential alternative pre-training options, and concludes that while pre-trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO-based pretraining. <sep> Pros: <sep> The paper presents an extensive empirical study that offers new insights on pre-trained language models with respect to a variety of sentence-level tasks. <sep> Cons: <sep> The primarily contributions of this paper is empirical and technical novelty is relatively weak. Also, the insights are based just on ELMO, which may have a relatively weak empirical impact. The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting. None of these is a deal-breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance. <sep> Verdict: <sep> Leaning toward reject due to relatively weak novelty and empirical impact. <sep> Additional note on the final decision: <sep> The insights provided by the paper are valuable, thus the paper was originally recommended for an accept. However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions. Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions."
"The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The cost function of the minimax GAN optimization problem is changed accordingly. Experimental results suggest that this approach leads to improved results, both visually and w.r.t. FID metric. <sep> Improving on the well known problem of mode collapse in GAN training scenarios is without a doubt an important endeavor. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission. <sep> With respect to the proposed method, I am not completely clear on how it can increase sample variety, if it does. In understand the arguments brought forward in Appendix B, but: <sep> Consider a minimax game involving one generator G and one discriminator D, where each batch of generated samples from G(Z), is split up into two parts, A and B, via selection without replacement using uniform sampling. Z is a matrix of noise inputs, where each column corresponds to one item of the batch. D is now tasked to differentiate whether a sample came from A or B. It seems intuitive to say that in this case, D can neither win, nor provide any useful signal to G, since the sets A and B were split randomly, and there is no influence on G during training. The variety of samples in A as well as in B will be identical to the variety in the set (A and B). <sep> Yet this random microbatch splitting is what seems to be happening here, if I understood Section 3 correctly; just with an ensemble of discriminators, and not just with one. <sep> While it is thus not completely clear to me *why* the proposed additional term seems to bring increased variety, experiments strongly suggest that it does. <sep> As described in Section 3.1, choice of the weighting parameter alpha seems crucial, and additionally alpha needs to depend on the iteration index. Different schedules are demonstrated, but optimality of either is not guaranteed. This makes the actual influence of the additional loss term even harder to judge and evaluate. <sep> Section 4.1 seems to confirm the increase in variety via the self-defined ""Intra FID"" measure. I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5. <sep> In Table 1, both min and mean FID are given over 50k iterations. Instead of reporting the minimum, it might be fairer to compare FIDs after a fixed number of iterations (i.e. 50k in this case). <sep> The method comparison in Section 5 is generally appreciated, but I think some of its flaws are: <sep> - Datasets, including ImageNet, are all downsampled to 32x32 pixels. We have seen generators in recent work that produce interesting high-resolution output in even megapixel size; the tiny size seems like a pessimization of overall approaches. <sep> - The proposed method is compared to other methods using only 2 discriminators, although Section 4.3 suggest a larger number is better. <sep> - MicroGAN does not compare favorably to many of the compared to methods in Table 2. This may not necessarily by a flaw of the MicroGAN contributions, but is rather a problem of an apples-to-oranges comparison, as the authors readily admit (""the use of more powerful architectures [...] plays a big role""). I question the value of such a comparison, if not only the method differ, but also implementation details such as network architectures. <sep> Overall the submission is quite interesting, but not without the above-mentioned flaws.",The paper proposes an approach to remedying mode collapse problem in GANs. This approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator. <sep> + preventing mode collapse in GAN training is an important problem <sep> - the exact motivation for the proposed techniques is not fully fleshed out <sep> - the evaluation and baselines used are lacking
"Summary: <sep> The authors offer a novel incremental learning method called SupportNet to combat catastrophic forgetting that can be seen in standard deep learning models. Catastrophic forgetting is the phenomenon where the networks don't retain old knowledge when they learn new knowledge.  SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. Furthermore, two regularizers, feature and EWC regularizer, are added to the network. The feature regularizer forces the network to produce fixed representation for the old data, since if the feature representation for the old data changes when the network is fine-tuned on the new data then the support vectors generated from the old feature representation of the old data would become invalid.  The EWC regularizer works by constraining parameters crucial for the classification of the old data, making it harder for the network to change them. SupportNet is compared to five methods (all data: network is re-trained with new and old data, upper bound for performance, iCaRL: state-of-the-art method for incremental learning, EWC: Only EWC regularizer added, Fine tune: Only new data, Random guessing: Random guess to assign labels) on six datasets (MNIST, CIFAR-10, CIFAR-100, Enzyme Function Prediction, HeLa Subcellular Structure Classification, Breast Tumor Classification). It shows some improvement in overall accuracy with each newly added class when compared to iCaRL, EWC, Fine Tune and Random guessing.  Additionally, they show that overfitting for the real training data (a chosen subset of old data and the new data) is a problem for the competition iCaRL and affects SupportNet to a much lesser degree. <sep> Pros: <sep> (1) <sep> The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. Additionally, they offer a feature regularizer that penalizes the network for changing the feature representation of the support data when training the network on new data and an EWC regularizer that constrains the parameters that are crucial for the classification of the old data and makes it harder to change them. <sep> (2) <sep> The authors use six different datasets and several other approaches (subsets of their method's components, other competing methods) to show these three components alleviate catastrophic forgetting and show improvement in overall accuracy. <sep> (3) <sep> The paper is well written and easy to follow. <sep> Cons: <sep> Major Points: <sep> (1) <sep> To show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data. <sep> (2) <sep> The authors claim that iCaRL suffers from overfitting on real training data (section 4.1) however Table 2 shows iCaRL only on the enzyme function prediction which is also the dataset where the difference in performance between iCaRL and SupportNet is the largest. To support the general overfitting claim made in section 4.1, the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller.  SupportNet also suffers from overfitting (Table 3, Accuracy: test data: 83.9%, real training data: 98.7%) although to a lesser extent than iCaRL. <sep> (3) <sep> The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. To prove their usefulness, add two methods to Figure 3: <sep> (a)A method that uses support points without any regularizer. <sep> (b) A method that uses support points with just the feature regularizer. <sep> Other points: <sep> (1) <sep> In section 2.3.2, EWC regularizer, Eq. 9: We think F(theta_new) should be F(theta_old) since we want to constrain parameters crucial for classification of old data and should be computing Fisher Information for the old parameters. <sep> (2) <sep> In section 2.1 Deep Learning and SVM: additional steps are needed to show how Eq. 3 is derived from Eq. 2. <sep> (3) <sep> In section 2.1 Deep Learning and SVM: In the line before Eq. 4. ""t represtent"" instead of ""t represents"". <sep> (4) <sep> Figures are small and hard to read. Please increase the size and resolution of the figures.","The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation)."
"This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. There is also an interesting notion of using an embedding based on the action history to aid the agent in avoiding violations. However, I do not believe this paper did a good enough job in situating this work in the context of prior work — in particular (Camacho 2017). There is a significant related work section that does an ok job of describing many other works, but to my knowledge (Camacho 2017) is the most similar to this one (minus the embedding), yet is not mentioned here. It is difficult to find all related work of course, so I would encourage revision with detailed description of the novelty of this work in comparison with that one. I would also encourage an more thoughtful examination of the theoretical ramifications of the reward shaping signal with respect to the optimal policy as (Camacho 2017) do and as is modeled in the (Ng 1999) paper. As of this revision, however, I'm not sure I would recommend it for publication. Additionally, I suggest that the authors describe the reward shaping mechanism a bit more formally, it was unclear whether it fits into Ng's potential function methodology at first pass. <sep> Comments: <sep> + It would be nice to explain to the reader in intuitive terms what ""no-1D-dithering"" means near this text. I understand that later on this is explained, but for clarity it would be good to have a short explanation during the first mentioning of this term as well. <sep> + It would be good to clarify in Figure 1 what . * (lr)^2 is since in the main text near the figure is is just (lr)^2 and the .* is only explained several pages ahead <sep> + An interesting connection that might be made is that Ng et al.'s reward shaping mechanism, if the shaping function is based on a state-dependent potential then the optimal policy under the new MDP is still optimal for the old MDP. It would be interesting to see how well this holds under this holds under this schema. In fact, this seems like analysis that several other works have done for a very similar problem (see below). <sep> + I have concerns about the novelty of this method. It seems rather similar to <sep> Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Decision-making with non-markovian rewards: From LTL to automata-based reward shaping."" In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017. <sep> Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping."" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017. <sep> However, that work proposes a similar framework in a much more formal way. In fact, in that work also a DFA is used as a reward shaping signal -- from what I can tell for the same purpose through a similar mechanism. It is possible, however, that I missed something which contrasts the two works. <sep> Another work that can be referenced: <sep> De Giacomo, Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. ""Reinforcement Learning for LTLf/LDLf Goals."" arXiv preprint arXiv:1807.06333 (2018). <sep> I think it is particularly important to situate this work within the context of those others. <sep> + General the structure of the paper was a bit all over the place, crucial details were spread throughout and it took me a couple of passes to put things together. For example, it wasn't quite clear what the reward shaping mechanism was until I saw the -1000 and then had to go back to figure out that basically -1000 is added to the reward if the constraint is violated. I would suggest putting relevant details all in one place. For example, ""Our reward shaping function F(x) was  { -1000, constraint violation, 0 otherwise}"".","The paper studies the problem of reinforcement learning under certain constraints on action sequences. The reviewers raised important concerns regarding (1) the general motivation, (2) the particular formulation of constraints in terms of action sequences and (3) the relevance and significance of experimental results. The authors did not submit a rebuttal. Given the concerns raised by the reviewers, I encourage the authors to improve the paper to possibly resubmit to another venue."
"IEA proposes to use multiple ""parallel"" convolution groups, which are then averaged to improve performance. <sep> This fundamental idea of ensembles combined with simple functions has been explored in detail in Maxout (Goodfellow et.  al., https://arxiv.org/abs/1302.4389) in the context of learning activation functions, and greater integration with dropout regularization. <sep> Under the lens of comparison to Maxout (which should be cited, and is a key comparison point for this work), a number of questions emerge. Does IEA also work for feedforward layers? Does IEA give any performance improvement or have some fundamental synergy with the regularizers used here? Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? The choice of the mean here seems insufficient for creating the types of complexity in activation which are normally desirable for neural networks, so some description of why a simple mean is a good choice would be beneficial since many, many other functions are possible. <sep> Crucially Maxout seems much too close to this work, and I would like to see an indepth comparison (since it appears to be use of mean() instead of max() is the primary difference). I would also significantly reduce the claims of novelty, such as ""We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures."" in the abstract, given that this is the exact idea explored in other work including followups to Maxout. <sep> For example, MNIST performance here matches Maxout (.45% for both, but Maxout uses techniques known in 2013). CIFAR-10 results are better, but again Maxout first appeared 5 years ago. There are more recent followups that continued on the line of work first shown in Maxout, and there should be some greater comparison and literature review on these papers. The CIFAR-10 baseline numbers are not ideal, and since IEA is basically ""plug and play"" in existing architectures, starting from one of these settings instead (such as Wide ResNet https://arxiv.org/abs/1605.07146) and showing a boost would be a stronger indication that this method actually improves results. In addition, there are a number of non-image settings where CNNs are used (text or audio), and showing this idea works on multiple domains would also be good. <sep> There seems to be a similarity between ResNet and this method - specifically assuming the residual pathway is convolution with an identity activation, the summation that combines the two pathways bears a similarity to IEA. With multiple combined paths (as in Dense ResNet) this equivalence seems stronger still. A discussion of this comparison in greater detail, or even derivation of IEA as a special setting or extension of ResNet (coupled with stronger performance on the datasets) would help ground the work in prior publication. <sep> The section on visualization and inspection of IEA features seems interesting, but too brief. A greater exploration of this, and possible reduction or removal of the ensemble selection section (which didn't have a clear contribution to the message of the paper, in my opinion) would strengthen the work - and again, comparisons to activations learned by Maxout and followups would make this inspection much stronger. <sep> My key concerns here are on relation to past work, greater comparison to closely related methods, and improvement of baselines results. Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here.","The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming ""inner ensembles"". <sep> Reviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method. <sep> The AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited. The AC also concurs that a full ensemble baseline would strengthen the paper's claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time."
"This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. The aim is to learn a continuous latent space, and an encoder and decoder to map both directions between the two architecture spaces. Two further regressors are trained to map from the continuous latent space to accuracy, and parameter count. By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors). <sep> The authors perform experiments on 4 standard datasets, and show that they can in some cases get a 20x reduction in parameters with negligible performance decrease. They show better Cifar10 results than a few baselines - I am not aware whether this is SOTA for that parameter budget, and the authors do not specify. <sep> Overall I really like the idea in this paper, the latent space is well justified, but I cannot recommend acceptance of the current manuscript. There are many notational issues which I go into below, but the key issue is experiments and reproducability. <sep> The search space is not clearly defined. Current literature shows that the performance of these methods depends a lot on the search space. The manuscript does make clear that a T-layer CNN is represented as a 5XT tensor, with each column representing layer type, kernel size etc. However the connectivity is not defined at all, which implies that layers are simply sequentially stacked. This seems to preclude even basic architectural advancement like skip connections / ResNet - the authors even mention this in section 3.1, and point to experiments on resnets in section 4.4, but the words ""skip"" and ""resnet"" do not appear anywhere else in the paper. I presume from the emphasis on topological sort that this is possible, but I don't see how. <sep> If this paper is simply dealing with linear chains of modules, then the mapping to a continuous representation, and accuracy regression etc would still be interesting in principle. However it does mean that essentially all the big architecture advancements post-VGG (ie inception, resnet, densenet...) are impossible to represent in this space. Most of the Architecture Search works cited do have a search space which allows the more recent advances. <sep> I don't see a big reason why the method could not be extended - taking the 5D per-layer representation and adding a few more dimensions to denote connectivity would seem reasonable. If not, the authors should clearly mention the limitations of their search space. <sep> In terms of experiments, Figure 3 is very hard to interpret. The axes labellings are nearly too small to read, but it's also unclear what loss this even is - I presume this is the 'train' loss of L_d + \\lambda_1L_a + \\lambda_2L_p, but it could also be the 'compress' loss. It also behaves very unusually - the lines all end up lower than where they started, but oscillate around a lot, making me wonder if the curves from a second set of runs would look anything alike. It's not obvious why there's not just a 'normal' monotonic decrease. <sep> A key point that is not really addressed is how well the continuous latent space actually captures what it should. I am extremely interested to know whether the result of 'compress', ie a new concrete architecture found by gradient descent in the latent space, actually has the number of parameters and the accuracy that the regressors predict. This could be added as columns in Table 1 - eg the concrete architecture for Cifar10 gets 20.33x compression and no change in accuracy, but does the regressor for the latents space predict this compression ratio / accuracy as well? If this is the case, then I feel that the latent space is clearly very informative, but it's not obvious here. <sep> It would also be really useful to see some concrete input / output values in discrete architecture space. Presumably along the way to 20x compression of parameter count, the optimisation passes through a number of progressively smaller discrete architectures - what do these looks like? Is it progressively fewer layers / smaller filters / ??? Given that the discrete architecture encoding appears to have a fixed length of T, it's not even clear how layers would be removed. Figure 1 implies you would fill columns with zeros to delete layers, but I don't see this mentioned elsewhere in the text. <sep> More minor points: <sep> Equation numbers would be extremely useful throughout the paper. <sep> Notation in section 3 is unclear. If theta represents trained parameters, then surely the accuracy on a given dataset would be a deterministic value. Assuming that the distribution P_{\\theta}(a | A, D) is used to represent the non-determinism of SGD training, is \\theta supposed to represent the initialised values of the weights? <sep> There are 3 functions denoted by 'g' defined on page 3 and they all refer to completely different things - this is unnecessarily confusing. <sep> The formula for expected accuracy - surely this should be averaging over N different training / evaluation runs, something like: <sep> E_{\\theta}[a | A, D] \\simto \\frac{1}{N} \\sigma_{i}^N g_{\\theta}(A, D, \\theta_i) <sep> The decoder computes a 6xT output instead of a 5xT output - what is this extra row for? <sep> In the definition of ""ground truth parameter count"" p^* - presumably the standard deviation here is the standard deviation of the l vector? This formulation is a bit surprising, as convolutional layers will generally have few parameters, and final dense layers could have many. Did you consider alternative formulations like simply taking the log of the number of parameters? Having a huber loss with scale 1 for this part of the loss function was also surprising, it would be good to have some justification for this (ie, what range are the p^* values in for typical networks?) <sep> In algorithm 1 line 4 - here you are subtracting \\bar{p} from num_params before dividing by standard deviation, which does not appear in the formulation above. <sep> In the experiments: <sep> How were the 1500 random architectures generated? I presume by sampling uniformly a lot of 5xT tensors, but this encoding is not clearly defined. x_i is defined as being in the set of integers, does this include negative numbers? What are the upper / lower limits, and is there anything to push towards standard kernel sizes like 3x3, 5x5, etc? These random architectures were then trained five times for 5 epochs - what optimizer / hyperparameters / regularization was used? Similarly, the optimization algorithm used in the outer loop to learn the {en,de}coders/regressors is not specified. <sep> I would move the lemma and theorem into the appendix - they seem quite unrelated to the overall thrust of the paper. To me, saying that an embedding is not uniquely defined, but can be learnt is not that controversial, and I don't need proofs that some architecture search space has a finite number of entries. Surely the fact that the architecture is represented as a 5xT tensor, and practically there are upper limits to kernel size, stride etc beyond which an increase has no effect, already implies a finite space? Either way, this section of the paper did not add much value from my perspective. <sep> I want to close by encouraging the authors to resubmit after addressing the above issues, I do believe the underlying idea here is potentially very interesting.","The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures. During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding. Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters. The optimized representation can then be mapped back into the discrete architecture space. <sep> Overall, the main idea of this work is very interesting, and the experiments show that the method has some promise. However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses. As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue."
"Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. Given source and target metrics for measuring similarity, an attack is deemed successful if the source difference is smaller than the relative decrease in target similarity to the reference. A first experiment measures correlation with human judgements of similarity between original and perturbed sentences, and concludes that chrF is better than BLEU and METEOR for this purpose. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. In comparisons on three language pairs from IWSLT,  the constrained attacks are found to preserve meaning and yield more successful attacks according to the current framework. The Transformer architecture was also found to deal less well with attacks under the 10-closest embedding constraint. Finally, adversarial training with the character-swap constraint confers some robustness to this attack, without degrading performance on normal text. <sep> I think it is a good idea to formalize a method for carrying out and assessing adversarial attacks, but the framework proposed here seems too narrow, as it excludes adversarial inputs that are sensible but not a close perturbation of an existing source/reference pair, or ones that contain varying amounts of noise. It is more difficult to measure output quality for such attacks, but that doesn't seem like a good reason for excluding them from what is intended to be a general framework. Note also that ""more difficult"" doesn't mean impossible, since good attacks can produce severely degraded output that is relatively easy to detect. <sep> I found some of the methodology questionable. Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. Unnecessary because knowing the class of perturbation already gives you a lot of information about semantic distance. Unlikely to succeed because automatic metrics are too coarse to reliably distinguish among different perturbations. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. The experiments that support the viability of automatic metrics in 4.2 do so by measuring correlation with human judgment when the number of perturbed tokens varies from 1 to 3. I think the good correlation is likely due to the metrics being able to detect that, eg, changing 3 tokens makes things worse than changing only one. To be convincing, the experiments would have to be repeated with number of perturbations fixed at 3, to match the setting in the remaining experiments. <sep> Apart from the interesting observation about the Transformer's performance on embedding-neighbor attacks mentioned above, it is difficult to know what conclusions to draw from the experiments. In 4.3 it seems obvious a priori that perturbations intended to be relatively meaning preserving would indeed preserve meaning better than unconstrained ones. Similarly, it is not surprising that character swaps that by design produce an OOV token will cause more damage than choosing a near neighbor in embedding space. In 5.3, training with OOVs (resulting from character swaps) is of course not likely to hurt performance on test sets containing few OOVs, and, as is known from previous work, it will improve robustness to the same kind of noise. A final comment about the experiments is that word-based systems are not state of the art, and it isn't clear how much we could expect any conclusions to carry over to sub-word models. <sep> To conclude, although this is an interesting initiative, both the framework and the methodology need to be tightened up. <sep> Details: <sep> End of 2.1: this would be easier to interpret if you had previously specified the allowed range for s_src. <sep> 3.2 For kNN, being semantically related doesn't imply that the relationship is synonymy, as would be required for meaning preservation. It also doesn't imply that the substitution will be grammatical, which could jeopardize meaning preservation even if the words are synonyms. <sep> CharSwap seems odd. If you're just going to replace a work with an OOV symbol in any case, why go to the trouble of swapping characters? No matter what actual semantic shift is caused by the swap, the model will always see exactly the same representation. <sep> 4.1 ""Following previous work on adversarial examples for seq2seq models (Belinkov & Bisk, 2018; Ebrahimi et al., 2018a)"" - this is misleading: Ebrahimi et al only work with classification, and don't use IWLST. <sep> 4.1 Should mention the size of the training sets in this section. <sep> Table 1, first sentence, CharSwap example omits ""faire"". <sep> 4.3, ""Adding Constraints Helps Preserve…"" last sentence: but here you need to reason in the opposite direction. <sep> 5.2 It would be good to also give absolute scores for table 6, so we can judge how much the systems actually benefited, and whether these gains were statistically significant.","This paper present a framework for creating meaning-preserving adversarial examples. It then proposes two attacks within this framework: one based on k-NN in the word embedding space, and another one based on character swapping. <sep> Overall, the goal of constructing such meaning-preserving attacks is very interesting. However, it is unclear how successful the proposed approach really is in the context of this goal. <sep> Additionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim."
"SUMMARY <sep> The paper presents a method for classification which takes into account the semantic hierarchy of output labels, rather than treating them as independent categories. In a typical classification setup, the loss penalizes the KL-divergence between the model's predicted label distribution and a one-hot distribution placing all probability mass on the single ground-truth label for each example. The proposed method instead constructs a target distribution which places probability mass not only on leaf category nodes but also on their neighbors in a known semantic hierarchy of labels, then penalizes the KL-divergence between a model's predicted distribution and this target distribution. This model is used for classification on ImageNet-1k, and for zero-shot classification on ImageNet-21k where a model must predict superclasses seen during training for images of leaf categories not seen during training. <sep> Pros: <sep> - Method is fairly straightforward <sep> - Modeling relationships between labels is an important problem <sep> Cons: <sep> - Missing references to key prior work in this space <sep> - Minimal comparison to prior work <sep> - Confusing experimental setup <sep> - Paper is difficult to read <sep> MISSING REFERENCES <sep> This paper is far from the first to consider the use of a semantic hierarchy to improve classification systems; see for example: <sep> Deng et al, ""Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition"", CVPR 2012 <sep> Deng et al, ""Large-scale object classification using label relation graphs"", ECCV 2014 (Best Paper) <sep> Jiang et al, ""Exploiting feature and class relationships in video categorization with regularized deep neural networks"", TPAMI 2017 <sep> None of these are cited in the submission. [Deng et al, 2014] is particularly relevant, as it considers not just ""is-a"" relationships as in this submission, but also mutual exclusion relationships between categories. Without citation, discussion, and comparison with some of these key pieces of prior work, the current submission is incomplete. <sep> COMPARISON TO PRIOR WORK <sep> The only direct comparison to prior work in the paper is the comparison to DeViSE on ILSVRC12 classification performance in Table 3. However since DeViSE was intended to be used for zero-shot learning and not traditional supervised classification, this comparison seems unfair. <sep> Instead the authors should compare their method against DeViSE and ConSE for zero-shot learning. Indeed, in Section 4.3 the authors construct a test set ""in a [sic] same manner defined in Frome et al"" but do not actually compare against this prior work. <sep> I suspect that the authors chose not to perform this comparison since unlike DeViSE and ConSE their method cannot predict category labels not seen during training; instead it is constrained to predicting a known supercategory when presented with an image of a novel leaf category. As such, the proposed method is not really ""zero-shot"" in the sense of DeViSE and ConSE. <sep> EXPERIMENTAL SETUP <sep> From Section 3.1, ""we adopt a subset of ImageNet the ILSVRC12 dataset which gather [sic] 1K classes [...]"". The 1000 category labels in ILSVRC12 are mutually exclusive leaf nodes; when placed in the context of the WordNet hierarchy there are 820 internal nodes between these leaves and the WordNet root. As a result, for the method to make sense I assume that all models must be trained to output classification scores for all 1820 categories rather than the 1K leaf categories. This should be made more explicit in the paper, as it means that none of the performance metrics reported in the paper are comparable to other results on ILSVRC12 which only measure performance on the 1K leaf categories. <sep> The experiments on zero-shot learning are also confusing. Rather than following the existing experimental protocol for evaluating zero-shot learning from [Frome et al, 2013] and [Norouzi et al, 2013] the authors evaluate zero-shot learning by plotting SG-hit vs SG-specificity; while these are reasonable metrics, they make it difficult to compare with prior work. <sep> POOR WRITING <sep> The paper is difficult to follow, with confusing notation and many spelling and grammatical errors. <sep> OVERALL <sep> On the whole, the paper addresses an important problem and presents a reasonable method. However due to the omission of key references and incomplete comparison to prior work, the paper is not suitable for publication in its current form.","The paper proposes to take into accunt the label structure for classification <sep> tasks, instead of a flat N-way softmax. This also lead to a zero-shot setting <sep> to consider novel classes. Reviewers point to a lack of reference to prior <sep> work and comparisons. Authors have tried to justify their choices, but the <sep> overall sentiment is that it lacks novelty with respect to previous approaches. <sep> All reviewers recommend to reject, and so do I."
"Summary: <sep> The paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). The paper suggests that the standard practice of training the student to imitate the behavior of the teacher *on the same training data* that the teacher was trained on is problematic and can lead to overfitting. Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression. <sep> Experimental results show that this idea seems to improve the performance of convnet student models on CIFAR-10 classification and random forest student models on tabular data from UCI and Kaggle. <sep> Another contribution of the paper is to propose an evaluation metric for generative model, called the compression score. This score evaluates the quality of generated data by using it in model compression: ""good"" synthetic data results in a smaller gap in performance between student and teacher models. <sep> Strengths: <sep> - The paper sheds a light on an interesting aspect in model compression. The idea of teaching a student model to imitate behavior of the teacher model on *new* data is interesting. In fact, it emphasizes the fact that we are mostly interested in imitating the teacher model's capability of generalizing to new examples rather than overfitting to training examples. <sep> - Experiments show that for several settings (model class, architecture and datasets), using synthetic data by a cGAN can be useful in reducing the gap between student and teacher models. <sep> - The paper is clearly written and easy to follow. <sep> Weaknesses: <sep> - The claim that reusing the same training data used for training the teacher model in model compression can lead to overfitting of student model is not very obvious and needs more experimental evidence in my opinion. One way to test this is to use some unseen real data (e.g. validation or a held-out part of training data) for model compression, and showing that it can indeed help in improving student performance. <sep> - The claim that cGAN can generate ""infinite"" amount of realistic data is too strong. In light of some well-known problems of GANs such as mode collapse [2] and low-support learned distributions [1], this assumption seems unrealistic. In fact, it is not too obvious how synthetic data by a generative model learned on *same training data as the teacher* can provide any additional information to real data. <sep> - While the idea of the proposed evaluation metric seems interesting, I believe it is not very practical, because: <sep> 1. It is computationally intensive (requires training a model from scratch on fake data) <sep> 2. It relies on performance of the compression mechanism, which might also have some idiosyncrasies that prefer some features in synthetic data which do not necessarily correspond to quality of generated data. <sep> Questions/Suggestions: <sep> - In addition to using held-out real data for model compression as suggested above, a useful baseline could be using standard data-augmentation techniques in model compression. <sep> - What would happen if a student model is very small and cannot possibly overfit training data? Would using synthetic data be still useful there? <sep> - I am actually confused about a claim made when presenting compression score in Section 5. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data. I would appreciate if authors can clarify this point. <sep> Overall recommendation: <sep> While the paper presents an interesting problem in model compression, I'm leaning towards rejecting the paper because of the weaknesses mentioned above. That being said, I am happy to reconsider my decision if there is any misunderstanding on my part. <sep> References: <sep> [1] Arora, Sanjeev, and Yi Zhang. ""Do GANs actually learn the distribution? an empirical study."" arXiv preprint arXiv:1706.08224 (2017). <sep> [2] Goodfellow, Ian. ""NIPS 2016 tutorial: Generative adversarial networks."" arXiv preprint arXiv:1701.00160 (2016). <sep> ----- <sep> Updated score and posted a comment to author response.","The authors propose a scheme to compress models using student-teacher distillation, where training data are augmented using examples generated from a conditional GAN. <sep> The reviewers were generally in agreement that 1) that the experimental results generally support the claims made by the authors, and 2) that the paper is clearly written and easy to follow. <sep> However, the reviewers also raised a number of concerns: 1) that the experiments were conducted on small-scale tasks, 2) the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor. The authors in their rebuttal address 2) by noting that the student training was not too expensive, but I believe that this cost is task specific. Overall, I think 1) is a significant concern, and the AC agrees with the reviewers that an evaluation of the techniques on large-scale datasets would strengthen the paper."
"This papers uses the label hierarchy to drive the search process over a set of labels using reinforcement learning. The approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons. <sep> However, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search. <sep> I tend to see hierarchical classification as an approach to multi-label classification justified by a greedy decomposition that reduced both training and test time. This view has been outmoded for more than an decade, first as flat approaches became feasible, and now as end-to-end  structured classification is implementable with DNNs (see for instance David Belanger work with McCallum) <sep> Compared to other structured classification approaches whose scope is limited by the complexity of the inference process, this approaches is very attractive. The authors open the optimization black box of the inference process by adding a few very clever tricks that facilitate convergence: <sep> - Intermediate rewards based on the gain on F1 score <sep> - Self critical training approach <sep> - ""Clamped"" pre-training enabled by the use of state embeddings that are multiplied my a transition to any state in the free mode, and just the next states in the hierarchy in the clamped mode <sep> - Addition of a flat loss to improve the quality of the document representation <sep> While those tricks may have been used for other applications, they seem new in the context of hierarchical/multi-label/structured classification. <sep> While the experiments appear thorough, they could be the major weakness of this paper. The results the authors quote as representative of other approaches seem in fact entirely reproduced on datasets that were not used on the original papers, and the authors do not try an apple-to-apple comparison to determine if this 'reproduction' is fair. None of the quoted work used the 2018 version of Yelp, and I could only find RCV1 Micro-F1 experiments in Johnson and Yang, who report a 84% micro-F1, far better than the 76.6% reported on their behalf here, and better than the 82.7% reported  by the authors. I read note 4 about the difference in the way the threshold is computed, but I doubt it can explain such a large difference. I did not check everything, but could not find and apple-to-apple comparison? <sep> Have the network architecture been properly optimized in terms of hyper-parameters? <sep> In particular, having tried Kim CNN on large label sets, I suspect the author settings using a single layer after the convolution is sub-optimal. I concur with the following paper than an additional hidden layer is essential: Liu et al ""Deep Learning for Extreme Multi-label Text Classification"". I also note the 32 batch size could be way too small for sparse label sets (I tend to use a batch size of 512 on this type of data).","This paper presents a reinforcement learning approach to hierarchical text classification. <sep> Pros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning. <sep> Cons: The major concensus among all reviewers was that there were various concerns about experimental results, e.g., apple-to-apple comparisons against prior art (R1), proper tuning of hyper-parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3). In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, e.g., what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem). <sep> Verdict: <sep> Reject. While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work."
"To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network. <sep> The paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness. <sep> However, my main concern about the paper is still about its significance. <sep> 1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, ""lower value of \\sigma^\\ell(s) are indicative of better separation between classes"", what is the reason of not directly penalizing this value, rather than requesting a ""stability"" property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper. <sep> 2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of ""Proposed + Parseval"". On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings. <sep> Other comments: <sep> 1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. <sep> 2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.","The paper proposes a new graph-based regularizer to improve the robustness of deep nets. The idea is to encourage smoothness on a graph built on the features at different layers. Experiments on CIFAR-10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization. The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization. Despite the revision, the reviewers maintain their concerns. For this reason this work is not ready for publication."
"The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments.  First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. <sep> In section 3, the authors then introduce the ""max-marginal regularization"" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word ""marginal"" instead of ""margin"" seems quite wrong to say the least.  Secondly, the stated definition seems to be wrong. In the definition the range of values for \\gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \\gamma not equals 0) below and show that both have issues: <sep> (a) \\gamma > 0: This seems to imply that when the log-likelihood of ground-truth is already \\gamma better than the log-likelihood of the random negative, the loss comes to life. Strange! <sep> (b) \\gamma < 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. <sep> I'm assuming the authors swapped y with y^{-} in the ""regularization"" part. <sep> Anyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well. <sep> Coming to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two ""Where is your location?"" outputs.  The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. <sep> A lot of typos/wrong phrasing/wrong claims and here are some of them: <sep> (a) Page 1, ""lead to the misrecognition of those common replies as grammatically corrected patterns""? - No idea what the authors meant. <sep> (b) Page 1, ""unconsciously preferred"" - I would avoid attaching consciousness before AGI strikes us. <sep> (c) Page 1, ""Above characters"" -> ""Above characteristics"" <sep> (d) Page 1, ""most historical"" -> ""most previous"" <sep> (e) Page 2, ""rest replies"" -> ""rest of the replies"" <sep> (f) Page 3, ""variational upper bound"" -> Not sure what's variational about the bound <sep> (g) ""Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance""? - No idea what the authors meant.","This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves."
"Summary <sep> ------- <sep> This paper proposes DL2, a framework for turning queries over parameters and input, output pairs to neural networks into differentiable loss functions, and an associated declarative language for specifying these queries. The motivation for this work is twofold. The first is to allow for the specification of additional domain knowledge during training. For example, if a user expects that the predicted probabilities of some output classes should be correlated for all predictions, this constraint can be enforced during weight learning. Second, it allows users to search for specific inputs that satisfy specified conditions. In this way, DL2 can capture popular applications like searching for adversarial examples by querying for inputs close to a known input of class A but that the network predicts is class B with high confidence. <sep> The paper provides a concise specification of the query language (a mixture of logical and numeric operators) and asserts a theorem that the given procedure for constructing the query loss produces a function such that anytime the function is 0, the constraints are satisfied. No proof is given, but I cannot see a counterexample. There is also a statement about the converse relationship, that when the loss is above some threshold it implies that the query is not satisfied. <sep> Experiments are conducted on supervised, semi-supervised, and unsupervised computer vision tasks. I particularly liked the experiment on semi-supervised learning with CIFAR-100. By replacing labeled examples with domain knowledge about the relationships among classes in CIFAR-100, the paper demonstrates a compelling use case for DL2. <sep> The primary technical challenge is the non-convex optimization required to search for a solution to a query. Experiments show that the loss functions created by DL2 are often solved quickly and correctly, but not always <sep> Strengths <sep> --------- <sep> The framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection. The experiments cover a range of these use cases, demonstrating that the constructed optimization objectives usually work as intended. <sep> Weaknesses <sep> ----------- <sep> The statement in Theorem 1 regarding the converse case is unclear, because it says that the limit of \\delta as \\epsilon approaches zero is zero, but it is not explained what \\epsilon is or how it changes. If \\epsilon is the threshold that can often be used in the query, it is not obvious that every query contains exactly one \\epsilon. If other cases exist, it is unclear how Theorem 1 applies. <sep> It remains unknown how to handle the case when queries fail. AS the paper points out, if a query fails, it cannot be determined whether no solution exists or if the optimization simply failed to find a solution. Of course, this is a computationally hard in general. <sep> Related Work <sep> ------------ <sep> There are a couple of points from related work that would be good to add to the paper. <sep> First, the paper ""Adversarial Sets for Regularising Neural Link Predictors"" (Minervini et al., UAI17) is a prior paper that generates adversarial examples to handle restrictions on inputs which may not exist in the training set. The paper claims DL2 is the first to do this, but I believe this paper is an earlier example that does so, albeit for a particular problem. DL2 is certainly more general. <sep> Second, the description of the limitations of rule distillation (Hu et al., ACL16), particularly in Appendix A is not fully accurate. The expressivity of PSL is greater than stated (see Bach et al., JMLR17 for a full description). In particular, the DL2 loss function for z = (1, 1) can be expressed exactly in PSL using what it calls arithmetic rules. It is not clear that this affects the findings of the semi-supervised learning experiment significantly, although I would appreciate a clarification of the authors. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well.","Unfortunately, this paper fell just below the bar for acceptance. The reviewers all saw significant promise in this work, stating that it is intriguing, ""novel and provides an interesting solution to a challenging problem"" and that ""many interesting use cases are clear"". AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training. A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints. The other two reviewers unfortunately felt that while the proposed approach was ""interesting"", ""promising"" and ""intriguing"", the quality of the paper, in terms of exposition, was too low to justify acceptance. Arguably, it seems the writing doesn't do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten."
"The authors consider the problem of determining the minibatch size for SGD by first fixing a set of candidate sizes, and then learning a distribution over those sizes using a MAB algorithm. A minibatch size is first sampled from the distribution, then one training epoch is performed. A validation error is then computed, and if it is lower than that of the last epoch, the cost of the minibatch is taken to be zero (otherwise one), and the distribution is updated. This is Algorithm 1. <sep> In Section 4.2, they prove a regret bound, but I don't think that regret is really the correct notion, here (although it's very close). This is a subtle point, so I'll set up some notation. Let w(b_1, ..., b_t) be the result at the tth epoch, if the batch sizes b_1, …, b_t were used at the 1st through tth epochs. Let y(w,b) be 0 if training one epoch starting at w with batch size b would improve the validation error, and 1 otherwise. <sep> They show (unnumbered inequality on the middle of page 5) that \\sum_t y(w(b_1,...,b_{t-1}),b_t) is close to \\sum_t y(w(b_1,...,b_{t-1}),b^*), where b_t is the batch size that was chosen at time t, and b^* is the best fixed batch size. The key point here is that the comparator (the second sum) starts each epoch at the result that was found by their adaptive algorithm, *not* what would have been found if a batch size of b^* had been used from the beginning. <sep> In other words, their result does *not* show that their algorithm is close to outperforming a fixed choice of batch size (for that to hold, the comparator would need to be \\sum_t y(w(b^*,...,b^*),b^*)). What they show is similar, but subtly different. They don't put too much weight on this theoretical result, and in fact don't even explicitly claim that the comparator in this result is that for a fixed choice of batch size, so really this is a minor issue, but I think that this is something that should be clarified, since it would be easy for a reader to draw an incorrect conclusion. <sep> With that said, their approach is well-motivated, and their experiments seem to show consistent small improvements in performance. I don't think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn't be much more computationally expensive than using a fixed minibatch size. Furthermore, their approach is potentially more robust, since you can presumably be less careful about choosing the set of candidate minibatch sizes, than you would be for choosing only one. So while the experiments don't show a big improvement, their proposal has other benefits.","It is a simple but good idea to consider the choice of mini-batch size as a multi-armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size. <sep> The main concerns from the reviewers are that (1) treating the choice of hyper-parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini-batch size, (2) the improvement in the test error is not significant. The authors' feedback did not solve the concerns raised by R2. <sep> This paper conveys a nice idea but as the current form it falls slightly below the standard of the *CONF* publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper-parameter selection problems."
"Overview: <sep> This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at *CONF*. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time. <sep> Pros: <sep> The paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track. <sep> Issues: <sep> * (p.5) ""R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients."" <sep> This statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian). <sep> Control variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook ""Simulation."" <sep> The fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s ""Mirage of Action-Dependent Baselines"", https://arxiv.org/abs/1802.10031. <sep> Since the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale. <sep> * The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: ""We choose the baseline ... to be a function of state ... it must be independent of the action ...."" and ""it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased."" In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited. <sep> Cons <sep> * Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. <sep> * I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice. <sep> * The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper. <sep> *EDIT: <sep> I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below: <sep> (1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that ""everyone knows"" what is meant when the actual claim is misleading. <sep> (2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue. <sep> (3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper. <sep> The variance of the policy gradient estimator, subject to a baseline ""phi,"" is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from ""phi(a,s)"", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper: <sep> ""We expect this to be the case when single actions have a large effect on the overall discounted return (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward)."" <sep> Please see Sec. 3, ""Policy Gradient Variance Decomposition"" of the Mirage paper for further details. <sep> The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. <sep> It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in ""A Better Second Order Baseline"" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper. <sep> (4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper. <sep> (5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation.","This paper extends the DiCE estimator with a better control variate baseline for variance reduction. <sep> The reviewers all think the paper is fairly clear and well written. However, as the reviews and discussion indicates, there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions. We encourage the authors to rewrite the paper to address these criticism. We believe this work will make a successful submission with proper modification in the future."
"This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. <sep> The proposed method is closely related to 1) ""Learning Montezuma's Revenge from a Single Demonstration"" a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma's revenge game 2) ""Reverse Curriculum Generation for Reinforcement Learning"" by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. <sep> The approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. <sep> The Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I'm slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. <sep> Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). <sep> I commend the authors for honestly reporting their method's shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the *CONF* standards in its current form.","-pros: <sep> - good, sensible idea <sep> - good evaluations on the domains considered <sep> - good analysis <sep> -cons: <sep> - novelty, broader evaluation <sep> I think this is a good and interesting paper and I appreciate the authors' engagment with the reviewers. I agree with the authors that it is not fair to compare their work to a blog post which hasn't been published and I have taken this into account. However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for *CONF* this year."
"Note: This is an emergency review. I managed not to look at existing comments/ratings for this paper before writing my review. <sep> Summary <sep> --- <sep> This paper studies deep multi-agent RL in settings where all of the agents must cooperate to accomplish a task (e.g., search and rescue, multi-player video games). It uses simple cooperative weighted voting games 1) to study the efficacy of deep RL in theoretically hard environments and 2) to compare solutions found by deep RL to a fair solution concept known in the literature on cooperative game theory. <sep> In a weighted voting game each agent is given a weight and the agents attempt to form teams. The first team whose total weights exceed a known threshold get the total reward, which is distributed amongst the team members. Given such a game, the __shapely value__ of an agent measures the importance of that agent. How much does it contribute to a team from this set of agents? How much payoff should it get? These have existed in the literature for over 60 years and appear to be widely known and used. <sep> All of this is agnostic to how the agents communicate to form teams: i.e., the communication protocol or the actions available in the environment. The protocol matters because it can allow certain teams to form more or less easily than others, even though the same team would get the same reward regardless of protocol. This can make an agent more or less effective under different protocols. Here two protocols are considered - one where agents suggest proposed teams directly and another where they suggest teams by congregating on a 2d plane. Both protocols result in games whose Nash equilibria are computationally intractable. <sep> The paper shows 4 results: <sep> 1) It considers a hand-designed bot similar to models from the game theory literature. Relative to a group of RL agents, an additional RL bot will outputperform a hand-designed bot in terms of average reward it receives. <sep> 2) The average reward of a bot is strongly correlated with that bots shapely value. <sep> 3) In the negotiation by congregation environment, a bot's spatial position can affect its ability to negotiate. <sep> 4) Shapely values can be predicted quite accurately from the weights and threshold that define a cooperative voting game, though these predictions have high variance. <sep> The paper concludes that deep RL is effective at learning agents for cooperative games in multiple ways: <sep> 1) Deep agents are better than a hand-designed agent. <sep> 2) Deep agents easily extend across negotiation protocols (something hand-designed agents don't do). <sep> 3) A popular result in cooperative game theory predicts how effective agents should be. Deep agents are just about that effective. <sep> Strengths <sep> --- <sep> * The paper does a pretty good job of reviewing relevant work from game theory. <sep> * Some of the organization is nice (e.g., the list of reasons classic game theory doesn't extend to practice; one section per experiment). <sep> Weaknesses mentioned in individual sections... <sep> Quality <sep> --- <sep> Overall, things were well thought through, but I would have liked more out of the experiment 4 section and I think a few minor details might have been missed. <sep> Details: <sep> Section 4.5/Experiment 4: The Shapely value comparison is the most important part of the paper.  This section is important because it tries to explain those results, but it seems like there's more work to be done here. I'm not sure capacity is eliminated as a concern, and there might be other concerns not listed like optimization error. <sep> * I'm not sure what conclusion to take from experiment 4. Shapely values can be computed from the cooperative games directly, independent of protocol. We're interested in __policies__ that get exactly the shapley values as their average reward. Policies depend on the protocol. Does being able to predict shapley values mean that a model with similar capacity can learn a policy that will have the desired shapley value? Was that the desired conclusion? <sep> Other comments: <sep> * The current hand-designed baseline uses weights to form a probability distribution. There should be another baseline that uses Shapley values instead of weights. <sep> * It's not clear exactly what the spatial nature of the Team Patches environment adds. It is good to try another environment just to have an additional notion of generalization. <sep> Clarity <sep> --- <sep> Overall, the motivation could be clearer. Is the point to do work on cooperative games or to compare to Shapley values? <sep> Presentation details: <sep> * The paper does not get to specific examples of agents acting in environments until about page 4. Providing a simple, brief example which leaves out some details at the beginning would go a long way toward aiding intuitions about the abstract concepts discussed. Here are some clarity issues I had that might have been helped with an example: <sep> * What exactly is it about a task which requires agents to form teams? How necessary are those teams? <sep> * What exactly is a negotiation protocol? <sep> * What does it mean to distribute/share a reward across agents? <sep> * When talking about shapely values, fairness seems to be emphasized somewhat often, but no concrete intuition about what fairness means in this setting is provided. <sep> * Intro para 4: What does the human data measure? And thus how might it be useful? <sep> * Intro para 7: People in the *CONF* community will be more familiar with this work. What is the difference between communication and team forming? <sep> * The section on Shapley values should provide more intuition about what they're thought about as measuring. (An agent's importance or what payoff it should expect, according to wikipedia.) <sep> * Instead of measuring correlation to Shapley values, the paper measures whether average reward approximates Shapley values. It seems like the two are on a different scale. Average reward is unbounded and Shapley values are in [0, 1]. How are they comparable? <sep> * The paper mentions how results vary over different types of boards (ones with higher and lower variance in the sampled weights). It does not show results to support this discussion. A conditional analysis of performance would be interesting and relevant, perhaps conditional versions of Fig. 3. <sep> Originality <sep> --- <sep> I do not know much about game theory and I'm only somewhat familiar with multi-agent deep RL, so I am not in a great position to judge novelty. Nonetheless, Given existing work in multi-agent RL, it is unsurprising that deep RL agents learn reasonable policies in these environments. <sep> As far as I know, the comparison of average reward to shapely values has not been done before. <sep> Significance <sep> --- <sep> Most work in multi-agent RL evaluates by 1) comparing to baselines or 2) measuring some environment/task-specific metric. The best thing about this work is that it evaluates by comparing actual performance to some external theory that suggests how well an agent should be able to do, falling into a 3rd category.  It's not alone in this category (e.g., paper compare to theoretically optimal baselines if they can), but it is interesting to see another example of this kind of evaluation. <sep> The community might possibly start to focus more on cooperative games because of this paper. A more interesting result would occur if others are inspired to implement more comparisons to how agents __should__ perform in theory. <sep> Justification for Final Rating <sep> --- <sep> I am unsure about novelty. As described above, the paper is lacking in clarity and quality (esp. section 4.5), but I don't think these concerns would invalidate the main result. I think the contribution is significant because of the kind of evaluation, but I'm not sure it will ultimately have a large impact. Thus I think some of the concerns above should be addressed before publication, but I would not be very disappointed if it were published as is.","This paper was reviewed by three experts. Initially, the reviews were mixed with several concerns raised. After the author response, there continue to be concerns about need for significantly more experiments. If this were a journal, it is clear that recommendation would be ""major revision"". Since that option is not available and the paper clearly needs another round of reviews, we must unfortunately reject. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue."
"Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. The authors also show why the assumptions on the permissible functions can not be relaxed. <sep> Quality: the paper seems to be technically correct. However, the authors do not discuss any consequence of their result. Why was it important to prove it? What does it tell us about the networks? While the proof may be of interest to the authors of [14] to correct their (possible) mistakes, I think the paper will go under the radar for most people and thus encourage the authors to heavily revise the paper. <sep> Clarity: the writing is clear in general. The proofs sometimes jump over non-trivial things and explain easy steps, but that maybe subjective. The paper spends no effort explaining the contribution and its consequences. <sep> Originality: the proven statements are novel and extend/fix the claims of [14] <sep> Significance: as said above, I believe that in the current form the paper will have little to no impact. The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that. <sep> Minor comments: <sep> * when introducing T{i,:,:} the <> notation is not clear. I could guess it from the later usage of the symbol, but these brackets can mean a lot of things, e.g. bracket mean (Section 2.1) <sep> * it would be beneficial to define the main objects, wide-network limits, in a more formal way (Section 2.3) <sep> * how the wide regime (large N) is interesting for studying deep NNs? [14] discusses that to some extent, but this should be explained here as well <sep> * q_0 is never defined <sep> * it's good practice to add numbers to all equations <sep> * I believe the claim in the appendix of [14] was meant to be conditionally independent (see also the reviews of [14]). It's clear that preactivations should not be independent and, while technically interesting, spending a page of theorem 10 and on plots seems unnecessary. Even in the paper's example preactivations are uncorrelated in the limit of large N. <sep> * I don't see the point of having experiments in this paper. The authors have already proven the fact. Also, it is not clear how to read the plots (no axes, little description) and come to the statements from page 9. <sep> ******************** <sep> After the authors' response: <sep> If the main motivation of the paper is to fix the mistakes in [14], then the paper should clearly state so, in addition to explaining why fixing is necessary. While I believe that pointing out other paper's mistakes and correcting them is important, the current state of the paper leads me to keeping my initial score and recommending to reject the paper.","I appreciate that the authors are refuting a technical claim in Poole et al., however the paper has garnered zero enthusiasm the way it is written. I suggest to the authors that they rewrite the paper as a refutation of Poole et al., and name it as such."
"It is often argued that one of the roles of pooling is to increase the stability of neural networks to deformations. This paper presents empirical evidence to contest this assertion, or at least qualify it. <sep> I appreciate empirical studies that question some of the widely accepted dogmas of deep learning. <sep> From this point of view, the present paper is certainly interesting. <sep> Unfortunately, the actual evidence presented is quite weak, and insufficient to draw far reaching conclusions. An obvious objection is the authors only consider two datasets, and a very small number of more or less standard pooling methodologies. The effect of pooling is evaluated in terms of cosine similarlity, which is not necessarily a good proxy for the actual performance of a network. <sep> A more serious issue is that they seem to very readily jump to unwarranted conclusions. For example, <sep> the fact that stability to deformations (by which I necessarily mean the specific type of deformations that they consider) tends to decrease in the middle layers of neural networks during training does not mean that starting with a neural network with less stability would be better. Maybe some kind of spontaneous coarse-to-fine optimization is going on in the network. Similarly, it is obvious that smoother filters are going to lead to more stable representations. However, they might be less good at discriminative tasks. Just because smoother filters are more stable does not automatically mean that they are more desirable. <sep> Stability to deformations is an important but subtle topic in computer vision. For starters, it is difficult to define what kind of deformations one wants to be insensitive to in the first place. A useful model would likely incorporate some notion of deformations at multiple different length scales. <sep> Just showing that one network is better than another wrt some arbitrarily defined simple class of deformations with no reference to actual recognition performance, speed of training, or interpretation of the nature of the deformations and the learned filters is not very convincing. I would particularly like to emphasize the last point. I would really like to understand what pooling actually does, not just at the level of ""if you turn it off, then cosine similarity will decrease by this much or that much.","This paper studies the role of pooling in the success underpinning CNNs. Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training. <sep> All reviewers agreed that this is a paper asking an important question, and that it is well-written and reproducible. On the other hand, they also agreed that, in its current form, this paper lacks a 'punchline' that can drive further research. In words of R6, ""the paper does not discuss the links between pooling and aliasing"", or in words of R4, ""it seems to very readily jump to unwarranted conclusions"". In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit."
"This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain. <sep> Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard. <sep> I have several further concerns about this work: <sep> * The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information. <sep> * The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead? <sep> * I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results. <sep> I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance. <sep> Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time. <sep> Other comments: <sep> * In the introduction, an adversarial criterion is referred to as a ""discriminative objective"", but ""adversarial"" (i.e. featuring a discriminator) and ""discriminative"" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative. <sep> * Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model. <sep> * Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability. <sep> * Introduction, top of page 2: should read ""does not learn"" instead of ""do not learns"". <sep> * Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction. <sep> * The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments. <sep> * I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms. <sep> * I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples""). <sep> * Section 3.1, ""amounts to optimizing"" instead of ""amounts to optimize"" <sep> * Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to ""Generating Sentences from a Continuous Space"" by Bowman et al. (2016). This should probably be cited instead. <sep> * ""circle-consistency"" should read ""cycle-consistency"" everywhere. <sep> * MMD losses in the context of GANs have also been studied in the following papers: <sep> - ""Training generative neural networks via Maximum Mean Discrepancy optimization"", Dziugaite et al. (2015) <sep> - ""Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"", Sutherland et al. (2016) <sep> - ""MMD GAN: Towards Deeper Understanding of Moment Matching Network"", Li et al. (2017) <sep> * The model name ""FILM-poi"" is only used in the ""implementation details"" section, it doesn't seem to be referred to anywhere else. Is this a typo? <sep> * The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train? <sep> * The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.","This paper proposes a VAE-based model which is able to perform musical timbre transfer. <sep> The reviewers generally find the approach well-motivated. The idea to perform many-to-many transfer within a single architecture is found to be promising. However, there have been some unaddressed concerns, as detailed below. <sep> R3 has some methodological concerns regarding negative transfer and asks for more extended experimental section. R1 and R2 ask for more interpretable results and, ultimately, a more conclusive study. R2 specifically finds the results to be insufficient. <sep> The authors have agreed with some of the reviewers' feedback but have left most of it unaddressed in a new revision. That could be because some of the recommendations require significant extra work. <sep> Given the above, it seems that this paper needs more work before being accepted in *CONF*."
"Summary-- <sep> The paper tries to address an issue existing in current image-to-image translation at the point that different regions of the image should be treated differently. In other word, background should not be transferred while only foreground of interest should be transferred. The paper propose to use co-segmentation to find the common areas to for image translation. It reports the proposed method works through experiments. <sep> There are several major concerns to be addressed before considering to publish. <sep> 1) The paper says that ""For example, in a person's facial image translation, if the exemplar image has two attributes, (1) a smiling expression and (2) a blonde hair, then both attributes have to be transferred with no other options"", but the model in the paper seems still incapable of transferring only one attribute. Perhaps an interactive transfer make more sense, while co-segmentation does not distinguish the part of interest to the user. Or training a semantic segmentation make more sense as the semantic segment can specify which region to transfer. <sep> 2) As co-segmentation is proposed to ""capture the regions of a common object existing in multiple input images"", why does the co-segmentation network only capture the eye and mouth part in Figure 2 and 3, why does it capture the mouth of different shape and style in the third macro column in Figure 4 instead of eyes? How to train the co-segmentation module, what is the objective function? Why not using a semantic segmentation model? <sep> 3) The ""domain-invariant content code"" and the ""style code"" seem rather subjective. Are there any principles to design content and style codes? In the experiments, it seems the paper considers five styles to transfer as shown in Table 1. Is the model easy to extend to novel styles for image translation? <sep> 4) What does the pink color mean in the very bottom-left or top-right heatmap images in Figure 2? There is no pink color reference in the colorbar. <sep> 5) Figure 5: Why there is similariy dark patterns on the mouth? Is it some manual manipulation for interactive transfer? <sep> 6) Though it is always good to see the authors are willing to release code and models, it appears uncomfortable that github page noted in the abstract reveals the author information. Moreover, in the github page, <sep> even though it says ""an example is example.ipynb"", the only ipynb file contains nothing informative and this makes reviewers feel cheated. <sep> Minor-- <sep> There are several typos, e.g., lightinig.","The paper received mixed ratings. The proposed idea is quite reasonable but also sounds somewhat incremental. While the idea of separating foreground/background is reasonable, it also limits the applicability of the proposed method (i.e., the method is only demonstrated on aligned face images). In addition, combining AdaIn with foreground mask is a reasonable idea but doesn't sound groundbreakingly novel. The comparison against StarGAN looks quite anecdotal and the proposed method seems to cause only hairstyle changes (but transfer with other attributes are not obvious). In addition, please refer to detailed reviewers' comments for other concerns. Overall, it sounds like a good engineering paper that might be better fit to computer vision venue, but experimental validation seems somewhat preliminary and it's unclear how much novel insight and general technical contributions that this work provides."
"The paper presents an anomaly detection method called MMOCGAN which is claimed to work well on high-dimensional datasets with limited, multimodal data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance: <sep> - Results are presented on a single private dataset, and I don't see any indication that the dataset will be shared with the community. This is problematic because there is no way for the community to reproduce and validate these results. I don't think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility. <sep> - Given the small number of non-pass products in the dataset (22), it's unclear to me whether a held-out test set was used, or if hyperparameter selection was performed on the full set of non-pass products. <sep> - The use of a pre-trained, domain-specific feature extractor is briefly mentioned but no details (what is the architecture, on which data it's been trained, etc.) are provided. <sep> - The central idea in the paper is to have the generator capture the ""complementary distribution"" of the data-generating distribution. The way in which this distribution is defined is not specific enough (it depends on hyperparameters C and epsilon for which there is no clear prescribed value). On a conceptual level it seems to me that for a data-generating distribution corresponding to a low-dimensional manifold embedded in a high-dimensional space the complementary distribution will essentially be uniform random noise, and in that case it's unclear to me how it's supposed to ""simulate anomalies"". <sep> - The way the proposed method is presented makes it look ad-hoc: several moving parts (InfoGAN, generator loss term encouraging it to learn the ""complementary distribution"", feature matching regularization term, pull-away loss term, discriminator entropy term) are connected together and their individual inclusion in the final loss is loosely justified. In practice, looking at the results it's impossible for me to tell which term is necessary and which is not. <sep> - The word ""modal"" is used throughout as a noun. I'm not sure if the authors mean ""model"", ""mode"", or ""modality"", but based on the context I assume they mean ""mode"" as in ""mode of the distribution"". <sep> - The use of an InfoGAN architecture and loss is not credited clearly enough to Chen et al. and may give the impression to a casual reader that the idea is novel to this paper. The paper also does not make it clear how the number of categories or modes for the latent variable should be chosen, and what was the value used for the experiments. <sep> - The paper is legible, but there are several grammatical errors and typos throughout that make it harder to read than necessary.","The authors propose a GAN-based anomaly detection method based on simulating anomalies (low density regions of the data space) in order to train an anomaly classifier. <sep> While the paper addresses an interesting take on an important problem, there are many concerns raised by reviewers including novelty, clarity, attribution, reproducibility, the use of exclusively proprietary data, and a multitude of textual mistakes. Overall, the paper shows promise but does not seem to be a mature and polished piece of work. As there has been no rebuttal or update to the paper I have no choice but to concur with the reviewers' initial assessments and reject."
"Summary <sep> ------ <sep> The authors propose an adaptation of the Adam method, with the AMSGrad correction and an additional parameter to p to exponentiate the diagonal conditioning matrix V (Padam). <sep> The proposed method changes two aspects: first, there is no need to retain two version of the rescaling matrix v, where amsgrad and Padam keeps the last monotone \\hat v)t and non-monotone version v_t. Secondly, a new parameter q is introduced, that replaces the q=2 in the moment estimation phase of (P)Adam. <sep> A regret analysis is proposed in the convex case, while a vanishing bound on the gradient is derived in the non-convex smooth case. <sep> Review <sep> ------ <sep> Although improving optimization methods is certainly important for the machine learning community, the reviewer have strong concerns about this paper. <sep> First of all, the paper is hard to read as it contains too many approximations. What does 'SGD is known to work reasonably well regardless of their problem structure' means ? Same thing for 'Its performance deteriorates when the gradients are dense due to a rapid decay of the learning rates.' The authors uses many times elliptical discourse to detail the course of their analysis, which is non informative: for instance, 'one can easily derive the upper bound expression', and 'It is not difficult to conclude that when G_t [...]'. This level of writing is not professional. Some completely irrelevant argument are proposed to justify the method: 'For instance the extension from l_2 norm to l_p norm and generalization from Cauchy-Schwart to Holder inequality'. <sep> The reviewer has interrogations about the relevance of the proposed algorithm. The additional parameter q needs to be tuned, which carries only the promise of further overfitting. I would have been convinced by an sequence of experiment where q is set automatically by considering a validation set, and then tested on a left out test set. However, the authors report only the results for the best q, with non significant differences (and not quantified, there is no result tables). Using q=2 at least made sense from the point of view of empirical Fisher matrix approximation. <sep> The review also have several concerns aout the correctness of the proposed arguments. First, the major argument of memory usage stems from 1) a miscalculation and 2) a misunderstanding of memory bottlenecks in deep learning. 1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \\hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. In contrast, the proposed model do not track v_{t-1}: this amounts to a memory saving of 20% considering all model related parameters. 2) more importantly, the most important memory usage in deep leaning comes from the activations that need to be kept in memory during the forward pass to perform the backward pass. Even the biggest model are less than 1GB, and most of the memory used during training is dedicated to intermediary activations. This makes the major argument of the paper less convincing, and misleads the reader. <sep> Second, even when disregarding the slightly abusive assumptions over the iterate sequences, that are common in the adaptive stochastic optimizers community, I think that the bound proposed in theorem 1 is non informative, as the second term behaves like T sqrt(T) assymptotically, due to the presence of 1 / \\alpha_t. This does not show the convergence of averaged regret R_T / T. <sep> Regarding the experiment section, I am afraid that testing a new optimizer over MNIST and CIFAR is not enough to show the relevance of the method for the whole deep learning community. An eperiment over a non-toy dataset (eg ImageNet), and on non computer-vision dataset (eg from NLP) would be a minimum, besides the overfitting concern described above. <sep> In conclusion, it is the reviewer's opinion that significant rework in term of presentation and strong improvement of the experiment section to make the case for the Game optimizer. <sep> Minor comments <sep> ------------ <sep> Table 1: what do you bound when you compare results ? I think there is a typo in Zhou et al. result: 1/2 should read p. <sep> Eq (1): it is rather surprising to use x_t as the model parameters in the *CONF* community. <sep> p 7: the dimension d could be larger than T when training large-scale neural networks: how does it relate to comparing sqrt(dT) to (dT)^s ?","The reviewers find the per difficult to read. Reviewers also had concerns regarding the correctness of various claims in the paper. The paper was also found lacking in experimental analysis, as it only tested on relatively small datasets, and only no a CNN architecture. Overall, the paper appears to be lacking in quality and clarity, and questionable in correctness and originality."
"Summary: <sep> This paper presents an RL approach to active learning that is generic across ML model being learned, and across dataset being used. The paper formulates the standard active learning problem as an MDP with the objective of minimizing the number of annotated labels required to meet a pre-specified prediction quality. <sep> The MDP state proposed by this paper is the current performance score on each sample in a hold-out set. The actions are specified by selecting a datapoint from the set of all un-annotated datapoints. The action feature vector consists of the current performance score of the model on the datapoint, and the average distance of that datapoint from every datapoint in the labeled set and every datapoint in the unlabeled set. <sep> Review: <sep> I do not recommend this paper for publication in *CONF* because I believe: <sep> 1) the work is too incremental <sep> 2) the comparison to baseline and competing methods is incomplete <sep> 3) some design decisions of the proposed method are not well motivated. <sep> I appreciated the clarity of the writting, and the paper organization. I also believe that the proposed method is quite intuitive, and is a good addition to the field. Finally, I appreciate that sufficient experimental details are available within the paper to be able to easily reproduce the results. <sep> Details: <sep> My points (1) and (2) are highly related, so I will discuss both simultaneously. I find that this paper makes only incremental forward progress from the Pang 2018 paper and the Konyushkova 2017 paper. The methodology here looks very similar to the SingleRL method, which Pang 2018 notes can be considered a special case of Konyushkova 2017's method. I think that the work in this paper would be sufficient to stand on its own if it performed a convincing comparison to SingleRL and/or MLP-GAL from Pang 2018. I recognize that this paper references why no such comparison currently exists, but I think this comparison would be extremely valuable to the paper. <sep> A further comment on my point (2), I do not find the comparisons to baseline methods to be entirely convincing. Of note, only the average performance for each method is reported. I'm curious of the variance---and more specifically the standard error and number of independent runs---of each of the reported results. On many of the datasets, the performance difference between the proposed method and uncertainty sampling is quite small in table 1. <sep> A final comment on point (2): I would have liked to see more exploration of different models. I think table 2 is quite informative, showing notable differences between simple baseline AL methods. I would have liked to see table 2 with more classifiers and with more competing AL methods. Because logistic regression is a simple model, the differences between AL methods may be more subtle. Perhaps a more complex model (say a single hidden layer NN) would show more notable differences. <sep> For point (3), I would have liked to see either an exploration of other design decisions or an explanation of given design decisions. For instance, why only use 30 hold-out samples for the state? I imagine the proposed method would be fairly sensitive to this choice.  Another unexplained design decision was using a maximum budget of 100 datapoints. Table 2 shows some extremely interesting interactions with this budget in its comparison between LogReg-100 and LogReg-200, and further explanation would have been useful. Finally, I would have liked to see some motivation for choice of stopping condition. Using the stopping condition of 98% of maximum performance may have some biasing effect of each method, and it would helpful to have some motivation behind this choice. <sep> Questions: <sep> - Why did uncertainty sampling have such limited benefits on LogReg-200 in table 2? This was a surprising result to me, as uncertainty sampling consistently outperformed most other methods. <sep> - Why is there a disparity between the results for the SVM in table 2 and the discussion in the first paragraph of section 4.3? <sep> - How does choice of final performance metric affect all methods? Choosing final performance to be 98% of maximum performance could have a major effect on each method. Because the proposed method is non-myopic, I would expect that it performs well when this value is large but would perform poorly with a smaller percentage of maximum performance. <sep> - Is the proposed method sensitive to number of samples used to compute the state? <sep> - What does figure 1 show? Are the same 30 samples used for all three subfigures? Perhaps this would more interpretable if, instead of showing the predicted class, this figure showed the prediction error. <sep> Minor nitpicks (did not influence decision): <sep> - The datasets are 1-based indexed sometimes and 0-based indexed sometimes, even with disparities within a single paragraph. <sep> - Figure 1 appears a long time before it is discussed, which made it difficult to understand what was going on.","This paper provides further insight into using RL for active learning, particularly by formulating AL as an MDP and then using RL methods for that MDP. Though the paper has a few insights, it does not sufficiently place itself amongst the many other similar strategies using an MDP formulation. I recommend better highlighting what is novel in this work (e.g., more focus on the reward function, if that is key). Additionally, avoid general statements like ""To this end, we formalize the annotation process as a Markov decision process"", which suggests that this is part of the contribution, but as highlighted by reviewers, has been a standard approach."
"Summary: <sep> The paper presents a method for ""learning an optimizer""(also in the literature Learning to Learn and a form of Meta-Learning) by using a Variational Optimization for the ""outer"" optimizer loss. The mean idea of the paper is to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weight them using a product of Gaussians formula for the mean. The method is simple and clearly presented. The paper also presents issues with the standard ""learning to learn"" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the ""exponential explosion of gradients"" which I think lacks enough justification as currently presented (see below for details). The ideas are clearly stated, although the work is not groundbreaking, but more on combining several ideas into a single one. <sep> Experiments: <sep> The authors evaluate their method on a single task which consists of optimizing a 3-layer convolutional neural network on downsampled images from ImageNet. A key idea, not new to this work, is to optimize the meta-optimizer with respect to the validation dataset rather than the training, which seems to be crucial for any meaningful training to happen. Although the experiments do show so promising results, they seem to be somewhat limited (see below for details). There is also a small ablation study on how do different features presented to the optimizer affect its performance. Given the still small-scale experiments, I'm not sure this is a significant result for the community. <sep> Conclusion: <sep> As a whole, I think the idea in the paper is a good one and worth investigating further. However, the objections I have on section 2.3 and the experiments seem to indicate that there needs to be more work into this paper to make it ready for publication. <sep> On section 2.3 and the explosion of gradients: <sep> There is a mistake in the equation on page 4 regarding the ""gradient with respect to the learning rate"". Although the derivation in Appendix A is correct, the inner product in the equation starts wrongly from j=0, where it should in fact start at j = i + 1. To be more clear the actual enrolled equation for dw^T/dt for 3 steps back is: <sep> dw^T/dt = (I - tH^{T-1})(I - tH^{T-2})(I - tH^{T-3}) dw^{T-3} - (I - tH^{T-1})(I - tH^{T-2}) g^{T-3} - (I - tH^{T-1}) g^{T-2} - g^{T-1} <sep> Hence the product must start at j = i + 1. <sep> It is correct that in this setting the equation is a polynomial of degree T of the Hessian, however, there are several important factors that the authors have not discussed. Namely, if the learning rate is chosen accordingly such that the spectral radius of the Hessian is less than 1/t then rather than the gradient exploding the higher order term will vanish. However, even if they do vanish for large T since the Hessian plays with smaller and smaller power to more recent gradients (after correcting the mistake in the equation) than the actual T-step gradient will never vanish (in fact even if tH = I then dw^T/dt = g^{T-1}). Hence the claims of exploding gradients made in this section coupled with the very limited theoretical analysis seem to unconvincing that this is nessacarily an issue and under what circumstances they are. <sep> The toy example with l(w) = (w - 4)(w - 3) w^2 is indeed interesting for visualizing a case where the gradient explosion does happen. However, surprisingly here the authors rather than optimizing the learning rate, which they analyzed in the previous part of the section, they are now optimizing the momentum. The observation that at high momentum the training is unstable are not really surprising as there are fundamental reasons why too high momentum leads to instabilities and these have been analyzed in the literature. Additionally, it is not mentioned what learning rate is used, which can also play a major role in the effects observed. <sep> As a whole, although the example in this section is interesting, the claims made by the authors and some of the conclusions seem to lack any significant justifications, in addition to the fact that usually large over-parameterized models behave differently than small models. <sep> Experiments: <sep> I have a few key issues with the experimental setup, which I think need to be addressed: <sep> 1. The CNN being optimized is quite small - only 3 layers. This allows the authors to train everything on a CPU. The key issue here, as well with previous work on Learning to Learn, is that it is not clear how scalable is this method to very Deep Networks. <sep> 2. Figure 1 - The setup is to optimize the problem for 10000 iterations, however, I think it is pretty clear even to the naked eye that the standard first-order optimizers (Adam/RMS/Mom) have not fully converged on this problem. Hence I think its slightly unfair to compare their ""final performance"" after this fixed period. Additionally using the curriculum the ""meta""-optimizer is trained explicitly for 10000 iterations. Hence, it is also unclear if it retains its stability after letting it run for longer. From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance - I hope this is the latter as that is the only way to fairly compare the two approaches. <sep> 3. Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction). Given however that these are plots for only 10000 iterations it is still unclear if this is scalable to larger problems.",The paper conveys interesting idea but need more work in terms of fair empirical study and also improvement of the writing. The AC based her summary only on the technical argumentation presented by reviewers and authors.
"This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach. <sep> Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference. <sep> Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1. <sep> PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.","The authors propose a framework for compressing neural network models which involves applying a weight distortion function periodically as part of training. The proposed approach is relatively simple to implement, and is shown to work for weight pruning, low-rank compression and quantization, without sacrificing accuracy. <sep> However, the reviewers had a number of concerns about the work. Broadly, the reviewers felt that the work was incremental. Further, if the proposed techniques are important to get the approach to work well in practice, then the paper would be significantly strengthened by further analyses. Finally, the reviewers noted that the paper does not consider whether the specific weight pruning strategies result in a reduction of computational resources beyond potential storage savings, which would be important if this method is to be used in practice. <sep> Overall, the AC tends to agree with the reviewers criticisms. The authors are encouraged to address some of these issues in future revisions of the work."
"This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory. <sep> While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy. <sep> Pros: <sep> - Novel defense technique against very challenging white-box attacks. <sep> - Sound threat model drawn from traditional security. <sep> - Clearly written. <sep> Cons: <sep> - Poor clean accuracy makes the technique very impractical. <sep> - Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.","This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written. <sep> However, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. <sep> Furthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. <sep> This concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. <sep> The paper thus requires significantly stronger baselines and attacks."
"I totally disagree with the authors that any of their observations are surprising. Indeed the fact that an RL agent does not generalizes to small modifications of the task (either visual or in the dynamics) is well known. If the agent should generalize though is a different question. And I do not mean this in the sense that it is an undesirable property but rather if it is outside of what ""learning one task"" means. Particularly I feel this is a very pessimistic view of RL and potentially not even in-line with what happens in supervised learning. <sep> I think one mantra of deep learning (and deep RL needs to obey by it) is that one should test in the same setting as training for things to truly work. For supervised, there is a distribution of data, and the test set are samples from the same distribution. However the testing scenario used here is slightly different. During training, if I do not see car accelerating, I think it makes no sense to expect to generalize to a new game that has this property as it is out-of-distribution. Of course it would be ideal if it could do that. And to clarify, while for us some of these extensions seem very similar and minimal changes, hence it should generalize to rather than transfer to, this is just the effect of imposing our own biases on the learning process. Deep Nets do not learn like we do, and in their universe they have never seen a car accelerating -- it makes sense that it might not to be able to generalize to it. Again, I'm not arguing that we don't want this, but rather if we should expect it as part of what the system should normally generalize to. <sep> To that end I think this paper enters in that unresolved dispute of what generalization should be versus what is transfer. At what point do we have truly a new task vs a sample from the same task. I don't think there is an answer. <sep> Going back to the observations in this work. I think the fact that the environment is not stochastic reinforces this overfitting (as in the extreme you end up with a policy that just repeats the optimal sequence of actions). I think in this particular case I can see how finetuning to a variation of the task fails. However true stochasticity in the environment (e.g. having a distribution of variations) like is done in Distral paper (where each episode is a different layout) can behave as a regularizer that will mitigate a bit the overfitting. That is to say that I believe the observed behaviour will be less pronounced in complex stochastic setting. <sep> Nevertheless the paper seems to highlight an important observation (and back it up with empirical evidence), namely we should use more regularization like L2 or otherwise in practice. Which is mostly absent from publications. And I think this on its own is valuable.","The authors have presented an empirical study of generalization and regularization in DQN. They evaluate generalization on different variants of Atari games and show that dropout and L2 regularization are beneficial. The paper does not contain any major revelations, nor does it propose new algorithms or approaches, but it is a well-written and clear demonstration, and it would be interesting to the deep RL community. However, the reviewers did not feel that the paper met the bar for publication at *CONF* because the experiments were not more comprehensive, which would be expected for an empirical study. The AC will side with the reviewers but hopes that the authors will expand their study and resubmit to another venue in the future."
"This paper proposes a new technique that can reduce the computational complexity of batch normalization. Several sampling methods called NS, BS, and FS are proposed, and additionally, VDN is proposed to generate random virtual samples.  Experiment results follow to support the authors' goal. <sep> pros) <sep> (+) The paper is clearly written and easy to follow. <sep> (+) The way of reducing the computational cost looks good. <sep> (+) The method can be easily adapted to BN or other batch-based methods. <sep> cons) <sep> (-) Any motivations or insights into NS, BS, and FS are not provided. Furthermore, the proposed sampling strategy looks heuristic without any studies. <sep> (-) For VDN, how to generate virtual samples is not clearly stated. I think the way of generating samples is critical to the performance of VDN but hard to find the exact way to do that. <sep> (-) How to determine the sampling ratio for each normalization method is not provided, and it would be better if the authors can show some studies about sampling ratio versus the speed gain. <sep> (-) It is hard to choose which normalization among FS and BS is better as looking at Table 2 and 3 only. So how about the speedup using BS+VDN? <sep> comments) <sep> - It is something strange why the authors used shallower ResNet on ImageNet and deeper ones on CIFAR datasets, maybe it was due to the training time, but the authors should clarify it. <sep> - What is the goal of the correlation analysis section? Especially, Figure 7 looks similar among BS, FS, VDN, and NS. Furthermore, the authors could include BN into the comparison. <sep> - This kind of paper should incoporate different ablation studies as much as the authors can, but it seems to be lacking. <sep> The paper has an interesting idea about sampling some features to speed up the batch normalization. However, it looks quite obvious and needs more experimental grounds such as ablation studies to support the idea.","This paper proposes a faster approximation to batch norm, which avoids summing over the entire batch by subsampling either random examples or random image locations. It analyzes some of the tradeoffs of computation time vs. statistical efficiency of gradient estimation, and proposes schemes for decorrelating the samples to make good use of smaller numbers of samples. <sep> The proposal is a reasonable one, and seems to give a noticeable improvement in efficiency. However, it's not clear there is a substantial enough contribution for an *CONF* paper. The idea of subsampling is fairly obvious, and various other methods have already been proposed which decouple the computation of BN statistics from the training batch. From a practical standpoint, it's not clear that the observed benefit is large enough to justify the considerable complexity of an efficient implementation."
"This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. This work was very difficult to follow, so it is somewhat unclear what were the main contributions (since much of this seems to be covered by other works as referenced within the paper and as related to similar unreferenced works below). Moreover, regarding the experiments, many things were unclear (some of the issues are outlined below). While the overall idea of using logic in this way to help with skill composition is interesting and exciting, I believe several things must be addressed with this work. This includes: situating this work more clearly against existing similar works which use logic in this way, clearly defining the novel contributions of this work as compared to those and others, overall making the methodology more clear and specific (including experimental methodology), and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., HRL multi-task learning, meta-learning) to emphasize the need/importance of this work — I am aware that at least 1 HRL work is mentioned, but this work is not really contrasted against it to help situate it. <sep> Questions/Concerns about Experiments: <sep> + Does Figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step, or 5 episodes, each from a separate run averaged together? It is a bit unclear especially because the main text and the figure caption slightly differ. Also, average discounted return is somewhat different than average return,  suggest updating the label to be clear also with the discount factor used. <sep> + What were the standard deviations for this across experiments? Even with averaging it seems that these runs are very high variance, would be good to understand what variance bounds to expect if using this method. <sep> + Why were average discounted returns reported in Figure 5 and not in Table 1? <sep> +  What were the standard deviations on success rate and training time? Also what about sample complexity? <sep> + To my understanding the benefit here is reusability of learned skills via the automata methods described here. It would have made sense to compare against other HRL or multi-task learning methods in addition to just SQL or learning from scratch. For example how would MAML compare to this? <sep> + It is also unclear whether the presented results in Table 1 and Figure 5 are on the real robot or in simulation. The main text says, ""All of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning."" So does this mean that Figure 5 is simulated results and Table 1 is on the real robot? <sep> Citations that should likely be made: <sep> + Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. ""Reinforcement Learning for LTLf/LDLf Goals."" arXiv preprint arXiv:1807.06333 (2018). <sep> + Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Decision-making with non-markovian rewards: From LTL to automata-based reward shaping.""  In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017. <sep> + Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping."" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017. <sep> Typos/Suggested grammar edits: <sep> ""Skills learned through (deep) reinforcement learning often generalizes poorly across tasks and re-training is necessary when presented with a new task."" —> Often generalize poorly <sep> ""We present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for convenient specification of complex temporal dependent tasks with logical expressions and construction of new skills from existing ones with no additional exploration."" —> Sentence kind of difficult to parse and is a run-on <sep> ""Policies learned using reinforcement learning aim to maximize the given reward function and is often difficult to transfer to other problem domains."" —> ..and are often.. <sep> ""by authors of (Todorov, 2009) and (Da Silva et al., 2009)"" —> by Todorov (2009) and Da Silva et al. (2009) Also several other places where you can use \\citet instead of \\cite","The authors present an interesting approach for combining finite state automata to compose new policies using temporal logic. The reviewers found this contribution interesting but had several questions that suggests that the current paper presentation could be significantly clarified and situated with respect to other literature. Given the strong pool of papers, this paper was borderline and the authors are encouraged to revise their paper to address the reviewers' feedback."
"This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. The model is a simple combination of GCN and existing framework for community detection. The proposed algorithm is compared to baselines on various datasets, and demonstrated to be accurate in many cases. <sep> I think the paper does not deal with one of the most important aspects of network modeling - the degree heterogeneity of nodes. Many works reported that lack of degree corrections would result in bad estimates of community structures [1,2,3]. Probably including the degrees as feature of nodes would be helpful. <sep> Regarding the stochastic gradient descent by edge subsampling, I think the authors should mention [4], where the idea of edge subsampling in stochastic gradient descent setting was introduced before this work. Also, it is worth noting that we may lose some important distributional properties in graphs if we naively subsample from it [5]. For instance, sampling from positive and negative pairs to balance the class contribution may distort the sparsity and degree distributions of subsampled graphs. <sep> If we choose to use Bernoulli-Poisson link function, we can reduce the time complexity of likelihood and gradient computation to O(N + E), where N is the number of nodes and E is the number of edges, with the auxiliary variable trick introduced in [6]. In that case we don't really have to worry about subsampling. Why didn't you consider applying this to your model? <sep> Regarding the experiments, I think some important baselines are missing [3, 6]. Also, I wonder whether the proposed algorithm would scale to the graphs with more than 100,000 nodes. <sep> References <sep> [1] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Physical Review E, 83(1):016107, 2011. <sep> [2] P. K. Gopalan, C. Wang, and D. Blei. Modeling overlapping communities with node popularities. NIPS 2013. <sep> [3] A. Todeschini, X. Miscouridou and F. Caron. Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities. CoRR 2016. <sep> [4] J. Lee, C. Heakulani, Z. Ghahramani, L. F. James, and S. Choi. Bayesian inference on random simple graphs with power law degree distributions. ICML 2017. <sep> [5] P. Orbanz. Subsampling large graphs and invariance in networks. CoRR 2017. <sep> [6] M. Zhou. Infinite edge partition models for overlapping community detection and link prediction. AISTATS 2015","The paper provides an interesting combination of existing techniques (such as GCN and and the Bernoulli-Poisson link) to address the problem of overlapping community detection. However, there were concerns about lack of novelty, evaluation metrics, and missing comparisons with previous work. The authors did not provide a response to address these concerns."
"This paper describes a recurrent model (LSTM specifically, but generalizable) which can produce variable-wise hidden states that can be further used for two types of attentions: 1) variable importance for the importance of each variable (not accounting for time), and 2) temporal importance of each variable for the importance of each variable over time. The proposed NN model (IMV-LSTM) does not seem to directly provide such importance. Rather, the outputs are ""decomposed"" for each variable/time that allows probabilistic inference on top of this. <sep> One of my main concerns (described in Cons/Comments below) is how it is not straightforward to grasp the quality of variable importance and temporal variable importance results despite this is the key strength of this paper. If this comes from my lack of understanding, I would appreciate if the authors could provide a little more explanation. <sep> Pros: <sep> 1. The overall quality of the paper is decent and mostly clear. <sep> 2. The experiments are quite extensive. <sep> 3. The fact that each variable should have different level of importance is interesting and practical. <sep> Cons/Comments: <sep> 1. The term ""tensor"" is used throughout the paper to describe the stacked matrices. While this is not technically wrong to describe 2>-dimensional structures, this term could potentially imply (and make the readers to expect) tensor-based schemes such as tensor decomposition. This is not necessarily bad, but to me, ""tensor"" and ""variable-wise correspondence"" do not seems to be associated too deeply since the ""tensor"" used in IMV-LSTM is a stack of matrices that are also independently used with respect to each other. <sep> 2. The variable importance experiments seem quite extensive and thorough, especially the lists of variable-wise temporal importance matrices provided in the appendix. However, the authors could provide some significance or relevance of the findings with respect to any domain knowledge or literature, it may help further appreciate and interpret the quality of the variable importance which is quite subjective to non-experts. Such information may not even need to be in the main paper; including a short description in the appendix. <sep> 3. Related to comments (2), the difference between IMV-Full and IMV-Tensor is hard to interpret since neither one is always better than the other (i.e., IMV-Full > IMV-Tensor in some experiments, vice versa). While the key difference is speculated to be from how the LSTM handles the variables, I am curious how this related to the differences in the results and how the differences variable importance results (i.e., Fig.3) can be in at least speculated. <sep> Questions: <sep> 1. Should \\tilde{h}_t in Figure 1 (a) be \\tilde{h}__{t-1} since this hidden state is from t-1? The figure itself currently implies that the hidden state for t is used, but this is computed from x_t using U_j. With \\tilde{h}__{t-1}, it follows Eq.(1). <sep> 2. In Equation set 2 for IMV-Tensor, are W and U (not W_j and U_j) also in tensor forms so each variable and hidden state get transformed correspondingly (i.e., W_1 for h^1_{t-1}, U_1 for x^1_t). <sep> 3. The IMV-Tensor version of IMV-LSTM (related to the question above) can be considered as a set of parallel LSTMs, one for each variable. Such independence could also be inferred from Figure 1. If that's the case, where do the variables ""interact"" with each other? Is this happening in the later stage where the hidden states across variable/time are aggregated in the attention stage (Eq.(8) and on)? <sep> 4. Up until Eq.(8), n was used for the variable index where n = 1,…,N. In Eq.(8), it seems to be still used as the variable index (i.e., h_T^n and g^n), but it is also a set of possible values for a random variable z_{T+1}. Is n used the same way for z_{T+1} as well? I am slightly confused on how z is used. Also, (just to clarify), if we use N variables, we are using y_t as well (i.e., [x_t^1,…,x_t^{N-1}, y_t])? <sep> 5. f_agg: Is this for aggregating over instances? For \\bar{\\alpha}^n, I'm guessing this is aggregated over instances for variable n for t=1,…,T_1. <sep> 6. I am not too familiar with the notion of ""time-lag"" in the experiments. If the authors could explain this a little bit, I would appreciate it.","The reviewers appreciated the clarity of writing, and the importance of the problem being addressed. There was a moderate amount of discussion around the paper, but the two reviewers who responded to the author discussion were split in their opinion, with one slightly increasing their score to a 6, and the other remaining unconvinced. The scores overall are borderline for *CONF* acceptance, and given that, no reviewer stepped forward to champion the paper."
"This is largely an experimental paper, proposing and evaluating various modifications of variational recurrent models towards obtaining sequence data representations that are effective in downstream tasks. The highlighted contribution is a ""stochastic generation"" training procedure in which the training objective evaluates the reconstruction of output sequence elements from individual latent variables independently. The main claim is that the resulting model, augmented with prior updating and/or hierarchical latent variables, improves results w.r.t. the baselines. <sep> My main concern is that the various choices are not motivated well, e.g. with examples or detailed descriptions of the issues addressed and that the resulting implications are not discussed in detail (see detailed comments below). This could perhaps be alleviated during the rebuttal discussion. <sep> Empirically, when used in conjunction with prior updating and/or hierarchical latent variables, the proposed ""stochastic generation"" approach improves upon the baselines, but not when used in isolation. This is OK, but it weakens the contribution since it's more unclear what the exact advantage ""stochastic generation"" is, how it takes advantage of prior updating, and so on. Could you maybe discuss this in the rebuttal? The fact that not all model variants considered are evaluated on all settings also contributes to this problem (again, see below). <sep> General questions: <sep> - ""dependence of observations at each time step on all latent variables"": Unfortunately, this means that the complexity of evaluating the model during training is O(n^2), where n is the sequence size, rather than linear in the standard case. Is that correct? I think this is what is alluded to on the top on page 4. Could you discuss this trade-off? <sep> - regarding section 2.1.: Multi-modal marginal probabilities are also used due their increased modeling power, and this again seems like a potential limitation of the proposed approach w.r.t. the baseline, and is not discussed. <sep> - ""the mean of z_t may have very small probability and thus may not be a good choice"": I think this statement requires more context. The mean of z_t can have low probability in both cases (e.g. if the posterior has a high variance). Are you suggesting that the low probability issue is exacerbated by to the sampling of previous z_{t-1}? Or are you comparing to the case where the mean z_{t-1} is used instead of sampling as well? <sep> Stochastic generation: <sep> - While I understand where it's coming from, the term ""stochastic generation"" is somewhat misleading, since stochasticity is already present in the generation process for VAEs; <sep> - Stochastic generation is introduced as a way to approximate the generation process. However, when it's introduced, it's not clear what the generation process that needs to be approximated is. Introducing the model in eq. (6-7), motivating its use and then showing how it is obtained through stochastic generation second would improve the clarity of the paper. <sep> - Related to the point above, the implications of using the model in eq. (6-7) are not discussed. The graphical model in Figure 1 suggests that x_k depends jointly on all the (z_t)_{t=1 ... sequence_size}. Instead, in eq. (6-7), each x_k is generated independently from each z_t (for t = 1 ... T, and k sampled from a distribution which depends on t). In particular, if I understand this correctly, the distribution p(x_k | z) = p(x_k | z_1 \\dots z_T) factorizes as p(x_k | z_1) p(x_k | z_2) ... p(x_k | z_T). Could you motivate this choice and its expected effect? It seems to me that this encourages each z_t to capture all the information needed to reconstruct each x_k in the corresponding window. <sep> Experimental results: <sep> - Table 2: I think this table since it includes most models, but it still misses RecRep (without delta = 0) and StocCon. Could you confirm whether StocCon vs. RecRep have the same setting except the use of recurrent stochastic connections in StocCon vs. using eq. (4) in RecRep with window size 1? <sep> - In Table 4, the difference between line 5 and line 6 is interesting and I wish it was discussed more, maybe used in the visualization experiment to show how/why ""stochastic generation"" with a larger window improves performance. <sep> - Figure 3, could it be that the use of hierarchical latent variables (H) accounts for the visual difference? Is a difference still observed when comparing lines 3 and 7 in Table 4, whose settings seem more comparable? <sep> - <sep> Minor issues: <sep> - the lack of parenthesis around citations makes the text hard to follow at times (maybe use \\citep whenever the citation mixes with the text?); <sep> - typo: ""for use in a downstream tasks"" <sep> - typo: ""with graphical model as described"" => ""with the/a graphical model as described""","This paper heavily modifies standard time-series-VAE models to improve their representation learning abilities. However, the resulting model seems like an ad-hoc combination of tricks that lose most of the nice properties of VAEs. The resulting method does not appear to be useful enough to justify itself, and it's not clear that the same ends couldn't be pursued using simpler, more general, and computationally cheaper approaches."
"The paper studies a particular task (the XOR detection problem) in a particular setup (see below), and proves mathematically that in that case, the training performs better when the number of features grows. <sep> The task is the following one: <sep> - consider a set of pairs of binary values (-1 or +1); <sep> - detect whether at least one of these pairs is (+1, +1) or (-1, -1). <sep> The design of the predictor is: <sep> - for each pair, compute 2k features (of the form ReLu(linear combination of the values, without bias)); <sep> - compute the max over all pairs of these features (thus obtaining 2k values); <sep> - return the k first values minus the k last ones. <sep> The training set consists only of examples having the following property [named 'diversity']: <sep> - if the example (which is a set of pairs) is negative (i.e. doesn't contain (+1,+1) nor (-1,-1)), then it contains both (-1,1) and (1,-1); <sep> - if the example is positive, it contains all possible pairs. <sep> The paper proves that, under this setup, training with a number of features k > 120 will perform better than with k = 2 only (while k = 2 is theoretically sufficient to solve the problem). While tackling an interesting problem (impact of over-parameterization), the proof is specific to this particular, unusual architecture, with a ""max - max"" over features independently computed for each pair that the example contains; it relies heavily on the fact that the input are binary, and that the number of possible input pairs is small (4), which implies that the features can take only 4 values. Note also that the probabilities in some theorems are not really probabilities of convergence/performance of the training algorithm per se (as one would expect in such PAC-looking bounds), but actually probabilities of the batch of examples to all satisfy some property (the diversity). <sep> Thus it is difficult to get from this study any insight about the over-parameterization / training ability phenomenon, for more general tasks, datasets or architectures. <sep> Though clearly an impressive amount of work has been done in this proof, I do not see how it can be generalized (there is no explanation in the paper in that regard either, while it would have been welcomed), and consequently be of interest for the vast majority of the *CONF* community, which is why I call for rejection.","`This paper tackles the problem of learning with one hidden layer non-overlapping conv net for XOR detection problem. For this problem the paper shows that over parametrized models perform better, giving insights into why larger neural networks generalize better - an interesting question to study. However reviews opined that the setting considered in this paper is too specific to this XOR problem and the simplified network architecture, and the techniques are not generalizable to other models. Generalizing these results to more complex architectures or other learning problems will make the paper more interesting."
"Overall this paper contributes many interesting insights into the specific application of empathetic dialog into chatbot responses.  The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. <sep> While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders. <sep> There is a lot in this paper, and I think it could have been better organized. <sep> I am more familiar with emotion related research and not language to language translation, so I would have appreciated a better explanation of the rationale for using BLEU scores.  I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity, it is unclear how they capture ""relevance"" at least according to the brief tutorial that I read (https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).  I did not see the paper describing the use of this score in the references but perhaps I missed it – could you please clarify why this is a good metric for relevance?  It seems that these scores are very sensitive to sentence variation.  I am not sure if you can measure empathy or appropriateness of a response using this metric. <sep> For your data collection you have 810 participants and 24,850 conversations.  Are the 810 participants all speakers or speakers and listeners combined?  How many conversations did each speaker/listener pair perform 32?  (one for each emotion) or 64? (two for each emotion) Was the number variable?  If so what is the distribution of the contribution – e.g. did one worker generate 10,000 while several hundred workers did only three of four?  Was it about even?  Just for clarity – how did you enroll participants?  Was it through AMT?  What were the criteria for the workers?  E.g. Native English speaker, etc. <sep> In your supplemental material, I found the interchanging of the words ""context"" and ""emotion"" confusing.  The word context is used frequently throughout your manuscript: ""dialog context,"" ""situational context"" - emotions are different from situations, the situational utterance is the first utterance describing the emotion if I read your manuscript correctly.  Table 6 should use ""Label"" or ""Emotion"" instead of the more ambiguous ""Context."" <sep> My understanding is that speakers were asked to write about a time when they experienced a particular feeling and they were given a choice of three feelings that they could write about.  You then say that workers are forced to select from contexts they had not chosen before to ensure that all of the categories were used.  From this I am assuming that each speaker/listener worker pair had to write about all 32 emotions – is this correct?  Another interpretation of this is that you asked new workers to describe situations involving feelings that had not been chosen by other workers as data collection progressed to ensure that you had a balanced data set.  This would imply that some emotional situations were less preferred and potentially more difficult to write about.  It would be interesting if this data was presented.  It might imply that some emotion labels are not as strong if people were forced to write about them rather than being able to choose to write about them. <sep> Were these dialogs ever actually annotated?  You state in section 2, Related Work ""we train models for emotion detection on conversation data that has been explicitly labeled by annotators"" – please describe how this was done.  Did independent third party annotators review the dialogs for label correctness?  Was a single rater or a majority vote used to decide the final label.  For example, in Table 1, the label ""Afraid"" is given to a conversation that could also have reasonable been generated by the label ""Anxious"" a word explicitly used in the dialog.  I am guessing that the dialogs are just labeled according to the label / provocation word and that they were not annotated beyond that, but please make this clear. <sep> In the last paragraph you state ""A few works focus.."" and then list 5.  This should rather be ""several other works have focused on "" … <sep> Conversely, you later state in section 3 ""Speaker and Listener"", ""We include a few example conversations from the training data in Table 1,"" this should more explicitly be ""two."" <sep> Also in section 3 when you describe your cross validation process, you state ""We split the conversations into approximately 80/10/10 partitions.  To prevent overlap of <<discussed topics>> we split the data so that all the sets of conversations with the same speaker providing the prompt would be in the same partition. <sep> In your supplemental material you state that workers were paired.  Each worker is asked to write a prompt, which also seems to be the first utterance in the dialog they will start.  You state each worker selects one emotion word from a list of three which is somehow generated (randomly?) form your list of 32 .  I am assuming each worker in the pair does this, then the pair has a two ""conversations"" one where the first worker is the speaker and another where the second worker is the speaker – is this correct?  It is not entirely clear from the description. Given that you have 810 workers and 24,850 conversations, I am assuming that each worker had more than one conversation.  My question is  - did they generate a new prompt / first utterance for each conversations.  I am assuming yes since you say there are 24,850 prompts/conversations.  For each user are all of the situation/prompts they generate  describing the same emotion context?  E.g. would one worker write ~30 conversations on the same emotion.  This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. ""fear"" several times.  If the same worker was allowed to select the same emotion context multiple times was it found that they re-used the same prompt several times?  I am assuming that this is the case and that this is what you mean when you say that you ""prevent overlap of discussed topics"" between sets when you exclude particular workers.  Is this correct?  Or did you actually look and code the discussed topics to ensure no overlap even across workers (e.g. several people might have expressed fear of heights or fear of the dark). <sep> In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt.  Calling these both prompts makes the statement about 24,850 prompts/conversations a bit ambiguous.  A better statement would be 24,850 conversations based on unique situational prompts/descriptions (if they are in fact unique situational prompts.  I am assuming they are not if you are worried about overlapping ""discussed topics"" which I am assuming are the situational prompts since the dialogs are very short and heavily keyed off these initial situational prompts) <sep> In your evaluation of the models with Human ratings you describe two sets of tests.  In one test you say you collect 100 annotations per model.  More explicitly, did you select 100 situational prompts and then ask workers to rate the response of each model?  Was how many responses was each worker shown?  How many workers were used?  Are the highlighted numbers the only significant findings or just the max scores?  Annotations is probably not the correct word here. <sep> Please also describe your process for assigning workers to the second human ratings task. <sep> Since the two novel aspects of your paper are the new dataset and the use of this dataset to create more empathetic chatbot responses (""I know the feeling"") I have focused on these aspects of the paper in my review. <sep> I found the inclusion of Table 7 underexplained in the text.  The emotion labels for all these datasets are not directly comparable so I would have liked to have seen more explanation around how these classifications were compared.  It would also be helpful to know how more similar emotions such as ""afraid"" and ""anxious"" were scored vs ""happy"" and ""sad"" confusions","The reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic (and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data), no methodological novelty in the presented work, and relatively uninteresting application with very limited real-world application (it remains unclear whether having better empathetic dialogues would be truly crucial for any real-life application and, in addition, all work is based on acted rather than real-world data). The authors' rebuttal addressed some of the reviewers' concerns but not fully (especially when it comes to usefulness of the data). Overall, I believe that the effort to collect the presented database is noble and may be useful to the community to a small extent. However, given the unrealism of the data and, in turn, very limited (if any) generalisability of the presented to real-world scenarios, and lack of methodological contribution, I cannot recommend this paper for presentation at *CONF*."
"Summary: This paper proposes a technique for quantizing the weights and activations of a CNN. The main contribution is in replacing the heuristic to find good quantization intervals of (Zhu et al, 2016) with a different heuristic based on a hierarchical clustering algorithm, and empirically validating its effectiveness. <sep> Strenghts: <sep> - The proposed nested-means heuristic is simple and makes sense intuitively. <sep> - The experiments on two modern architectures seem solid and demonstrate good empirical performance. <sep> Weaknesses: <sep> - The main weakness is the limited novelty of this paper. The proposed setup is almost identical to the one in (Zhu et al, 2016), except for the replacement of the heuristic to find quantization intervals with another one. While the experiments demonstrate the empirical effectiveness of the method as a whole, what is missing is a direct, controlled comparison between the original heuristic and the proposed one. Now it is hard to tell whether the accuracy increases are obtained through the proposed adaptation or because of other factors such as a better implementation or longer training. <sep> - In section 4, it is not made clear whether the activations are quantized according to the same scheme as the weights (apart from the issue of selecting a good clipping interval, which is addressed). <sep> - The paper is a bit short on references, considering the many recent works on quantized neural networks. <sep> Minor comments and questions: <sep> - The wording is sometimes imprecise, making some arguments hard to follow. Two examples: <sep> -- ""Lowering the learning rate for re-training can diminish heavy changes in the weight distribution, at the cost of longer time to converge and the risk to get stuck at plateau regions, which is especially critical for trainable scaling factors"" <sep> -- ""This approach is beneficial because it defines cluster thresholds which are influenced by large weights that were shown to play a more important role than smaller weights (Han et al., 2015b)"" <sep> - The title says ""for compression and inference acceleration"", so it would be nice if the paper reports some compression and timing metrics in the experiments section. <sep> - The notation in section 3.1 overly complicated, could probably be simplified a bit for readability. <sep> - Section 3.3: ""However, having an additional hyperparameter t_i for each scaling factor alpha_i renders the mandatory hyperparameter tuning infeasible."" -> From section 4.2 in (Zhu et al, 2016), I believe the constant factor t is shared across all layers, making it only a single hyperparameter. <sep> - Last paragraph of section 4: ""(Cai et al., 2017) experimentally showed that the pre-activation distribution after batch normalization are all close to a Gaussian with zero mean and unit variance. Therefore, we propose to select a fixed clipping parameter gamma."". -> But what about the activations *before* the batchnorm layer where the assumption of zero mean and unit variance does not hold?","The submission proposes a hierarchical clustering approach (nested-means clustering) to determine good quantization intervals for non-uniform quantization. An empirical validation shows improvement over a very closely related approach (Zhu et al, 2016). <sep> There was an overall consensus that the literature review was insufficient in its initial form. The authors have proposed to extend it somewhat. Other concerns are related to the novelty of the technique (R4 was particularly concerned about novelty over Zhu et al, 2016). <sep> Two reviewers were against acceptance, and one was positive about the paper. Due to the overall concerns about the novelty of the approach, and that these concerns were confirmed in discussion after the rebuttal, this paper is unlikely to meet the threshold for acceptance to *CONF*."
"This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments: <sep> 1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada. <sep> Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018 <sep> 2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. <sep> Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017. <sep> Johnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016. <sep> Kim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016. <sep> 3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth?",This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non-medical datasets. <sep> Reviewers raise multiple concerns around the following: <sep> - Novelty (all reviewers) <sep> - Inadequate comparison baselines (all reviewers) <sep> - Inadequate citations. (R2 & R3) <sep> Authors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal.
"The authors analyze the convergence performance of Riemannian gradient descent based algorithm for the dictionary learning problem with orthogonal dictionary and sparse factors. They demonstrate a polynomial time convergence from a random initialization for a smooth surrogate objective for the original non-smooth one. The problem and the analysis are of interest, but I have several questions regarding the paper as follows. <sep> My first concern is that the analysis is on a smooth surrogate of the non-smooth sparse minimization for solving the dictionary learning problem, so it is not clear what is relationship between the global minimizer of the smooth problem to the underlying true dictionary. More specifically, how far is the global minimizer of problem (1) or (2) to the true dictionary parameter, and whether they share (approximately) the space or components regarding the sparse factors. Without clarifying this, it is not well motivated why we are interested in studying the problem considered in this paper at the beginning. Intuitively, since the recovered factors are not sparse anymore, it will impact the dictionary accordingly due to the linear mapping, which may lead to a very different set of dictionary components. Thus, explicit explanation is necessary here to avoid such degenerate case. <sep> My second concern is the eligibility of assuming the dictionary A to be an identity matrix and extending it to the general orthogonal matrix case. The analysis uses the fact that rows of A are canonical basis, i.e., each row only has one non-zero entry. I do not see a straightforward extension by replacing A to be an orthogonal matrix as the authors claimed on page 3, since then the inner product of one row of A and one column of X is not just the corresponding entry of the column of X. It will be helpful if the authors can explain this explicitly or adjust the analysis accordingly to make this valid. <sep> Another issue is the clarity of the paper. Some statements in the paper are not very clear. Form example, on page 3, third paragraph of Section 3, what does row(Y) = row(X_0) mean? Also, in eqn (1), y_k means k-th column of Y, and in eqn (2), q_i means i-th entry of q? Since both are bold lower case letters, clear distinction will help. Moreover, the reference use (. ) instead [ .], which can be confusing sometimes.",It seems that the reviewers reached a consensus that the paper is not ready for publication in *CONF*. (see more details in the reviews below. )
"Summary: <sep> This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic. <sep> Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off. <sep> - Quality: Paper is technically complex, but based on simple ideas. In the case of infinite mixtures, it is not clear what is done in the end in the experiments. <sep> Experimental results are rather poor, given state-of-the-art. <sep> - Clarity: The paper is not hard to understand. What is done, is done cleanly. <sep> - Originality: The idea of putting a mixture model on the global parameters is not surprising. Important questions, such as how to make this faster, are not addressed. <sep> - Significance: The only comparative results on miniImageNet are worse than the state-of-the-art by quite a margin (admittedly, the field moves fast here, but it is also likely these benchmarks are not all that hard). This is even though better performing methods, like Versa, are much cheaper to run <sep> While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea. <sep> State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: <sep> - Versa: https://arxiv.org/abs/1805.09921. <sep> Importantly, this method uses a simpler model (logistic regression head models) <sep> and is quite a bit faster than MAML, so much faster than what is proposed here <sep> - BMAML: https://arxiv.org/abs/1806.03836. <sep> This is also quite complex and expensive, compared to Versa, but provides good results. <sep> Other points: <sep> - You use a set of size N+M per task update. In your 5-way, 1-shot experiments, <sep> what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5, <sep> then I wonder why results are branded as 5-way, 1-shot, which to mean means that each update can use exactly 5 labeled points. <sep> Please just be exact in the main paper about what you do, and what main competitors do, in particular about the number of points to use in each task update. <sep> - Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and uses further approximations (ICM, instead of Gibbs sampling). <sep> Can be seen as a heuristic to evolve the number of components. <sep> What is given in Algorithm 2, is not compatible with Section 4. How do you merge your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid that there is always one more (L -> L+1) components? Some threshold must be applied somewhere. <sep> An alternative would be to use split&merge heuristics for EM. <sep> - Results reported in Section 5 are potentially interesting, but entirely lack a reference point. The first is artificial, and surely does not need an algorithm of this complexity. The setup in Section 5.2 is potentially interesting, but needs more work, in particular a proper comparison to related work. <sep> This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based.","This paper is extending the meta-learning MAML method to the mixture case. Specifically, the global parameters of the method are now modeled as a mixture. The authors also derive the elaborate associated inference for this approach. <sep> The paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth. <sep> The results do not convince any of the three reviewers. Rev3 asks for a clearer exposition of the results to increase convincingness. Rev2 and Rev1 also make similar comments. <sep> Rev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated. Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect. <sep> The reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness. Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade-offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights. <sep> Finally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them)."
"Clarity: <sep> The work is a clear introduction/overview of this area of research. The reviewer enjoyed the connections to Multiple-Gradient Descent and clear distinctions/contrasts with previous approaches to weighting the outputs of multiple discriminators. All in all, the paper is quite clear in what its contributions are and how it differs from previous approaches. The details and motivations of the Hypervolume Maximization  (HVM) method (especially as it relates to and interacts with the slack method of picking the nadir point) were a bit harder to follow intuitively given the standalone information in the paper. <sep> Originality: <sep> Adapts a technique to approximate MGD called HVM (Miranda 2016) and applies it to multi-discriminator training in GANs. As far as the reviewer is aware, this is a novel application of HVM to this task and well motivated under the MGD interpretation of the problem. <sep> Significance: <sep> Unclear. This work in isolation appears to present an improvement over prior work in this sub-field, but it is not obvious that the findings in these experiments will continue to be robust in more competitive settings. For instance, the worst performing model on CIFAR10, WGAN-GP (according to the experiments run) WGAN-GP also holds near SOTA Inception scores on CIFAR10 when appropriately tuned. Without any experimental results extending beyond toy datasets like MNIST and CIFAR10 the reviewer is not confident whether fundamental issues with GAN training are being addressed or just artifacts of small scale setups. Closely related previous work (Neyshabur 2017) scaled to 128x128 resolution on a much more difficult dataset - Imagenet Dogs but the authors did not compare in this case. <sep> Quality: <sep> Some concerns about details of experiments (see cons list and significance section for further discussion). <sep> Pros: <sep> + The work provides a clear overview of previous work on approaches using multiple discriminators. <sep> + The connections of this line of work to MGD and the re-interpretation of various other approaches in this framework is valuable. <sep> + The author provides direct comparisons to similar methods, which increases confidence in the results. <sep> + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. <sep> Cons: <sep> - Performance of GANs is highly dependent on both model size and compute expended for a given experiment (see Miyato 2018 for model size and training iterations and Brock 2018 for batch size). Training multiple discriminators (in this paper up to 24) significantly increases compute cost and effective model size. No baselines controlling for the effects of larger models and batch sizes are done. <sep> - The paper lacks experiments beyond toy-ish tasks like MNIST and CIFAR10 and does not do a good job comparing to the broader established literature and contextualizing its results on certain tasks such as CIFAR10 (reporting ratios to a baseline instead of absolute values, for instance). The absolute inception score of the baseline DCGAN needs to be reported to allow for this. Is the Inception Score of the authors DCGAN implementation similar to the 6 to 6.5 reported in the literature? <sep> - Figure 3 is slightly strange in that the x axis is time to best result result instead of just overall wallclock time. Without additional information I can not determine whether it is admissible. Do all models achieve their best FID scores at similar points in training? Why is this not just a visualization of FID score as a function of wallclock time? A method which has lower variance or continues to make progress for longer than methods which begin to diverge would be unfairly represented by the current Figure. <sep> Additional comments: <sep> In section 3.1 Eq 5 appears to be wrong. The loss of the discriminator is presented in a form to be minimized so exponentiating the negative loss in the softmax weighting term as presented will do the opposite of what is desired and assign lower weight to higher loss discriminators. <sep> In Fig 6 FID scores computed on a set of 10K samples are shown. The authors appear to draw the line for the FID score of real data at 0. But since it is being estimated with only 10K samples there will be sampling error resulting in non-zero FID score. The authors should update this figure to show the box-plot for FID scores computed on random draws of 10K real samples. I have only worked with FID on Imagenet where FID scores for random batches of 10K samples are much higher than 0. I admit there is some chance the value is extremely low on CIFAR10 to make this point irrelevant, however.","The reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature. The proposed method for using multiple discriminators in a multi-objective setting to train GANs seems interesting and compelling. However, all the reviewers found the paper to be on the borderline. The main concern was the significance of the work in the context of existing literature. Specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in GAN training."
"Summary: <sep> The paper proposes a hybrid approach which combines evolution and RL. The key idea is to conduct tournament selection over a population of architectures with learned mutations. The mutations are defined as the output of an RNN controller which either reuses or alters the sequence descriptor of the parent at each step. The proposed hybrid architect is evaluated on both synthetic and text classification tasks and then compared against pure evolutionary and RL-based agents. <sep> Pros: <sep> * The method can be viewed as a generalization of conventional evolution by replacing the handcrafted (uniform) distribution of mutations with a learned one. On the one hand, this should hopefully improve the sample efficiency of pure genetic methods since the population can evolve towards more meaningful directions, assuming useful patterns can be learned by the mutation controller. On the other hand, mutating existing architectures seem a easier task than sampling the entire architecture from scratch. <sep> * The synthetic experiment is interesting, though it's hard to draw any conclusions based a single task. <sep> Cons: <sep> * To my knowledge, all text classification tasks used in 5.2 are quite small. There is no evidence that the method can scale to and work well on large-scale tasks, where improving the sample efficiency becomes truly crucial and challenging. <sep> * It is good to see comparisons against pure evo and RL within the authors' own search space. However, the advantage of the proposed evo-NAS, especially when evaluated on real-world text classification tasks, does not seem significant enough. In particular, there is a clear overlap between the performance of architectures found by NAS, evo and evo-NAS (Figure 4). The advantage of evo-NAS is even smaller if we compare the very best model (as can be read from Figure 4) instead of the average among the top 10 (as reported in Table 2). In my option, performance of the strongest model is arguably more interesting than the averaged one in practice. <sep> * Since no results on CIFAR or ImageNet are provided as in most prior works in the literature, it is impossible to empirically compare the method with the state-of-the-art. The experiments would be more convincing if a comparison can be provided on those benchmarks. Otherwise, it is possible that the current search space & hyperparameters are tailored towards evo-NAS and it remains unclear whether the method can generalize well to other domains and/or search spaces.",Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
"This paper develops an unsupervised classification algorithm using the idea of CycleGAN. Specifically, it constructs a piece-wise linear mapping (the Connector Network) between the discriminator network and the generator network. The learning objective is based on the cycle-consistency loss. Experiments show that it can achieve reasonable loss. This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. However, the paper is not in a good shape for publication in its current form. <sep> First, the paper is not well written and many of the key ideas are not clear. It devotes more than half of the pages to the review of the preliminary materials in Sections 2-3 while only briefly explained the main algorithm in Section 4. Many of the details are missing. For example, why L1-loss is used in (5)-(7) in Algorithm 1? What is the ""random-walk Laplacian matrix L_{sym}"" (never defined)? More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification. However, the ideas (regarding the footprints and footprint mask etc.) are totally unclear. It is assumed that all different classes have equal probabilities. In this setting, it is unclear (in its intuition) why it is possible to assign a cluster index to its true class labels. What is the key signal in the data that enables the algorithm to relate different clusters to their corresponding classes, especially when all classes have equal probability? Furthermore, it is not clear why the mapping from H to Z can be written as a sum of C_1,…,C_k in Proposition 3.2. If the final mapping is piece-wise linear, how can it be written as a sum of linear mappings? Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix? (Only linear mapping can be expressed as a matrix.) <sep> Second, the experiment is insufficient. None of the experiment details are presented in the paper. Only the final accuracy of 0.874 is given without any further details. What is the model architecture and size? More experimental analysis should be presented. For example, there are many different hyperparameters in the algorithms. How are the \\lambda_D, \\lambda_G chosen when there is no labeled validation set? How sensitive is the algorithm to different model architecture and model size? Furthermore, none of the baselines results are presented and compared against. <sep> A lot of related works are missing. There have been a lot of emerging works related to unsupervised classification recently, which should be discussed and compared: <sep> [1] G. Lample, L. Denoyer, and M. Ranzato.  Unsupervised machine translation using monolingual corpora only. *CONF*, 2018. <sep> [2] M. Artetxe, G. Labaka, E. Agirre, and K. Cho.  Unsupervised neural machine translation. *CONF*, 2018. <sep> [3] Y. Liu, J. Chen, and L. Deng.  Unsupervised sequence classification using sequential output statistics. NIPS, 2017 <sep> [4] A. Gupta, A. Vedaldi, A. Zisserman. Learning to Read by Spelling: Towards Unsupervised Text Recognition. arXiv:1809.08675, 2018. <sep> [5] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato. Phrase-based & neural unsupervised machine translation. EMNLP 2018. <sep> The presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and English usage issues as well as other unclear statements. Just to name a few examples below: <sep> - (1st paragraph of Introduction): ""…imagine the learned objectconstruct…"" <sep> - The last paragraph in Section 1 is not in the right position and should be placed somewhere else in the introduction. <sep> - In the first paragraph of Section 2.2, ""one of the clustering algorithm"" should be ""one of the clustering algorithms"". <sep> - In the first paragraph of Section 3, it is not clear what it means by ""we can make the tuples (Z,X,H) for the whole dataset"". <sep> - At the end of the first paragraph of Section 3, there is a missing reference in ""network in section()"". <sep> - In the third paragraph on page 4, there is a grammar issue in ""H and Z have greater than convexity than X…"" and in ""it allows the linear combination on these two manifolds in the feature spaces H and Z are and"". <sep> - In the first paragraph of Section 3.2, it is not clear what it means by ""two feature spaces will be trained by the cycle consistency loss to obtain the tuples (Z,X,H) with the correct permutation, where all of elements is in the same-class manifold and shares same learned features."" <sep> - In the first paragraph of page 5, ""cycle-consistency loss z C(D(G(z))) and backward cycle consistency loss x G(C(D(x)))"" does not read well. It sounds like z is the loss C(D(G(z))) and x is the loss G(C(D(x)))? <sep> - Typo in Figure 4: ""a shows"" should be ""(a) shows"".","Following the unanimous vote of the submitted reviews, this paper is not ready for publication at *CONF*. Among other concerns raised, the experiments need significant work, and the exposition needs clarification."
"The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal. <sep> 1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below. <sep> 2) Properties of the minimizer <sep> The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying. <sep> 3) Scaling property <sep> I find this section confusing. Specifically, <sep> a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate? <sep> b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this? <sep> c) The idea of ""restarting"" is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don't see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this ""property"" seem to be used to simply rescale the a and w parameters. <sep> d) The authors claim that ""the scaling law (Proposition 3.2) should play a significant role"" to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models? <sep> 4) Convergence rate <sep> It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically, <sep> a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn't a constant step size also yield convergence in that case? <sep> b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work. <sep> 5) Saddles for neural nets <sep> The authors claim they ""have not encountered convergence to saddles"" for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly? <sep> 6) Extension of the analysis to deep neural networks <sep> The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.? <sep> 7) Experiments <sep> How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?","The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. The AC thus proposes ""revise and sesubmit""."
"This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent. The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning. The mind of the demonstrating agent is modeled as a latent space representation from a neural net. This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods. <sep> General comments, in no particular order: <sep> 1. The authors should provide more details on how the hand-crafted demonstrator agents were made. I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task? <sep> 2. The demonstrated tasks are (gridworld and algorithmic) which are very simple RL taks with low-dimensional (non-visual) state-spaces.  It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco. <sep> 3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. Because the latent vector is modeled as a non-linear function, distances between latent vector representations do not necessarily correspond to similar distances between behavior policies (for example, KL distances between two policy distributions). Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors. <sep> 4. The biggest flaw that I see in this method is the practicality of it's use. This method relies on the ability to obtain or gain access to a demonstration agent to learn from. In very simple tasks, such as the one presented here, the authors were able to hard-code their own demonstration agent. However, in harder tasks, this will not be feasible. If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.  In reality, for sufficiently difficult tasks, a human would be the demonstration agent (as is done in most robotics tasks). In practice, imitation learning from a human works well since the learning can be done offline (i.e., post-hoc after a set of demonstrations are collected from the human). However, this task requires the learning to be interactive and thus the demonstrator needs to be present during the learning.  Interactively learning from a human becomes a problem if the learning takes tens of thousands of episodes of training since a human cannot reasonably be expected to be present for that amount of time. Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)? <sep> 5. My previous comment relates mainly to the application of improved imitation learning. However, I do think this is still very useful in the context of multi-agent reinforcement learning for collaborative and competitive tasks (sections 4.6 and 4.7). I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy. <sep> Overall, I think the paper presents a really nice idea of how to improve modeling of agents. essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations.   This work sounds novel to me from a reinforcement learning perspective, however, I'm not well versed on theory of mind research.","The submission proposes a setting of two agents, one of them probing the other (the latter being the ""demonstrator""). The probing is done in a way that learns to imitate the expert's behavior, with some curiosity-driven reward that maximizes the chance that the probing agents has the expert do trajectories that the probing agent hasn't seen before. <sep> All the reviewers found the idea and experiments interesting. The major concern is whether the setup and the environments are too contrived. At least 2 reviewers commented on the fact that the environments/dataset seemed engineered for success of the given method, which is a concern about how this method would generalize to something other than the proposed setup. <sep> I also share the concern with R3 regarding the practicality of the proposed method: it is not obvious to me what problems this would actually be *useful* for, given that the method requires online interaction with an expert agent in order to succeed. The space of such scenarios where we can continuously probe an expert agent many many times for free/cheap is very small and frankly I'm not entirely sure why you would need to do imitation learning in that case at all (if the method was shown to work using only a state, rather than requiring a state/action pair from the expert, then maybe it'd be more useful). <sep> It's a tough call, but despite the nice results and interesting ideas, I think the method lacks generality and practical utility/significance and thus at this point I cannot recommend acceptance in its current form."
"This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. <sep> The authors suggest that one can predict future frames from a vector comprised of an observation encoding and an action. To train the model, they suggest using a linear combination of three different losses: (1) an adversarial loss that encourages the generated sample to look similarly to training data, (2) an InfoGAN-inspired loss that is supposed to maximise mutual information between the conditioning (e.g. action) and the generated sample, and (3) a content loss, taken to be the mean-squared error of the prediction and ground-truth in the VGG feature space. <sep> The major contribution of this work seems to be using these three losses in conjunction, while doing conditional frame prediction at the same time. While interesting, there exist very similar approaches that also use adversarial losses [1] as well as approaches using different means to reach the same goal [2, 3]. None of these are mentioned in the text, nor evaluated against. It is true that [1] is not action-conditional, but adding actions as conditioning could be a simple extension. <sep> Experimental section consists of an ablation study, which evaluates importance of different components of the loss, and a qualitative study of model predictions. With no comparison to state of the art (e.g. [1, 3]), it is hard to gauge how valuable this particular approach is. <sep> The qualitative evaluation starts with §4.4¶1 ""we follow the customary GAN literature to include some qualitative results for illustration"", as if there was no other reason for including samples than to follow the custom. Since the paper is about action-conditional prediction, it would be interesting to see predictions conditioned on the same initial sequence but different actions, which are not present, however. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting. <sep> The paper is clearly written, but some claims in the text are not supported by any citations (e.g. §1¶2 ""More recently, several papers have shown that forward modelling…"" without a citation).  Some claims are misleading (e.g. §1¶3 says that by using adversarial training we don't need to use task-specific losses and it does not put constraints on input modality. While true, using MSE loss is equally general). Some other claims are not supported at all or may not be true (e.g. §3.2¶1 ""ResNet … aims at compressing the information in the raw observation"" - to the best of my knowledge, there is no evidence for this). <sep> To conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements. I recommend to reject this paper. <sep> [1] Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., & Levine, S. (2018). Stochastic Adversarial Video Prediction. CoRR, abs/1804.01523. <sep> [2] Eslami, S.M., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N.C., King, H., Hillier, C., Botvinick, M.M., Wierstra, D., Kavukcuoglu, K., & Hassabis, D. (2018). Neural scene representation and rendering. Science, 360, 1204-1210. <sep> [3] Denton, E.L., & Fergus, R. (2018). Stochastic Video Generation with a Learned Prior. ICML. <sep> [4] Buesing, L., Weber, T., Racanière, S., Eslami, S.M., Rezende, D.J., Reichert, D.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., & Wierstra, D. (2018). Learning and Querying Fast Generative Models for Reinforcement Learning. CoRR, abs/1802.03006.","The paper presents an action conditioned video prediction method that combines previous losses in the literature, such as, perceptual, adversarial and infogan type of losses. The reviewers point out the lack of novelty in the formulation, as well as the lack of experiments that would verify its usefulness in model based RL. There is no rebuttal thus no ground for discussion or acceptance."
"Pros: <sep> This paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \\phi_i, and then is formulated as a GP. <sep> Cons: <sep> However, there are some key issues. <sep> 1. The so-called ""infinite width"" is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details. <sep> [S1] Generalization Properties of Learning with Random Features, NIPS 2017. <sep> 2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing. <sep> Summary: <sep> The paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results. <sep> Response to rebuttal: <sep> The authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. <sep> Therefore the paper is very borderline. However, I would like to bump my rating a bit higher.","The paper studies how to construct infinitely deep infinite-width networks from a theoretical point of view, and uses the results of its theoretical analysis to design a weight initialization scheme for finite-width networks. While the idea is interesting and the paper may contain novel theoretical contributions, the experimental results are weak, as pointed out by all three reviewers from several different perspectives. In particular, it seems that the presented theoretical analysis is useful mainly for weight initialization and hence has limited potential impacts. In addition, the authors have responded to neither the AC's question, nor a detailed anonymous comment that challenges the value of Proposition 1 given the previous work by Aronszajn."
"This paper study the trajectory of H(\\hat{y}) versus H(\\hat{y}|y) on the information plane for stochastic gradient descent methods for training neural networks. This paper was inspired by (Ziv and Tishby 17'), but instead of measuring the mutual information I(X;T) and I(Y:T), this paper proposed to measure H(\\hat{y}) and H(\\hat{y}|y), which are much easier to compute but carries similar meaning as I(Y;T) and I(X;T). <sep> The interesting part of this paper appears in Section 4, where the author makes a connection between the SGD training process and \\alpha-SMLC(strong Markov learning chain). SMLC is just simply linear combination of the initial distribution and the final stable distribution of the labels. The authors show that the trajectory of the real experiment is similar to that of SMLC. <sep> Generally I think the paper is well-written and clearly present the ideas. Here are some pros and cons. <sep> Pros 1: The trajectory presented in this paper is much more reliable than that in (Ziv and Tishby 17'), since measuring the entropy and conditional entropy of discrete random variables are much easier. Also it is easy for people to believe that the trajectory holds for various neural network structure and various activation functions. <sep> Pros 2: The connection to SMLC is interesting and it may contain lot of insights. <sep> Cons 1: One of my major concern is --- if you look at the trajectory of the experiment v.s. SMLC (Figure 3), they look similar at first glance. But if you look at it carefully, you will notice that the color of them are different! For SGD, the trajectory goes to the turning point very soon (usually no more than 10% of the training steps), whereas SMLC goes to the turning point much slower. How do the authors think about this phenomenon and what does this mean? <sep> Cons 2: This paper is going to be more meaningful if the author can provide some discussions, especially about (1) what does the shape trajectory mean (2) what do the connection between the trajectory and Markov chain means (3) how can these connections be potentially useful to improve training algorithm? I understand that these questions may not be clearly answerable, but the authors should make this paper more inspiring such that other researchers can think deeper after reading this paper. <sep> Cons 3: I suggest the authors using SGD instead of GD throughout the paper. Usually GD means true gradient descent, but the paper is talking about batched stochastic gradient descent. GD does not have Markovity. <sep> Generally, I think the paper is on the borderline. I think the paper is acceptable if the author can provide more insights (against Cons 2).","The paper proposes a quantity to monitor learning on an information plane which is related to the information curves considered in the bottleneck analysis but is more reliable and easier to compute. <sep> The main concern with the paper is the lack of interpretation and elaboration of potential uses. A concern is raised that the proposed method abstracts away way too much detail, so that the shapes of the curves are to be expected and contain little useful information (see AnonReviewer2 comments). The authors agree to some of the main issues, as they pointed out in the discussion, although they maintain that the method could still contain useful information. <sep> The reviewers are not very convinced by this paper, with ratings either marginally above the acceptance threshold, marginally below the acceptance threshold, or strong reject."
"The authors propose to formulate the neural network architecture as a collection of multivariate categorical distributions. They further derive sample-based gradient estimators for both the stochastic architecture and the deterministic parameters, which leads to a simple alternating algorithm for architecture search. <sep> Pros: <sep> + Intuitions and formulations are easy to comprehend. <sep> + Simpler to implement than most prior methods. <sep> + Appealing results (on CIFAR-10) as compared to the state-of-the-art. <sep> Cons: <sep> - Limited technical novelty. The approach is a straightforward extension of Shirakawa et al. 2018. The main algorithm is essentially the same except minor differences in gradient derivations. <sep> - Lack of theoretical justifications. It seems all the derivations at the beginning of Section 2 assume the architecture is optimized wrt the training set. However, the authors ended up splitting the dataset into two parts in the experiments and optimize the architecture wrt a separate validation set instead. This would invalidate all the previous derivations. <sep> - The method is a degenerated version of ENAS. A closer look at eq (2) and (3) suggests the resulting iterative algorithm is almost the same as that in ENAS, where the weights are optimized using GD wrt the training set and the architecture is optimized using the log-derivative trick wrt the validation set. The only distinction are (i) using a degenerated controller/policy formulated as categorical distributions (ii) using the validation loss instead of the validation accuracy as the reward (according to eq. (3)). This is also empirically reflected in Table 1, which shows the proposed PDAS is similar to ENAS both in terms of efficiency and performance. The mathematical resemblance with ENAS is not necessarily bad, but the authors need to make it more explicit in the paper. <sep> Minor issues: <sep> * I'm not sure whether it's a good practice to report the ""best"" test error among multiple runs in Table 1. <sep> * The method is not really ""parameterless"" as claimed in the introduction. For example, a suitable learning rate adaptation rule can be task-specific thus requires manual tuning/design. The method also consists of some additional hyperparameters like the \\lambda in the utility transform.","The paper presents an architecture search method which jointly optimises the architecture and its weights. As noted by reviewers, the method is very close to Shirakawa et al., with the main innovation being the use of categorical distributions to model the architecture. This is a minor innovation, and while the results are promising, they are not strong enough to justify acceptance based on the results alone."
"The paper aimed at improving the performance of recommendation systems via reinforcement learning. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; (2) the trajectory manager (TM) that determines how to roll out the IC under the planning strategy and produces a set of imagined item trajectories; (3) the imagination-augmented executor (IAE) that aggregates the internal data resulting from imagination and external rewarding data to update its action policy. <sep> Strengths of the paper: <sep> (1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people's daily lives. <sep> (2) Experiments were conducted on a publicly available dataset. <sep> (3) Robustness to cold-start scenario was tested and evaluated in the experiments. <sep> Weaknesses of the paper: <sep> (1) The motivations of applying reinforcement learning techniques are not convinced to me. There are a lot of supervised learning algorithms to the task of recommendations. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? Is it because reinforcement learning based methods work better than traditional machine learning based ones? The motivations of integrating A3C (Asynchronous Advantage Actor-Critic) but not other techniques into the proposed model are not convinced to me as well. <sep> (2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines. <sep> (3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. In equations (2) and (3), what is theta_v? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)? <sep> (4) The contributions of the paper in terms of theory are somewhat not significant. It seems that the proposed algorithm is built based on and combined by existing algorithms such as A3C. <sep> Minor comments: <sep> (1) It would be better if the authors can test the proposed model on more datasets. There are many publicly available datasets for testing the performance of recommendation systems. <sep> (2) Figure 2 is not straightforward. It would be better if the authors can draw the figure in other ways. (I am not sure if the authors have expressed the underlying ideas clearly with Figure 2).","This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective. The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences. The problem suffers from delayed and sparse rewards, which the authors propose to address using self-supervised prediction. The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge. <sep> The reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice. The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted. R1 also commends the authors' decision to address the challenging cold-start problem. <sep> The reviewers and AC also note several potential weaknesses. The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated. This is needed, as many supervised learning (and other types) approaches to the problem exist. A performance comparison to current state-of-the-art RL baselines is missing. The proposed approach is related to both imagination augmented (I2A, Racaniere et al. 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al. 2016), but does not compare to either method. Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches. A thorough comparison to these baselines in a real-world application like session-based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess. Reviewers also noted lack of clarity. Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it's conceptual and empirical differences from existing reinforcement learning approaches. R3 mentions missing related work, some of which the authors include in the revision. The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem. <sep> Overall, the paper was assessed as borderline by the reviewers. The ACs view is that there are too many concerns for acceptance at *CONF* in the present form, and that the paper will benefit from a thorough revision."
"This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent.  In other words, representations where there is no information conveyed by combinations of latents that is not conveyed by considering each latent in isolation.  As the resultant target is intractable to evaluate, a number of approximations are employed for practical training. <sep> The high-level idea is quite interesting, but the paper itself is quite a long way of convincing me that this is actually a good approach.  Moreover, the paper is a long way of the level of completeness, rigor, clarity, and polish that is required to seriously consider it for publication.  In short, the work is still at a relatively early stage and a lot more would need to be done for it to attain various minimum standards for acceptance.  A non-exhaustive list of specific examples of its shortfalls are given below. <sep> 1. The paper is over a page and a half under length, despite wasting large amounts of space (e.g. figures 3 and 4 should be two lines on the same plot) <sep> 2. The experimental evaluation is woefully inadequate.  The only quantitative assessment is to compare to a single different approach on a single toy dataset, and even then the metric being used is the one the new method uses to train for making it somewhat meaningless. <sep> 3. The introduction is completely generic and says nothing about the method itself, just providing a (not especially compelling) motivation for disentanglement in general.  In fact, the motivation of the introduction is somewhat at odds with the work -- correctly talking about the need for hierarchical representations which the approach actually actively discourages. <sep> 4. There are insufficient details on the algorithm itself in terms of the approximations that are made to estimate the synergistic mutual information.  These are mostly glossed over with only a very short explanation in the paragraph after equation 15.  Yes there are algorithm blocks, but these are pretty incomprehensible and lack accompanying text.  In particular, I cannot understand what A_w is supposed to be.  This is very important as I suspect the behavior of the approximation is very different to the true target.  Similarly, it would be good to provide more insight into the desired target (i.e. Eq 15).  For example, I suspect that it will encourage a mismatch between the aggregate posterior and prior by encouraging higher entropy on the former, in turn causing samples from the generative model to provide a poor match to the data. <sep> 5. The repeated claims of the approach and results being ""state-of-the-art"" are cringe-worthy bordering on amusing.  Writing like this serves no purpose even when it justified, and it certainly is not here. <sep> 6. There are a lot of typos throughout and the production values are rather poor.  For example, the algorithm blocks which are extremely messy to the point where they are difficult to follow, citep/citet mistakes occur almost every other citation, there is a sign error in Equation 16. <sep> This is a piece of work in an exciting research area that,  with substantial extra work, could potentially result in a decent paper due to fact that the core idea is simple and original.  However, it is a long way short of this in its current state.  Along with addressing the specific issues above and improving the clarity of the work more generally, one thing in particular that would need to address in a resubmission is a more careful motivation for the method (ideally in the form of a proper introduction). <sep> Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.  Forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.  As you say in the first line of your own introduction, hierarchy and composition are key parts of learning effective and interpretable representations and this is exactly what you are discouraging.  A lot of the issue here is one of the disentanglement literature at large rather than this paper (though I do find it to be a particularly egregious offender) and it is fine to have different opinions.  However, it is necessary to at least make a sensible case for why your approach is actually useful. <sep> Namely, is there actually any real applications where such a simplistic disentanglement is actually useful?  Is there are anyway the current works helps in the longer vision of achieving interpretable representations?  When and why is the synergistic information a better regularizer than, for example, the total correlation?  The experiments you have do not make any inroads to answering these questions and there are no written arguments of note to address them.  I am not trying to argue here that there isn't a good case to be made for the suggested approach in the context of these questions (though I am suspicious), just that if the work is going to have any lasting impact on the community then it needs to at least consider them.","The paper introduces a form of variational auto encoder for learning disentangled representations. The idea is to penalise synergistic mutual information. The introduction of concepts from synergy to the community is appreciated. <sep> Although the approach appears interesting and forward looking in understanding complex models, at this point the paper does not convince on the theoretical nor on the experimental side. The main concepts used in the paper are developed elsewhere, the potential value of synergy is not properly examined. <sep> The reviewers agree on a not so positive view on this paper, with ratings either ok, but not good enough, or clear rejection. There is a consensus that the paper needs more work."
"Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image.  Their argument is backed by outperforming their baseline and getting competitive results on few shot learning tasks. Their method is much more computationally heavy than the baseline matching networks. In order to make training feasible, in practice they train with 90% dropout of test pixels embedding & 80% dropout of reference pixels embedding. <sep> The caveats: <sep> -> Using hyper-columns is related to adding residual connections. The question remains how much performance can be gained by just adding residual connections (with dropout) to the matching networks and letting the network automatically (or with a probability) choose to embed higher layers or lower ones. Adding the residual connection and just comparing the final layer embeddings is a cleaner method than ABM which  provides a richer embedding than baseline and could potentially close the performance gap between ABM and final layer matching. <sep> ->It is strictly designed for one-shot learning. It does not benefit from few shots (extra shots) and the fact that these different shots are getting classified as the same label. Vinyal et al mitigates this shortcoming by adding the FCE. However FCE is not directly applicable anymore. Author's don't suggest any alternatives either. Their smaller gains (or even worse than baseline without self-regularization) in the 5-shot cases is an evidence of this shortcoming. <sep> The fact that SNAIL (TCML Mishra et al. (2017)) consistently outperforms this method puts a question mark on the significance of this work. If it was computationally feasible, authors could have used SNAIL and replaced the 64 dimensional embedding of each picture with the 10-20% hypercolumns. Essentially due to computational costs authors are sacrificing a more thorough matching system (non-greedy) for a richer embedding and they don't get better results. <sep> On the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. Illustrations like fig. 3 for example shows that the model is not matching semantically similar points and can be used to debug & improve the model. While understanding why a blackbox matching network is making a mistake and improving, is  harder. <sep> It would have been nice if authors used this added interpretability in some manner. Such as getting an idea about a regularizer, a prior, a mask, etc. and improved the performance. <sep> I would argue for accepting this paper for two reasons. <sep> -> Given that they beat their baseline and  they get comparable performance to sota even with a greedy matching (min-pooling followed by average pooling), is impressive. Furthermore, it is orthogonal to methods like SNAIL if the computational cost could be resolved. <sep> -> They not only provide which image is a match but how they are matched, which could be interesting for one-shot detection as well as classification. <sep> Question: At test/validation: do you still only categorize with 10,20% samples or do you average the full attention map for all test pixels? <sep> Nit: The manuscript needs several passes of proofreading, spell & grammar checking. A few examples in the first couple of pages: <sep> -> The citing format needs to be fixed (like: LSTMsRavi, there should be () around citations). <sep> -> are not incompatible: are compatible <sep> ->incomprehensible sentence with two whiles: ABM networks outperforms these other state-of-the-art models on the open-set recognition task while in the one-shot setting by achieving a high accuracy of matching the non-open-set classes while maintaining a high F1 score for identifying samples in the open-set. <sep> -> add dots to the end of contribution list items. <sep> -> we first look pairwise matchings: we first look at the pairwise matchings","The reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication. There is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments. One of the main concerns of the method's effectiveness in practice is the computational cost. There is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched. The authors provide some justification for why this wouldn't happen, and this should be put in a future draft. Even better would be to show statistics to demonstrate empirically that this doesn't happen. <sep> There were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues. There is also a typo in the title that should be fixed."
"This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. <sep> In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn't tested in robots. However, to my understanding the REINFORCE baseline isn't really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. <sep> Moreover, I think the description of the experiments doesn't provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn't say what they were. <sep> Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix). <sep> Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.'s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X ) <sep> Comments/Thoughts: <sep> + I think in the introduction there are some statements that probably need citations. For example, ""But the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient."" —> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? <sep> + ""Yes, 54 environments but no real-world physical robots"" —> this and the intro seems like a blogpost at times. That can be fine (some would argue it's a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation. <sep> + ""Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient."" —> citation/backing? it might be nice to point to the experiment section here to back it (e.g., ""As will be shown in Section X and in \\citet{something}, REINFORCE can be quite sample inefficient"") <sep> + ""In practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to? <sep> + ""regress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section"" —> regress to \\sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier <sep> + What is the actual loss function used for the baseline? Is it the same as Pathak et al.? <sep> + What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? <sep> + Was a variance-reducing baseline used in REINFORCE? <sep> + What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation? <sep> + ""Hence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. "" —> what were the learning rates? <sep> Linguistic/Typos: <sep> Also, some minor, but frequent, grammatical issues/typos that I've added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I've tried to point out below. <sep> + ""This leads to a significantly sample efficient exploration policy. "" —> significantly more (?) sample efficient ? <sep> ""Why is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent"" —> by the agent? <sep> ""Forward model fθF is trained to minimize its loss which amounts to minimizing rti with respect to θF"" —> the forward model <sep> ""However, policy is optimized to maximize the objective"" —> However, the policy <sep> ""We can also optimize  for policy parameters θP via differentiable loss function"" —> We can also optimize for (the) policy parameters \\theta via (a) differentiable loss function? <sep> ""To optimize policy to maximize a discounted sum "" —> To optimize the policy <sep> ""How good is Forward Prediction Model"" —> How good is the forward prediction model <sep> There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues.","The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward. The motivation is that this approach will lead to more sample efficient exploration for real robots. The use of a differentiable loss for policy optimization is interesting and has some novelty. However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims. Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper."
"This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words. <sep> While the idea is interesting, there are several important limitations. First, the paper is difficult to understand, and some of the explanations are not convincing. For example, in section 4.1.1, it says ""... our method assumes that the documents are produced from a single topic ... Our assumption aligns well with human intuition that most documents are generated from a single main topic."" This goes very much against the common assumption of a generative topic model, such as LDA, which the model compares against. I don't mean to argue either way, but if the paper presents a viewpoint which is quite different from the commonly accepted viewpoint (within the specific research field), then there needs to be a much deeper explanation, ideally with concrete evidence to support it. Another sentence from the same paragraph states that their ""model outperforms LDA because LDA is a statistical model, while our generator is a deep generative model."" This argument also seems flawed and without concrete evidence. There are other parts in the paper where the logic seems strange and without evidence, and they make it difficult to understand and accept the major claims of the paper. <sep> Second, the model does not offer much novelty. It seems that the two-stage model simply puts the two pieces, a GAN-style generator and an LSTM sequence model together. Perhaps I am not understanding the model, but the model description was also not clear nor easy to understand with respect to its novelty. <sep> Third, the evaluation is somewhat weak. There are two main evaluations tasks: text classification and text generation. For the first task, classification is not the main purpose of topic models, and while text classification _is_ used in many topic modeling papers, it is almost always accompanied by other evaluation metrics such as held-out perplexity and topic coherence. This is because the main purpose of topic modeling is to actually infer the topics (per-topic word distribution and per-document topic distribution) and model the corpus. Thus I feel it is not a fair evaluation to just compare the models using text classification tasks. The second evaluation task of text generation is not explained enough. For the human evaluation, who were the annotators, and how were they trained? How many people annotated each output, and what was the inter-rater agreement? How many sentences were evaluated, and how were they chosen? Without these details, it is difficult to judge whether this evaluation was valid. <sep> Lastly, the results are mediocre. Besides the classification task, the others do not show significant improvements over the baseline models. Perplexity (table 3) shows similar results for DBPedia and worse results (than WGAN-gp) for Gigaword. Table 4 shows slightly better results for ""Preference"" for TopicGAN with joint training, but ""Accuracy"" is measured only for the proposed model and not the baseline model.","This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag-of-words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words. <sep> Pros: <sep> It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation. <sep> Cons: <sep> There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component-wise evaluation (e.g., text classification from topic models) and insufficient GAN-based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong. <sep> Verdict: <sep> Reject. Many technical details require clarification and experiments lack sufficient comparisons against prior art."
"This paper aims to address the problem of lacking sufficient demonstrations in inverse reinforcement learning (IRL) problems. They propose to take a meta learning approach, in which a set of i.i.d. IRL tasks are provided to the learner and the learner aims to learn a strategy to quickly recover a good reward function for a new task that is assumed to be sampled from the same task distribution. Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. The proposed algorithm is evaluated on a synthetic grid-world problem, SpriteWorld. The experimental results suggest the proposed algorithm can learn to mimic the optimal policy under the true reward function that is unknown to the learner. <sep> Strengths: <sep> 1) The use of meta learning to improve sample efficiency of IRL is a good idea. <sep> 2) The combination of MAML and MaxEnt IRL is new to my knowledge. <sep> 3) Providing the gradient expression is useful, which is the main technical contribution of this paper. (But it needs to be corrected; see below.) <sep> 4) The paper is well motivated and clearly written ""in a high level"" (see below). <sep> Weakness: <sep> 1) The derivation of (5) assumes the problem is tabular, and the State-Visitations-Policy procedure assumes the dynamics/transition of the MDP is known. These two assumption are rather strong and therefore should be made explicitly in the problem definition in Section 3. <sep> 2)  Equation (8) is WRONG. The direction of the derivation takes is correct, but the final expression is incorrect. This is mostly because of the careless use of notation in derivation on p 15 in the appendix (the last equation), in which the subscript i is missed for the second term. The correct expression of (8) should have a rightmost term in the form  (\\partial_\\theta r_\\theta) D  (\\partial_\\theta r_\\theta)^T, where D is a diagonal matrix that contains \\partial_{r_i} (\\E_{\\tau} [ \\mu_\\tau])_i and i is in 1,...,|S||A|. <sep> 3) Comparison with imitation learning and missing details of the experiments. <sep> a) The paper assumes the expert is produced by the MaxEnt model. In the experiments, it is unclear whether this is true or not, as the information about the demonstration and the true reward is not provided. <sep> b) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. In imitation learning, it is known that the expert policy is often sub-optimal, and therefore the goal in imitation learning is mostly only to achieve expert-level performance. Given this, the way this paper evaluate the performance is misleading and improper to me, which leads to an overstatement of the benefits of the algorithm. <sep> c) It would be interesting to compare the current approach with, e.g., the policy-based supervised learning approach to imitation learning (i.e. behavior cloning). <sep> 4) The rigorousness in technicality needs to be improved. While the paper is well structured, the writing at the mathematical level is careless, which leads to ambiguities and mistakes (though one might be able to work out the right formula after going through the details of the entire paper). Below I list a few points. <sep> a) The meta-training set {T_i; i=1,...,N} and the meta-test set {T_j; i=1,...,M} seems to overload the notation. I suppose this is unintentional but it may appear that the two sets share the first T_1,.., T_M tasks, e.g., when N>=M, instead of being disjoint. <sep> b) The set over which the summation is performed in (4) is unclear; alpha in (4) is not defined, though I guess it's a positive step size. <sep> c) On p4, ""we can view this problem as aiming to learn a prior over the intentions of human demonstrators"" is an overstatement to me. At best, this algorithm learns a prior over rewards for solving maximal entropy IRL, not intention. And the experiment results do not corroborate  the statement about ""human"" intention. <sep> d) On p4,  ""since the space of relevant reward functions is much smaller than the space of all possible rewards deﬁnable on the raw observations"" needs to be justified. This may not be true in general, e.g., learning the set of relevant functions may require a larger space than learning the reward functions. <sep> e) The authors call \\mu_\\tau the ""state"" visitation, but this is rather confusing, as it is the visiting frequency of state and action (which is only made clear late in the appendix). <sep> f) On p5, it writes ""... taking a small number of gradient steps on a few demonstrations from given task leads"" But the proposed algorithm actually only takes ""one"" gradient step in training. <sep> g) The convention of derivatives used in the appendix is the transpose of the one used in the main paper. <sep> Minor points: <sep> 1) typo in (2) <sep> 2) p_\\phi is not defined, L_{IRL} is not defined, though the definition of both can be guessed. <sep> 3) T^{tr} seems to be typo in (11) <sep> 4) A short derivation of (2) in the Appendix would be helpful.","This work proposes to use the MAML meta-learning approach in order to tackle the typical problem of insufficient demonstrations in IRL. <sep> All reviewers found this work to contain a novel and well-motivated idea and the manuscript to be well-written. The combination of MAML and MaxEnt IRL is straightforward, as R2 points out, however the AC does not consider this to be a flaw given that the main novelty here is the high-level idea rather than the technical details. <sep> However, all reviewers agree that for this paper to meet the *CONF* standards, there has to be an increase in rigorousness through (a) a more close examination of assumptions, sensitivity of parameters and connections to imitation learning (b) expanding the experimental section."
"￼ <sep> To my understanding, this paper builds on prior work from Chow et al. to apply Lyapunov-based safe optimization to the policy-gradient setting. This seems is similar to work by Achiam 2017. While this work seems like an interesting framework for encompassing several classes of constrained policy optimization settings in the Lyapunov-based setting, I have some concerns about the evaluation methodology. <sep> It is claimed that the paper compares against ""two baselines, CPO and the Lagrangian method, on several robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost."" Then it is stated in the experimental section ""However since backtracking line-search in TRPO can be computationally expensive, and it may lead to conservative policy updates, without loss of generality we adopt the original construction of CPO to create a PPO counterpart of CPO (which coincides with SPPO) and use that as our baseline."" This seems directly to contrast to the earlier statement which states that it is unclear how to modify the CPO methodology to other RL algorithms. Moreover, is this really a fair comparison? The original method has been modified to form a new baseline and I'm not sure that it is ""without loss of generality"". <sep> Also, it is unclear whether the results can be accepted at face value. Are these averaged across several random seeds and trials? Will performance hold across them? What would be the variance? Recent work has shown that taking 1 run especially in MuJoCo environments doesn't necessarily provide statistically significant values. In fact the original CPO paper shows the standard deviations across several random seeds and compares directly against an earlier work in this way (PDO). Moreover, it is unclear why CPO was not directly compared against and neither was the ""equivalent"" baseline not compared on similar environments as in the original CPO paper. <sep> Comments: <sep> Figure 3 is difficult to parse, the ends of the graphs are cut off. Maybe putting the y axis into log format would help with readability here or having the metrics be in a table.","This is an interesting direction but multiple reviewers had concerns about the amount of novelty in the current work, and given the strong pool of other papers, this didn't quite reach the threshold."
"The paper presents a pool-based active learning method that achieves sub-linear runtime complexity while generating high-entropy samples, as opposed to linear complexity of more traditional uncertainty sampling (i.e., max-entropy) methods. <sep> This is achieved by using a generative adversarial network (GAN) to generate high-entropy samples that are then used by a nearest neighbor method to pick samples from a pool, that are closest to the generated samples. The sub-linear complexity is achieved through the use of a k-d tree, combined with the fact that similarity is computed on the feature space and samples can thus be indexed once (as the feature space does not change while training). <sep> The proposed idea builds on top of previously published work on Generative <sep> Adversarial Active Learning (GAAL). The main difference is the added nearest neighbor component, as GAAL is directly using the generated examples, thus achieving constant runtime complexity, rather than sub-linear. <sep> I like the overall direction and the idea of being able to perform uncertainty sampling in sub-linear time. The approach is interesting. However, the results presented in the paper are not strong and I do not see whether or not I should be using this method over uncertainty sampling. Most importantly, the results are strongest only for the MNIST experiments, which are over a small dataset. <sep> Given that the method is motivated by the scalability argument, I would like to see at least one large scale experiment where it performs well, and more specifically, outperform random sampling. Also, I would really like to see a more principled and thorough experimental investigation with justifications for the configurations used and with more comparisons to alternative approaches, <sep> such as GAAL, which has constant complexity. <sep> I believe that it would be better for your paper if you work a bit more on the experimental evaluation and submit a revised version at a later deadline. <sep> == Background and Method == <sep> The background and method sections are clear and easy to follow. One improvement <sep> I can see is making figure 1 more clear, by maybe explicitly stating in the figure what ""G"" and ""F"" are. One more point of interest is that the way you perform sample matching makes some smoothness assumption about the feature space as related to the classifier uncertainty. I perceive this as a smoothness assumption on the decision boundary of the classifier and I do not know how true is may be for deep neural networks, but I can see how it may be true for logistic regression models and support vector machines (SVMs), depending on the kernel used. I believe that this point and main assumption may be worth further discussion, given that it is also about the main difference your method has with respect to GAAL. <sep> I do not have any other major comments for these sections as my main concerns are about the experiments section. <sep> == Experiments == <sep> In the experiments, it would be very useful to have plots against execution time, given that the main motivation for this method is scalability. For example, the method outperforms random sampling for small datasets, based on number of samples, but what happens when you look at execution time? Given that random sampling is very cheap, I imagine that it probably does better. Also, as mentioned earlier, I would like to see at least one experiment using a big dataset, where the method outperforms random sampling, as I am not currently convinced of its usefulness. <sep> Also, you present a lot of results and list observations but I felt there was not much discussion as to why you observe/obtain some of the results. Given that your method is not working very well for CIFAR, I would like to see a more thorough investigation as to why that may be the case. This investigation could conclude with some ""tips"" on when it may be a good idea to use your method over <sep> GAAL, or uncertainty sampling, for example. <sep> Regarding the experimental setup, I find lots of configuration choices very arbitrary and have difficulty understanding how they were chosen. For example: <sep> - For the two-class MNIST you use classes ""5"" and ""7"" and for the two-class <sep> CIFAR you use classes ""automobile"" and ""horse"". Why is that? How did you pick the two classes to use in each case? Do the results match for other class pairs? <sep> - ""learning rate of 0.01 that we decay by a factor of 10 at the 130th and <sep> 140th epochs"" -- end of page 6 <sep> - ""In contrast to the previous experiments we use a residual Wasserstein GAN <sep> with gradient penalty and soft consistency term"" -- page 7 -- why do you make that change? <sep> Questions: <sep> - Why do you think using Wasserstein GANs perform better than using DCGANs? -- section 5.3.1 <sep> - Why not compare to GAAL in all of figures 3 and 4? <sep> - How/why were the number of samples you start with and sample in each round, <sep> chosen? Do you observe any difference if you increase/decrease the number of samples sampled in each round or if you start with fewer samples? <sep> - How/why were these model architectures chosen?","The paper proposes adversarial sampling for pool-based active learning. <sep> The reviewers and AC note the critical potential weaknesses on experimental results: it is far from being surprising the proposed method is better than random sampling. Ideally, one has to reduce the complexity under keeping the state-of-art performance. Otherwise, it is hard to claim the proposed method is fundamentally better than prior ones, although their targets might be different. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish."
"The authors propose a more unified view of disparate methods for training sequence models. Specifically, a multi-term objective L(q,theta) consisting of: <sep> 1) The standard reward maximization objective of policy gradient, E_{p_\\theta}[R], <sep> 2) A weighted (weight alpha) reverse KL divergence of the parametric policy and a non-parameteric policy q, <sep> 3) A weighted (weight beta) entropy term on q, <sep> Is proposed for sequence training (see equation (1). L can be iteratively optimized by solving for q given p_\\theta, and the \\theta given q (see eq. 2). <sep> This framework mathematically generalizes softmax-policy gradient (SPG, alpha=1, beta=0), and reward-augmented maximum likelihood (alpha=0, beta=temperature), and also standard entropy regularized policy gradient (alpha=0), among other algorithms. <sep> The paper is well written, and the approach sensible. However, combining SPG and RAML by introducing their respective regularization terms is a rather straightforward exercise, and so seems quite incremental. <sep> Other major concerns are: <sep> 1) the true utility of the model, and <sep> 2) the integrity of the experiments. <sep> Wrt: <sep> 1), While RAML was a significant contribution at the time, it is now well established that RAML generally doesn't perform well at all in practice due to exposure bias (not conditioning on it's own previous predictions during training). Moreover SPG, as the authors point out, was supposed to address the need for ML pre-training, but required much engineering to work. The fact is that REINFORCE-based policy gradient methods are still more effective than these methods, provided they have a good baseline to reduce varince. Which brings me to point <sep> 2) Was MIXER run with a learned baseline and judiciously optimized? Table 1 suggests that MIXER can outpeform ML by only 0.1 Bleu points, and outpeformed by RAML? Something is wrong with your implementation then. Moreover, there are techniques like self-critical sequence training (SCST), which far outpeform MIXER, and we haven't even discussed Actor-Critic baselines... <sep> In summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Also, a paper on essentially the same approach was submitted and rejected from *CONF* 2018(https://openreview.net/pdf?id=H1Nyf7W0Z), although this paper is better written, and puts the method more fully in context with existing work, I think that several of the concerns with that paper apply here as well. <sep> Look forward to the authors' feedback, and additional/corrected results - will certainly update my score if these concerns are addressed. In particular, if this generalization can significantly outpeform existing methods it generalizes with non-degenerate settings, this would overcome the more incremental contribution of combining SPG and RAML. <sep> Current Ratings: <sep> Evaluation      2/5: Results are not consistent with previous results (e.g. MIXER results). Stronger baselines such as SCST and AC are not considered. <sep> Clarity         5/5: Clear paper, well written. <sep> Significance    3/5: RAML and SPG have not been established as important methods in practice, so combining them is less interesting. <sep> Originality     2/5: RAML and SPG are fairly straightforward to combine for experts interested in these methods. <sep> Rating          4/10 Okay but not good enough, reject. <sep> Confidence      5/5 <sep> Pros: <sep> - Generalizes RAML and SPG (and also standard entropy-regularized policy gradient). <sep> - Well written paper, clean generalization. <sep> Cons: <sep> - RAML and SPG have not been established as important methods in practice. <sep> - generalization of RAML and SPG is straightforward,  incremental. <sep> - Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML) <sep> - Stronger REINFORCE-based algorthms like SCST, as well as Actor-critic algorithms, have not been compared. <sep> Update after author responses: <sep> -------------------------------------------- <sep> Authors, thank you for your feedback. <sep> While it is true that generalizing RAML and SPG into a common framework is not trivial, the presented framework simply augments the dual form of SPG (i.e. REPS [16] in the SPG paper) with a RAML term. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally. <sep> Wrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. However, the unusally poor performance of MIXER remains unexplained, and falls short even of scheduled sampling (SS), which suggests a lingering major issue. REINFORCE techniques rely on 1) strong baselines, verified by the authors, 2) larger batch sizes to reduce variance, and 3) pre-training to reduce variance and facilitate efficient exploration. If the MLE is undertrained or overtrained (the latter the more likely issue given the plots), then MIXER will perform poorly. Actually, it is now standard practice to pre-train with MLE+SS before RL training, and this is really the (also dynamically weighted objective) baseline that should be compared against. The current REINFORCE results (MIXER or otherwise) really need to be updated (or at the least removed, as they are not captured by the framework, but the comparison to PG methods is important!). <sep> More generally, I feel that the experiments are not yet comprehensive enough. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is. <sep> Overall I feel that the paper is still in need of substantial maturation before publication, although I have revised my score slightly upward.","I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG. While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at *CONF*. <sep> In the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. It is important to describe how the parameters are tuned. Given the additional hyper-parameters, one may consider giving all of the algorithms the same budget of hyper-parameter tuning. I also agree with reviewers that the policy gradient baseline seems to underperform typical results. One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper-parameters."
"While overall the writing quality of the paper is high, the paper itself is a strong rejection.  I believe the analysis of the paper is at points flawed, and the experiments are minimal. <sep> This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017.  Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone.  To combat this they use the noise-regularized mutual information estimator (I(X; L+eps)).  To actually estimate the mutual information the authors use the MINE estimator of Belghazi (2018).  Here they suggest using the neural network itself as a structural element in the form of the discriminator to take advantage of the specific circumstances in this case.  Doing this ensured that their estimator diverged in the zero noise limit as expected.  From here they show some experimental results of the effect of their objective on an MNIST / CIFAR10 classification task. <sep> This paper fits into what is an increasingly large discussion in the literature, surrounding Information Bottleneck.  The paper itself does a very good job of citing recent relevant work.  Technically however I take issue with the framing of previous work in the last paragraph of the ""Deep neural nets"" subsection of Section 2.  Technically Achille & Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a ""single bottleneck layer"" as stated in the paper.  More generally at the end of that paragraph it is implied that the single bottleneck layer scheme ""deviates from the original theory"".  This is a misleading characterization of the original information bottleneck (Tishby et al 1999) in which there was a single random variable, a representation of the data (Z) satisfying the Markov conditions Z <- X -> Y.   I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017).  In general the paper does a poor job of distinguishing between the Shwartz-Ziv & Tishby paper and the rest, but this is a distinction that should be maintained.  The original information bottleneck may and has demonstrated utility regardless of whether the information bottleneck generally can help explain why ordinary deterministic feed forward networks trained with cross entropy and sgd generalize well. <sep> This also raises one of the main problems with the current work. The title, abstract and especially the conclusion (""This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning"") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks.  Here the authors modify the ordinary cross entropy objective, and so their networks are necessarily not ordinary and so they cannot claim they have helped clarify our understanding of the vast majority of neural networks currently being trained.  Again, this is distinct and should be kept distinct from the utility of their proposed objective, itself inspired by the information bottleneck.  Here too the paper falls flat.  If instead of attempting to comment on networks as they are designed today they aim to proposed a new information bottleneck inspired objective they really ought to directly compare other attempts along those lines (such as the ones they themselves cite  Alemi et al. 2018, Kolchinsky et al. 2017, Chalk et al. 2016, Achile & Soatto 2018, Belghazi et al. 2018) but there are no comparative studies. <sep> The experiments are extremely lacking, not only are any of their cited alternatives compared, they don't compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.  Their reported numbers are not very impressive with their top MNIST number at 98.09 and their baseline at 97.73. These numbers are worse than many of the papers they themselves cite.  Only a single comparative results for both a limited training set run and the full one are shown, as well as only a single choice of beta.  The CIFAR10 numbers are not very good either.  There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against. <sep> Technically a deterministic function need not have infinite mutual information, if it is non-invertible, i.e. the sign function, or just floating point discretization. <sep> Their own results in Figure 2 and the main body of the text highlight that the authors believe the true mutual information between the activations of the intermediate layers and the input is infinite.  If the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information? <sep> Just plugging in the Discriminator for the objective (equation (7)) is flawed.  The discriminator, if optimal would learn to approximate the density ratio 1 + log p(x,y)/(p(x) p(y)) .   ( see f-GAN, Norowin et al. 2016).  How does this justify using the individual elements of the discriminator in the functional form of the IB objective? <sep> At the bottom of page 6 they rightfully say that mutual information is invariant to reparameterizations, but their noise regularized mutual information estimator is not (by their own reference (Saxe et al 2017). <sep> The discussion at the center of page 8 is confusing.   They claim that Figure 5 (a) is more 'quantized' than (b) and ""has reduced entropy"".  I think it should be the other way.  More clusters should translate to a higher KL divergence, or higher entropy.  If you need only identify which cluster an activation is in, that should require log K nats where K is the number of clusters.  (a) shows more clusters and so seems like it should cost more and have a higher entropy not a lower one. <sep> Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well.","This paper does two things. First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+ε), where ε is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X. Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L) − βI(X;L+ε), layerwise instead of using cross-entropy and backpropagation. Experiments on MNIST and CIFAR-10 show improvements for the layerwise training over cross-entropy training. The penalty on I(X;L+ε) is described as being analogous to weight decay. The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong. In the AC's opinion, R1's critique that ""[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?"" is critical, and the authors' reply that ""this quantity is in fact a more appropriate measure for ""compactness"" or ""complexity"" than the mutual information itself"" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information. The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise. Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers' receiving several reminders that the discussion is a defining feature of the *CONF* review process."
"This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN). The intended research direction on tabular data is essential and promising. However, the proposed technique does not seem to be handling the problem foundationally well. It seems heavily dependent on GBDT. It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results. Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem. <sep> Pros: <sep> -This is a crucial line of research direction that aims to make DNNs applicable to many real-world problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search. <sep> -The starting point of using GBDT seems like a good choice. <sep> -The Paper is mostly well written except occasional repetitions and missing acronym definitions. <sep> Cons: <sep> -The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well. I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented. The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times). This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for *CONF*. <sep> -The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data. <sep> -In the provided benchmark data sets the depth of the analysis seems to be enough. However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features  (e.g., unstructured raw text for the search queries.) In such scenarios, it had been showed that wide-and-deep NNs perform decently. However such problems are entirely missing in the results section. I also think that this is a lost opportunity for the authors as they could be showing that it is the NN part contributing.",All reviewers agree in their assessment that this paper has merits but is not yet ready for acceptance into *CONF*. The area chair commends the authors for their responses to the reviews.
"This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. This is a very important and necessary problem. <sep> However, this paper lacks in terms of experimental evaluation and has some technical flaws. <sep> 1. Morphological properties deals with only the ""shape"" properties of the image object. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. Additionally, there are lot of low level pixel relations that the model learns to fit the distribution of the given images. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Latent space features could be affected by the color or texture of the image as well. <sep> 2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. However, it becomes really difficult for other datasets such as CIFAR or some real world images. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. <sep> 3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? There is no guarantee on generalizability or extensibility of the work.","This paper presents a dataset for measuring disentanglement in learned representations. It consists of MNIST digits, sometimes transformed in various ways, and labeled with a variety of attributes. This dataset is used to measure statistics of various learned models. <sep> Measuring disentanglement is certainly an important problem in our field. This dataset seems to be well designed, and I would recommend its use for papers studying disentanglement. The experiments are well-designed. While the reviewers seem bothered by the fact that it's limited to MNIST, this doesn't strike me as a problem. We continue to learn a lot from MNIST, even today. <sep> But producing a useful dataset isn't by itself a significant enough research contribution for an *CONF* paper. I'd recommend publication if (a) it were very different from currently existing datasets, (b) constructing it required overcoming significant technical obstacles, or (c) the dataset led to particularly interesting findings. <sep> Regarding (a), there are already datasets of similar complexity which have ground-truth attributes useful for measuring disentanglement, such as dSprites and 3D Faces. Regarding (b), the construction seems technically straightforward. Regarding (c), the experimental findings are plausible and consistent with past findings (which is a good validation of the dataset) but not obviously interesting in their own right. <sep> So overall, this seems like a useful dataset, but I cannot recommend publication at *CONF*."
"This paper discusses the effect of L1 penalization for deep neural network. In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. <sep> The perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. If the coefficients of the linear equation are distributed in general positions, then the number of variables should not be larger than the number of equations. <sep> While I mostly like the paper, I would like to point out some possible issues: <sep> main concerns: <sep> 1. the columns of V may not be independent during the optimization(training) process. In this situation, I am not quite sure if the assumption of ""general position"" still holds. I understand that in literatures of Lasso and sparse coding it is common to assume ""general position"". But in those problems the coefficient matrix is not Jacobian from a learning procedure. <sep> 2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. It is against the empirical observation that when lambda is extremely small, effect of the regularizer tends to be almost zero. Can authors also show this effects empirically, i.e., when the regularization coefficients decrease, the nnz does not vary much? (Maybe there is some optimization details or approximations I missed?) <sep> Some minor notation issues: <sep> 1. in theorem 1: dim(W^{(j)})=d should be dim(vec(W^{(j)}))=d <sep> 2. in theorem 1: Even though I understand what you are trying to say, I would suggest we describe the jacobian matrix V in details. Especially it is confusing to stack vec(X^J) (vec(W^j)) in the description. <sep> 3. the notations of subgradient and gradient are used without claim","This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non-zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed."
"This paper introduces an approach to pruning the parameters of a trained neural network. The idea is inspired by the Optimal Brain Surgeon method, which relies on second derivatives of the loss w.r.t. the network parameters. Here, the corresponding Hessian matrix is approximated using the Fisher information to make the algorithm scalable to very deep networks. <sep> Strengths: <sep> - The method does not require hyper-parameter tuning. <sep> - The results show the good behavior of the approach. <sep> Weaknesses: <sep> Novelty: <sep> - In essence, this method relies on the work of Marten & Grosse to approximate the Hessian matrix used in the Optimal Brain Surgeon strategy. This is fine, but not of great novelty. <sep> Method: <sep> - It is not clear to me why the notion of binary parameters gamma is necessary. Instead of varying the gammas from 1 to 0, why not directly zero out the corresponding network parameters w? <sep> - In essence, the objective function in Eq. 5 adds an L_1 penalty on the gamma parameters, which would be related to an L_1 penalty on the ws. Note that this strategy has been employed in the past, e.g., Collins & Kohli, 2014, ""Memory Bounded Deep Convolutional Networks"". <sep> - It is not clear to me how zeroing out individual parameters will truly allows one to reduce the model afterwards. In fact, one would rather want to remove entire rows or columns of the matrix W_l, which would truly correspond to a smaller model. This was what was proposed by Wen et al., NIPS 2016 and Alvarez & Salzmann, NIPS 2016, ""Learning the Number of Neurons..."". <sep> - In the past, when dealing with the Hessian matrix, people have used the so-called Pearlmutter trick (Pearlmutter, Neural Computation 2014, ""Fast exact multiplication by the Hessian"". In fact, in this paper, the author mentions the application to the Optimal Brain Surgeon strategy. Is there a benefit of the proposed approach over this alternative strategy? <sep> Experiments: <sep> - While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer.  This does not guarantee entire channels to be removed. As such, I would not know how to make the model actually smaller in practice. It would seem relevant to show the true gains in memory usage and in inference speed (both measured on the computer, not theoretically). <sep> Summary: <sep> I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. However, novelty of the approach is limited, and I am not convinced of its actual benefits in practice.","The authors propose a technique for pruning networks by using second-order information through the Hessian. The Hessian is approximated using the Fisher Information Matrix, which is itself approximated using KFAC. The paper is clearly written and easy to follow, and is evaluated on a number of systems where the authors find that the proposed method achieves good compression ratios without requiring extensive hyperparameter tuning. <sep> The reviewers raised concerns about 1) the novelty of the work (which builds on the KFAC work of Martens and Grosse), 2) whether zeroing out individual connections as opposed to neurons will have practical runtime benefits, 3) the lack of comparisons against baselines on overall training time/complexity, 4) comparisons to work which directly prune as part of training (instead of the train-prune-finetune scheme adopted by the authors). <sep> In the view of the AC, 4) would be an interesting comparison but was not critical to the decision. Ultimately, the decision came down to the concern of lack of novelty and whether the proposed techniques would have an impact on runtime in practice."
"The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. Auxiliary task is used for the sole purpose of helping the main one. In other words, auxiliary task performance is not of interest. The simple and sensible approach proposed in the paper is using cosine similarity between the gradients of two loss functions and incorporating the auxiliary one if it is positively aligned with the main gradient. Authors suggest to further scale loss functions using the cosine similarity but it only experiments with the simpler case of binary decision of using both gradients or only the main one. Authors provide a convergence guarantee (without any convergence rate) by simply extending the convergence of gradient method. <sep> The paper is definitely addressing an important problem as the authors cite many previous work which uses the setup of set of auxiliary tasks helping a main one. The method is simple and easy to implement. Hence, it has a potential to be useful for the community. <sep> One major issue for me is the experimental setup. The authors cite many interesting, realistic and practical setups (Zhang et al., 2016; Jaderberg et al., 2017; Mirowski et al., 2017; Papoudakis et al., 2018), but do not use any of these setups in their experiments. Instead, paper uses set of toy experiments. This is very puzzling to me as all these papers set existing baselines for interesting problems which authors can easily compare. I think the paper needs to be experimented and compared with these established methods. <sep> Another major issue is the weak multi-task learning baseline used in the paper. There have been many interesting developments in adaptive scaling of multiple loss functions in the literature. However, paper does not compare with them. Example of these methods are: [GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks, ICML 2018] and [Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics, CVPR 2018]. Although these methods addresses the case of all tasks being important, it is a valid baseline and need to be compared. Similar to my first points, these papers also use very realistic and interesting experiments which would fit better than the toy experiments in the paper. <sep> Final major issue is the fact that experimental results are suggesting the method is not effective. In ImageNet experiment, auxiliary tasks actually hurt the final performance as the single task is better than all methods including the proposed one. Proposed method does not guarantee that auxiliary tasks will have no harm. The GridWorld experiment is sort of a sanity check to me as it is very hand-crafted. For Breakout experiment, single task actually outperforms all baselines and this means the proposed method results in a harm similar to ImageNet case. For Breakout+MSPacMan experiment, multi task and the proposed method performs almost exactly same. I do not get why the performance on Breakout is relevant for this case since it is not a main task. The paper clearly states that only performance of an interest is the main one which is MSPacMan in this case. Also, in this experiment clearly all methods are still learning as the curve did not plateau yet. I am curious, why the learning is stop there. I do not think we need the method to be effective to be published; but, the negative result should be explained properly. <sep> MINOR NITPICKS <sep> - Algorithm 1&2 are crucial to understand the paper, they should be in main text <sep> - ImageNet class IDs change between years. So, actual wordnet IDs or class names is a better thing to state <sep> - What happens if there are multiple auxiliary tasks? <sep> - Does the theory still hold for loss functions which are not Lipschitz as the Cauchy's gradient method requires that for convergence <sep> In summary, the paper is proposing a sensible method for an important problem. However, it is only tested for toy problems although there are interesting existing setups which would be ideal for the method to be tested. Moreover, it is only compared with the most-naive multi task learning baselines. Even this limited experimental setup does not confirm what the paper is claiming (using auxiliary tasks only when they help). And the paper fails to explain this failure cases. The method needs to be experimented with a more realistic setup with more realistic baselines. <sep> ------ <sep> After rebuttal: <sep> I gave detailed responses to each part of the rebuttal below. Here is the summary: <sep> Although the response addresses some of my concerns. There are still major issues with the experimental study. 1) there are existing, relevant and well-studied multi-task setups with negative interference. Method should be experimented with some of those setups. 2) Multi-task baseline in the paper is naive and far from state-of-the-art. Paper need strong baselines as discussed. Hence, I am keeping my score. Paper needs to be improved with a stronger experimental study and need to be re-submitted.","This paper tackles the problem of using auxiliary losses to help regularize and aid the learning of a ""goal"" task. The approach proposes avoiding the learning of irrelevant or contradictory details from the auxiliary task at the expense of the ""goal"" tasks by observing cosine similarity between the auxiliary and main tasks and ignore those gradients which are too dissimilar. <sep> To justify such a setup one must first show that such negative interference occurs in practice, warranting explicit attention. Then one must show that their algorithm effectively mitigates this interference and at the same time provides some useful signal in combination with the main learning objective. <sep> During the review process there was a significant discussion as to whether the proposed approach sufficiently justified its need and usefulness as defined above. One major point of contention is whether to compare against the multi-task literature. The authors claim that prior multi-task learning literature is out of scope of this work since their goal is not to measure performance on all tasks used during learning. However, this claim does not invalidate the reviewer's request for comparison against multi-task learning work. In fact, the authors *should* verify that their method outperforms state-of-the-art multi-task learning methods. Not because they too are studying performance across all tasks, but because their method which knows to prioritize one task during training should certainly outperform the learning paradigms which have no special preference to one of the tasks. <sep> A main issue with the current draft centers around the usefulness of the proposed algorithm. First, whether the gradient co-sine similarity is a necessary condition to avoid negative interference and 2) to show at least empirically that auxiliary losses do offer improved performance over optimizing the goal task alone. Based on the experiments now available the answers to these questions remains unclear and thus the paper is not yet recommended for publication."
"\\clarity & quality <sep> The paper is easy to follow and self-contained. <sep> However, the motivation for minimizing the upper bound is not so clear for me. <sep> As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. <sep> In the introduction, the author just mentioned that ""the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence."" <sep> For me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly. <sep> \\originality & significance <sep> Although the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting. <sep> However, it is hard to evaluate the usefulness of the proposed method from the current experiments. <sep> It seems that there are two merits about the proposed method as above. <sep> The only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence. <sep> About the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper. <sep> So I cannot tell whether the proposed objective is really useful to learn the deep generative models. <sep> I think further experimental results are needed to validate the proposed method. <sep> \\Question <sep> In page 4,  the variance of the p(y|x) and p_\\theta(y|z) are set to be the same. What is the intuition behind this trick? <sep> Since this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.","The paper proposes a new method for training generative models by minimizing general f-divergences. The main technical idea is to optimize f-divergence between joint distributions which is rightly observed to be the upper bound of the f-divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution. The basic ideas in this work are not completely novel but are put together in a new way. <sep> However, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach. The only quantitive results are in table 2, which is only a simple Gaussian example. It essential to have more substantial empirical results for supporting the new algorithm."
"Summary <sep> ========= <sep> The authors present an extension to the VAE model by exploring the possibility of using the label space to create a new embedding space, which they call Probabilistic Semantic Embedding (PSE). <sep> They present two different extension of PSE, PSE and PSE*. <sep> The idea of additionally supporting the latent embedding, created by a VAE, by using available textual descriptors seems promising. <sep> The proposed model was evaluated on two tasks, label-to-image generation and image annotation. <sep> Although the work is interesting, there are a few questions that I am not clear about and have several comments. <sep> Questions <sep> ========= <sep> 1. How was the word2vec model trained? Did you use an existing pretrained model (e.g. available as download) or did you train the embedding model yourself? If so, on what data? <sep> 2. The major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding Wes was neither introduced in the Related Work section nor was it clearly explained in the text. <sep> 3. Why did the authors focus on the w2v model instead of more promising approaches as fastText or ELMo? <sep> 4. How does your model deal with OOV word(s) as input? For example, when used as Image Generator. <sep> 5. Table 1 shows results achieved on MNIST but not Fashion-MNIST; was the evaluation performed on MNIST only? <sep> 6. Table 2 presents the impact of the use of pretrained embeddings (word2vec) instead of one-hot vectors for labels. Which one do the models presented in Table 1 use? <sep> 7. Could you explain the small difference between using one-hot vs pretrained label encodings, presented in Table 2? <sep> 8. Also, can you explain how the numbers in table 2 were achieved (e.g. sum over all, average of all, etc.). When comparing the values presented here to the values of the same measure in table 1, one does notice the big difference between them. <sep> Comments <sep> ========= <sep> 1. Section 2, page2: ""As derived in the original paper…"" references which paper (i.e. Kingma et al)? <sep> 2. VAE or beta-VAE model is not referenced (mentioned on page 5); <sep> 3. Authors do agree that the corpora used is not optimal for the adequate evaluation of the proposed model. It would be interesting to see the use of this approach on a more realistic data set; <sep> 4. Unclear sentence: ""Compared with the VAE, latent codes where images with the same labels are clustered.""; <sep> 5. The authors claim that one of the results of this work is the possibility to generalize for unseen cases (zero-shot learning). It would be interesting to see the performance of this model compared to SOTA in CV in terms of the zero-shot learning task; <sep> 6. Figure 2 visualizes proposed embedding space (2D) but it shows VAE and beta-VAE models and omits to show PSE. VAE and beta-VAE are neither introduced nor referenced in text; <sep> 7. Table 1: mark best performing with bold. It does, however, outperform other evaluated models when using 20D embedding space; <sep> 8. Page 7: in text you mention generation accuracy and in Table 1 the same value is defined as Generation Correctness (%).",mnist and small picture variants are not that impressive. <sep> it is a minor extension of VAEs which also are not common in sota systems.
"The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. <sep> The paper is well written and easy to follow. Various experiments are conducted to support the contributions of the paper.  Nevertheless, the technical novelty seems a bit weak to me. The method basically moves the interpolating process from input space as in MixUp to randomly selected hidden states. More importantly, some of the paper's claims are not very convincing to me in its current form. <sep> Major remarks: <sep> 1. The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. <sep> The authors theoretically prove that with the proposed training cost in Manifold Mixup, the representation for each class will lie on a subspace of dimension dim (h) –d +1 (h and d are the hidden dimension and number of classes, respectively). I did not get the idea of how such dimension reduction relates to the ''flattening'' of the manifold and in particular how such representations (representations for each class ""concentrating into local regions"") can avoid the class collision issues as that in Mixup. <sep> Experimentally, from Figures 3 and 4, it seems the class collision issue could be worse than that of Mixup. For example, for mixing ratio of 0.6 (meaning the created image has almost half labels from the two original images), MixUp clearly shows, for instance in the second row, that there are two overlapped images (Horse and Plane), but Manifold Mixup seems to have only the Plane in the mixed image with a soft label. <sep> 2. The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning. In this way, the paper would be much more convincing because the proposed method is so close to MixUp and the observation here is contradictive. <sep> 3. I wonder how sensitive is the parameter Alpha in Manifold Mixup. For example, how the mixing rate Alpha impacts the results for NLL and Semi-supervised learning in section 5.2? <sep> 4. It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper. <sep> Minor remarks: <sep> 1. In Table2, the result from AdaMix seems missed. <sep> 2. Why not using Cifar100, but with a new dataset SVHN for the semi-supervised learning in section 5.2? <sep> 3. In related work, regarding regularizing deep networks by perturbing the hidden states, the proposed method may relate to AgrLearn (Guo et al., Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks) as well.","The paper contains useful information and shows relative improvements compared to mixup. However, some of the main claims are not substantiated enough to be fully convincing. For example, the claims that manifold mixup can prevent can manifold collision issue where the interpolation between two samples collides with a sample from other class is incorrect. The authors are encouraged to incorporate remarks of the reviewers."
"This paper studies the ability of SGD to learn dynamics of a linear system + non-linear activation. That is, in the standard LTI setting, the dynamics of a system evolve according to h_{t+1} = Ah_t + Bu_t, <sep> on input u_t. <sep> In addition, this paper considers the setting where the evolution is: <sep> h_{t+1} = \\phi(Ah_t + Bu_t) <sep> for \\phi a non-linear activation function. <sep> This is a difficult problem. Though system identification was for many decades a large and active area in the control community, the understanding of system identification from a modern statistical perspective (understanding sample complexity and computational complexity simultaneously) is surprisingly lacking. This is evidenced by the fact that the first results along these lines for the simplest possible (SISO, LTI) system, came only recently (Hardt, Ma, Recht '16). <sep> This paper attacks a more general setting, due to the presence of the nonlinearity. <sep> However, the present setting is significantly limited in another sense: the authors assume that the state is observed directly. This is in contrast to the typical situation where we observe only a projection of the state, or possibly even a noisy such projection. Indeed, this is one of the critical complications in the work of Hardt, Ma and Recht. Without it, i.e., under the assumption that the entire state trajectory can be directly observed, much more is possible, and indeed much more has been done. For example, work by Bento, Ibrahimi and Montanari '10, solves a more difficult problem in that they estimate sparse dynamics (in appropriate sample complexity). Jalali and Sanghavi '11 generalized the work of Bento et al., to the setting where some of the components of the state are not all observed, but rather some are latent. <sep> The motivating application for this work is estimating RNNs. In this case, the state variable represents the critical information that is carried from one time to the next in the RNN. Presumably the setting here is to show that if indeed data are generated by an RNN, then we can compute this using SGD and backprop. Towards this, the assumption of having access to the internal state is a difficult one. On the one hand, this is a hard and important problem. On the other, we really won't have access to such an internal state. There are of course other problematic aspects, such as robustness, the inability to use ReLU (Defn 3.1). But the observation model seems important. Again, I believe this is especially so, because the considerable complications present in Hardt, Ma, Rect '16 specifically seemed to be a consequence of the observation model being partial. <sep> The inability to use ReLU at first look does not seem like a great limitation. But then one problematic aspect here seems that the proof concept and direction critically rely on this, as they basically reduce to the setting of linear activations — something which, presumably, is impossible for something like ReLU. So it is not only the results, but also the developed machinery, that seem to be inherently limited. <sep> This is, overall, an interesting paper, attacking an important and also very challenging area. As with all papers in this vein, we are left with having to make a judgement call on whether this simplified scenario is indeed a good first step towards solving the problems we are hoping to solve. Is it developing the right insight, right tools, etc. While I find there is a lot of interesting and good work in this paper, I am not completely convinced about this last point.","This paper shows convergence of stochastic gradient descent for the problem of learning weight matrices for a linear dynamical system with non-linear activation. Reviewers agree that the problem considered is both interesting and challenging. However the paper makes many simplifying assumptions - 1) both input and hidden state are observed, a very non standard assumption, 2) analysis requires increasing activation functions, cannot handle ReLU functions. I agree with R2 and think these assumptions make the results significantly weaker. R1 and R3 are more optimistic, but authors response does not give an insight into how one might extend this analysis to the setting where hidden state is not observed. Relaxing these assumptions will make the paper more interesting."
"Quality is good, just a handful of typos. <sep> Claritys above average in explaining the problem setting. <sep> Originality: scan refs... <sep> Significance: medium <sep> Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). Further, for inference, the authors propose an explicit procedure. It seems like a noveel approach to demixing which is exciting. <sep> Cons: The experiments do not push the limits of their method. It's difficult to judge the demixing 'power' of the method because it's difficult to tell how hard the problem is. Their method seems to easily solve it (super low MSE). The classification measure is clearly improved by denoising, which is totally unsurprising-- There should definitely be comparison with other denoising methods. <sep> In general, they don't compare to any other methods. Actually in the appendix, comparisons are provided for a basic compressive sensing problem, but their only comparator is ""LASSO"" with a ""fixed regularization parameter"", and vanilla GAN. Since the authors ""main contribution"" (their words) is demixing, I'm surprised that they did not compare with other demixing approaches, or try on a harder problem. Could you  give some more details about the LASSO approach? How did you choose the L1 parameter? <sep> I have another problem with the demixing experimental setting. On one hand, both the sinusoids and MNIST have ""similar characteristics"" in the sense that they are both pretty sparse, basically simple combinations of primary curves. This actually makes the problem harder for a dictionary learning approach like MCA (referenced in your paper). On the other hand, both signals are very simple to reconstruct. For example, what if you superimposed the grid of digits onto a natural image? Would you be able to train the higher resolution GAN to handle a more difficult setting? The other demixing setting of adding 1's and 2's has a similar problem. <sep> The authors need to provide (R)MSE  results that show how well the method can reconstruct mixture components on average over the dataset. The only comparison is visual, and no comparators are provided. <sep> Conclusions: <sep> I'm actually torn on this paper. On one hand this paper seems novel and clearly contributes to the field. On the other hand, HOW MUCH contribution is not addressed experimentally, i.e. the method is not properly compared with other denoising or demixing methods, and definitely not pushed to its limits. It's hard to assess the difficulty of the denoising problem because their method does so well, and it's hard to assess the difficulty of demixing because of the lack of comparators. <sep> Caveats: <sep> I am knowledgeable about iterative optimization approaches to denoising and demixing, especially MCA (morphological component analysis), but *not knowledgeable about GAN-based approaches*, though I have familiarity with GANs. <sep> ********************* <sep> Update after author response: <sep> I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. I think this is an exciting contribution to dually learning component manifolds for demixing.","The paper proposes two simple generator architecture variants enabling the use of GAN training for the tasks of denoising (from known noise types) and demixing (of two added sources). While the denoising approach is very similar to AmbientGAN and could thus be considered somewhat incremental, all reviewers and the AC agree that the developed use of GANs for demixing is an interesting novel direction. The paper is well written, and the approach is supported by encouraging experimental results on MNIST and Fashion-MNIST. <sep> Reviewers and AC noted the following weaknesses of the paper: a) no theoretical support or analysis is provided for the approach, this makes it primarily an empirical study of a nice idea. <sep> b) For an empirical study, the experimental evaluation is very limited, both in terms of dataset/problems it is tested on; and in terms of algorithms for demixing/source-separation that it is compared against. <sep> Following these reviews, the authors added the experiments on Fashion-MNIST and comparison with ICA which are steps in the right direction. This improvement moved one reviewer to positively update his score, but not the others. <sep> Taking everything into account, the AC judges that it is a very promising direction, but that more extensive experiments on additional benchmark tasks for demixing and comparison with other demixing algorithms are needed to make this work a more complete contribution."
"This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. <sep> I find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. <sep> Here I detail my main concerns: <sep> 1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, p(y|x)=∫p(y|x,w)p(w|Θ)dw. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). <sep> In the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound ln⁡p(y∗|x∗)>=Eq[ln⁡p(y∗|x∗,w)]−KL(q|p) <sep> In this case, q(w) = p(w|\\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the Θ are learnt by maximum log-likelihood and do not have a q associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time. <sep> My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. <sep> 3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. <sep> So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. <sep> Minor comments: <sep> 1. The generative model for Variational dropout is the same than the generative model for the ""conditional model"", eq. (3). <sep> 2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2. <sep> 3. Section 3.3. I find some parts are difficult to understand. ""suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation."" Later, I can understand authors are referring to the possibility of reducing the dropout rate.","The paper tried to introduce a new interpretation of dropout and come with improved algorithms. However, the reviewers were not convinced that the presented arguments were correct/novel, and they found the paper difficult to follow. The authors are encouraged to carefully revise their paper to address these concerns."
"The paper considers adaptive regularization, which has been popular in neural network learning.  Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix. <sep> When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?  As a matter of good implementation, one never takes the inverse of anything.  Instead, on solves a linear system, via other means.  Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up. <sep> There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.  The latter may be important in practice, but it is orthogonal to the full matrix theory. <sep> There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.  Instead, it is a low-rank approximation to the full matrix.  If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters. <sep> The discussion of convergence to first order critical points is straightforward. <sep> Adaptivity ratio is mentioned in the intro but not defined there.  Why mention it here, if it's not being defined. <sep> You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.  It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems.. <sep> It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks. <sep> The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.  If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.  More papers should present this, and those that do should do it more systematically. <sep> You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)","This paper shows how to implement a low-rank version of the Adagrad preconditioner in a GPU-friendly manner. A theoretical analysis of a ""hard-window"" version of the proposed algorithm demonstrates that it is not worse than SGD at finding a first-order stationary point in the nonconvex setting. Experiments on CIFAR-10 classification using a ConvNet and Penn Treebank character-level language modeling using an LSTM show that the proposed algorithm improves training loss faster than SGD, Adagrad, and Adam (measuring time in epochs) and has better generalization performance on the language modeling task. However, if wall-clock time is used to measure time, there is no speedup for the ConvNet model, but there is for the recurrent model. The reviewers liked the simplicity of the approach and greatly appreciated the elegant visualization of the eigenspectrum in Figure 4. But, even after discussion, critical concerns remained about the need for more focus on the practical tradeoffs between per-iteration improvement and per-second improvement in the loss and the need for a more careful analysis of the relationship of this method to stochastic L-BFGS. A more minor concern is that the term ""full-matrix regularization"" seems somewhat deceptive when the actual regularization is low rank. The AC also suggests that, if the authors plan to revise this paper and submit it to another venue, they consider the relationship between GGT and the various stochastic natural gradient optimization algorithms in the literature that differ from GGT primarily in the exponent on the Gram matrix."
"Pros: <sep> + Improving joint training of non-differentiable pipelines is a meaningful and relevant problem <sep> + Using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea <sep> Cons: <sep> + The main result of the paper concerning sufficient conditions for optimality of the method seems dubious <sep> + It is not obvious why this method would outperform simple baselines, and baselines for joint training were tried <sep> + The notation seems unnecessarily bloated and overly formal <sep> + The exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing <sep> The submission describes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector), so that gradient-based methods may be applied to jointly train all the parameters of the pipeline.  In particular, the proposal involves recasting the pipeline as a stochastic computation graph (SCG), adding stochastic nodes to this graph, and then using REINFORCE-style policy gradients to perform parameter learning on the SCG.  It is claimed that under certain conditions, the optimal parameters of the resulting SCG are also optimal for the original pipeline.  The method is applied to optimizing the parameters of Faster-RCNN. <sep> I think making non-differentiable pipelines differentiable is an intuitively appealing concept.  A lot of important, practical machine learning systems fall into this category, so devising a nice way to do global parameter optimization for such systems could potentially have significant impact.  In general, we can't hope to make much meaningful progress on the problem of optimizing general nonlinear, differentiable functions, but it is plausible that a method that targets key non-differentiable components for smoothing—such as this paper—could outperform a generic black-box optimizer.  So, I think the basic idea here is plausible and addresses an important problem. <sep> Unfortunately, I think this work loses sight of that high-level goal: to me, the key question is whether the proposed approach outperforms any other simple method for global parameter optimization in the presence of nonlinearities and nondifferentiability.  The paper fails to answer this question because no baselines for global parameter optimization were tried.  We can just treat the pipeline as a black box mapping parameters to training set performance, and so any black-box optimization method can be applied to this problem.  It is not clear that the proposed method would outperform an arbitrary black box optimization method such as simulated annealing, Nelder-Mead, cross-entropy method, etc. <sep> I think there are also much simpler methods in a similar vein to the proposed method that might also perform just as well as the proposal.  One key conceptual issue here is that reducing the problem to a reinforcement learning problem, as the submission does, is not much of a reduction at all.  First, if the goal is to do global parameter optimization, then we don't really have to smooth the pipeline itself: we can just smooth the black box mapping parameters to performance, and then optimize that with SGD.  There are many ways to do this--if we want to use policy gradient, we can just express the problem as something in this form: <sep> min_\\phi E_{\\theta ~ q_\\phi} C(\\theta) <sep> where C is the black-box mapping parameters \\theta to a performance index (such as mean AP), q_\\phi is a distribution over parameters (e.g., Gaussian), and \\phi are the distribution parameters (e.g., mean, covariance of the Gaussian).  We can then optimize this using REINFORCE policy gradients. <sep> If we want to really smooth the pipeline itself, then it is also easy to do this by devising a suitable MDP and then applying REINFORCE with the usual MDP structure.  We simply identify the state s_t at time t with the output of the t'th pipeline stage, introduce a new 'action' variable a_t representing a 'stochastified output', and trivial dynamics (P(s_{t+1} | s_t, a_t) = \\delta(s_{t+1} - a_t)).  If the policy is a Gaussian (P(a_t | s_t) = N(a_t; s_t, \\Sigma)), then this is similar to relaxing the constraint that one stage's output is equal to the input of the next stage, and somehow quadratically penalizing their difference.  In fact, there is a neural network training method based explicitly on this penalization view [A], and it would make yet another great baseline to try. <sep> In fact, the proposed method is essentially similar to what I have just described, but it is unfortunately described in an overcomplicated way that obscures the true nature of the method.  I think the whole SCG framework is overkill here.  Too much of the paper is spent just rehashing the SCG framework, and the very heavy notation again just obscures the essential character of the method. <sep> If there were, as the paper claims, some interesting condition under which the method produces solutions that are optimal under the original pipeline, that would be remarkable and interesting.  However, I have serious doubts about this part of the paper.  The key problem is the statement that ""It follows that c(k_c, DEPS_c - k_c) = c(…) + z_c"".  The paper seems to be claiming that if E z = 0, then c(k + z) = c(k) + z, which can't possibly be true in general. <sep> The heavy and opaque notation makes it very difficult to understand this section.  Perhaps it would help to consider a very simple example.  Suppose we want to minimize E_{x ~ q} c(y(x)) (where x ~ q means x is distributed as q).  We can introduce only one new stochastic node (k = y + z), between y and c.  Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0. <sep> In summary, I think the submission needs a lot of work on multiple axes before it can make a significant impact.  The most important issues are a complete lack of relevant baselines and the dubious claims about sufficient conditions for optimality.  The idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [A]) as well as the simple baselines I have mentioned.  The presentation also needs to be revised to find the simplest expression of the method and to focus on the interesting parts. <sep> [A] Taylor, Gavin, et al. ""Training neural networks without gradients: A scalable admm approach."" International Conference on Machine Learning. 2016.","The work proposes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector) using policy gradient. Unfortunately, the reviewers identified a number of critical issues, including no significant improvement beyond existing works. The authors did not provide a rebuttal for these critical issues."
"This paper introduces a new framework for learning an interpretable representation of images and their attributes. The authors suggest decomposing the representation into a set of 'template' latent features, and a set of attribute-based features. The attribute-based features can be either 'free', i.e. discovered from the data, or 'fixed', i.e. based on the ground truth attributes. The authors encourage the decomposition of the latent space into the 'template' and the 'attributes' features by training a discriminator network to predict whether the attributes and the template features come from the same image or not. <sep> While the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate. Furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so I am still confused about the exact implementation of the attribute features \\phi(x, m) for example. The authors do point to the Appendix for their Experiments section, however this is not a good idea. The paper should be self-contained and the authors should not assume that their readers will read the information presented in the Appendix, which is always optional. <sep> Unfortunately, even the experimental section presented in the Appendix is not comprehensive enough to evaluate the proposed method. The authors train the model on a single dataset (MNIST), no baseline or ablation results are presented, and all the results are purely qualitative. Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. I recommend that the authors present quantitive results in the updated version of their paper (i.e. disentanglement metric scores, the log-likelihood of the reconstructions), including new experiments on a dataset like dSprites or CelebA, where the ground truth attributes are known.","The authors propose a generative model based on variational autoencoders that provides means to manipulate the high-level attributes of a given input. The attributes can be either pre-defined ground truth attributes or unknown attributes automatically discovered from the data. <sep> While the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by AC as a critical issue: (1) very limited experimental evaluation (e.g. no baseline or ablation results, no quantitative results); comparisons on other more complex datasets and more in-depth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work – see, for example, R3's suggestion to use other dataset like dSprites or CelebA, where the ground truth attributes are known; (2) lack of presentation clarity – see R2's latest comment how to improve. <sep> A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarification, more empirical studies and polish to achieve the desired goal."
"Two high-level points about my review before going into the details: <sep> 1. This paper was a thoroughly enjoyable and insightful read. Kudos to the authors for attempting such a comprehensive overview of likelihood-based vs. likelihood-free learning. <sep> 2. I'll be more than happy to revise my current rating if my concerns are addressed by the authors. <sep> With regards to the technical assessment of this work, the idea of using a nearest neighbors objective for learning a generative model is both intriguing and appealing. What makes this work even more interesting are its connections with maximum likelihood estimation. Novelty aside, I believe there are major theoretical, algorithmic, and empirical concerns in the current work which I discuss below: <sep> Theorem 1 <sep> - The third condition is true for location-scale family of distributions e.g., Gaussian. But the distribution learned by a generative model p_theta is far from Gaussian or other location-scale distributions. <sep> - More importantly, I don't think the upper bound is tight in practice because the likelihoods can vary significantly across the dataset. Take MNIST for example. Compare the log-likelihoods of an autoregressive model or ELBOs of a VAE across the different classes of digits. Straight digits (like 1s) have much higher log-likelihoods on average than curved digits. <sep> Algorithm <sep> - While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it's hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation. <sep> - Similarly, I was a bit disappointed by the choice of Euclidean distance in a pixel space as the choice of distance metric. The argument that you do not want to use ""auxiliary sources of labelled data or leverage domain-specific prior knowledge"" is indeed necessary for fair comparisons, but also points to a limitation of the current approach. <sep> Empirical evaluation <sep> - Seems too outdated both in terms of baselines and metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches. <sep> - While it is arguably well-established that Parzen window estimates are misleading (Theis et al.), that's the only quantitative estimate in this work (Table 1). Hard to think of any recent published work (last 1-2 years) in generative modeling that even reports these estimates. <sep> - The baselines in Table 1 are all from 2013-15. Clearly, much has happened in the last 3 years that merit the inclusion of more recent baselines. <sep> -  Even for sample quality, there has been a lot of research in designing and improving metrics. E.g., Inception scores, Frechet Inception Distance, Kernel Inception Distance. I am not looking for state-of-the-art numbers, showing heavily zoomed out samples without any of these metrics is slightly disingenuous. <sep> - As mentioned before, reporting the computation time/per iteration and number of iterations for convergence for the proposed algorithm in comparison with other approaches  is important. <sep> - Similarly, the argument about the method avoiding even the other GAN problems (e.g., vanishing gradients, stability in training) can and should be supported by empirical evidence. <sep> Analysis and discussion <sep> - One family of generative models that is crucially missing from this work is normalizing flow models. <sep> - This is somewhat debatable, but I do not agree that the tradeoff between likelihoods and sample quality is due to model capacity. As far as I can tell, the cited work of Grover et al., 2017 provides evidence contrary to what the authors claim. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. So model capacity isn't necessarily the key differentiating factor (which is same for both training algorithms in their experiments), it's more about the choice of the objective function and the optimization procedure. <sep> Minor points for improving presentation: <sep> - Section 3 can be made more concise and to the point. I'd be especially interested if the precision and recall discussion in this section and elsewhere can be formalized. <sep> - Use numbered lists instead of bullets for assumptions in Theorem 1, so that the discussion of the assumptions right after the theorem statement are easy to follow. <sep> - The citation of Grover et al. seems outdated? The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models. <sep> - In general, avoid making somewhat hard assertions that are speculative. Some of them I've highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.).","The manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation. Overall, reviewers and AC agree that the general problem statement is timely and interesting, and the subject is of interest to the *CONF* community <sep> The reviewers and ACs note weakness in the evaluation of the proposed method. In particular, reviewers note that the Parzen-based log-likelihood estimate is known to be unreliable in high-dimensions. This makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated. Reviewers also expressed concerns about the strengths of the baselines compared. Additional concerns are raised with regards to scalability which the authors address in the rebuttal."
"The paper proposes a model to improve adversarial training, by introducing random perturbations in the activations of one of the hidden layers. Experiments show that robustness to attacks can be improved, but seemingly at a significant cost to accuracy on non-adversarial input. <sep> I have not spent significant time on adversarial training, and review the paper under the following understanding: It was observed that the decision regions of a class are sprinkled with ""holes"" that get misclassified. These holes are neither naturally occuring. Their existence allows a potential attacker to coerce a model into mis-classifying by providing specially crafted inputs, in order to attain a benefit. Therefore, those holes are called ""adversarial"" examples. The risk is heightened by the fact that adversarial examples are commonly not mis-classified by humans (or even detectable by the eye). To ""plug"" the holes, one includes adversarial examples in the training, called ""adversarial training."" A resulting system should now have a much improved accuracy for the ""holes"", while ideally not affecting classification accuracy for the natural examples, which will continue to constitute nearly 100% of the samples the system will be used on. (The ""hole"" metaphor may not be entirely appropriate, since the space of adversarial examples that are neither misclassified by humans nor detectable is likely much larger than the space of naturally occuring samples.) <sep> The paper proposes a way of plugging the hole by quantizing layer activations. The results show that this makes the system robust to adversarial attacks. <sep> Clarity: <sep> I spent a lot of time figuring out, as someone who has not spent a lot of time with this, what is being evaluated. It is very unclear whether the non-clean systems in Tables 1 and 2 do apply FGSM etc. also in training (in combination with SQA), or only to the test samples. Table 4, the wording in 4.2, and the wording of the Conclusion indicate that they are. But then, where do I find the accuracy on the naturally-occuring (non-manipulated) samples? <sep> The only combination of interpretations that makes sense in the end is to parse ""The networks are all trained with fast single-step adversaries"" as to mean ""The networks are all trained with FGSM"", and that the non-Clean columns in Table 1 refer to test data perturbed by the respective method, while the Clean column shows the accuracy on the natural data. This *must* be clarified in the final version, as it took way too long to understand this. I strongly suggest to do this with the naming: change small_full to small_FGSM, and small_SQA to small_SQA+FGSM. <sep> Assuming I figured this out right, the tables still lack the baseline accuracy of doing nothing (clean-clean), so one can know how much the nearly-100% use case gets affected. <sep> Results: <sep> The second concern I have is that, assuming my reading of the results as described above is correct, that the SQA method quite severely affects accuracy on the clean test data, e.g. increasing the error rate on CIFAR by 72% (from 12.33% to 17.06%). There must be a discussion on why such severe performance hit is worth it, especially since there often is an accuracy cliff below which there is a steep loss of usability of a system. For example, according to my personal experience in speech recognition, the difference between 12% and 17% is the difference between decent and unacceptable user experience (also considering that a few percent of errors are caused by ambiguities in the ground-truth annotations themselves, which should be the case for CIFAR as well). <sep> Figure 1 seems a little misleading in this regard since the areas of good accuracy are very condensed. It should be rescaled, as only the area close to the optimum performance is relevant. It does not matter whether we degrade from 99.x% to 77% or 58%, or even 95-ish. All of those hurt performance to the point of not being useful. <sep> It would be nice to discuss what an accuracy metric would be that is useful for the end user. It would have to be a combination of the expected cost of a misclassification of a natural image and the expected cost caused by attacks. A good method would improve this overall metric. A paper attempting to address adversarial attacks should at least discuss this topic briefly, in my view. <sep> Technical soundness: <sep> A technical question I have is whether the min-max normalization may be too susceptible to outliers. A single extreme activation can drastically shift the threshold for \\lambda=1. How about a mean-var normalization? If there is batch or layer normalization in the system, your activations may already be scaled into a consistent range anyway, that might allow you to use a constant scaling on top of that. <sep> Another question I have is: quantization is often modeled as adding uniform noise. Why not add noise directly? And why uniform noise? For example, would compute g = h + Gaussian noise with std dev=(max-min)/lambda work equally well? What is special about quantization? <sep> And another technical question: My guess is that the notable loss of accuracy is caused by the strong quantization (two values only in the case of \\lambda=1). I think the paper should show results for larger lambdas, specifically whether there is a better trade-off point between the accuracy loss from quantization vs. robustness to adversarial samples. <sep> Section 3/SQA: ""This is the reason why we rescale g^i to the original range of h^i"" This seems wrong. I think the main reason is that one would not want to totally change the dynamic ranges of the network, as it may affect convergence merely by scaling. You'd want to limit any impact on convergence to the quantization itself. <sep> Significance: <sep> I think the significance is limited. Given that the accuracy impact of the mitigation method is very large, I do not consider this paper as substantially solving the problem, or even bringing a practical solution much closer in reach. <sep> Pros: <sep> - tnteresting idea; <sep> - comparison against various attacks. <sep> Cons: <sep> - Hard to understand because it was left unclear what is evaluated, at least to readers who are not familiar with a possibly existing implied convention; <sep> - The method seems to harm accuracy on clean data a lot, which is the main use case of such a system. <sep> I would in the current form reject the paper. To make it acceptable, the clarity of presentation, especially of the results, must be improved, but more importantly, more work seems necessary to reduce the currently significant accuracy hit from the method, and the trade-off of quantization level vs. robustness should be addressed. <sep> Minor feedback: <sep> Please review the paper for grammar and spelling errors (e.g. ""BinaryConnect constraints"" or the use of ""make"", which is often not correct). <sep> In Algorithm 1, I suggest to not use 'g', as it may be mis-read as ""gradient."" Unless this is a common symbol in this context. <sep> ""Thus, we propose SQA"" warrants another \\subsubsection{}, to indicate where \\subsubsection{BinaryConnect} ends. <sep> Section 2.2's early reference to SQA is a little confusing, since SQA has not formally been defined. I would smooth this a little, e.g. change ""SQA can be considered"" to ""We will see that our SQA, as introduced in the next section, can be considered"" <sep> ""an alternative is to approximate it"" probably should be ""our approach is to approximate it","While the paper contains interesting ideas, the reviewers suggest improving the clarity and experimental study of the paper. The work holds promises but is not ready for publication at *CONF*."
"This paper proposes a method to train a machine translation system using weakly paired bilingual documents from Wikipedia. A pair of sentences from a weak document pair are used as training data if their cosine similarity exceeds c1, and the similarity between this sentence pair is c2 greater than any other pair in the documents, under sentence representations formed from word embeddings trained with MUSE. The neural translation model learns to translate from language X to Y, and from Y to X using the same encoder and decoder parameters, but the decoder is aware of the intended target language given an embedding of the intended language. The model is also trained to minimise the KL divergence between the distribution of terms in the target language document and the distribution of terms in the current model output. The model also uses the denoising autoencoding and reconstruction objectives of Lample et al. (2017). The results show improvements over the Lample et al. (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents. <sep> Positives <sep> - Large improvement over previous attempts at unsupervised MT for the En-De language pair. <sep> - Informative ablation study in Section 4.4 of the relative contribution of each part of the overall objective function (Eq 9). <sep> Negatives <sep> - The introduction gave the impression that this method would be applied to low-resource language pairs but it was applied to two high-resource language pairs. Because you have not evaluated on a low-resource language pair, it's not clear how your proposed method would generalise to a low-resource setting. <sep> Questions <sep> - Can you give some intuition for why you remove the first principal component from the word embeddings in Equations 1 - 3? <sep> - Are the Supervised results in Table 2 actually a fair reflection of a reasonable NMT model trained with sub-word representations and back translated data? <sep> - What is the total number of sentences in the weakly paired documents in Table 1? It would be useful to know the proportion of sentences you managed to extract to train your models. <sep> Comments <sep> - Koehn et al. (2003) is not an example of any kind of neural network architecture.","This paper proposes a new method to mine sentence from Wikipedia and use them to train an MT system, and also a topic-based loss function. In particular, the first contribution, which is the main aspect of the proposal is effective, outperforming methods for fully unsupervised learning. <sep> The main concern with the proposed method, or at least it's description in the paper, is that it isn't framed appropriately with respect to previous work on mining parallel sentences from comparable corpora such as Wikipedia. Based on interaction in the reviews, I feel that things are now framed a bit better, and there are additional baselines, but still the explanation in the paper isn't framed with respect to this previous work, and also the baselines are not competitive, despite previous work reporting very nice results for these previous methods. <sep> I feel like this could be a very nice paper at some point if it's re-written with the appropriate references to previous work, and experimental results where the baselines are done appropriately. Thus at this time I'm not recommending that the paper be accepted, but encourage the authors to re-submit a revised version in the future."
"The authors here introduces a novel  graph pooling technique called StructPool that uses the underlying graph's structural information to behave as a node clustering algorithm and learns a node clustering matrix. <sep> Graph level classification requires learning good graph level representation, especially for  aggregating low level information for high . Recent work in pooling does not take advantage of important structural information of the relationship between different  nodes. Here, the authors formulate  graph pooling as a structured prediction problem, control clique set in the CRFs and use mean filed approximation to calculate assignments. <sep> A cluster assignment matrix assigns each node in the original graph to a cluster in the new graph. The assignment not only depends on the node features but also on the cluster assignment of the other nodes. The authors therefore draw connection with finding the optimal assignment to minimizing the Gibbs energy. The authors propose to learn clustering assignment via CRF conditioned on the global feature representation of the nodes. <sep> The unary potentials of the cliques are computed used the GCN to measure energy of each node. The novelty in accommodating topology information is in using l hop connectivity based on adjacency A to define pairwise cliques thus building pairwise relationship between pairs of nodes thus allowing the Gibbs energy formulation of the cluster assignment thereby using GCN to also compute this pairwise energy. <sep> I have  a few questions as below: <sep> I think the authors can better elucidate the motivation for using  the attention matrix over Gaussian kernels to measure pairwise energy in section 3.3; an  empirical experiment for drawing comparison wrt to the computational time and number of feature dimensions on a toy problem seems important. <sep> How is the computation  of the unary potential and pairwise energy influenced by the connectivity of the graph G for the datasets considered? It would be interesting to see how the pairwise energy, unary energy varies over different layers of GCNs. <sep> Further, how is the cluster assignment affected by the l-hop connectivity? <sep> Is there a notion of the minimum value of 'k' in the context of convergence? <sep> What happens in case of very different graph features, or structural assumptions where the cliques are not enforced? <sep> Is there a notion of how the method performs on datasets with a high percentage of isomorphism bias: repeating instances or repeating instances with different labels? <sep> It will be interesting to see a discussion on how the performance varies with respect to the depth of the overall architecture,  positioning of the structpool and some results on how effective they are on  hierarchical features and multiple pooling ops as in architectures such as Graph UNet. <sep> Avoid repetition in 2.2 Related work section and in other sections throughout. Otherwise, the paper is rather well written and has clarity.","The paper proposed an operation called StructPool for graph-pooling by treating it as node clustering problem (assigning a label from 1..k to each node) and then use a pairwise CRF structure to jointly infer these labels. The reviewers all think that this is a well-written paper, and the experimental results are adequate to back up the claim that StructPool offers advantage over other graph-pooling operations. Even though the idea of the presented method is simple and it does add more (albeit by a constant factor) to the computational burden of graph neural network, I think this would make a valuable addition to the literature."
"Summary: the authors introduce a method to learn a deep-learning model whose loss function is augmented with a DPP-like regularization term to enforce diversity within the feature embeddings. <sep> Decision: I recommend that this paper be rejected. At a high level, this paper is experimentally focused, but I am not convinced that the experiments are sufficient for acceptance. <sep> **************************** <sep> My main concerns are as follows: <sep> - Many mathematical claims should be more carefully stated. For example, the authors extend the discrete DPP formulation to continuous space. It is not clear to me, based on the choice of the kernel function embedding, that the resulting P_k(X) is a probability (Eq. 1). If it is not (using a DPP-based formulation as a regularizer does not require a distribution), the authors should clarify that fact; more generally, the authors should be more careful throughout the paper (for example, det=0 if features are proportional, not necessarily equal; the authors inconsistently switch between DPP kernel L and marginal kernel K throughout computations.) <sep> - The authors do not describe their baselines for several experiments. In tables 1, 2, 3, the baseline is never described (I assume it's the same setup without regularization); I did not find a description of DCH (Tab 4) in the paper (Deep Cauchy Hashing?). The mAP-k metric should also be defined. Furthermore, the authors do not report standard deviations for their experiments. <sep> - A key consideration when using DPPs is their compulational cost: most operations involving them require SVD (which seems to be used in this work), matrix inversion, and often both. This, unsurprisingly, limits the applications of DPPs, and has driven a lot of research focused on improving DPP overhead. I would like to see more discussion in this paper focused on to which extent the DPP's computational overhead remains tractable, and which methods were used (if any) to alleviate the computational burden. <sep> - Finally, the paper itself appears to be somewhat incomplete: sentences are missing or incomplete (Section 4), and numbers are missing in some tables (Table 5). <sep> *********************** <sep> Questions and comments for the authors: <sep> - When computing the proper sub-gradient, are you computing the subgradient as inv(L + \\hat L)? <sep> - You state that by avoiding matrix inversion, your method is more feasible. However, it seems like your method requires SVD, which is also O(N^3); could you please provide more detail for this? <sep> - Could you report number of trials and standard deviations for your experiments? <sep> - Do you have any insight into why DPPs do more poorly than the DCH baseline in Table 4 for mAP-k metrics? <sep> - You might be able to save space by decreasing the space allocated to the exponentiated quadratic kernel.","Most reviewers seems in favour of accepting this paper, with the borderline rejection being satisfied with acceptance if the authors take special heed of their comments to improve the clarity of the paper when preparing the final version. From examination of the reviews, the paper achieves enough to warrant publication. Accept."
"SUMMARY: explore the use of CNN in a binary task on images of zebrafish <sep> It is important to note that researchers in the field of AI and deep learning are themselves aware of the fallacies of deep learning, and are striving everyday to overcome these themselves. The hype over deep learning has caused certain disdain among a section of the research community over the workings of deep neural networks. This is evident in this paper with the authors calling CNNs ""black box"" and the learnings of a neural network ""cheating"". Perhaps the authors are not aware that CNNs are hardly black boxes, their inner workings quite transparent in mathematical terms, which the submitted paper itself explores. Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the intention of the authors is to bring more relevance to the dangers of spurious correlations, especially when the applications are critical. I hope the community can work together in improving the state of the art while improving transparency and explainability. <sep> BACKGROUND: <sep> Hypothesis: Prey movements in zebrafish are characterized by specific motions, that are triggered by a specific pathway involving an area called AF7. <sep> Validation: Semmelhack et. al. (2014) removed the AF7 neuropil and observed that they failed to respond to prey stimuli <sep> AI: the prey stimuli was a characteristic movement that an SVM was trained to detect. <sep> Good to know that the authors will share their code. <sep> It is not explained why the authors chose to pretrain on ImageNet, since ImageNet does not have any image classes that are comparable to the dataset the authors use. They then proceed to do a hyperparameter sweep to fine-tune on their dataset. <sep> It is also not explained why they chose to average outputs of 500 output nodes to get two outputs, instead of simply replacing the last layer with a 2-neuron layer and finetune, as is general practice. <sep> The level of detail in the training procedure is very helpful to reproduce the setting as well as establish a reference for any future work in this direction. This itself is a notable achievement and a good use of significant research time. Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN. The conclusion has many repeated points from previous sections, but it is a good summary. <sep> Given their premise about explainability in machine learning, perhaps more significance was to be given to DTD, and other methods also performed to check if the results coincide with those from DTD.","This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts. The supervised learning task is zebrafish bout classification which is relevant for biological experiments. The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition. This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a ""clever Hans"" mechanism. By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier. More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification. <sep> The reviewers found the paper to be well-written and the experiments to be well-designed. The reviewers suggested a some changes to the phrasing in the document, which the authors adopted. In response to the reviewers, the authors also clarified their use of ImageNet for pre-training and examined alternative approaches for building saliency maps. <sep> This paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice."
"The authors present a new first order optimization method that adds a corrective term to Nesterov SGD. They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. In the full-batch (non deterministic) setting, their algorithm boils down to the classical formulation of Nesterov GD. Their approach is justified by a well conducted theoretical analysis and some empirical work on toy datasets. <sep> Positive points: <sep> - The approach is elegant and thoroughly justified. The convergence to Nesterov GD when the batch size increase is comforting. <sep> - The empirical evaluation, even if it is still preliminary and larger scale experiments will have to be conducted before the method could be widely adopted, are suitable and convincing. <sep> - Some interesting observations regarding the convergence regimes (in respect to the batch size) are made. It would have been interesting to see how the results from fig3 generalize to the non convex problems considered in the paper. <sep> Possible improvements: <sep> - In H2, it is mentioned that the algorithm is restarted (the momentum is reset) when the learning rate is annealed. Was this also done for SGD+nesterov? Also, I think it is an important implementation detail that should be mentioned outside of the appendix <sep> - Adam didn't get the same hyper-parameter tuning as MaSS did. It is a bit disappointing, as I think the superior performance (in generalization) of non-adaptive methods would still hold and the experiment would have been more convincing. Rate of convergence is also not reported for Adam in fig 5. <sep> I think this is definitely a good paper that should be accepted. I'm looking forward to see how it performs on non-toy models and if the community adopt it.","The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over-parametrized settings. Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood. This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters."
"Summary <sep> This paper studied the expressive power of graph NNs, specifically, their universality and limitations under the non-anonymous setting, via the theory of distributed computations. For the universality, it proved the Turing completeness of graph NNs if messaging and aggregation functions are sufficiently strong. For the limitation, it characterized the lower bound of width for solving graph-theoretic tasks (such as subgraph detection, subgraph verification, approximate, and exact optimization problems) using graph NNs. The key idea is to reduce the computation model of graph NNs to LOCAL (for Turing completeness) or CONGEST (for limitations), which are well-studied in the literature of distributed computations and use the known results for these models. <sep> Decision <sep> This paper gave us a new approach to analyzing the expressive power of graph NNs. Not only does this paper give new theoretical results, but also it opens the door to a new research direction by bridging the theories of graph NNs and distributed computations. However, I cannot confirm the correctness of the proof of Theorem 3.1 (see Suggestions section). For now, I am tending to accept the paper. But I want to determine the final decision after I am certain that the proof of the theorem is correct. <sep> We can roughly divide existing approaches for studying the expressive power of graph NNs into two. One is to compare the power of discriminating non-isomorphic graph pairs with isomorphism tests such as the WL isomorphism test (Xu et al., 2019). The other one is to theoretically justify the oversmoothing phenomena (Li et al., 2018). The proof techniques the authors used are different from both of the two. It related a graph NN to the computational models LOCAL and CONGEST, and enabled to incorporate the theory of distributed computations. By doing so, the authors successfully derived many lower bounds in a systematic way, proving the effectiveness of their strategy. I think we can expect that a more refined analysis inspired by this approach will appear in the future. <sep> Regarding the Experience Assessment: I have published several papers in graph NNs (4). But I do not know much about the area of the theory of distributed algorithms (1--3). <sep> Suggestions <sep> - Section 3.2 <sep> - Theorem 3.1 proves the equivalence of GNN_n and LOCAL. However, the definition of equivalence is missing. Please write it in the main part, since this theorem is the key result of this paper. <sep> - I could not find any reference for the Turing completeness of the LOCAL model. Could you add the reference for it? <sep> - The description of the CONGEST model is only available in the appendix informally (Appendix B.3). Could you write it in the main part? <sep> - The authors emphasized the importance of the universality and limitation results in the introduction and paragraph after Corollary 3.1. In my opinion, the importance of such tasks is in machine learning community (Cybenko's paper on the universality of MLPs (Cybenko, 1989) is one of the most cited papers in the community). Rather, I think many graph NN researchers who are expected to read this paper are not familiar with the theory of distributed computations. Therefore, I would recommend to use page resources to explain the basic concepts of the distributed computation theory. <sep> Questions <sep> - Is there any existing work which tries to solve the graph theoretical tasks using graph NNs? If there is, can the theorems in this paper give explanations for the results? <sep> [Cybenko, 1989] Cybenko, George. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems 2.4 (1989): 303-314. <sep> [Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelli- gence, pp. 3538–3545, 2018. <sep> [Xu et al., 2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.","This paper provides a theoretical background for the expressive power of graph convolutional networks. The results are obviously useful, and the discussion went in the positive way. All reviewers recommend accepting, and I am with them."
"This work proposes to leverage a pre-trained semantic segmentation network to learn semantically adaptive filters for self-supervised monocular depth estimation. Additionally, a simple two-stage training heuristic is proposed to improve depth estimation performance for dynamic objects that move in a way that induces small apparent motion and thus are projected to infinite depth values when used in an SfM-based supervision framework. Experimental results are shown on the KITTI benchmark, where the approach improves upon the state-of-the-art. <sep> Overview: <sep> + Good results <sep> + Doesn't require semantic segmentation ground truth in the monodepth training set <sep> - Not clear if semantic segmentation is needed <sep> - Specific to street scenes <sep> - Experiments only on KITTI <sep> The qualitative results look great and the experiments show that semantic guidance improves quantitative performance by a non-trivial factor. The qualitative results suggest that the results produced with semantic guidance are sharper and more detailed. However, it is not clear that using features from a pre-trained semantic segmentation network is necessary. The proposed technical approach is to use the pixel-adaptive convolutions by Su et. al. to learn content-adaptive filters that are conditioned on the features of the pre-trained semantic segmentation network. These filters could in principle be directly learned from the input images, without needing to first train a semantic segmentation network. The original work by Su et. al. achieved higher detail compared to their baseline by just training the guidance network jointly.  Alternatively, the guidance network could in principle be pre-trained for any other task. The main advantage of the proposed scheme is that the guidance path doesn't need to be trained together with the depth network. On the other hand, unless shown otherwise, we have to assume that the network needs to be pre-trained on some data that is sufficiently close to the indented application domain. This would limit the approach to situations where a reasonable pre-trained semantic segmentation network is available. <sep> The proposed heuristic to filter some dynamic objects is very specific to street scenes and to some degree even to the KITTI dataset. It requires a dominant ground plane and is only able to detect a small subset of dynamic motion (e.g. apparent motion close to zero and object below the horizon). It is also not clear what the actual impact of this procedure is. Section 5.4.2 mentions that Abs. Rel decreases from 0.121 to 0.119, but it is not clear to what this needs to be compared to as there is no baseline in any of the other tables with an Abs. Rel of 0.121. Additionally, while the authors call this a minor decrease, the order of magnitude is comparable to the decrease in error that this method shows over the state-of-the-art (which the authors call statistically significant) and also over the baselines (c.f. Table 2). Can the authors clarify this? <sep> Related to being specific to street scenes: The paper shows experiments only on the KITTI dataset. The apparent requirement to have a reasonable semantic segmentation model available, make it important to evaluate also in other settings (for example on an indoor dataset like NYU) to show that the approach works beyond street scenes (which is one of the in practice not so interesting settings for monocular depth estimation since it is rather easy to just equip cars with additional cameras to solve the depth estimation problem). <sep> Need for a reasonable segmentation model: It is not clear in how far the quality of the segmentation network impacts the quality for the depth estimation task. What about the domain shift where the segmentation model doesn't do so well? Even if the segmentation result is not used directly, the features will still shift. How much would depth performance suffer? <sep> Summary: <sep> While the results look good on a single dataset, I have doubts both about the generality of the proposed approach as well as the need for the specific technical contribution. <sep> === Post rebuttal update === <sep> The authors have addressed many of my initial concerns and provided valuable additional experimental evaluations. While I'd like to upgrade my recommendation to weak accept, I strongly encourage the authors to provide additional experiments on different datasets (at least NYU).","The paper proposes a using pixel-adaptive convolutions to leverage semantic labels in self-supervised monocular depth estimation. Although there were initial concerns of the reviewers regarding the technical details and limited experiments, the authors responded reasonably to the issues raised by the reviewers. Reviewer2, who gave a weak reject rating, did not provide any answer to the authors comments. We do not see any major flaws to reject this paper."
"This paper studies the emergence of compositional language in neural agents. They propose an iterated learning method that consists of three phases: a supervised learning phase for a randomly-initialized speaker and listener, a self-play phase (where both agents are updated together), and a phase where a new dataset is created based on the current speaker's language. This dataset is then passed on to the next 'generation' of speaker and listener. The paper finds that this procedure, with the right hyperparameters, leads to the emergence of more compositional languages in a simple symbolic referential game. <sep> The question of how to emerge a compositional language is indeed interesting. This paper does a good job of conducting a careful set of ablations and analyzing the results. In my mind, the main scientific contribution of this work is the empirical verification of the principle 'compositional languages are easier to learn'. While this principle is intuitive, it's good to see it confirmed via experiments. The paper's description of the 'interval of advantage' --- the range of updates where a compositional language performs better on the task than a non-compositional language --- is insightful to me. <sep> I do have concerns for this paper around utility and novelty. As the paper mentions, it has already been shown that iterated learning procedures give rise to more compositional languages in non-neural models. While there are some things to consider in adapting this to neural networks, to my eye they seem rather straightforward (i.e. tuning the number of updates of the speaker and listener, the values I_a and I_b), contrary to the paper's assertion.  From a utility perspective, the paper doesn't go into how this might be practically applied in general to train neural agents to learn compositional languages in more complex environments (where they might be simultaneously speakers and listeners), as they stick to a very simple symbolic referential game. The main contribution of this paper is really: ""studying how neural networks behave when trained in an iterated learning setting in a simple referential game"". I think this is a nice contribution, but the main question for me is whether this is enough for an *CONF* acceptance. <sep> My other concern is around the length of the paper. In my opinion, while the paper is well-written, it's quite bloated and there is a lot of repetition. I think the paper could easily be condensed to 8 pages and retain the same information. Alternatively, some of the graphs in the Appendix (which are quite nice) could be added to the main paper to give more insight about how neural networks behave in this iterated learning procedure. <sep> Finally, the paper shows that compositional languages generalize better to the held-out validation set. While this is also an intuitive result, it's nice to have in the paper. I'd encourage the authors to remove the 'zero-shot' terminology though (which usually refers to predictions on new samples outside of the training distribution), and just stick to what is actually being shown, which is improved generalization. <sep> Overall, I like the paper, but due to the concerns mentioned above I think it's borderline, with a slight lean towards rejection.","This paper examines the correspondence between topological similarity of languages (correlation between the message space and object space) and ability to learn quickly in a situation of emergent communication between agents. <sep> While this paper is not without issues, it does seem to present a nice contribution that all of the reviewers appreciated to some extent. I think it will spark further discussions in this area, and thus can recommend it for acceptance."
"# 1. Summary <sep> The paper introduces a pre-training procedure for visual-linguistic representations. The model is an extension of BERT (with transformer backbone) to deal with visual input. Images are encoded using object detectors which regions are masked at pixel level. Experiments show state-of-the-art results on different downstream tasks. <sep> Strengths of the paper: <sep> * State-of-the-art results on 3 vision-language tasks <sep> The weak reject decision was mainly guided by the following two weaknesses of the paper: <sep> * Clarity of the paper needs to be improved to make the readers understanding the details of the model (see point 2 below) <sep> * Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below) <sep> # 2. Clarity <sep> The paper reads quite well, although some points need to be improved: <sep> * How were words split in sub-words (Sec 3.2)? <sep> * ""For each input element, its embedding feature is the summation of four types of embedding, ..."": it is not clear how you sum embeddings. E.g., token embedding has 30k dimensions while image one has 2048 dimensions. <sep> * ""It is attached to each of the input elements, which is the output of a fully connected layer taking the concatenation of visual appearance feature and visual geometry embedding as input"" -> this is not clear; what output are we talking about? What is the geometry embedding? I suggest to describe the two features first and then say at the end of the paragraph that the representation is the concatenation. <sep> * ""For the non-visual elements, the corresponding visual appearance features are of features extracted on the whole input image"" -> what is the intuition of having the full image here? Some terms do not need to have an image associated (e.g., verbs or articles). Do you take care somehow of that? <sep> * Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked? To my understanding the answer is no: what's the intuition of this? <sep> * Segment embedding: is this important? This should be easy to show with an experiment in the ablation study of Table 4? <sep> * It seems that there is a semantic asymmetry of input to the loss during training when considering only the text information (bookscorpus) and the image-text information (conceptual captions): how is training coping with this? Doesn't it make more sense to have 2 pre-training phases: first on text information only and then on image-text information? <sep> # 3. Novelty and Motivation <sep> The novelty of the paper is quite limited. It strongly relies on transformer networks and then recent success of BERT in the NLP domain. The proposal is an extension of these two ideas to visual domain. <sep> Moreover, there is a body of concurrent work that is very similar to the proposed idea with slight differences (ViLBERT, VisualBERT, LXBERT, UNITER, B2T2), i.e., using transformers with masking operation on the RoIs. It is not clear what is the intuition related to the differences between the methods, i.e. <sep> * Why one is better than the other; why should someone prefer this pre-training technique wrt others? <sep> * Why a unified network (this work) is preferred wrt a two-stream one (ViLBERT, LXMERT)? <sep> It seems that everything heavily depends on the experiments and empirical results obtained by trying many variants during the prototyping phase. It is missing a bit of understanding and intuition on the reasons why this technique should be used. <sep> # 4. Experimentation <sep> Experiments are the strength of the paper showing state-of-the-art results on 3 vision-language tasks. Some additional analysis is missing: <sep> * If masking is conducted on the raw pixel, this makes training much slower since you need to perform inference many times. What is the impact in terms of accuracy? Did you carried out an experiment showing that it is better to mask raw pixels instead of conv maps? <sep> * How long is the model trained for? <sep> * What is the performance/accuracy on the pre-training tasks? <sep> * How important is the segment embedding? <sep> * Footnote 2 should be in the main text (Sec 4.1). It is too hidden, but very important to let the reader knowing about it.",The paper proposed a new pretrained language model which can take visual information into the embeddings. Experiments showed state-of-the-art results on three downstream tasks. The paper is well written and detailed comparisons with related work are given. There are some concerns about the clarity and novelty raised by the reviewers which is answered in details and I think the paper is acceptable.
"This paper addresses the problem of unsupervised model selection for disentangled representation learning. Based on the understanding of ""why VAEs disentangle"" [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. Experimental results clearly show that UDR is a good approach for hyperparameter/model selection. <sep> Overall, I think a reliable metric for model selection/evaluation is needed for the VAE-based disentangled representation learning. According to comprehensive experimental studies performed in this paper, UDR seems to be a potentially good choice. <sep> However, I am not sure if very good disentangled representations must benefit (general) subsequent tasks, though the authors provide experimental evidence on fairness classification and data efficiency tasks. Actually, the data generation process in the real-world may consist of different generative factors that are not independent of each other. Though good disentangled representation provides good interpretability, it needs not to be better than entangled representation for concrete tasks. Specifically, for concrete supervised classification tasks, VAE with beta smaller than 1 (not towards disentanglement) might be the best (Alexander A. Alemi et al. 2017, Deep VIB). <sep> Another concern is about the choice of some key ""hyperparameters"". <sep> For the KL divergence threshold in equation 3, you set it to be 0.01. It looks like the choice would control how much the UDR favors a ""sparse representation map"". The larger the value, the few ""informative dimensions"" would be considered. <sep> In supplementary material, you say that ""uninformative latents typically have KL<<0.01 while informative latents have KL >>0.01"". Is this judgment based on ""qualitative feeling""? For me, as you are contributing a ``quantitative measurement"", it is interesting and important to see how this threshold would generally affect UDR's behavior in one (or more) datasets you have tried. <sep> Another hyperparameter I cared is P (number of models for pairwise comparison). In the paper, you validate the effect of P in the range [5,45]. How would P smaller than 5 affect UDR? According to Table 1, if I was using UDR, I'd rather using P>=20 (or at least 10) rather than 5. <sep> Also, it seems to me P would grow up due to the size of factors that generate the data. Thus, I also have a little concern about the computation cost of the proposed metric (as also mentioned by the authors). <sep> Others concerns: <sep> -- As a heavy experimental paper, most experimental results are in supplementary material, while the authors spent a lot of time in the main text explaining the conclusions found in other papers. <sep> -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse).","The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations. They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks. After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed. There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly. All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem."
"This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks. The first modification enforces the distribution of predicted labels to match the distribution of labeled data. The second modification is adding a learned data augmentation strategy, and adapting the method to work with strong data augmentation. The final method is titled ReMixMatch, and improves significantly over MixMatch, especially in low-data regime. <sep> The main contribution of the paper is really strong empirical results. The method achieves state of the art results or close to that on multiple benchmarks, with especially large gains in settings with very scarce labeled data, like 40 labels on CIFAR-10. <sep> Another important contribution is the learned data augmentation strategy, which as far as I understand is novel and overcomes some of the limitations of  existing learned data augmentation techniques. However, the explanation of the strategy wasn't very clear for me, and the authors didn't frame it as a major contribution. <sep> The main drawback of the paper is that it seems to be more engineering-focused, and doesn't provide much insight into semi-supervised learning. The paper can be summarized as adding two modifications to mix-match, and getting better results. The final method becomes fairly involved. Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3). <sep> For the reasons above, I think the paper is borderline, but I am currently voting for acceptance based on the strong empirical performance. At the same time, I think the paper can be made stronger and more interesting to read, if the authors added some experiments aimed at understanding the proposed modifications. <sep> One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part. For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4. It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance. <sep> For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives. Just analyzing the learned data augmentation in different settings and adding more intuition for what happens would make the paper more insightful and interesting to read. <sep> On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it's being reported as 6.24. What is the reason for the difference? Another paper, [2], reports very competitive results on CIFAR-10 for 4k labels. I would recommend discussing these results briefly in the paper. At the same time the empirical performance of ReMixMatch is really impressive, and I don't think the results in [1] and [2] affect their significance. <sep> [1] MixMatch: A Holistic Approach to Semi-Supervised Learning <sep> David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel <sep> [2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average <sep> Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson","This works improves the MixMatch semi-supervised algorithm along the two directions of distribution alignment and augmentation anchoring, which together make the approach more data-efficient than prior work. <sep> All reviewers agree that the impressive empirical results in the paper are its main strength, but express concern that the method is overly complicated and hacking together many known pieces, as well as doubt as to the extent of the contribution of the augmentation method itself, with requests for better augmentation controls. <sep> While some of these concerns have not been addressed by authors in their response, the strength of empirical results seems enough to justify an acceptance recommendation."
"Overview: <sep> This paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. During training, only a few training items are annotated for these types of properties; for items where these labels are not given, the variables are marginalised out. TTS experiments are performed and the approach is evaluated objectively by training classifiers on top of the synthesised speech and subjectively in terms of mean opinion score. <sep> I should note that, although I am a speech researchers, I am not a TTS expert, and my review can be weighed accordingly. <sep> Strengths: <sep> The proposed approach is interesting. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output. I agree that this is a principled way to impart interpretability on latent spaces which are obtained through unsupervised modelling aiming to disentangle properties like affect and speaking rate. <sep> Weaknesses: <sep> This work misses some essential baselines, specifically a baseline that only makes use of the (small number of) labelled instances. In the experiments, the best performance is achieved when gamma is set very high, which (I think) correspond to the purely supervised case (I might be wrong). Nevertheless, I think a model that uses only the small amount of labelled data (i.e. without semi-supervised learning incorporating unlabelled data) should also be considered. <sep> As a minor weakness, the evaluation seems lacking in that human evaluations are only performed on the audio quality, not any of the target properties that are being changed. For affect specifically, it would be helpful to know whether the changes can be perceived by humans. As a second minor weakness, some aspects of the paper's presentation can be improved (see below). <sep> Overall assessment: <sep> The paper currently does not contain some very relevant baselines, and I therefore assign a ""weak reject"". <sep> Questions, suggestions, typos, grammar and style: <sep> - p. 1: ""control high level attributes *of of* speech"" <sep> - p. 2: It would be more helpful to state the absolute amount of labelled data (since 1% is somewhat meaningless). <sep> - p. 2: I am not a TTS expert, but I believe the last of your contributions have already been achieved in other work. <sep> - Figure 2: It would be helpful if these figures are vectorised. <sep> - p. 4: ""*where* summation would again ..."" <sep> - Figure 4: Is there a reason for the gamma=1000 experiment, which performs best in (a), not to be included in (b) to (d)? <sep> - Section 5: Table 1 is not references in the text. <sep> - Section 5.1: ""P(x|y,z_s,z_u)"" -> ""p(x|y,z_s,z_u)"" <sep> - In a number of places, I think the paper meant to cite [1] but instead cited the older Kingma & Welling (2013) paper; for instance before equation (6) (this additional loss did not appear in the original VAE paper). <sep> References: <sep> [1] https://arxiv.org/abs/1406.5298 <sep> Edit: Based on the author's response, I am changing my rating from a 'weak reject' to a 'weak accept'.","The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi-supervised way, with a small amount of labeled data with the variables of interest labeled. The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers."
"# UPDATE after rebuttal <sep> I have changed my score to 8 to reflect the clarifications in the new draft. <sep> Summary: <sep> This paper presents a family of architectural variants for the recurrent part of an RL controller. The primary claim is that simple components which compute elementwise sum/average/max over the activations seen over time are highly robust to noisy observations (as encountered with many RL environments), as detailed with various empirical and theoretical analyses. By themselves these aggregators are incapable of storing order dependent information, but by combining an LSTM with an aggregator, and pushing half the LSTM activations through the aggregator, and concatenating the the other half of the activations with the aggregator output, the resulting output contains order dependent and order independent content. <sep> The motivation is very clear (many of the most challenging modern video games used as RL environments clearly have noisy observations, and many timesteps for which no new useful information is observed) and the related work is comprehensive. An increase in training speed and stability, apparently without any major caveats, would be of great interest to any practitioner of Deep Reinforcement Learning. <sep> The experiments provided are good, and vary nicely between actual RL runs and theoretical analysis, all of which convinces me that this could well become a standard Deep RL component. I do have a range of questions & requests for clarification (see below) but I believe the experiments as presented, plus some additions before camera ready, will make for a good paper of wide interest. <sep> Desicion: Accept. I would give this a 7 if I was able to. The idea is very simple (indeed I find it slightly hard to believe no one has tried this before, but I don't know of any references myself) and the results, particularly Figure 4, are compelling. It's nice to see a very approachable more mathematical analysis as well, it would be good to see more papers proposing new neural components with this kind of rigour. I look forward to trying this approach myself, post publication. <sep> Discussion: <sep> AMRL-Avg coming out the best in Figure 8 makes a lot of sense to me, as I can see how average provides a stable signal of unordered information. One thing that really doesn't make sense to me is why Max and Sum would also be good - obviously their SNR / Jacobians are quite similar, but fundamentally there is a risk of huge values being built up in activations (especially with Sum), which at least have the potential to cause numerical instability, provide weird gradient magnitudes (which then mess up moment tracking optimizers like Adam, etc). Thre does not seem to be any mention of numerical stability, or whether any considerations ned to be taken for this. Maybe we could hope that 'well behaved' internal neron activations are zero mean, so the average aggregator will never get too big - but is this always the case, at all points in training, in every episode? I appreciate the straight through estimator might ameliorate this, but it is not made entirely clear to me in the text that this is the reason for using it. Addressing this point would increase the strength of the argument. <sep> Given that DNC was determined to be the strongest baseline in a few of the metrics, and that AMRL combines these aggregators with LSTM, an experiment that I'm surprised is missing would be to make AMRL containing a DNC instead of an LSTM. Is there a reason why this wasn't attempted? <sep> It would have been good to see a wider set of RL experiments - the Atari suite is well studied, easily available, and there are many open source implementations to which AMRL could easily be slotted into (eg IMPALA, openai baselines, etc). <sep> The action space for Minecraft is not spelled out clearly - at first I assumed this was the recent project Malmo release, which I assume to have continuous actions (or at least, the ability to move in directions that are not compass points), but while Malmo is mentioned in the paper, the appendix implies that the action space is (north, east, west) etc. I'm aware that there is precedent in the literature (Oh et al 2016) for 'gridworld minecraft', but I think it would improve the paper to at least acknowledge this in the main text, as I feel even most researchers in 2019 would read the text and assume the game to be analogous to VizDoom / DeepMind Lab, when it really isn't. Note that most likely the proposed method would give an even bigger boost with ""real"" minecraft, as there are even more non-informative observations between the interesting ones, and furthermore I think the environment choice made in this paper is fine, as it still demonstrates the point. A single additional sentence should suffice. <sep> Minor points: <sep> NTM / DNC were not ""designed with RL in mind"" per se, the original NTM paper had no RL and the DNC paper contained both supervised and RL results. Potentially the statement at the bottom of page 1 was supposed to refer mainly to stacked LSTMs - either way I feel it would be better to slightly soften the statement to ""...but also for stacked LSTMs and DNCs (cite), which have been widely applied in RL."" <sep> In the RL problem definition, the observation function and the state transition function are stated as (presumably?) probability distributions with the range [0,1], but for continuous state / observation spaces it is entirely possible for the probability density at a point to exceed 1. <sep> Also in the RL section - the notation of τt∈Ωt should probably also contain the sequence of t−1 actions taken throughout the trajectory - this is made clear in the text, but not in the set notation. <sep> The square bracket ""slicing"" notation used for slicing and concatenating is neat, but even though I spend all day writing python it still took me a while to realise what this meant, as I haven't seen this used in typeset maths before. Introducing this notation (as well as the pipe for concatenation) at first usage would help avoid tripping up readers unnecessarily. <sep> Note that this pdf caused several office printers I tried it on to choke on later pages, and I get slow scrolling in Chrome on figures such as figure 11 - possibly the graphics are too high resolution (?) <sep> typos: A5, feed forward section: ""later"" should be ""layer"" I believe.","This paper introduces a way to augment memory in recurrent neural networks with order-independent aggregators. In noisy environments this results in an increase in training speed and stability. The reviewers considered this to be a strong paper with potential for impact, and were satisfied with the author response to their questions and concerns."
"The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required. <sep> Please see my detailed comments below. <sep> Positive points： <sep> 1. The proposed inference example weighing method yields promising results and does not require any re-training. <sep> 2. The combination of batch and group normalization makes it possible to train deep models with very small batch size. <sep> 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. <sep> Negative points: <sep> 1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, …, x_{B-1}? <sep> 2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods? <sep> 3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2. <sep> 4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions. <sep> Reference: ""Bag of tricks for image classification with convolutional neural networks."" CVPR, 2019. <sep> 5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. <sep> 6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case? <sep> 7. Some closely related work should be discussed in the paper, such as <sep> [1] ""Decorrelated Batch Normalization."" CVPR, 2018. <sep> [2] ""Double Forward Propagation for Memorized Batch Normalization."" AAAI, 2018. <sep> [3] ""Differentiable Dynamic Normalization for Learning Deep Representation."" ICML, 2019. <sep> [4] ""Iterative Normalization: Beyond Standardization towards Efficient Whitening."" CVPR, 2019. <sep> Minor issues: <sep> 1. In Section 1, the third contribution is not a complete sentence. <sep> 2. There are many typos in the paper. <sep> (1) In Section 2, ""Layer Normalization, which has found use in many natural language processing tasks."" Should ""which has found use"" be ""which has been used""? <sep> (2) In Section 3.1, ""Batch Normalization has a disparity in function between training inference"". ""between training inference"" should be ""between training and inference"". <sep> (3) In Section 3.1, ""we need only figure out …"" should be ""we only need to figure out …""","This paper proposes techniques to improve training with batch normalization. The paper establishes the benefits of these techniques experimentally using ablation studies. The reviewers found the results to be promising and of interest to the community. However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough. We encourage the authors to properly address these issues before the camera ready."
"This paper introduces Precision Gating, a novel mechanism to quantize neural network activations to reduce the average bitwidth, resulting in networks with fewer bitwise operations. The idea is to have a learnable threshold Delta that determines if an output activation should be computed in high or low precision, determined by the most significant bits of the value. Assuming that high activations are more important, these are computed at higher precision. <sep> I agree that the following three key contributions listed in the paper are (slightly re-formulated): <sep> 1. Introducing Precision Gating (PG), the first end-to-end trainable method that enables dual-precision execution of DNNs and is applicable to a wide variety of network architectures. <sep> 2. Precision gating enables DNN computation with a better average bitwidth to accuracy tradeoff than other state-ofthe- <sep> art quantization methods. Combined with its lightweight gating logic, PG demonstrates the potential to reduce DNN execution costs in both commodity and dedicated hardware. <sep> 3. Unlike prior works that focus only on inference, precision gating achieves the same sparsity during back-propagation as forward propagation, which reduces the computational cost for both passes. <sep> These contributions are novel and experimental evidence is provided for multiple networks and datasets. The paper is well-written and provides insightful figures to showcase the strengths of the present method. Related work is adequately cited. The paper does not contain much theory, but wherever possible equations are provided to illustrate in detail how the method works. <sep> Experimental results are shown for the datasets CIFAR-10 with ResNet-18 and ShiftNet-20, and ImageNet with ShuffleNet V2 0.5x. On both datasets, PG outperforms uniform quantization, PACT, Fix-Threshold and SeerNet in terms of top-1 accuracy and average bitwidth. <sep> What I am missing is information about the variability of results, since there are no error bars. Are the results averaged over multiple trials (if yes how many?), and is there a difference in variance between the methods? I realize that adding standard deviations to all results in the tables might be infeasible, but a qualitative statement would be interesting. In particular, the random initialization of the hb bits could play a bigger role than lb bits. <sep> The two variants of PG, with and without sparse backpropagation are also investigated, showing that sparse backpropagation leads to more sparsity. To show that the resulting lower average bitwidth gained with PG leads to increased performance, the authors implement it in Python (running on CPU) and measure the wall clock time to execute the ResNet-18 model. Speedups >1 are shown for every layer when using PG. Evidence from other papers is cited to argue that similar speedups are expected on GPUs. <sep> Even though at the moment it is unclear to me how statistically significant the results are, and I strongly recommend commenting on this in the paper, I think the idea of PG and the demonstrated benefits make the paper interesting enough to be accepted at *CONF*. <sep> I also have a few questions that I could not get completely from the paper: <sep> 1. I am a bit confused by what you call features. Fig. 2 shows by example how the method works for an input I. Is I, a single number, i.e. a single entry of your input vector, or do you mean the complete input vector? <sep> 2. Could you give a bit more insight, how you tuned your hyperparameters, especially δ and σ? <sep> 3. What exactly does e.g. δ=−1 mean? The network ideally should compute at high precision, when the result when only considering the most significant bits is above -1? <sep> From a hardware point of view, the paper focuses on GPU implementations. I would have hoped for a discussion of suitable custom hardware that could support PG most efficiently. <sep> Minor comments that I would be interested in but did not influence my score <sep> - It seems to me that on the top-left image of Fig. 3, one blue circle (the second largest) is too much? First part shows 8 dots, middle and right only seven? <sep> - Can you please cite a source that DNN activations have outliers (Sec. 3.4)? <sep> - You could also define e.g. one δ and Δ per layer, couldn't you? Would be interesting to see if  e.g. thinning out the precision over depth is possible / has advantages.","The submission proposes an approach to accelerate network training by modifying the precision of individual weights, allowing a substantial speed up without a decrease in model accuracy. The magnitude of the activations determines whether it will be computed at a high or low bitwidth. <sep> The reviewers agreed that the paper should be published given the strong results, though there were some salient concerns which the authors should address in their final revision, such as how the method could be implemented on GPU and what savings could be achieved. <sep> Recommendation is to accept."
"This paper proposes a combined architecture for image-text modeling. Though the proposed architecture is extremely detailed, the authors explain clearly the overarching concepts and methods used, within limited space. The experimental results are extremely strong, especially on sub-domains where conditional generative models have historically struggled such as images with angular, global features - often mechanical or human constructed objects. ""Computers"" and ""cars"" images in Figure 2 show this quite clearly. The model also functions for tagging and annotating images - performing well compared to models designed *only* for this task. <sep> The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. Combined with the included code release, this paper should be of interest to many. <sep> My chief criticisms come for the density of the paper - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. <sep> As usual, more experiments are always welcome, and given the strengths of GAN based generators for faces a text based facial image generator could have been a great addition. The existing experiments are more than sufficient for proof-of-concept though. <sep> Finally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. Being able to read the code online, without downloading and opening locally can be nice, along with other benefits from open source release. However the authors should release the code however they see fit, this is more of a personal preference on the part of this reviewer. <sep> To improve my score, the primary changes would be more editing and re-writing, focused on clarity and brevity of the text in the core paper.","This paper proposes a bidirectional joint image-text model using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE-GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance."
"This paper proposes a knowledge graph advantage actor critic (KG-A2C) model to allow an agent to do reinforcement learning in the interactive fiction game. Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. Experiments on Zork1 game environment are done to verify the effectiveness of the proposed method. <sep> Overall, this paper presents a novel contribution to reinforcement learning with augmented memory/world-state. However, I do have a few concerns regarding the baselines and other details. Given these clarifications and or comparisons in an author response, I would be willing to increase the score. <sep> Pros: <sep> 1, I like the idea of constructing the knowledge graph as the agent roll out. I think it is a better way to construct a structural representation of the world rather than assuming the agent gonna learn everything via single hidden state vector. It also permits more explainable policy in the future. Authors do make good progress along this line. <sep> 2, The paper is well written and the design of the proposed new model seems technically reasonable. <sep> Cons & Questions: <sep> 1, The main concern I had is regarding to the baseline. I think it is more convincing to have a baseline which leverages the same entity extraction and template-action space. In particular, it should have the same model architecture except that it uses maybe a LSTM to decode the action rather than a GAT applied on knowledge graph. Note that the baseline I am referring to is different from the LSTM-A2C baseline reported in the paper as: (1) with entity extraction, although you may not get a graph mask, but you can still have a object-mask which also reduces the action space; (2) it is not clear to me that LSTM-A2C uses the same template-action mechanism as the KG-A2C, e.g., the valid action construction procedure described in section 4.1. Without such a baseline, it is hard to fully judge how helpful the knowledge graph is. <sep> 2, How do you test the generalization of the proposed models? In particular, do you use different maps during training and testing? If the model is merely trained on one map as shown in figure 5, it may just memorizes it in the knowledge graph and overfit to this map. <sep> 3, The details of the interaction fiction problem setup are sparse. It would be very helpful to explain what exactly the observations are in the example of Figure 2. For example, what are the game description, game feedback are in this case? <sep> 4, In the caption of figure 1, ""Solid lines represent gradient flow"" is misleading. If I understood correctly, solid lines refer to the computation flow which has gradient back-propagated in the backward pass. <sep> 5, Could you explain why KG-A2C converge slower than DRRN in figure 3? <sep> 6, Do you think having a fully differentiable mechanism of building knowledge graph would help or not? Why? <sep> ====================================================================================================== <sep> Most of my concerns are addressed by the authors' response. I increased my score.","This paper applies reinforcement learning to text adventure games by using knowledge graphs to constrain the action space. This is an exciting problem with relatively little work performed on it. Reviews agree that this is an interesting paper, well written, with good results. There are some concerns about novelty but general agreement that the paper should be accepted. I therefore recommend acceptance."
"This paper considers learning low-dimensional representations from high-dimensional observations for control purposes. The authors extend the E2C framework by introducing the new PCC-Loss function. This new loss function aims to reflect the prediction in the observation space, the consistency between latent and observation dynamics, and the low curvature in the latent dynamics. The low curvature term is used to bias the latent dynamics towards models that can be better approximated as locally linear models. The authors provide theory (error bounds) to justify their proposed PCC-Loss function. Then variational PCC is developed to make the algorithm tractable. The proposed method is evaluated in 5 different simulated tasks and compared with the original E2C method and the RCE method. <sep> The paper is well-written. <sep> Pros: <sep> - The idea in this paper is quite original. The three principles used to formulate the loss function provide some new insights. <sep> - The authors have proposed a theory to justify the use of their loss function. The technical quality of this part seems solid. <sep> - Simulations have been used to show that the proposed PCC method outperforms E2C and RCE. <sep> -The paper is well written. <sep> Cons: <sep> - The tasks in this paper are not that complicated. It is unclear whether the proposed method outperforms other model-based RL methods such as Solar and DSAE for practical robotic applications. More comparisons are needed. <sep> - It is also not that clear why one wants to SOC3 to be close to SOC1 in the first place. It seems the true optimization problem should be posed on the space of the original state s. SOC1 is just a surrogate problem for the original problem. <sep> - There seems to be a gap between the proposed theory  and the algorithm implementation. This makes the theory part less useful. <sep> Overall, I think the idea in this paper is interesting. The authors have made a serious effort in coming up principles for model-based RL control. But at this moment it is not that convincing the proposed method will be the best model-based RL method for practical robotic applications. If the authors can address my comments, I will be willing to increase my score. <sep> Minor Comments: <sep> - It seems that for the task the authors have tested their method, it is not that difficult to directly estimate the state. Am I correct here? Can the authors make a comment on this? How to compare their approach and a more direct control approach using estimation of state s? <sep> - I have never seen the curvature principle in any control papers. Any control reference on why this is a good principle? It seems that the linearization works well when the control inputs are around the reference points. Does the curvature really matter that much for ILQR to work? <sep> - How to justify the Markovian assumption on x? Just by observation or there is a more principle way to test this assumption on the buffered images? <sep> ==================================================== <sep> Post-Rebuttal: <sep> After reading the authors' response, I am changing my score to weak accept. Lemma 4 is nice. I have not seen anything similar to this in the controls literature before. The authors have addressed most of my concerns. I still have a few comments for preparing the final version of this paper. <sep> 1. I still don't see why SOC1 is the ""original problem."" Yes, it is assumed that the true state cannot be directly observed. But if the observations are Markov eventually, then some estimated version of the states can be obtained, right? I think treating SOC1 as the original problem is one possible way of doing things and clearly the authors have built a principled framework for doing things in this way. But treating SOC1 as the original problem seems not the only way of doing things. I hope the authors can clarify this and do not oversell the proposed approach. <sep> 2. I think it is still worth comparing SOLAR and PCC empirically. This will help the readers to choose algorithms when they need. <sep> 3. The comment on the verification of the Markov assumption is hand-waving.  The authors said ""A simple test would be to see if a control algorithm with the Markovian assumption works well with our representation or not."" Does this mean that the users will not be able to verify this assumption before using the proposed approach to obtain controllers? It will be helpful if the authors can explain this step for one specific example in details.","This paper studies optimal control with low-dimensional representation. The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions."
"I. Summary of the Paper <sep> This paper studies robustness to adversarial examples from the perspective of having 'topology-aware' generative models. Next to some experiments on data sets with a manifold structure, the main contribution of the paper is a tandem of theorems that state the conditions under which models can recover the topology---or the number of connected components---of a data set correctly, <sep> thereby making them more robust to adversarial examples. <sep> II. Summary of the Review <sep> This paper provides a novel perspective on adversarial examples through the lens of Riemannian Geometry and topology. I appreciate novel research that employs topology-based methods, but at present, I cannot fully endorse accepting the paper. Specifically, I see the following issues: <sep> - Missing clarity: while the appendix is very comprehensive, which <sep> I appreciate, the main text could be improved; some statements appear redundant, while others need to be re-formulated to build intuition <sep> - This paper appears to span both theory and applications. I appreciate this attempt, knowing full well that this is no easy feat to accomplish. However, the main theoretical result on the number of connected components only applies to mixtures of Gaussian distributions, but the purported scope of the paper is the analysis of manifold-based defences in general. I would expect a more in-depth discussion of the limitations of the theorem. Can we expect this to generalise? Moreover, 'topology' is reduced to 'connected components' <sep> in this paper. While this is perfectly adequate in the sense of connected components being a particular concept from topology, I would expect this to be clarified much earlier in the paper. In addition, <sep> connected components are a very basic and coarse concept, so I am wondering to what extent it is sufficient to describe models purely based on that information. <sep> - As a sort of corollary to the previous point, the experiments could be improved. I like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; I would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more in-depth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large? <sep> Is there a dependency between the number of connected components and vulnerabilities---are models with a very small number of connected components more vulnerable than models with a very larger number of connected components? The present experimental section is lacking this depth. <sep> - The same statements apply to the INC example. I found this super instructive, but it is only _one_ case on _one_ manifold---I would like to see more details here; maybe some of the experiments in the appendix could be moved to the front? I have some suggestions for shortening the paper (see below). <sep> Despite these issues, I think this paper can be a strong contribution if properly revised; since I am positive that at least some of these suggestions could be performed within a revision cycle, I want to be upfront and state that I will definitely consider raising my score, <sep> provided that my concerns are addressed appropriately! <sep> I have to state that I am _not_ an expert in adversarial examples, but an expert in topology-based methods; I consider this paper to belong to the latter field given its theoretical contributions about 'recovering' <sep> the correct density distribution. <sep> III. Clarity <sep> The paper provides an extensive background to Riemannian geometry, which <sep> I appreciated as a reference. Nevertheless, there are improvements to the main text that I would suggest: <sep> - Please consider changing the title to 'On the need...' <sep> - The manifold assumption is that data lie _on_ a manifold or _close to_ <sep> a manifold whose intrinsic dimension is much lower than that of the ambient space. This is not stated in sufficient precision in the paper; please correct the usage on p. 1 and p. 2 <sep> - In terms of notation, why use pM to denote the density on the whole of \\mathdsRn? I would expect pM to refer to the density on M <sep> rather than the density of the whole space. <sep> - If M is a disjoint union of manifolds, please consider using a '\\cupdot' operator to make this more clear. <sep> - If the pairwise manifolds are disjoint, how can the resulting data distribution still contain any ambiguities? I find this hard to harmonise with the statement in Section 3.4 about the existence of a classifier that separates the manifolds. Please clarify the meaning behind the term 'ambiguities' here. <sep> - The '(R0)' requirement definition and the discussion in Section 3.2 <sep> strike me as needlessly complex. Would it be possible to shorten this or move some content to the appendix? I think it would be sufficient to have Eq. 1 and mention how it could be solved. <sep> - Section 3.3 could be shortened as well, if I am not mistaken; while it is good to know how such models look, the 'change of variable formula' <sep> is not used directly any more in the paper; I think it might be easier to write down a generic form of the density for each model. <sep> - The salient points of Section 3.4 seem to be the projection point; <sep> maybe this could also be shortened somewhat in the interest of having more space for experiments. The relevant information of this section was to learn how projections work for different models, but it would be sufficient to keep Eq. 3 and discuss Eq. 4 in the appendix <sep> - The 'topological differences' mentioned in Section 4 could be clarified: the paper talks about differences in connected components. <sep> - The λ-density set appears to be a superlevel set, if I am not mistaken: the level set would be defined for a single threshold only, <sep> while this paper introduces the pre-image of an interval. <sep> - I would expect the pre-image to be defined as p−1, not p−1; <sep> the latter strikes me as somewhat non-standard usage <sep> - The statements preceding Definition 1 require some more intuition; <sep> what is the purpose of these assumptions? <sep> - The notation for the Euclidean balls should be briefly introduced before Definition 1. I recognise this as a standard notation, but since this is the first appearance of the symbol, it should be mentioned at least briefly. <sep> - Definition 1 could also be phrased more intuitively; additional sentence behind each definition would be useful, such as: <sep> δλ is the largest δ such that the full superlevel set is contained in a ball of radius δλ. <sep> Figure 1 is already helpful in that regard; it should ideally precede the definition and/or be made larger to be more illustrative <sep> - Maybe the results of Theorem 2 could already be stated earlier; it could probably be explained reasonably well when describing the number of connected components as a sort of 'baseline' topological complexity that a model has to satisfy. <sep> On a more abstract level, could it also be summarised as 'the inclusion of prior knowledge is a necessary condition for robustness'? <sep> - I would not state that the main goals of the experiments are to <sep> 'check the correctness of Theorem 2'---the proofs should be responsible for this! I would rather say that the main goals are to provide empirical evidence for the *relevance* or *applicability* of the Theorem. <sep> - The paragraph on 'Latent vector distributions' contains the most relevant information, viz. the knowledge of what the paper considers to be a 'topology-aware' and 'topology-ignorant' model; this should be highlighted more; maybe the figures could be extended to contain information about nX? <sep> Overall, I like the idea of having one overarching question in a paper that is subsequently answered or discussed under different aspects. I <sep> very much commend the authors of the paper for choosing this sort of writing style! <sep> IV. Experimental setup <sep> As mentioned above, the experiments require more depth. I would propose adding more repetitions of the training process for different data sets and analysing the impact of the 'parameters' used in the theorems. <sep> In particular, the 'INC' experiments show great promise for multiple repetitions. Why not choose more data sets and more staring positions and visualise the trajectories of _multiple_ draws, as shown for a single draw in Figure 4 a,b,c? <sep> Also, please consider moving the additional experiments from the appendix to the main paper. <sep> More compelling examples would also be helpful. Why not generate data sets consisting of more than two manifolds? At present, the largest issue I see in this section is that the conceptual 'leap of faith' <sep> between the theory and the applications is simply too large. Would it not be possible to perform the same experiments on a simple digits data set, say MNIST? <sep> V. Minor issues <sep> The paper is well-written overall. There are only a few typos and minor style issues that I would recommend fixing: <sep> - please check the usage of quotes; it should be ``pulled back'', not ''pulled back'' in LaTeX <sep> - please check the usage of citations; if '\\citet' is used, citations can be used as nouns directly (for example: 'in Pennec (1999)' instead of 'in (Pennec, 1999)'. <sep> - etc.. --> etc. <sep> - it approximates posterior distribution --> it approximates a posterior distribution <sep> - for compound distribution --> for a compound distribution <sep> - a relatively simpler distribution --> a simpler distribution <sep> - near manifold --> near a/the manifold <sep> - we simply the minimum --> we denote (?) the minimum <sep> - satisfies the followings --> satisfies the following properties <sep> - number of connected component --> number of connected components <sep> - Experimental result --> Experimental results <sep> VI. Update after the rebuttal <sep> The authors managed to address the majority of my comments. Overall, I still would like to see a more detailed/in-depth experimental setup, but I realise that this not directly possible within the timeframe allotted during the rebuttal period. I am thus raising my score.","This paper studies the role of topology in designing adversarial defenses. Specifically , the authors study defense strategies that rely on the assumption that data lies on a low-dimensional manifold, and show theoretical and empirical evidence that such defenses need to build a topological understanding of the data. <sep> Reviewers were initially positive, but had some concerns pertaining to clarity and limited experimental setup. After a productive rebuttal phase, now reviewers are mostly in favor of acceptance, thanks to the improved readibility and clarity. Despite the small-scale experimental validation, ultimately both reviewers and AC conclude this paper is worthy of publication."
"In this paper authors propose a novel idea (called Filter Summary, FS)  how to compress convolutional neural networks with 2D convolutions (kernels are 3D tensors, it is also applicable to the 1D convolutions). Compression of the convolution operation is done with weight sharing: unwrapped kernel (channel-major) is packed into 1D vector by having intersected segments (shared weights). <sep> The strong sides of the paper are the following: <sep> - Proposed algorithm for efficient computation of convolution operation with FS (less number of multiplication operations with comparison to the standard convolution). <sep> - Via experiments it is demonstrated that proposed approach provides compression of the model while having close to the baseline quality for image classification and object detection tasks and for small and large datasets. <sep> - Proposed method gives better compression factor and better quality than state-ot-the-art models for classification and object detection tasks. <sep> - It is experimentally showed that architecture search works for proposed convolution. <sep> There are still ways to improve the paper: <sep> - In the introduction section it is written that ""FS is quantization friendly"". I would say the main concern should be ""FS quantization achieves the same quality as original FS while having higher compression factor"". Standard convolutions are also quantization friendly, no? At the same time I don't see huge gain to do quantization of the model. Yes, to store it we need less memory, however, for the inference we still use float-point computation. How will this quantization be helpful for inference on CPU/GPU (memory/speed)? <sep> - Did you implement CUDA kernel to have efficient FS convolution (I suspect the answer yes)? It would be interesting to see the training time per epoch/forward/backward comparison for standard CNN and FS CNN as soon as you are using the same training process (for models you trained to have an idea for practical usage). Also it is interesting to see comparison of inference time on CPU/GPU. <sep> - For illustrations (figs 1, 2, 3) of input and filters packing it would be very helpful to mark where are channel and spatial dimensions on the figure for simpler understanding the packing. Typos for the figures: ""in Figure 2, two slices, marked in green and blue"" -> ""in Figure 2, two slices, marked in green and red""; ""Figure 3 by dashed lines in green and blue that cross A, where the dashed line in red"" -> ""Figure 3 by dashed lines in green and blue that cross A, where the dashed line in green"" (on Figure 3 there are blue and green lines only). <sep> - In the paragraph where the number of elements of matrix A is specified to be computed, it would be better to have detailed explanation why we skip C_in*S_1 elements. It is not so trivial when you read the paper the first time. <sep> - In section 2.3 for complexity of the three stages: seems the third stage doesn't have float-point computations (this is addition for S_2 locations in matrix A), however, the third stage is included into analysis of multiplication operations (this doesn't affect the final result of complexity) <sep> - For DFSNet are filters ordered for FS F (monotonically located)? Or now they can be located independently and starting point defined by its alpha? <sep> - Typo: page 7 ""in Table 9"" -> ""in Table 2"" <sep> - In Table 4 it would be better to have results for FSNet-1 too (without quantization). <sep> - In section 3.3 there is no details how the alpha parameter from DFSNet is used in DARTS. <sep> - In Tables 10 and 11 there are linear quantization, what does it mean? what is the difference in quantization procedure with FSNet-WQ (maybe I missed something in the paper).","The paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer. A fast convolution algorithm is also designed for the convolution layer with this approach. Experimental results show (i) effectiveness in CNN compression, (ii) acceleration on the tasks of image classification, object detection and neural architecture search. While the authors addressed most of reviewers' concerns, the weakness of the paper which remains is that no wall-clock runtime numbers (only FLOPS) are reported - so efficiency of the approach in practice in uncertain."
"There has been a great deal of interest and research into reduced numerical precision of weights, activations and gradients of neural networks. If, for example, 16 bit floating point can be used instead of 32 bit floating point, then the memory bandwidth is halved along with significant gains in computational performance. <sep> In this work the authors propose an 8-bit floating point format (denoted S2FP8) for tensors. In general, computing activations and gradients with such low precision at training time, has generally proved challenging without a number of tricks such as scaling the loss of each minibatch to within a reasonable range. Such ``tricks'' can be difficult to tune for each problem. <sep> The key idea here is that for each tensor of 8-bit numbers, two 32 bit floating point statistics are recorded as well. These determine (in log-space) a scale and an offset for the 8-bit numbers (eq 1). This means that in this format tensors of significantly different scales can be well-represented (although larger scales necessarily implies low precision). <sep> Matrix multiplications are done in FP32 precision and then converted in S2FP8 format. This requires an additional step to accumulate the summary statistics of each tensor in order to convert from FP32 to S2FP8 (the mean and the max of the tensor elements in log space). <sep> The weights of the network are stored in FP32 and the gradients and activations are computed in S2FP8 and used to update the weights. <sep> They test this approach in ResNet, a small transformer and a MLP for collaborative filtering. They find it reaches similar performance to FP32 where standard FP8 format has worse performance or results in Nan's. <sep> Improvements in computational efficiency, both at training and inference, are active areas of research and this work contributes a novel approach using summary statistics. However, there are a several ways this work could be improved. <sep> 1. There is no comparisons with bfloat16, which is becoming a widely used approach to lower precision and is gaining significant hardware support [1]. <sep> 2. Discussion and analysis regarding the need to gather summary statistics after each matrix multiplication (or other tensor operation). It is claimed that this brings minimal HW complexity, but this doesn't seem well justified. For a large tensor, this additional reduction to compute statistics may be expensive (in memory bandwidth and computation), particularly since this is done with FP32. <sep> 3. Even with the current implementation on a GPU, it should be possible with kernel fusions to gain some significant savings in memory bandwidth (and therefore computational speed), but there is no attempt anywhere to show any runtime benefit on current hardware. <sep> Minor issues: <sep> Some captions are very terse and the figures would benefit from a clearer explanation (e.g. figure 6). <sep> [1] https://en.wikipedia.org/wiki/Bfloat16_floating-point_format","Main description: paper focuses on training neural networks using 8-bit floating-point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption. <sep> Discussions <sep> reviewer 3: gives a very short review and is not knowledagble in this area (rating is weak accept) <sep> reviewer 4: well written and convincing paper, some minor technical flaws (not very knowledgable) <sep> reviewer 1: interesting paper but argues not very practical (not very knowledgable) <sep> reviewer 2: this is the most thorough and knowledable review, and here the authors like the scope of the paper and its interest to *CONF*. <sep> Recommendation: going mainly by reviewer 2, i vote to accept this as a poster"
"Disclosure on reviewer's experience: I am not an expert on adversarial attack methods or defenses, but I am well read in the general literature on robustness and uncertainty in deep neural networks. <sep> The authors present a biologically inspired sleep algorithm for artificial neural networks (ANNs) that aims to improve their generalization and robustness in the face of noisy or malicious inputs. They hypothesize that ""sleep"" could aid in generalization by decorrelating noisy hidden states and reducing the overall impact of imperceptible perturbations of the input space. The proposed sleep algorithm broadly involves 1) converting the trained ANN to a ""spike"" neural network (SNN), 2) converting the input signal (pixels) to a Poisson distributed ""spike train"" where brighter pixels have higher firing rates than darker pixels, 3) propagating the neuronal spikes through the SNN, updating weights based on a simplified version of spike-timing-dependent plasticity (STDP), and 4) converting the network back to an ANN after the sleep phase has finished. They present a detailed comparative study spanning three datasets, four types of adversarial attacks and distortions, and two other baseline defense mechanisms, in which they demonstrate significant improvements (in some cases) of the sleep algorithm over the baselines. <sep> The core concept behind the authors' work is novel and interesting, and the experimental design is thorough and well controlled. Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying ""sleep"" and other relevant ideas from neuroscience to the problem of robustness in deep neural networks. I have some questions and concerns which I will detail per-section below, but overall, I believe that this paper is a valuable contribution to the literature and should be accepted once the authors have made a few necessary revisions. <sep> Section 1: Introduction <sep> ""We report positive results for four types of adversarial attacks tested on three different datasets (MNIST, CUB200, and a toy dataset) ..."" <sep> It's debatable whether or not the results from the CUB-200 dataset are positive. The sleep algorithm fails to outperform the baselines for each attack type (except for an almost negligible advantage in accuracy on JSMA) and barely even outperforms the control network in most cases (2/4 attacks it actually underperforms the control). I think the authors should consider rephrasing this statement to better reflect the actual results. <sep> Section 2: Adversarial Attacks and Distortions <sep> FGSM: The notation used here is somewhat inconsistent with the source paper. Goodfellow et al use epsilon to denote what I think the authors call eta, and call the second term, epsilon*sign(grad(J)), eta. Furthermore, the authors state that ""this represents the direction to change each pixel in the original input in order to decrease the loss function."" But this doesn't make sense. An adversary should want to *increase* the loss function enough to cause a misclassification. Goodfellow et al use this expression to formulate a L1-like regularization term and describe the training procedure ""minimizing the worst case error when the data is perturbed by an adversary"", which seems more sensible. This section should be rewritten to be more consistent with the source. <sep> Section 3: Adversarial defenses <sep> Regarding distillation: ""We use T=50 to compare with the sleep algorithm"" <sep> The authors should elaborate a bit more on the reasoning for this choice. It seems very arbitrary. <sep> Sectioin 4: Sleep algorithm <sep> 1. Algorithm 1: Why is line 9 inside of the for loop? It doesn't seem to be at all dependent on t. One would expect the input to only need to be converted once. Additionally, in lines 11-13, the l's in W(l,l-1) and similar should be unbolded. It's confusing that the format changes (unless I am missing something and it's actually a different variable). <sep> 2. Spike trains should be more rigorously defined, preferably with formalized notation. It's a bit unclear exactly what they are from the current text. Are they just parameters for a Poisson? Or outputs from a poison over T time steps? Or something else? <sep> 3. ""weights are scaled by a parameter to induce high firing rates in later layers"" <sep> It would be good to include more details on this parameter, how the values are chosen, and the intuition behind this idea. I assume it's because of higher level feature representations in later layers of deep neural networks. <sep> Section 5: Results <sep> 1. It's confusing that sometimes accuracy refers to classification accuracy and sometimes adversarial attack accuracy. I would recommend assigning a different name to the latter, or making sure that a qualifier precedes every reference to ""accuracy"" in this section. <sep> 2. In the second section of the results table (which is missing a label), why is the JSMA value for Defensive Distillation bolded? The distance measures for both the control network and for fine-tuning are higher. It seems like fine-tuning should be the one bolded. <sep> 3. Figure 1: caption is incorrect; it states ""adversarial attack accuracy"" and it should be ""classification accuracy"", otherwise the plots make no sense. <sep> 4. ""we observe that in the Patches and CUB-200 dataset, sleep has beneficial results in moving the accuracy function above the other defense methods"" <sep> It should be noted that this is only true for eta < 0.1. After that, sleep and the control both converge to 50% accuracy. Also this sentence should be reworded to be less visual and more quantitative (e.g. sleep tends to have higher median accuracy scores than the other methods for eta < 0.1). <sep> 5. ""We observe that performance continued to drop after a sufficiently large amount of noise was added"" <sep> More than that, the other methods converged to a small band of accuracy values; sleep continued to deteriorate. This is a significant difference. It would be a good idea to re-run this experiment with a binary classification problem (e.g. only two digits of MNIST) and see if this phenomenon still occurs. Then, the noisy sleep classifier predictions could simply be inverted to get improved accuracy scores. <sep> 6. In the analysis of JSMA, as noted before,, it's rather dubious to claim that sleep had any kind of significant effect on the attack success rate (or distance) for CUB-200. I would rewrite this section to better represent the results. <sep> 7. Figure 2 formatting: Legend is overflowing out of the first figure. Additionally, the legend colors should be made to match across all three figures, and the legend should either appear in all three (if necessary for some reason) or only in one. <sep> 8. Figure 2: The caption is incomplete and possibly incorrect. It's not clear why the first and last figures differ from each other, and the caption does not indicate this. The caption also only mentions two datasets, even though it says ""for the following three datasets"". <sep> Appendix: <sep> General formatting needs improvement. A lot of figures are off-centered, text misaligned, missing axis labels, etc.","Sleep"" is introduced as a way of increasing robustness in neural network training. To sleep, the network is converted into a spiking network and goes through phases of more and less intense activation. The results are quite good when it comes to defending against adversarial examples. Reviewers agree that the method is novel and interesting. Authors responded to the reviewers' questions (one of the reviewers had a quite extensive set of questions) satisfactorily, and improved the paper significantly in the process. I think the paper should be accepted on the grounds of novelty and good results."
"Summary: <sep> This paper proposes visualization techniques for the optimization landscape in GANs. The primary tool presented in this paper is a quantity called path-angle, which looks at the angle between the game vector field and the linear path between a point away from a stationary point and a point near a stationary point. The paper present examples of the visualization for dynamics with pure attraction, pure rotation, and a mix of attraction and rotation. Along with this, the authors propose to look at the eigenvalues of the game Jacobian and the individual player Hessian's to evaluate convergence in GANs. The paper presents application of the tools on GANs trained with NSGAN and WGAN-GP objectives on a mixture of Gaussians, MNIST, and CIFAR10. The primary observation is that the generator performance is good, but the algorithms converge to non-Nash stable attractors. Moreover, it is shown using the path-angle plots that GANs exhibit rotational behavior around stable points. <sep> Review: <sep> There has been a lot of work in the past few years (and ongoing) on principled training approaches for GANs. The objective of the algorithms is typically to converge a differential Nash equilibrium and/or to reach a stable point of the dynamics quickly. In my view, this work fills some of the gap on the empirical side of things with respect to each goal. <sep> Notably, a main idea to speed up convergence in GANs is to change the gradient play dynamics so rotational components are neutralized. The path angle visualization provides a novel tool to evaluate the empirical ability of any dynamics proposed for GANs to cancel out rotational components. Since it is generally known that gradient play dynamics are susceptible to cycling, I would have been interesting in seeing the path angle plots for some recently proposed algorithms such as consensus, symplectic gradient adjustment, stable opponent shaping, local symplectic surgery, etc to see how they compare. This would have made the experiments using the path angle visualization stronger in my view. Nonetheless, the path angle tool is useful and I can foresee it being commonly used in the future. <sep> Aside from neutralizing rotational components, dynamics have been proposed with the goal of avoiding non-Nash stable attractors and converging only to differential Nash equilibria. However, to my knowledge, there has not been much, if any, evaluation in GANs to see if the methods are in fact converging to Nash equilibria as theory may predict. While simple, I found it interesting to evaluate the eigenvalues of the relevant quantities at convergence. I am curious why the authors evaluate the top-k eigenvalues in terms of magnitude? The scipy package referenced in the appendix can compute the largest and smallest real eigenvalues, which is what it seems like you would want to evaluate the definiteness of the game Jacobian and the individual player Hessians. The most interesting empirical result in the paper to me was that it is common to converge to non-Nash stable attractors using standard training techniques and at such stable points the generator performance is strong. This is an important observation and  may cause some consideration of what points should be sought in GANs. I am not fully convinced this is always what the dynamics would always converge to depending on the network, learning rates, optimization methods, etc, but showing that it can be the case is useful. <sep> Overall, I think this paper introduces some useful tools to interpret the performance in GANs and to help understand the behavior of training dynamics. The main tool introduced was the path angle visualization and the primary empirical result was that standard GAN methods may reach non-Nash stable attractors and perform well. The paper probably be condensed in the first 4 pages, so that more experimental results could be presented and this would make the paper stronger. <sep> Post Response: Thanks for the response. I believe this paper should be accepted.",This is an interesting contribution that sheds some light on a well-studied but still poorly understood problem. I think it might be of interest to the community.
"This paper studies the situation in which a two-layer CNN with RELU nonlinearity is fit to a single image and the observation that it is able to fit a ""natural"" image in fewer iterations than a ""noisy"" image. Theorems on the convergence of this fitting are discussed and proven in the appendices. Intermediate results study the convergence of fitting a linear model to an image plus noise and fitting a single-layer CNN. Denoising is demonstrated on two images with additive white Gaussian noise and the approach under study is shown to provide a better signal-to-noise ratio than BM3D, another untrained denoising approach. The main result is that the use of upsampling via a fixed interpolation filter provides an inductive bias towards ""natural"" images. <sep> The bibliography does a good job of positioning this paper within the recent set of articles exploring the curious behavior of fitting a CNN to a single image. The problem is interesting, timely, and surprising. The analysis does provide some insight into what is going on. <sep> But, the paper would do well to embrace the Fourier domain and first discuss what is meant by ""natural images"" and ""noise"" in terms of their frequency content. In particular, something like Simoncelli and Olshausen (2001) could be used as a description of the spectra of natural images. Additive white Gaussian noise has a flat spectrum, which is never mentioned in the paper. Thus, the theoretical result mainly highlights the fact that natural images have a low-pass spectrum, while white noise has a flat spectrum, and CNNs using interpolation also have a low-pass spectrum in some sense. This makes sense from a frequency perspective, because interpolation increases the sampling rate of a signal without changing its frequency content, i.e., it adds high frequencies with no energy. <sep> The ""trigonometric basis"" used in the paper, which consists of sines and cosines at each frequency, could be more cleanly described as a basis of complex exponentials, i.e., the Fourier basis, which doesn't require partitioning the basis into a cosine half and a sine half. The discussion of triangular and Gaussian smoothing functions has been very well explored in the signal processing literature discussing windowing functions and their Fourier transforms, e.g., Harris (1978). I don't think Figure 3 showing some sinusoids is necessary. <sep> The main body of the paper goes into the 10th page, but the appendices make up another 18 pages. The main body of the paper does not include any of the proofs of the provided theorems. This seems rather excessive. <sep> Harris, F. J. (1978). On the use of windows for harmonic analysis with the discrete Fourier transform. Proceedings of the IEEE, 66(1), 51-83. <sep> Simoncelli, E. P., & Olshausen, B. A. (2001). Natural image statistics and neural representation. Annual review of neuroscience, 24(1), 1193-1216. <sep> Minor comments: <sep> Several citations are in the wrong form (\\citet instead of \\citep) throughout the paper. <sep> There is a link to a figure in the appendix that is broken on page 5 <sep> In figure 7, one one set of y-axis labels is shown, but it appears that each subplot uses an independent y-axis, just without labels. Please plot them all on the same y-axis. <sep> Similarly, figures 1, 4, and 6 show similar things on different y-axes when they could be plotted at the same scale on the y-axis to make them more easily comparable visually. <sep> Typos: ""over-paramtrized"" and ""spacial"" both on page 1. <sep> After discussion: <sep> The authors have addressed my concerns, so I am changing my decision from Weak Reject to Weak Accept.","This paper studies the question of why a network trained to reproduce a single image often de-noises the image early in training. This an interesting question and, post discussion, all three reviewers agree that it will be of general interest to the community and is worth publishing. Therefore I recommend it be accepted."
"This paper deals with turning a 2.5D video representation into a 3D representation of an environment or a scene. The authors introduce self-supervised methods to pretrain the 2d-3d projection with a contrastive loss, which is the neural backbone for multiple other tasks. They then assess their approach on numerous tasks such as 3D-object detection, 3D-moving object detection, and 3D motion estimation. The authors also evaluate the transferability of the features in a challenging sim2real setting. <sep> It is dense paper with multiple modules (2d-3d, ergomotion, memory, etc.) and concept. Still, the authors make it accessible by concise paragraphs, highlighting key equations (The enum + eq 1 and 2 are quite useful), and well-designed sketch (Figure1). I had some difficulties digging into the visual head component for each task as I was not familiar with this topic. However, the authors always explain their choices in a few lines and refer to the related papers for technical details in a meaningful way. <sep> I am pretty convinced with the experiments, especially Sim2Real, in Tab1, where the baselines are clears and make sense. <sep> I appreciated the limitation section, which is transparent and honest, and clearly states the strength and weaknesses (such as image downscaling) of the approach. Besides, the code and the data should be released, which is always a positive point. <sep> Remarks: <sep> - Latent map update: running average is a simple and efficient mechanism, it also makes sense as you are dealing with big 3D tensors. Yet, have you tried other update mechanisms? <sep> - A natural follow-up to this paper is Contrastive Predictive Losses (which had several successes in pure vision setting[1]). Did you already try this approach? <sep> - In visual CPC papers [1] (or since the early days of visual representation learning!), data transformation has been applied to improve model performance. Would it make sense to apply it to I_{n+1}, D_{n+1} ? <sep> - Although the authors assess their approach with RGB-D, the models were still trained on 2.5D video. It would have been useful also to assess a pretraining on pure RGB-D data <sep> However, I have two (somehow related) concerns. First of all, the machine learning novelties are rather small, contrastive losses are now widespread, and the models are closed to Tung et al. as mentioned by the authors.  However, I believe the paper to be a substantial contribution in vision, as they show the feasibility of their approach on large scale scenarios and over a highly diverse set of tasks. Again, the authors also release the code, making the paper a valuable baseline for the following work. On my side, I am impressed by the sim2real env. <sep> My second concern is the following, it is a high quality vision paper, and I am curious why the authors chose *CONF* over CVPR. Besides, the tasks are vision-oriented, and 2.5D vision is not common in the ML community. Having said that, the proposed approach is pretty generic, can be applied to RGB-D (more common in ML), and require few expert knowledge in vision (only the Egomotion module). <sep> As a result, I would advocate for clear accept if we assess vision-based contribution for *CONF*; otherwise, I would only recommend weak accept the paper is solely based on ML contributions (the paper is still sound, well-written, with numerous experiments and with a semi-generic architecture)","The authors propose to learn space-aware 3D feature abstractions of the world given 2.5D input, by minimizing 3D and 2D view contrastive prediction objectives. The work builds upon Tung et al. (2019) but extends it by removing some of the limitations, making it thus more general. To do so, they learn an inverse graphics network which takes as input 2.5D video and maps to a 3D feature maps of the scene. The authors present experiments on both real and simulation datasets and their proposed approach is tested on feature learning, 3D moving object detection, and 3D motion estimation with good performance. All reviewers agree that this is an important problem in computer vision and the papers provides a working solution. The authors have done a good job with comparisons and make a clear case about their superiority of their model (large datasets, multiple tasks). Moreover, the rebuttal period has been quite productive, with the authors incorporating reviewers' comments in the manuscript, resulting thus in a stronger submission. Based in reviewer's comment and my own assessment, I think this paper should get accepted, as the experiments are solid with good results that the CV audience of *CONF* would find relevant."
"Summary: This paper proposes a new loss function: curriculum loss, which is a meta-loss function that we can still specify an existing surrogate loss to use this loss function. This meta-loss function guarantees to be tighter than using a traditional pointwise-sum loss function as used in the empirical risk minimization framework. Intuitively, the proposed CL loss embed the sample selection process in the objective function. The authors suggest that it is robust against label corruption because it is tighter and provided promising experimental results. <sep> ======================================================== <sep> Clarity: <sep> The paper is well-written and easy to follow. <sep> ======================================================== <sep> Significance: <sep> The proposed paradigm is interesting and I am convinced that it can be useful under label noise. The experiments look promising. Future work about the analysis of NPCL/CL is also interesting to consider (e.g., which surrogate loss to use, rigorous theoretical guarantee, etc.). I think the proposed method is impactful. <sep> ======================================================== <sep> Comments: <sep> The proposed method is interesting and can give a tighter bound for any surrogate loss by using this method (CL). Moreover, the author suggested a simple extension of CL for label corruption (NPCL) and the performance is impressive. I would like to vote accept for this paper but the following point highly concerns me and I am not sure about the correctness (see the concern below). It is about the motivation not the proposed method. <sep> Concerns about motivation: <sep> I disagree with the original motivation of this paper. The authors used the result of Hu et al. 2018 to motivate the use of CL. To my knowledge, the main point raised by Hu et al. is as follows: <sep> In classification, minimizing the adversarial risk yields the same solution as using the standard empirical risk. This suggests that minimizing the adversarial risk may not enhance the robustness of a classifier. Yet, it may still be useful when we consider regression (other settings but not classification). As a result, in classification, we should try other methods to make a robust classifier. Then, Hu et al. considered to utilize some kind of structural assumption to make a robust classifier. From their title: ""Does Distributionally Robust Supervised Learning Give Robust Classifiers?"", I think they suggested ""No"" as an answer and the discussion about 0-1 loss in the curriculum loss paper will be contradicted to them from the motivation perspective. <sep> Furthermore, regarding the adversarial risk, it is not focusing on the label noise but rather the noise of the feature-label pair, i.e, perturb (x,y) adversarially within an f-divergence ball. However, in my opinion, if we randomly flip the label of the data regardless of x (as the authors and existing work did in experiments when considering label corruption: symmetric, partial, etc.), we cannot be confident to state that the f-divergence between test distribution and corrupted training distribution is small under label noise. <sep> Another point to motivate the use of 0-1 loss that the author mentioned is when we have outliers (Masnadi-Shirazi & Vasconcelos, 2009). This makes sense and this is a famous argument to discourage the use of too steep loss functions, e.g., exponential loss. I think this motivation is fine but it is not directly related to label corruption because we do not add out-of-distribution data but rather the label noise. Furthermore, the authors did not inject any outliers in the experiments in my understanding. I think this is totally no problem because we are focusing on label noise here, but this makes the motivation about outliers less important when we are talking about label noise. <sep> I think the most important direction both in theory and experiments about the robustness to label noise of the 0-1 loss is that 0-1 loss satisfies a ""symmetric property"", i.e., \\ell(z)+\\ell(-z) = Constant for a margin-based loss function in binary classification. Under symmetric label noise, ""the minimizer of the expected symmetric noise risk (a risk that the label is corrupted by coin flipping noise) is identical to the minimizer of the clean risk (normal risk)"". Although it is not empirically but the expected version, it gives a good insight about the advantage of directly minimizing 0-1 loss under label noise. This is first pointed out by <sep> [1] Manwani et al.: Noise tolerance under risk minimization, IEEE Transactions on Cybernetics 43 (2013) <sep> [2] Ghosh et al.: Making risk minimization tolerant to label noise Neurocomputing 160 (2015): 93-107. <sep> ([1] focused on the 0-1 loss while [2] extended it to symmetric losses.) <sep> Then, it was extended to the multiclass loss by the following paper: <sep> [3] Ghosh et al.: Robust loss functions under label noise for deep neural networks. AAAI2017. <sep> The advantage of symmetric losses is also discussed in this paper that the authors already cited in the symmetric noise experiment section. <sep> [4] van Rooyen et al.: Learning with symmetric label noise: The importance of being unhinged, NeurIPS2015 <sep> The advantage of the symmetric condition and 0-1 loss is also discussed in a more general noise scenario and more evaluation metrics: <sep> [5] van Rooyen et. al: An average classification algorithm. arXiv:1506.01520, 2015 <sep> [6] Charoenphakdee et al.: On symmetric losses for learning from corrupted labels, ICML2019 <sep> And the following paper that was also cited in the submitted work and compared: <sep> [7] Zhang and Sabuncu: Generalized cross-entropy loss for training deep neural networks with noisy labels, NeurIPS2018 <sep> is also inspired by the robustness of the symmetric losses (including 0-1 loss). They argued that although the symmetric loss (MAE) for multiclass proposed by Ghosh AAAI2017 is robust, it is hard to train for challenging datasets, and they try to relevate this condition while making it easier to train.  This paper outperformed [7] and I think it is clearer and better to build a story along this line. <sep> In short, here is the key message why I think the current motivation does not feel right. When we have noisy labeled data, instead of motivating the use of 0-1 loss by suggesting that <sep> ""If we have clean labeled data, minimizing the ""adversarial"" ERM risk using ""clean"" labeled data yields the same minimizer as minimizing the ""standard"" ERM risk using ""clean"" labeled data"", <sep> I believe the story to motivate the robustness of 0-1 loss under label noise should be <sep> ""If we have noisy labeled data, minimizing the ""standard"" or ""modified"" risk using ""noisy"" labeled data yields the same minimizer as minimizing the ""standard"" ERM risk using ""clean"" label data"" <sep> The latter statement corresponds to the literature I suggested. <sep> Apart from the motivation raised by the authors, as we can see from this curriculum loss paper, NPCL nicely outperformed generalized cross entropy loss in [7], which is impressive. <sep> ======================================================== <sep> Decision. <sep> I strongly feel that motivating the noise robustness of 0-1 loss by discussing about the adversarial risk (Hu et al.) is misleading. Nevertheless, I feel the proposed method itself makes a lot of sense and I am impressed by the results. If the author can convince me that using the current motivation of the paper is suitable, I am happy to improve the score. Another way is to agree to modify the motivation part. Given the experiments were done, it is not to difficult to change the motivation of the paper. At this point, I have decided to give a weak reject. <sep> ======================================================== <sep> Questions: <sep> 1. Is it straightforward to combine NPCL with Co-teaching/Mentornet/Co-teaching+? <sep> 2. Does the traditional theory about classification-calibration (Zhang, 2004, Bartlett+, 2006) can guarantee the Bayes-optimal solution if we use NPCL? <sep> ======================================================== <sep> Minor comments: <sep> 1. Page 9: Both our NPCL and Generalized Cross Entropy(GCE) << space missing between Entropy and ( <sep> Update: I have read the rebuttal. Although I am still not fully convinced with the motivation of the paper and still doubting whether NPCL works well because of the given motivation, I still believe that the proposed NPCL should give a new perspective to deal with noisy labels. I like the idea of the paper. Thus, I change the score to Weak Accept.","This paper studies learning with noisy labels by integrating the idea of curriculum learning. <sep> All reviewers and AC are happy with novelty, clear write-up and experimental results. <sep> I recommend acceptance."
"Updated review: I am overall happy with the response of the authors. I can appreciate the contributions of the paper and I am happy to recommend accept. The empirical study offers some insights into deep RL methods for ATARI games and raises some key questions. I feel the current version of the paper does not build upon these insights to propose a new method. <sep> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- <sep> Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. The paper concludes that methods that perform well on Montezuma's revenge do not necessarily perform well on the other games, sometimes, even worse than the eps-greedy approach. This also leads to the conclusion that recent results on the game Montezuma's revenge can be attributed to architectural changes instead of the exploration method. <sep> I think this is a-ok paper in that it does what it says it does. The paper is clear and well-written. <sep> I think the main contribution of the paper is that it raises some questions over existing methods/trends in solving exploration problems in reinforcement learning by comparing the performance of multiple methods across various games in ATARI suite. <sep> I think this is relevant to the *CONF* community and will be appreciated by it. <sep> However, I also feel that while the paper runs a satisfactory empirical analysis, it was all too much focussed on the existing methods. Throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various ATARI games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses. For example, one could easily investigate in the CTS method if the factor by which exploration bonus dies N^{alpha} (alpha=-1/2 by default) changes, then does it do better or worse (more below on this). <sep> I can understand that might not be the aim of the paper but still. <sep> Here are a couple of points that I felt conflicted/confused about the paper: <sep> - The conclusion of the paper is that 'progress of exploration in ATARI suite is obfuscated by good results in single domain'. I am confused if the paper is making a narrow point that (1) dont focus on Montezuma's revenge OR (2) is it admitting a broader point that focussing on even ATARI is probably not a good choice. I am not saying that I know the answer to this question, but I am unclear as to what is the question the paper is trying to raise. If it is saying (1st) then I find it contradictory that it is not ok to focus on MR but it is ok to focus on ATARI as a single domain; if it is saying the second then also it is contradictory because the paper only experiments with the ATARI suite. <sep> - It is interesting to note that noisy networks are most robust to hyperparameter optimization on a separate set of games when tested on a different set of games. It is also interesting to note that noisy networks are the only exploration bonus method that does not decrease/reduce exploration as the experience of the agent increases. I would have liked to see if the paper had made an attempt to investigate this. I feel such a hypothesis would have been easy to investigate with simple modifications to the CTS methods. Currently, the exploration bonus goes down by the factor of 1/sqrt(N)  in the CTS method. A comparison that showed the performance of CTS for a couple more values of factors such as (1/N) or (1/N)^{1/4} would have been nice to see if that mattered. <sep> - One of the comparisons I did not particularly find fair was when the hyperparameters of various methods were tuned to play MR and then the hyperparameters were fixed and the method were tested on other ATARI games. <sep> - Another point I felt was missing was checking if rainbow DQN is really the reason behind the observed performance of the methods. It would have been interesting to know how the methods performed when combined with the original DQN algorithm.","This paper presents a detailed comparison of different bonus-based exploration methods on a common evaluation framework (Rainbow) when used with the ATARI game suite. They find that while these bonuses help on Montezuma's Revenge (MR), they underperform relative to epsilon-greedy on other games. This suggests that architectural changes may be a more important factor than bonus-based exploration in recent advances on MR. <sep> The reviewers commented that this paper makes no effort to present new techniques, and the insights discovered could be expanded on. Despite this, it is an interesting paper that is generally well argued and would be a useful contribution to the field. I recommend acceptance."
"The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. This algorithm has complexity bounds that would open up (for instance) the possibility of exponentially large filter banks, and the authors show through a simple, classical simulation approach that the resulting network is also likely to be trainable. <sep> Feedback: <sep> A few typos/formatting issues: <sep> - The title accidentally includes ""Conference Submissions"" <sep> - The in-text citation format frequently has the parentheses in the wrong place; this is surprisingly distracting! <sep> Preliminaries: <sep> - Maybe explain what the ith vector in the standard basis is in terms of |0> and |1>? I assume the answer is along the lines of |000>, |001>, |010>, etc.? <sep> Main results: <sep> - The sentence ""a speedup compared to the classical CNN for both the forward pass and for training using backpropagation in certain cases"" is ambiguous; does ""in certain cases"" qualify only training speed or also forward pass speed? <sep> - There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks? <sep> - Can you briefly justify (or cite) the claim that ""most of the non linear functions in the machine learning literature can be implemented using small sized boolean circuits""? <sep> - I'm a little confused about the discussion of quantum importance sampling on page 4. Could you give some intuition for the relationship between eta and the fraction of output values that are on average flushed to zero (is this 1 minus sigma?), and perhaps connect this to the literature about activation pruning and sparse NNs? <sep> - Maybe define what you mean by ""tomography"" for ML folks without the quantum background? <sep> - I'm convinced by the simulations, even though I shouldn't really be convinced by anything on MNIST... It just seems like the perturbations you're applying are all things that modern neural networks take in stride. <sep> - The discussion of using a sigma-based classical sampling rather than the eta-based quantum importance sampling mentions a ""Section C.1.15"" which does not exist (I think you mean the end of Section C.1.5). <sep> - Re: ""We will use this analogy in the numerical simulations (Section 6) to estimate, for a particular QCNN architecture and a particular dataset of images, which values of σ are enough to allow the neural network to learn."" My understanding is that you're getting empirical estimates of which values of sigma are enough; it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks). <sep> - The sampling procedure based on sigma might be inefficient in your PyTorch implementation, but it's certainly something that GPUs are fairly well suited to computing. There might be other PyTorch operators that would help here (perhaps Bernoulli sampling?) or if nothing else you could write a small custom CUDA kernel.","Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission. Especially, the authors should take care to make this paper accessible (understandable) to the ML community as *CONF* is a ML venue (rather than quantum physics one). Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future."
"In this paper, the authors take a MobileNet v2 trained for ImageNet classification, and adapt it either (i) semantic segmentation on Cityscapes, or (ii) object detection on COCO. They do this by first expanding the network into a ""supernet"" and copy weights  in an ad-hoc manner, then, they perform DARTS-style architecture search before fine-tuning for the task at hand. <sep> There is no TLDR for this paper, and I must admit, on reading the abstract and introduction I wasn't entirely sure what this paper was doing at first. Perhaps I was being slow. <sep> From a narrative perspective, one of the main selling points is not needing to perform any expensive ImageNet pre-training; however, a pre-trained MobileNetv2 is being utilised. While this was off-the-shelf, it still incurred an initial training cost, so it isn't really fair in e.g. Table 4 to put pre-training cost as zero. On a related note, the authors write that this network is used for its ""generality"". I'd argue that MobileNetv2 is a highly engineered network specialised for mobile computation; a standard ResNet-50 would be more general really. <sep> I would like to see a comparison to a random search, as there are several papers (https://arxiv.org/abs/1902.07638, https://arxiv.org/abs/1902.08142) indicating that this is a very strong baseline. <sep> As mentioned earlier, the choices for remapping weights seem very ad-hoc. I can't really tell what's going on in Table 5 (why is PR in the NE and PA row?) so the ablation study of how effective this weight mapping is lost on me.  The stuff in Table 6 is pretty interesting however, if convoluted. <sep> I find the odd choices of hyperparameters (tau as 45, gamma as 10, lambda as 9e-3) rather alarming. How important are these? Would this technique work under any other circumstances? <sep> Error bars would be a welcome inclusion, particularly in Table 3 where you have 0.1% separating FNA and MNasnet-92. I appreciate that this can be expensive however. <sep> Pros: <sep> - Some promising results <sep> - Good figures <sep> Cons: <sep> - Ad-hoc design choices <sep> - Not a fair comparison regarding pre-training. <sep> - Very specific to one network choice <sep> - Lack of error bars or comparison to random search. <sep> I am giving this paper a weak reject, as there is insufficient experimental evidence that the technique works, or generalises beyond Mobilenetv2. I am also concerned about the ad-hoc hyperparmaters or weight-mapping. A comprehensive ablation study, along with error bars, and another choice of seed network would do much to strengthen this paper.","Main content: Paper proposes a fast network adaptation (FNA) method, which takes a pre-trained image classification network, and produces a network for the task of object detection/semantic segmentation <sep> Summary of discussion: <sep> reviewer1: interesting paper with good results, specifically without the need to do pre-training on Imagenet. Cons are better comparisons to existing methods and run on more datasets. <sep> reviewer2: interesting idea on adapting source network network via parameter re-mapping that offers good results in both performance and training time. <sep> reviewer3: novel method overall, though some concerns on the concrete parameter remapping scheme. Results are impressive <sep> Recommendation: Interesting idea and good results. Paper could be improved with better comparison to existing techniques. Overall recommend weak accept."
"The authors consider the alignment problem for multiple datasets with side information via entropic optimal transport (Sinkhorn). The authors formulate it as a transport cost learning in optimal transport framework with constraints giving by side information. Empirically, the authors illustrated the effectiveness of the proposed approach over state-of-the-art on several applications (e.g. single-cell RNA-seq, marriage-matching, MNIST with its perturbed one. <sep> The paper show good results of the proposed method for dataset alignment on several applications. However, the motivation of the dataset alignment seems quite weak (e.g. it is unclear the advantage for jointly analysis when one merge datasets by alignment). The side information is not clearly easy to obtain in practice, since it simply enforces the constraints on the transportation polytope (i.e. how one knows that which subset of a given class C_A on dataset A is matched with a certain subset of another class C_B on dataset B, it seems easy to obtain the side information about matching over the whole class C_A with C_B, and it seems nontrivial to obtain the subset matching). Additionally, some important information about the proposed method is not in the main text but in the appendix which makes the paper hard to read without checking appendix constantly (especially the main proposed method). Overall, I lean on the negative side. <sep> Below are some of my concerns: <sep> 1) As stated above, I am curious how one can obtain the side information about the subset matching? For example, in Figure 1, it seems trivial to have a side information about the whole class matching and quite nontrivial to obtain a matching for subsets of any given class. <sep> 2) The side information enforces the constraints on the feasible set of the transportation polytope. It seems to me that one can solve the variant of optimal transport (or Sinkhorn) with those extra constraints directly, it is unclear to me how this kind of side information can navigate the transport cost learning. Moreover, given a new pairs of input, the proposed method needs to optimize the transport cost again which make the benefits to learn to transport cost quite limited. Overall, a trivial baseline for the proposed method is to solve directly optimal transport with those extra constrains (from side information) directly (maybe with the Euclidean cost metric), but not the optimal transport without extra constraints as in the current setting. <sep> 3) The authors may need to consider to reorganize the presentation. Much important information of the proposed method is not in the main text, but in the appendix which make the paper hard to follow without checking the appendix constantly. The derivation of gradient in line 6 of Algorithm 1 is not trivial. It is better if the authors give more detail information and discussion. I am quite confused about the idea to back propagate the ""gradient"" through Sinkhorn-Knopp iterations? <sep> 4) It seems that the complexity of gradient update of Algorithm 1 is quite high. The authors should give a comparison about time consumption in the experiments (besides the accuracy matching) <sep> 5) It seems that there is nothing in the code sharing folder?","The paper proposes an algorithm for learning a transport cost function that accurately captures how two datasets are related by leveraging side information such as a subset of correctly labeled points. The reviewers believe that this is an interesting and novel idea. There were several questions and comments, which the authors adequately addressed. I recommend that the paper be accepted."
"This paper aims at improving the computational cost of variance reduction methods while preserving their benefits regarding the fast provable convergence. The existing variance reduction based methods suffer from higher per-iteration gradient query complexity as compared to the vanilla mini-batch SGD, which limits their utility in many practical settings. This paper notices that, for many models, as the training progresses the gradient vectors start exhibiting structure in the sense that only a small number of coordinates have large magnitude. Based on this observation, the paper proposes a modified variance reduction method (by modifying the SpiderBoost method), where a 'memory vector' keeps track of the coordinates of the gradient vectors with large variance. Let d be the size of the model parameter. During each iteration, one computes the gradient for k1 coordinates with the highest variance (according to the memory vector) and an additional k2 random coordinates. <sep> The paper shows that in the worst case, the proposed method has the same gradient query complexity as the SpiderBoost variance reduction method. Assuming that the proposed method can track the sparsity of the gradient vector, the proposed method achieves a gradient query complexity which is O((k1+k2)/d) times that of the SpiderBoost method. The paper demonstrates the gradient query complexity improvement over the SpiderBoost method on MNIST and CIFAR-10 data set. <sep> The paper presents novel results by utilizing the ideas from the field of communication-efficient distributed optimization. As far as the reviewer can tell, the results in the paper are correct. That said, there is quite a bit of room for improvement in terms of the writing of the paper. <sep> The paper appears to have way too many typos. For example, <sep> In Section 2.1: <sep> - Why is k introduced? <sep> - S denotes a random subset with size k ---> k2? <sep> - drawn from the set ℓ:|yℓ|<|y(k)| ----> ℓ:|xℓ|<|x(k2)|? <sep> - rtop(x,y)=(0,12,0,0,1) --> rtop(x,y)=(0,16,0,0,1) <sep> In Lemma 1: <sep> - while defining top−k1(x,y), "".... if |x_{\\ell}| >= |x_{(k_1)}|"" ----> "".... if |x_{\\ell}| <= |x_{(k_1)}|""? <sep> In Section 2.2: <sep> - What are g0,g1,...,gL−1? Shouldn't these be ϕ0,ϕ1,...,ϕL−1? <sep> In A1: <sep> - right after (5), what is x~0 in the definition of Δf? <sep> The authors may also consider making the empirical evaluation more comprehensive by considering tasks from the NLP domain, e.g., language modeling. This would further help asses the utility of the proposed method.",Congratulations on getting your paper accepted to *CONF*. Please make sure to incorporate the reviewers' suggestions for the final version.
"This paper studies how to generate transferable adversarial examples for black-box attacks. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). The first method adopts Nesterov optimizer instead of momentum optimizer to generate adversarial examples. And the second is a model-augmentation method to avoid ""overfitting"" of the adversarial examples. Experiments on ImageNet can prove the effectiveness of the proposed methods. <sep> Overall, this paper is well-written. The motivation of the proposed methods are generally clear although I have some questions. The experiments can generally prove the effectiveness. <sep> My detailed questions about this paper are: <sep> 1. The motivation in Section 3.1, which regards generating adversarial examples as training models, and transferability as generalizability, is first introduced in Dong et al. (2018). The authors should acknowledge and refer to the previous work to present the motivation. <sep> 2. It's not clear why deep neural networks have the scale-invariant property. Is it due to that a batch normalization layer is usually applied after the first conv layer to mitigate the effect of scale change? <sep> 3. It's not fair to directly compare DIM with SI-NI-DIM (also TIM vs. SI-NI-TIM; TI-DIM vs. SI-NI-TI-DIM), since SI-NI needs to calculate the gradient over 5 ensembles. It's better to compare the performance of two methods with the same number of gradient calculations. <sep> 4. Is there an efficient way of calculating the gradient for scale-invariant attacks like translation-invariant attacks in Dong et al. (2019)?","Under the optimization formulation of adversarial attack, this paper proposes two methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM adapts Nesterov accelerated gradient into the iterative attacks to effectively look ahead and avoid the ""missing"" of the global maximum, and SIM optimizes the adversarial perturbations over the scale copies of the input images so as to avoid ""overfitting"" on the white-box model being attacked and generate more transferable adversarial examples. Empirical results demonstrate the effectiveness of the proposed methods. The ideas are sensible, and the empirical studies were strengthened during rebuttal."
"The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs).  Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing. <sep> I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty. <sep> Pros: <sep> Simple, intuitive method <sep> Draws from existing literature relating to dropout-like methods <sep> Little computational overhead <sep> Solid experimental justification <sep> Some theoretical support for the method <sep> Cons: <sep> Method is somewhat heuristic <sep> Mitigates, rather than solves, the issue of oversmoothing <sep> Limited novelty (straightforward extension of dropout to graphs edges) <sep> Unclear why dropping edges is ""valid"" augmentation <sep> Followup-questions/areas for improving score: <sep> It would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization). <sep> As brought up in the comments, edges to drop out to the graph's properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. <sep> p2: ""First, DropEdge can be considered as a data augmentation technique"" Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases. <sep> Deeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name ""layer-independent"" for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the ""layer dependent"" regime, edges dropped out do *not* depend on the layer). <sep> Comments: <sep> Figure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice. <sep> Use ""reduce"" in place of ""retard"" <sep> p2 "" With contending the scalability"" improve phrasing p2 ""By recent,"" -> ""Recently,"" <sep> p2 ""difficulty on"" -> ""difficulty in"" <sep> p2 "" deep networks lying"" -> ""deep networks lies"" <sep> p3 ""which is a generation of the conclusion"" improve phrasing p3 "" disconnected between"" -> ""disconnected from"" <sep> p4 ""adjacent matrix"" -> ""adjacency matrix"" <sep> p4 ""severer "" -> ""more severe"" <sep> p5 ""but has no help"" -> ""but is no help"" <sep> p5 ""no touch to the adjacency matrix"" -> improve phrasing","The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post-rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept."
"## Updated review <sep> I have read the rebuttal. First I'd like to thank the authors for the detailled rebuttal. <sep> The latest version of the paper adressed all my concerns, hence I change my rating to Accept. <sep> ## Original review <sep> This paper presents a new variation of the Transformer model, named Compressive Transformer. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. This improves the long-range dependencies modelling capabilities of the approach. The model is evaluated on two common language modelling benchmarks and yields state of the art results in both of them. The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. The model is also evaluated on two other tasks: speech generation and reinforcement learning on videos. <sep> I think this paper should be accepted, mainly because: <sep> - The proposed model is novel as far as I can tell. <sep> - The presented approach is significant, as modelling long-range dependencies is an important milestone in sequence modelling. <sep> - The new benchmark is a good addition. <sep> - The comparison with the relevant literature is thorough and well done. <sep> - The experiments are convincing and demonstrate the viability of the approach, although some aspects can be improved (see below). <sep> Detailed comments: <sep> - About the character-level language modelling on Enwik8, the improvement is very small, it seems that the task doesn't benefit from have long-range memory, could it be because character-level modelling is less dependent on the long-range past? can the authors comment on that? It would also been interesting to evaluate the gain of the memory, for instance by varying the size of the compressed memory from 0 to 1152. <sep> - The WikiText-103 evaluation is interesting, specially Table 6, which shows the advantages of the model. However when comparing with the literature, it's not clear if the performance gain is due to the compressed memory or to the network capacity. A study with different lengths of the compressed memory (starting at 0) would bring some insights about that. <sep> - In Section 5.6.2: can the authors justify why the attention weights were split in only 6 bins? creating a trended curve on only 6 points could be problematic, and I don't see why more bins couldn't be used. <sep> - The speech analysis section (5.7) is not very insightful. It shows that the proposed model is on par with WaveNet on unconstrained speech generation, which is not very useful and feels a bit half-finished. I think that the authors should either commit to this study by constraining the model with linguistic features like in (Oord et al. 2018) and evaluate it in a TTS framework with subjective evaluation or discard this section entirely.","The paper proposes a ""compressive transformer"", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory. Both memories can be queried using attention weights. Unlike TransfomerXL that discards the oldest memories, the authors propose to ""compress"" those memories. The main contribution of this work is that that it introduces a model that can handle extremely long sequences. The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets. They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word-level perplexity. In addition, the authors also present evaluations on speech, and image sequences for RL. <sep> Initially the paper received weak positive responses from the reviewers. The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions. After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept). <sep> The authors have provided a thorough and well-written paper, with comprehensive and convincing experiments. In addition, the ability to model long-range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem. Thus, acceptance is recommended."
"The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. This is an interesting problem and the results are promising. Firstly they prove the convergence rate of the algorithm on non-convex smooth functions, which shows a nearly linear speedup. <sep> Second, on the practical part, there have 3 main results: <sep> 1. They compare CHOCO-SGD under various compression schemes with the baseline. The results show the algorithm generally outperforms the baseline. <sep> 2. They implement it over a realistic peer-to-peer social network and show a great communication performance under such a network with limited bandwidth. <sep> 3. In a datacenter setting, they compare the algorithm with all-reduce, which is a centralized communication method. The results show a strong training reduction for CHOCO-SGD. <sep> Also, the paper is mostly nicely written. <sep> However, there have several issues: <sep> 1. In the introduction, they introduce their experiments with the order from ""datacenter experiment"" to ""peer-to-peer experiment"", which is different from the actual presenting order. <sep> 2. In the description of Algorithm 1, the representation of initial values should be x{(-1/2)}_{i} instead of x{(0)}_{i} since line 2 using the term x^{t-1/2}_{i} with the range of t from 0 to T-1. <sep> 3. About ""datacenter setting"" experiment, it seems not an apple to apple comparison between CHOCO-SGD and all-reduce method since CHOCO-SGD stands for the decentralized algorithm with compression and all-reduce stands for a centralized algorithm without compression. It's better to compare with at least one centralized algorithm with a compression scheme (like QSGD[1], signSGD[2], DGC[3]). <sep> 4. Although they compare with the baseline (DCD and ECD) on Cifar-10 dataset,  it's worth to compare with them on the ImageNet since the result may be different under large-scale training. <sep> Overall, this could be a great paper if fixing the issues above. <sep> [1] D. Alistarh, D. Grubic, J. Z. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017. <sep> [2] Bernstein J, Zhao J, Azizzadenesheli K, Anandkumar A. signSGD with majority vote is communication efficient and fault tolerant. arXiv. 2018 Oct 11. <sep> [3] Lin Y, Han S, Mao H, Wang Y, Dally WJ. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887. 2017 Dec 5.","The authors present an algorithm CHOCO-SGD to make use of communication compression in a decentralized setting. This is an interesting problem, and the paper is well-motivated and well-written. On the theoretical side, the authors prove the convergence rate of the algorithm on non-convex smooth functions, which shows a nearly linear speedup. The experimental results on several benchmark datasets validate the algorithm achieves better performance than baselines. These can be made more convincing by comparing with more baselines (including DeepSqueeze and other centralized algorithms with a compression scheme), and on larger datasets. The authors should also clarify results on consensus."
"Summary: <sep> This paper is about developing VAEs in non-Euclidean spaces. Fairly recently, ML researchers have developed non-Euclidean embeddings, initially in hyperbolic space (constant negative curvature), and then in product spaces that have varying curvatures. These ideas were developed for embeddings, and recent attempts have been made to build entire models that operate in non-Euclidean spaces. The authors develop VAEs for the product spaces case. <sep> There's largely two aspects here: one is to be able to write down the equivalents for the operations in models (e.g., the equivalent of adding or multiplying matrices and vectors in Euclidean space have to be lifted to other spaces which no longer have a linear structure). The other are VAE-specific choices, particularly choosing a normal distribution on the manifolds. The authors consider several of these choices and then run a variety of experiments on small latent-dimension cases for VAEs. These reveal that sometimes non-Euclidean and in particular product spaces improve performance. <sep> Strengths, Weakness, Recommendation <sep> I like what the authors are trying to do here; embeddings and discriminative models on non-Euclidean spaces have been developed, offer credible benefits, and generative models are the next step. The authors push forward the machinery needed to do this, and the results seem like there's something there. <sep> On the other hand, the entire work seems quite preliminary. It's hard to say what the takeaway is, or any suggestions for users. The paper is written in a pretty frustrating way. There's an enormous amount of stuff in a sprawling appendix (there are 43 results in the first appendix?!), and checking all of these details will take a great deal of time. <sep> Overall, I recommended weak accept, since a lot of these issues seem like they can be cleaned up. <sep> EDIT: I increased my score based on the authors' response. <sep> Comments: <sep> - The approach taken here is quite similar to another *CONF* submission this year, which basically does the same thing but applies these operations to GCNs instead of VAEs. <sep> - A better way to define curvature is just to talk about the sectional curvature, instead of the Gaussian curvature the authors mention at the beginning of section 2. Fortunately for the constant case all of these definitions will be the same. <sep> - It's not quite clear in Section 2.1 why we should care about the fact that you can't fully take K->0 there---why does this hurt anything? You can approximate flat curvature arbitrarily well even without K exactly 0. <sep> - On a similar theme, what's the point of doing the product of {E,S,D,H,P}, instead of just {E,S,H} or {E,D,P}? Seems a bit weird to consider all 5, given the equivalence between S-D and H-P. <sep> - In 2.3, the products of spaces section, the distance decomposition in the 2nd paragraph should have squares (it's an l2): d_M(x,y)^2 = \\sum_{i=1}^k d_{M_k_i^n_i)^2(x^i,y^i). <sep> - The discussion in 2.3 should be expanded and made more concrete (some of these you can write out the expressions for), and more pros and cons explained, e.g., which theoretical properties are lost for the wrapped distributions? <sep> - On page 6, I don't understand the first problem with the learnable curvature approach. Why is there no gradient w.r.t to K? Isn't the idea that you'll write this thing as a piecewise function (presumably it's continuous, since that's why the authors built those models that deform to flat), and differentiate the whole thing? Why wouldn't there be a gradient at ELBO(K)? Is it not differentiable at K=0? That doesn't follow directly from just saying the curvature is 0. <sep> -  What's the intuition for the component learning algorithm using 2 dimensions for each of the spaces? <sep> - The experiment section was written in a way where I couldn't understand why the choices being made were there. Why 6 and 12 dimensions here? More clarity here would be great. Also, are there any other models to compare against for these datasets? I'm not a VAE expert; what do other models typically obtain in the authors' regime?","This paper studies generalizations of Variational Autoencoders to Non-Euclidean domains, modeled as products of constant curvature Riemannian manifolds. The framework allows to simultaneously learn the latent representations as well as the curvature of the latent domain. <sep> Reviewers were unanimous at highlighting the significance of this work at developing non-Euclidean tools for generative modeling. Despite the somewhat preliminary nature of the empirical evaluation, there was consensus that the paper puts forward interesting tools that might spark future research in this direction. Given those positive assessments, the AC recommends acceptance."
"This paper greatly reduces the gap between binarized and real valued imagenet, using a variety of techniques. The most significant contributions of this paper are engineering based, and the careful combination and integration of approaches from previous papers. I believe that this is of significant practical importance to the field. I particularly appreciate the effort put into developing a very strong baseline that combined ideas from many previous papers! <sep> My biggest concern is that ResNet is itself a very wasteful architecture in terms of compute and parameter count. If the goal is to develop a compute- and memory-efficient architecture, it would be good to also consider real-valued-network baselines that were proposed with computational and/or memory efficiency as a design goal. <sep> Additionally, the specific choices for the new student-teacher loss, and new scaling network architecture, seem fairly ad-hoc. <sep> Detailed comments: <sep> ""this implies a reduction of 32× in memory usage"" , assuming the parameter count is held constant. <sep> Fig 1 right: This is motivated in terms of preserving scaling factors that are lost by the binarization, but the functional form for this makes it look a lot like a learned gating operation. If the sigmoid is dropped from the architecture, does performance worsen? It would be nice to see some discussion of the degree to which this is helpful because it reverses information loss due to binarization, vs. introduces a new architectural feature which is itself helpful. <sep> Add a sentence describing what ""double skip connections"" are. I wasn't familiar with this phrase. <sep> eq. 2: <sep> This functional form is pretty weird. <sep> Why is Q a square norm rather than a norm? Square error on an already-squared property is an unusual choice. <sep> Why is the denominator itself a norm? Taking the norm of a square norm is similarly an unusual choice. (eg, why not just take an average or sum over Q) <sep> Say what Q_s and Q_t are (student and teacher network from context) <sep> ""thus, at test time, these scaling factors will not be fixed but rather inferred from data"" nit: Would not generally call this an inference process. ""Inference"" typically refers to values that are computed indirectly (eg by Bayesian reasoning), while in this case the values are computed directly. Would rather say that scaling factors are a function of data, or are determined by data, or similar. <sep> ""By doing so, more than 1/3 of the remaining gap with the real-valued network is bridged."" text is shifting back and forth between using % and fractional gap to describe benefits. Would just use one measure consistently. <sep> Computational cost analysis: <sep> This is very useful. <sep> Note though that ResNet is a very wasteful architecture in terms of compute! It would be good to include a comparison to imagenet architectures that have computational and memory efficiency as a design goal. (eg, MobileNet comes to mind) <sep> Very nice on the ablation studies.",This paper proposes methodology to train binary neural networks. <sep> The reviewers and authors engaged in a constructive discussion. All the reviewers like the contributions of the paper. <sep> Acceptance is therefore recommended.
"Summary: <sep> In this paper, the author propose a provable pruning method, and also provide a bound for the final pruning error.  Among most heuristics prune method,  pruning with mathematics guarantee is indeed more convincing. We expect this work can help people devoting some effort into more solid theoretical study in understanding the over-parameterized training. <sep> Intuitively speaking, the sensitive neuron has greater contribution for the final output, reusing the corresponding filter and carefully rescale its value require many empirically attempts. To achieve a more reasonable algorithm, author prune the redundant channel by controlling the deviation of the summation statistically small,  and reusing the filter by important sampling the given channel. Experiment show that this method can reach a competitive prune radio against other pruning algorithm, and show robustly in retained parameters vs error experiment. <sep> Weakness: <sep> 1. experiment is too weak <sep> ImageNet model has great impact on most CV problem, and the current release models are flooding in the open source world. Author should at least provide a imagenet model and make this work more convincing. Besides, Author should also consider an experiment in modern lightweight network, vgg and resnet like model are out of fashion and so big that any one can make a sound result on it. <sep> 2. lack of a comparing experiment for random select the top-k norm. <sep> Important sampling require an input of probability [p1, p2, p3, ... pn],  if those probabilities are nearly uniform, important sampling will behave like a random sampling method. In most case, if we want to prune the large channel network,  picking the top-1 significant filter or random sampling top-k filter will almost do the same thing. <sep> 3. lack of further theory consideration author only consider the single layer reconstruction, without discussing the overall accumulative error. Unlike the other deterministic method, sampling skill suffer variance propagation problem, the pre-layer variance will affect the sampling probability of next layer, how this pruning work if we change status of the pre-layer,  I didn't find any theoretical guarantee and only find a proof of single layer reconstruction bound.","This paper presents a sampling-based approach for generating compact CNNs by pruning redundant filters. One advantage of the proposed method is a bound for the final pruning error. <sep> One of the major concerns during review is the experiment design. The original paper lacks the results on real work dataset like ImageNet. Furthermore, the presentation is a little misleading. The authors addressed most of these problems in the revision. <sep> Model compression and purring is a very important field for real world application, hence I choose to accept the paper."
"This paper is under the topic of hierarchical reinforcement learning. The motivation of this paper is ""most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task."" The paper proposes a method to learn higher-level skill selection and lower-level skill improvement jointly. <sep> What I like in this paper: <sep> 1. The paper, in general, is well-written so that I can understand it well. <sep> 2. Experiments are question-driven and provide interesting results. <sep> 3. Theories are closely related to the algorithm. <sep> Key reasons for my rejection: <sep> 1. My biggest concern is the motivation of this paper. <sep> The joint learning of higher-level policy and lower-level skill discovery is not rare in modern literature. Some works are even cited in this paper, for example, option-critic, feudal network, etc. These methods fix their skills in the new task, not because they are inherently not able to do so, but because they want to demonstrate that the learned skills can be reused in new tasks, even if there is no further adaptation. I agree with the author that the agent needs to adapt its skills when faced with new tasks. But I don't think most works are limited in this aspect, as claimed by the paper in the abstract. <sep> 2. I think the author didn't justify his key design choices well. <sep> This paper is under the research area of ""hierarchical reinforcement learning."" However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 ""In the context of HRL, a hierarchical policy with a manager πθh(zt|st) selects every p time-steps one of n sub-policies to execute.""). I would like to see the paper takes the responsibility to justify the reason it follows this particular way. There are two more key decisions the paper proposed but not fully justified and analyzed. <sep> 1. Why is random length a valid choice? The paper doesn't tell readers the consequence of this design choice. For example, what about the optimality of the solution? Since bounding the random length needs prior knowledge, how difficult is it to come up with the prior knowledge. Is the algorithm sensitive to prior knowledge? <sep> 2. Why is it fine to assume ""for each action, there is just one sub-policy that gives it high probability""? What would be the consequence of this assumption? Well, the extreme case is each action is only being chosen by one sub-policy. Therefore, executing the sub-policy becomes executing a repeated sequence of the same action. Obviously, this is a kind of temporal abstraction but is a very limited one. <sep> Other small issues: <sep> Section 2: ""... maximize the discounted expected reward ..."" should be ""... maximize the discounted expected return ..."". <sep> Section 2: the horizon T in the definition in \\eta should be H. <sep> Section 4: the advantage function is not defined.","This paper considers hierarchical reinforcement learning, and specifically the case where the learning and use of lower-level skills should not be decoupled. To this end the paper proposes Hierarchical Proximal Policy Optimization (HiPPO) to jointly learn the different layers of the hierarchy. This is compared against other hierarchical RL schemes on several Mujoco domains. <sep> The reviewers raised three main issues with this paper. The first concerns an excluded baseline, which was included in the rebuttal. The other issues involve the motivation for the paper (in that there exist other methods that try and learn different levels of hierarchy together) and justification for some design choices. These were addressed to some extent in the rebuttal, but I believe this to still be an interesting contribution to the literature, and should be accepted."
"[Post-rebuttal update] <sep> Having read the rebuttals and seen the new draft, the authors have answered a lot of my concerns. I am still unsatisfied about the experimental contribution, but I guess producing a paper full of theory and good experiments is a tall ask. Having also read through the concerns of the other reviews and the rebuttal to them, I have decided to upgrade my review to a 6. <sep> *Paper summary* <sep> The paper combines attention with group equivariance, specifically looking at the p4m group of rotations, translations, and flips. The basic premise is to use a group equivariant CNN of, say, Cohen and Welling (2016), and use self-attention on top. The authors derive a form of self-attention that does not destroy the equivariance property. <sep> *Paper decision* <sep> I have decided that the paper be given a weak reject. The method seems sound and I think this in itself is a great achievement., But the experiments lack focus. Just showing that you get better accuracy results does not actually test why attention helps in an equivariant setting. That said, I feel the lack of clarity in the writing is actually the main drawback. The maths is poorly explained and the technical jargon is quite confusing. I think this can be improved in a camera-ready version or in submission to a later conference, should overall acceptance not be met. <sep> *Supporting arguments* <sep> I enjoyed the motivation and discussion on equivariance from a neuroscientific perspective. This is something I have not seen much of in the recent literature (which is more mathematical in nature) and serves as a refreshing take on the matter. There was a good review of the neuroscientific literature and I felt that the conclusions, which were draw (of approximate equivariance, and learned canonical transformations) were well motivated by these paper. <sep> The paper is well structured. That said, I found the clarity of the technical language at times quite difficult to follow because terms were not defined. By way of example, I still have trouble understanding terms like ""co-occurence"" or ""dynamically learn"". In the co-occurence envelope hypothesis, for instance, what does it mean for a learned feature representation to be ""optimal in the set of transformations that co-occur"". Against what metric exactly would a representation be optimal? This is not defined. <sep> That said, I feel that the content and conclusions of the paper are technically sound, having followed the maths, because the text was too confusing. <sep> *Questions/notes for the authors* <sep> - I would like to know whether the co-occurence envelope hypothesis is the authors' own contribution. This was not apparent to me from the text. <sep> - I'm not sure what exactly the co-occurence envelope is. It does not seem to be defined very precisely. What is it in layman's terms? <sep> - I found the section ""Identifying the co-occurence envelope"" very confusing. I'm not sure what the authors are trying to explain here. Is it that a good feature representation of a face would use the *relevant* offsets/rotations/etc. of visual features from different parts of the face, independent of global rotation? <sep> - Is Figure 1 supposed to be blurry? <sep> - At the end of paragraph 1 you have written: sdfgsdfg asdfasdf. Please delete this. <sep> - I believe equation 4 is a roto-translational convolution since it is equivariant to rotation *and translation*. Furthermore, it is not exactly equivariant due to the fact that you are defining input on a 2D square grid, but that is a minor detail in the context of this work. <sep> - Now that we have automatic differentiation, is the section on how to work out the gradients in Equations 5-7 really necessary? <sep> - In equation 8 (second equality), you have said f_R^l(F^l) = A(f_R^l(F^l)). How can this be true if A is not the identity? Giving the benefit of the doubt, this could just be a typo. <sep> - Please define \\odot (I think it's element-wise multiplication). <sep> - Are you using row-vector convention? That would resolve some of my confusion with the maths. <sep> - You define the matrix A as in the space [0,1]^{n x m}. While sort of true, it is more precise to note that each column is actually restricted to a simplex, so A lives in a subspace of [0,1]^{n x m}. <sep> - I think it would have been easier just say that you are using a roto-translation or p4m equivariant CNN with attention after each convolution. Then you could derive the constraint on the attention matrices to maintain equivariance. It would be easier to follow and make easy connections with existing literature on the topic.","The paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co-occurring features. It instantiates the approach with rotation and reflection transformations, and reports results on rotated MNIST and CIFAR-10. All reviewers have found the idea of using self-attention on top of equivariant feature maps technically novel and sound. There were some concerns about readability which the authors should try to address in the final version."
"After reading all the reviews and the comments, I feel more positive about the paper. I appreciate the feedback of the Authors and I have decided to increase the rating. <sep> ============================ <sep> The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection. The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D). The projection of the latent space then goes to a decoder that reconstructs the input. <sep> A transformation matrix A is trained jointly with the autoencoder. Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector. <sep> The paper claims to generalize the existing RSR framework to the nonlinear case. However, the linear RSR is applied to the latent space of the autoencoder. In addition to that, all the following discussion and proofs are limited to the linear case. <sep> Since the proposed method is using RSB as it's core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups). However, there is no such comparison. <sep> Since autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer. <sep> The matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur: <sep> - The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn't cause data loss. <sep> - The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible. <sep> It is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities. <sep> My point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. <sep> Some citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN 'Robust, Deep and Inductive Anomaly Detection' ECML 2017;   'Adversarially Learned One-Class Classifier for Novelty Detection' CVPR 2017; DSVDD 'Deep one-class classification.' ICML, 2018; ODIN  'Enhancing The Reliability  Of Out-of-distribution Image Detection  In Neural Networks' *CONF* 2018; 'Generative Probabilistic Novelty Detection with Adversarial Autoencoders' NeurIPS 2018.","Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission."
"This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. <sep> This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically: <sep> - How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities?  In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty. <sep> - What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? <sep> - An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldn't that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice. <sep> - For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is ""correct"". How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative. <sep> - For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network). <sep> - What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did ""B"" correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive? <sep> - How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. <sep> Other comments <sep> - It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2]. <sep> - In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters. <sep> - What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the \\hat{sigma}^2(x)). <sep> - I believe that a comparison against a simple variationally trained BNN would make the results more convincing. <sep> Misc <sep> - Second page, ""Figure 1, top two plota"" -> ""Figure 1, top two plots"" <sep> - Third page, ""[…] introduced in equation 2 denotes the posterior covariance [….]"" -> ""[…] introduced in equation 2 denotes the posterior variance […]"" <sep> - Fifth page, ""[…] this makes it is reasonable for W large enough […]"" -> ""[…] this makes it reasonable for W large enough […]"" <sep> - Sixth page, ""Corollary 1 and proposition 2""; where is corollary 1? Do you mean Proposition 1? <sep> - Seventh page, ""[…] inspired by, an builds on, […]"" -> ""inspired by, and builds on, […]"" <sep> - Ninth page ""montonicity"" -> ""monotonicity"" <sep> Overall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly. <sep> [1] Eric Nalisnick, José Miguel Hernández-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019 <sep> [2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016 <sep> [3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014 <sep> [4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014","The paper provides theoretical justification for a previously proposed method for uncertainty estimation based on sampling from a prior distribution (Osband et al., Burda et al.). <sep> The reviewers initially raised concerns about significance, clarity and experimental evaluation, but the author rebuttal addressed most of these concerns. <sep> In the end, all the reviewers agreed that the paper deserves to be accepted."
"[Summary] <sep> This paper proposes to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. The method introduces a predictor-corrector scheme, which employs a hierarchical structure that temporally divides the problem into more manageable subproblems, and uses models specialized in different time scales to solve the subproblems recursively. <sep> For dividing the problem into subproblems, they use an observation predictor network to predict the optimal center point between two states. To scale the scheme to sequences of arbitrary length, the number of models scales with O(log N). For each subproblem, the authors propose to use a corrector network to estimate the control force to follow the planned trajectory as close as possible. <sep> They have compared their method with several baselines and demonstrated that the proposed approach is both more effective and efficient in several challenging PDEs, including the incompressible Navier-Stokes equations. <sep> [Major Comments] <sep> Predicting the middle point between two states for modeling the dynamics via deep neural networks is not new, but I did not know any other works that use this idea for controlling PDEs. <sep> I like the idea of splitting the control problem into a prediction and a correction phase, which leverages the power of deep neural networks and also incorporates our understanding of physics. The introduction of the hierarchical structure alleviates the problem of accumulating error in single-step forwarding models and significantly improves the efficiency of the proposed method. The videos for fluid control in the supplement materials also convincingly demonstrate the effectiveness of the technique. <sep> I still have a few questions regarding the applicability and the presentation of the paper. Please see the following detailed comments. <sep> [Detailed Comments] <sep> In Section 3, the authors claim that their model ""is conditioned only on these observables"" and ""does not have access to the full state."" However, the model requires a differentiable PDE solver to provide the gradient of how interactions affect the outcome. These seem to contradict each other. Doesn't the solver require full-state information to predict the behavior of the system? <sep> Related to the previous question, how can we make use of the differentiable PDE solver if we are uncertain or unknown of the underlying physics, i.e., partially observable scenarios. <sep> The algorithm described in Section 5 seems to be the core contribution of this work. Instead of describing the algorithm in words, I think it would make it more clear if the authors can add an algorithm block in the main paper. It would also be better if the authors can include a few sentences describing the algorithm in the abstract to inform the readers of what to expect. <sep> Figure 4 is a bit confusing, and it would be better if the authors can include the label for the x-axis. Besides, in the caption, the authors said that they show ""the target state in blue."" However, there are a lot of blue lines in the figure, and it is hard to know, at first glance, which one of them is the target. <sep> In Table 1, the bottom two methods are using the same execution scheme and training loss, but the results are different. Is there a typo? Also, it would be better to bold the number that has the best performance.","The paper proposes a method to control dynamical systems described by a partial differential equations (PDE). The method uses a hierarchical predictor-corrector scheme that divides the problem into smaller and simpler temporal subproblems. They illustrate the performance of their method on 1D Burger's PDE and 2D incompressible flow. <sep> The reviewers are all positive about this paper and find it well-written and potentially impactful. Hence, I recommend acceptance of this paper."
"Update: I thank the authors for their response and I will maintain my score, my main hesitation being the overall clarity and readability of the paper. <sep> Summary: <sep> This paper proposes the use of two intrinsic rewards for exploration in MARL settings. The first one is an information-theoretic influence (EITI) bonus and a decision-theoretic influence (EDTI)  bonus. EITI uses mutual information to capture the influence of one agent on the transition dynamics of others,  while EDTI uses an intrinsic reward called Value of Interaction (VoI) to quantify the influence of one agent's behavior on expected returns of other agents. <sep> Main Comments: <sep> Overall, I think this paper would be a good contribution for *CONF* 2020 and I lean towards accepting it. The experimental section is thorough, the authors include relevant ablations, baselines and popular algorithms used in MARL settings. The use of the decision-theoretic influence is novel as far as I can tell and it also seems to be quite effective on the tasks used for evaluation. Although the method uses a series of approximations and assumptions, I believe most of them are clearly stated and fairly well-motivated (plus they are not very far from those of other recent work in the deep MARL literature). I also appreciated the fact that the authors explicitly derived the main mathematical results used in the paper. <sep> I only have some minor comments and questions regarding some assumptions and notation. <sep> I  also encourage the authors to proof-read the paper as some parts of it are a bit hard to follow. I would very much like to see a more an edited version of this paper with more precise language. <sep> Can you discuss in more detail the difference between EITI and the intrinsic reward based on social influence used in Jacques et al. (2018)? They seem to be quite similar conceptually and the related work part related to this is rather vague. Please clarify the distinction. <sep> Minor Questions / Comments: <sep> There are a few typos throughout the paper: <sep> 1. On page 3 after equation (3), I believe part of the sentence that should describe the I term in the equation is missing. <sep> 2. The phrase right after equation (16) which defines the EDTI reward does not seems to not match the  above expression. Can you please explain why the transition would be conditioned on the influence term? While reviewing, I've been assuming this was just a mistake in writing but please double check and clarify. <sep> 3. On page 4, after equation (8), you refer to  a_1, a_1 and s_2' which do not appear in the above equation. Can you please use the same notation or motivate your choice for referring to those variables instead?","The paper presents a new take on exploration in multi-agent reinforcement learning settings, and presents two approaches, one motivated by information theoretic, the other by decision theoretic influence on other agents. Reviewers consider the proposed approach ""pretty elegant, and in a sense seem fundamental"", the experimental section ""thorough"", and expect the work to ""encourage future work to explore more problems in this area"". Several questions were raised, especially regarding related work, comparison to single agent exploration approaches, and several clarifying questions. These were largely addressed by the authors, resulting in a strong submission with valuable contributions."
"This paper presents a semantic parser that operates over passages of text instead of a structured data source.  This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).  The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.  This is excellent work, and it should definitely be accepted.  I have a ton of questions about this method, but they are good questions.  The rest of this review focuses on things that I thought could be more clear, or that raise new questions, and might sound negative.  Please understand them, however, in terms of my overall score and what I said above. <sep> The three claimed contributions are (1) better numbers, (2) better compositionality / domain applicability, and (3) better interpretability. <sep> (2) and (3) sound a bit like overclaiming in the introduction to me, as there isn't a whole lot of nested composition in the language used by NeRd, and the BERT calculator in principle is almost as compositional and interpretable (also, e.g., NAQANet can add and subtract an arbitrary number of numbers, also, and it tells you which ones they are, just as NeRd does).  Later in the paper the specifics of those claims are made more clear, and while they are justified, they are very narrow claims.  To me, someone who is intimately familiar with this research area, the key contributions (the things that I learned) are (1) using passage-span and key-value predicates actually works, (2) how much difference hard EM and thresholding make, and (3) the data augmentation in this work is pretty clever.  (2) was intuitively clear to me after seeing Dasigi's iterative search paper and Min's hard EM paper, but the difference in results presented here is pretty striking. <sep> Compositionality: <sep> The authors claim that their method is compositional and domain agnostic, while all previous methods had hand-crafted modules for specific question types.  However, I see little reason to believe there's much of a difference here.  You also defined operations that are tailored to the dataset, and are basically identical to the operations that others have used.  I see no evidence that NeRd actually generalizes to program types that are beyond what is captured by other methods.  It's possible that this happens, but there is no evaluation that discusses this, and from all of the examples I'm led to believe that this is basically also just learning a few program templates, the same ones learned by previous methods.  With the weak supervision that you have, are you actually able to find more complex programs during your search?  Some kind of demonstration of actual compositionality on the more complex questions in DROP would make a very strong argument for your claims; without that, they ring a little hollow. <sep> Interpretability: <sep> The use of passage-span as a predicate is really interesting, and it raises a lot of questions.  This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser.  For example, your first example in table 2 ostensibly requires filtering the numbers in the passage to those that are percentages associated with groups, then filtering them again to those where the percentage is larger than 16, then returning the associated groups. But your method jumps straight to returning a set of passage spans.  This is hardly interpretable.  (In fairness, no prior method provides interpretable reasoning for this kind of operation either.)  But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP).  But how does the network decide which to do?  Any argmax or max question, and many count questions, could be answered by passage-span alone.  With only weak supervision, and with the parser having the ability to shortcut these more interpretable operations, how often are you actually getting the interpretable one, and what's causing the model to choose it? <sep> Similarly, how often does an argmax or a max operation actually operate on the full set that you would expect it to?  Or does it just do the argmax internal to the network, and output only one item as an argument to the argmax?  If the later, this again hurts your claims of better interpretability over prior methods, as the logic is just as opaque as before.  This also seems like a really hard search problem in how you've set up your DSL - what would make your search over programs actually select all of the correct arguments?  Because you're selecting passage spans directly instead of performing some kind of matching operation, you have to have your search select all of the appropriate spans for this to be ""interpretable"", and not just hiding the logic inside of the network.  But that seems like a totally intractable search.  You found a clever way to get around this for count questions (even though that still implicitly hides a bunch of filtering logic, as noted above), but I don't know how to make it work for maxes and argmaxes. <sep> Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality.  At one extreme, you end up with something like NABERT (or even less compositional), where basically everything is inside the network.  At the other extreme, where you don't have passage-span, you are left with a crippled semantic parser that can't handle most of the questions.  But using the predicate introduces tension in the model between interpretability and flexibility.  How do we resolve this tension?  (This isn't something I expect your paper to address, it's just a really interesting and important question raised by your work.) <sep> Parser: <sep> Prior work has found benefit in using runtime constraints on parser outputs, or grammar-based decoding.  It looks like you are doing neither of those, yet you're able to output specific token indices and number indices in your programs.  Are you really not doing anything special to handle those?  How does the decoder know token indices?  I feel like something must be missing here, or a simple LSTM decoder is more magical than I thought. <sep> Evaluation: <sep> Why only show results on DROP dev, and not on the test set?  It's possible that your higher numbers are because you were better able to overfit to the dev set, which you presumably used during training.  (I don't think that that's likely, but it's a concern that would be easily avoided by evaluating on test.)","Main content: <sep> Blind review #1 summarizes it well: <sep> This paper presents a semantic parser that operates over passages of text instead of a structured data source. This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different). The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. This is excellent work, and it should definitely be accepted. I have a ton of questions about this method, but they are good questions. <sep> -- <sep> Discussion: <sep> The reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems. <sep> -- <sep> Recommendation and justification: <sep> This paper should be accepted. Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of *CONF*)."
"Thank you for your revision and for the rebuttal. This a  strong submission with insightful angle on natural gradients and with provable guarantees. The authors improved a lot the manuscript and incorporated reviewers feedback. I am increasing my score to 8. <sep> ### <sep> Summary of the paper: <sep> The paper provides a way to estimate the natural Wasserstein gradient using Kernel estimators. The idea is neat and novel. Natural Wasserstein Gradient similar to the so called natural fisher gradients preconditions the gradient using a matrix that uses the local curvature of the manifold of the parametric distribution. <sep> Authors give variational forms of the Fisher information matrix of an explicit model , using  the variational form of the chi squared or the Fisher Rao divergence. Similarly authors give a variational form of the wasserstein natural gradient . Let theta be the parameter of the parametric implicit model, theta in R^q.  For a descent direction u, the variational form is obtained via finding an objective S,  supf∈C∞cS(f,u)=u⊤GWu, where GW is a form of  ""Wasserstein information matrix"". <sep> Authors then propose to learn the function f in an RKHS and propose to find the descent direction by solving minu<u,∇θLoss(pθ)>+supf∈RKHSS(f,u)+r(u)−λ||f||rkhs2 <sep> where r(u) is a quadratic regularizer on u. <sep> The sup problem has a closed form solution and can be approximated  using Nystrom approximation and randomization on dimensions. The problem in u has also a closed form solution , and one used u as the proxy to the natural  descent. <sep> Authors under some assumption  show that the  estimated natural W gradient in RKHS  is concentrated around the true one. <sep> Experiments on synthetic data and in classification on CIFAR 10 and CIFAR 100 shows that the preconditioning of the gradients that the method offers allows faster convergence in both well conditioned and ill conditioned initialization of the weights of the neural network. <sep> Review : <sep> The paper is not easy to follow and the high level intuition how the method works is not well explained. <sep> It would be easier for the reader, to motivate the natural wasserstein descent from how one defines natural Fisher descent , where one seeks a first order approximation of KL(pθ,pθ+ϵu) as we perturb in the parameter space and this well known that this epsilon u⊤Fu. <sep> Hence natural Gradient descent is : <sep> minu<u,∇θLoss(pθ)>+KL(pθ,pθ+u)≈minu<u,∇θLoss(pθ)>+u⊤Fu <sep> Now for the wasserstein distance one has also similarly: <sep> minu<u,∇θLoss(pθ)>+W22(pθ,pθ+u) <sep> and it is known that as epsilon goes to zero we have: <sep> W22(pθ,pθ+ϵu)/ϵ=||pθ−pθ+ϵu||H−1(pθ)2+o(ϵ)=supf∫f(pθ−pθ+ϵu)−12Epθ||∇xf(x)||2+o(ϵ) <sep> Now replacing with the implicit model as epsilon goes to zero we get the expression given in the paper using a simple taylor expansion: <sep> =supf∫<∇θhθ⊤∇xf(hθ),u>dν−12Epθ||∇xf(x)||2 <sep> in a sense the paper is proposing to linearize W22 around the perturbation in the parameter space of the implicit model and this can be done using  ||.||H−1(q) , as pointed and used in many recent works.  then the paper proposes to approximate ||.||H−1(q) in RKHS which was already proposed in Mroueh et al  in Sobolev Descent.  AISTATS 2019. <sep> We encourage the authors to layout in the beginning the derivations form this point of view which will make the paper easier to digest, the expression in Equation 7 seems mysterious and pulled out of a hat, but it is easier to understand by going to perturbation analysis usually done on KL for Fisher Natural gradient and to do it also here starting from the linearization of W2 with ||.||H−1(q)  , and how to approximate it in RKHS as it was already proposed in the literature in Mroueh et al Sobolev Descent. <sep> I read carefully the proofs of Proposition 1, 2, 3. I did not ready full the proofs of the concentration of the estimator , but they seem sensible as they follow usual bounding strategies in this context. <sep> Questions: <sep> - There is nothing special about the wasserstein natural gradient flow variational form and implicit model, once can apply the same to the variational form of Fisher, that would be probably more efficient? It would be great to baseline this one ? <sep> -the constraint ∫f(x)pθ(x)=0 is not imposed in the kernelized version? <sep> - the method comes disappointing since it seems that the preconditioning that the Wasserstein gradient gives is not enough and r(u)=u⊤Du is need where D is diagonal depends on T. Have you tried with D=Identity? it might be that the scaling of the gradients is coming only from that D−1? <sep> - Can you give timings for computing each gradient update and how it compares to regular SGD or diagonal approximation of Fisher natural gradient? <sep> - Does one need preconditioned gradient if the network was self normalized (like batch norm or spectral norm etc)? <sep> Overall assessment: <sep> That is a good theoretical work with provable guarantees. The computational complexity of each gradient estimate is large which makes the method not quite appealing in practice.","This is a very interesting paper which extends natural gradient to output space metrics other than the Fisher-Rao metric (which is motivated by approximating KL divergence). It includes substantial mathematical and algorithmic insight. The method is shown to outperform various other optimizers on a neural net optimization problem that's artificially made ill-conditioned; while it's not clear how practically meaningful this setting is, it seems like a good way to study optimization. I think this paper will be of interest to a lot of researchers and could open up new research directions, so I recommend acceptance as an Oral."
"Summary - The paper addresses the problem of hierarchical explanations in deep models that handle compositional semantics of words and phrases. The paper first highlights desirable properties for importance attribution scores in hierarchical explanations, specifically, non-additivity and context independence, and shows how prior work on additive feature attribution and context decomposition doesn't accurately capture these notions. After highlighting the said properties in context of related work, the authors propose an approach to calculate the context-independent importance of a phrase by computing the difference in scores with and without masking out the phrase marginalized over all possible surrounding word contexts (approximated by sampling surrounding context for a fixed radius under a language model). Furthermore, based on the above, the authors propose two more score attribution approaches -- based on integrating the above sampling step with (1) the contextual decomposition pipeline and (2) the input occlusion pipeline. Experimentally, the authors find that the attribution scores assigned by the proposed approach are more correlated with human annotations compared to prior approaches and additionally, the generated explanations turn out to be more trustworthy when humans evaluate their quality. <sep> Strengths <sep> - The paper is well-written and generally easy to follow. The authors do a good job of motivating and highlighting the desired properties of importance attribution scores and developing the proposed scoring mechanism. The proposed scoring mechanism ties in seamlessly with the existing contextual decomposition and occlusion pipelines and leads to improved performance when the generated explanations are evaluated. <sep> - The proposed approach involving masking out the phrase and marginalizing over possible surrounding word-concepts is novel and offers an interesting perspective on how to approach context independent scoring of phrases -- (1) phrases don't exist independent of the surrounding context and therefore marginalizing over all possible surrounding concepts makes sense and (2) replacing the intractable enumeration over all possible surrounding concepts with samples from a language model makes the score attribution process faster and more scalable modulo the learnt language model. <sep> - Sec. 4.4 offers interesting insights. I like that the authors performed this ablation given that the expectation over surrounding contexts is computed approximately via samples under a language model. There's a clear increase in terms of the attribution scores as the number of samples increases and the neighborhood size is increased. It is interesting to note that there is an approaching plateau region where increasing the neighborhood size won't affect the assigned scores. This experiment provides a holistic picture of the behavior of the interpretability toolkit (manifesting in terms of attribution scores) given the approximations involved. I would encourage the authors to flesh this out even more. <sep> Weaknesses <sep> Having said that, there are some minor comments that I'd like to point out / get the authors' opinion on. Highlighting these below: <sep> - While SOC and SCD don't always end up outperforming other approaches (specifically Statistic) on the SST-2, Yelp and TACRED datasets (Table. 1), for the human evaluation experiments, the authors only compare with CD, Direct Feed, ACD and GradSHAP. Do the authors have any insights on how well does Statistic perform on the human-evaluation set of experiments? <sep> - While inspiring trust in users is one aspect of evaluating explanations via humans, it's slightly unclear what 'trust' in this sense inherently identifies. Although, it might implicitly capture some notion of reliability (and predictability of the explanations by humans), asking users to rank explanations across a spectrum of 'best' to 'worst' doesn't explicitly capture that. Another possible aspect to look into could be -- ''Do the generated explanations help humans predict the output of the model?'' This captures reliability in a very explicit sense. Do the authors have any thoughts on this and potential experiments that might address this? I don't think not addressing this is necessarily detrimental to the paper but I'm curious to hear the thoughts of the authors on the same. <sep> Reasons for rating <sep> Beyond the above points of discussion, I don't have major weaknesses to point  out. I generally like the paper. The authors do a good job of identifying the sliver in which they make their contribution and motivate the same appropriately. The proposed phrase attribution scoring mechanism is motivated from a novel perspective and has a reasonable approximation characterized appropriately by the ablations performed. The strengths and weaknesses highlighted above form the basis of my rating.","The authors present a hierarchical explanation model for understanding the underlying representations produced by LSTMs and Transformers. Using human evaluation, they find that their explanations are better, which could lead to better trust of these opaque models. <sep> The reviewers raised some issues with the derivations, but the author response addressed most of these."
"Summary <sep> The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation). The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity. The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task. <sep> Review <sep> The overall quality of the paper is high. The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure. Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures. <sep> The proposed approach is novel. It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term. Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix. Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal. This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods. However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work. <sep> The paper is generally easy to follow. Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices. The experimental pipeline is also clear. <sep> Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines. Providing training plots might increase the quality of the paper. <sep> The experimental results are convincing. First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate. Then, the authors evaluate K-matrices on ImageNet dataset. In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer. The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix. Next, the authors show that K-matrices can be used to train permutations in image classification domains. In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset. With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10. Finally, the authors demonstrate that K-matrices can be used instead of the decoder's linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score. <sep> Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations). <sep> Improvements <sep> 1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis. <sep> 2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.",The paper generalizes several existing results for structured linear transformations in the form of K-matrices. This is an excellent paper and all reviewers confirmed that.
"The paper introduces CATER: a synthetically generated dataset for video understanding tasks. The dataset is an extension of CLEVR using simple motions of primitive 3D objects to produce videos of primitive actions (e.g. pick and place a cube), compositional actions (e.g. ""cone is rotated during the sliding of the sphere""), and finally a 3D object localization tasks (i.e. where is the ""snitch"" object at the end of the video).  The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. <sep> A variety of models from recent work are evaluated on the three proposed tasks, demonstrating the validity of the above motivation for the construction of the dataset.  The primitive action classification task is ""solved"" by nearly all methods and only serves for debugging purposes.  The compositional action classification task is harder and shows that incorporating LSTMs for temporal reasoning leads to non-trivial performance improvements over frame averaging.  Finally, the localization task is challenging, especially when camera motion is introduced, with much space for improvement left for future work. <sep> I am positive with respect to acceptance of this paper.  It is a well-argued, thoughtful dataset contribution that sets up a reasonable video understanding dataset.  The authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with real-world data, but still it can serve a useful and complementary role similar to the one CLEVR has served in image understanding. <sep> I have a few minor comments / questions / editing notes that would be good to address: <sep> - The random baseline isn't described in the main text, it would be good to briefly mention it (this will also help to clarify why the value is particularly high for tasks 1 and 2) <sep> - The grid resolution ablation results presented in the supplement are actually quite important -- they demonstrate that with a small increase in granularity of the grid the traditional tracking methods begin to be the best performers. As this direction (of increased resolution to make the problem less artificial) is likely to be important, a brief discussion of this finding from the main paper text would be appropriate <sep> - p3 resiliance -> resilience <sep> - p4 objects is moved -> object is moved <sep> - p6 actions itself -> actions themselves; builds upon -> build upon <sep> - p7 looses all -> loses all; suited our -> suited to our; render's camera parameters -> render camera parameters; to solve it -> to solve the problem <sep> - p8 (Xiong, b;a) and (Xiong, b) -> these references are missing the year; models needs to -> models need to <sep> - p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world","The paper proposed a new synthetically generated video dataset (CATER) for benchmarking temporal reasoning. The dataset is based on the CLEVR dataset and provides videos make up of primitive actions (""rotate"", ""pick-place"", ""slide"", ""contain"") that can be combined to form for complex actions. <sep> The paper also benchmarks a variety of methods on three proposed tasks (atomic action classification, composite action classification, and 'snitch' localization) and demonstrates that while it is possible to get high performance on atomic action classification, the other two task are still challenging and requires temporal modeling. <sep> Overall, all reviewers found the paper to be well written and easy to follow, with care given to the dataset construction, as well as the task definitions and experiment setup and analysis. The paper received strong scores from all reviewers (3 accepts). Based on the reviewer comments, the authors further improved the paper by adding additional relevant datasets for comparison and providing missing details pointed out by the reviewers. After the rebuttal, the reviewers remained positive."
"RNA Secondary Structure Prediction by Learning Unrolled Algorithms <sep> This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima). The former does not work for all molecules and the latter can be difficult to optimize. The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community. <sep> The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. These constraints limit the wide RNA search space. The first component of the method is a ""Deep Score Network"" which uses a stack of Transformer encoders (with relative and exact positional embeddings) followed by 2D convolutional layers to output a L x L symmetric matrix describing the ""scores"" of base pairing. As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the ""Deep Score Network"" to enforce constraints. This network starts with a transformation that symmetrizes the matrix and applies a constraint-enforcing mask. The problem is transformed into an unconstrained problem by using Lagrange multipliers; it is then solved using a proximal gradient. Finally, a recurrent cell is defined that implements this algorithm in a deep learning framework. This method is creative, could be applied to other tasks with constraints, and would be interesting to the wider deep learning community. <sep> In addition to developing the deep score network and post-processing network, the authors also develop a differentiable F1 loss, so that the network can directly optimize for precision and recall on the task. The performance of this method significantly outperforms previous methods. There was a fruitful discussion on OpenReview regarding whether this was a result of overfitting on the task. Indeed, it is critical in deep learning applications to carefully construct train/test sets to avoid high performance by memorization alone. To address this, the authors train on RNAStralign and test on ArchiveII. As the original ArchiveII dataset contains subsequences of other RNA sequences, which can result in overfitting, the authors re-ran their experiment with that removed, and similar results were achieved. To support the hypothesis that ArchiveII and RNAStralign capture different distributions, they perform a permutation test on the unbiased empirical Maximum Mean Discrepancy estimator, finding that the distributions are different. I do wonder why they did not check if P(ArchiveII) = P(ArchiveII) as they do check if P(RNAStr_train) = P(RNAStr_train). On the specific task of pseudoknot prediction, the method also performs well (F1 is >0.23 over the baseline). On sequence length-weighted F1, the model does even better. <sep> The paper is rich with ablations. The analysis of the number of unrolling iterations T helps support the use of an unrolled method and builds intuition for its importance - it would be useful to include this in the appendix of the paper. I also appreciated the visualizations, which are a good sanity check that the model correctly handles pseudoknots. The performance of the method is broken down by RNA family, which is also quite interesting -- the method outperforms LinearFold on all classes, besides 5S RNA, SRP, and Group I intron. Further analysis is required to better understand why the method is weaker on those datasets. Additionally, further work should explore training on one set of families and testing on a held-out set of families. This was pointed out by public comments on this paper. This is potentially a limitation of E2EFold (the authors do not seem to have tried this suggested experiment) and further exploration is required. Exploring this limitation (even if it is not overcome) would make this paper even more rich. <sep> That said, I recommend acceptance of this paper due to the extensive experiments, polished writing, novel method, and strong results, which can inspire future research.","This paper proposes a RNA structure prediction algorithm based on an unrolled inference algorithm. The proposed approach overcomes limitations of previous methods, such as dynamic programming (which does not work for molecular configurations that do not factorize), or energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima). <sep> Reviewers agreed that the method presented here is novel on this application domain, has excellent empirical evaluation setup with strong numerical results, and has the potential to be of interest to the wider deep learning community. The AC shares these views and recommends an enthusiastic acceptance."
"Summary <sep> --- <sep> (motivation) <sep> Lots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN. <sep> These methods produce scores that highlight regions that are in a vague sense ""important."" <sep> While that's useful (relative importance is interesting), the scores don't mean anything by themselves. <sep> This paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits. <sep> Non-highlighted regions contribute 0 bits of information to the task, so they are clearly irrelevant in the common sense that they have 0 mutual information with the correct output. <sep> (approach - attribution methods) <sep> An information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output. <sep> In particular, Z is a convex combination of the feature map (e.g., conv2) with Gaussian noise with the same mean and variance as that feature map. <sep> The weights of the combination are found so they minimize the information shared between the input and Z and maxmimize information shared between Z and the task output Y. <sep> These weights are either optimized on <sep> 1) a per-image basis (Per-Sample) or <sep> 2) predicted by a model trained on the entire dataset (Readout). <sep> (approach - evaluation) <sep> The paper uses 3 metrics with differing degrees of novelty: <sep> 1) The bbox metric rewards attribution methods that put a lot of mass in ground truth bounding boxes. <sep> 2) The original Sensitivity-n metric from (Ancona et al. 2017) is reported with a version that uses 8x8 occlusions. <sep> 3) Least relevant image degredation is compared to most relevant image degredation (e.g., from (Ancona et al. 2017)) to form a new occlusion style metric. <sep> (experiments) <sep> Experiments consider many of the most popular baselines, including Occlusion, Gradients, SmoothGrad, Integrated Gradients, GuidedBP, LRP, Grad-CAM, and Pattern Attribution. They show: <sep> 1) Qualitatively, the visualizations highlight only regions that seem relevant. <sep> 2) Both Per-Sample and Readout approaches put higher confidence into ground truth bounding boxes than all other baselines. <sep> 3) Both Per-Sample and Readout approaches outperform all baselines almost all the time according to the new image degredation metric. <sep> Strengths <sep> --- <sep> The idea makes a lot of sense. I think heat maps are often thought of in terms of the colloquial sense of information, so it makes sense to formalize that intuition. <sep> The related work section is very well done. The first paragraph is particularly good because it gives not just a fairly comprehensive view of attribution methods, but also because it efficiently describes how they all work. <sep> The results show that proposed approaches clearly outperform many strong baselines across different metrics most of the time. <sep> Weaknesses <sep> --- <sep> * I'm not sure why the new degredation metric is a useful addition. What does it add that MoRF and LeRF don't capture on their own independently? <sep> * I think [1] would be a nice addition to the evaluation section as it tests for something qualitatively different than the various metrics from section 4. It would also be a good addition to the related work. <sep> Missing Details / Points of Confusion <sep> --- <sep> * I think there's an extra p(x) in eq. 11 in appendix D. <sep> * I think the variable X is overloaded. In eq. 1 it refers to the input (e.g., the pixels of an image) while in eq. 2 it refers to an intermediate feature map (e.g., conv2) even though it later seems to refer to the input again (e.g., eq. 3). Different notation should be used for intermediate feature maps and inputs. <sep> Presentation Weaknesses <sep> --- <sep> * In section 3.1 is lambda meant to be constrained in the range [0, 1]? This is only mentioned later (section 3.2) and should probably be mentioned when lambda is introduced. <sep> * ""indicating that all negative evidence was removed."" I think this should read ""indicating that only negative evidence was removed."" <sep> Suggestions <sep> --- <sep> ""The bottleneck is inserted into an early layer to ensure that the information in the network is still local"" <sep> I'd like this to be explored a bit more. Though deeper feature maps are certainly more spatially coarse they still might be somewhat ""local"". To what degree to they loose localization information? My equally vague alternative intuition goes a bit differently: The amount of relevant information flowing through any spatial location seems like it shouldn't change that much, only the way its represented should change. If the proposed visualizations were the same for every choice of layer then it would confirm this intuition. That would also be an interesting result because most if not all of the cited baseline approaches (where applicable) produce qualitatively different attributions at different layers (e.g., see Grad-CAM). <sep> [1]: Adebayo, Julius et al. ""Sanity Checks for Saliency Maps."" NeurIPS (2018). <sep> Preliminary Evaluation <sep> --- <sep> Clarity: The paper is clearly written. <sep> Originality: The idea of using the formal notion of information in attribution maps is novel, as is the bbox metric. <sep> Significance: This method could be quite significant. I can see it becoming an important method to compare to. <sep> Quality: The idea is sound and the evaluation is strong. <sep> This is a very nice paper in all the ways listed above and it should be accepted! <sep> Post-rebuttal comments <sep> --- <sep> The author responses and other reviews have only increased my confidence that this paper should be accepted.","All three reviewers strongly recommend accepting this paper. It is clear, novel, and a significant contribution to the field. Please take their suggestions into account in a camera ready version. Thanks!"
"Summary: <sep> The authors propose to learn reusable options to make use of prior information and claim to do so with minimal information from the user (such as # of options needed to solve the task, which options etc). The claim is that the agent is first able to learn a near-optimal policy for a small # of problems and then is able to solve a large # of tasks by such a learned policy.  The authors build on the idea that minimizing the number of decisions made by the agent results in discovering reusable options. The options are learned offline by learning to solve a small number of tasks. Their algorithm introduces one option at a time until introducing a new option doesn't improve the objective further. The ideas are interesting, However, the paper as it stands is lacking in thorough evaluation. <sep> Detailed comments: <sep> The proposed approach offers two key contributions: <sep> -an objective function to minimize the decision states <sep> -incrementally constructing an option set that can be reused later, without the a priori specification of the # of options needed. <sep> The introduction is well written, however, given the intuitions behind the objective function; in some sense, the idea here is to minimize the decisions or terminations intuitively relates to terminating only at critical or bottleneck states. It would be useful to provide such motivation in the introduction. <sep> Intuitively the objective criterion is interesting. With a cursory look at the proofs, they seem fine, although I have to admit I have not looked in detail into the proofs. <sep> Paper writing could be significantly improved. Several points are not clear and need further clarification: <sep> -The term near-optimal is mentioned several times, but it is not clear the policies are near-optimal with respect to what? The task or a set of tasks? <sep> -How does the proposed approach ensures that they are near-optimal? Please clarify. <sep> -""We can obtain the estimate for equation 1 by averaging over a set of near-optimal trajectories"" The aim as states is to learn options that are capable of generating near-optimal trajectories (by using a small # of terminations). The authors then say that ""given a set of options, a policy over options, a near-optimal sample trajectory, we can calculate.."" Where does the near-optimal sample trajectory come from? Please provide clarifications. <sep> In experiments: FR rooms experiments are interesting, in the visualization of the option policies, do the figures here show the flattened policy of the options? What do the termination functions look like? <sep> Atari experiments are limited in nature in that they show only two games. Moreover, It is a bit confusing as to what is multi-task in the ATARI experiments. The authors mention the training of options and then talk about the results in the plots (4) show the training curves. However, they do not mention what are ""novel tasks for Breakout/Amidar"" in this context. <sep> Considering the proposed approach is closely related to the idea of selective terminations of options, it is natural to expect a comparison with Harb, 2018 and Hartyuanm 2019. The work could benefit by comparing with the aforementioned baselines. In particular, the visualization in 4b showing options learned in Amidar does not show much improvement from what was observed before in Harb, 2018. <sep> With the motivation of this paper, I am unable to convince myself about options being ""reusable"" for multi-task here. It would be very useful for the reader to clarify what ""novel tasks"" are here to appreciate what is learned. Looking deeper into the appendix, I understand that the authors ""first learned a good performing policy with A3C for each game and sample 12 trajectories for training."" This is not at all clear in the main paper. Besides, what does it mean by a ""good"" policy? If we already have that, it is unclear what gains do we get from the proposed method. <sep> One obvious limitation here is that they also have a hard imposed constraint here is that the options cannot run for more than 20 time-steps in total, to make the objective function a suitable choice. <sep> Overall: <sep> An interesting objective function, Learn not only option set but also the number of options needed and incrementally learn new options. <sep> Paper writing does not convey clearly what are novel tasks and could be significantly improved. <sep> Since the paper claims multi-task and mentions several lifelong learning works like [1], I was expecting rigorous baselines showing performance over multiple tasks. The experiments are lacking in that evidence except for four rooms domain, which is much simpler a domain. <sep> Near-optimal property is very much lacking the clarity to the best of my knowledge. <sep> [1]Ammar, Haitham Bou, et al. ""Online multi-task learning for policy gradient methods."" International Conference on Machine Learning. 2014.","This paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks. <sep> The primary concern with this paper was with a number of issues around the experiments. Specifically, the reviewers took issue with the definition of novel tasks in the Atari context. A more robust discussion and analysis around what tasks are considered novel would be useful. Comparisons to other option discovery papers on the Atari domains is also required. <sep> Additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion. <sep> While this is really promising work, it is not ready to be accepted at this stage."
"The paper studies active learning in graph representations. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. First it argues that one should consider region-based measures rather than single nodes. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. <sep> Overall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset. <sep> Strength: <sep> - One of the ideas of the paper, using region entropy over single node entropy makes sense to me. <sep> - The paper evaluates on 6 datasets and compares different variants as well to related work on 2 datasets. <sep> Weaknesses: <sep> 1. The paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: <sep> For example: <sep> 1.1. ""APR outperforms all other methods at low sampling fractions"".  This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing <sep> 1.2.  ""We have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used"", this is not the case on CiteSeer in Table 1 <sep> 1.2.1. Also e.g. ""Region Margin"" is worse than random on 5% Email-EU; or ""Region Margin AE"" on 3% SubeljCora (Table 2) [It is unclear how to select with or without AE] <sep> 1.3. ""We outperform all existing methods in the Cora dataset, and get very similar results to the best accuracy obtained by Chang et al methods:"" <sep> 1.3.1. The difference to Cai et al. on Cora is very small (improvement by only 0.002), while on Citeseer the performance is comparatively bigger (Cai et al. is by 0.016 better) <sep> 1.3.2. It should be ""Cai et al"" <sep> 2. Clarity: I found the paper rather difficult to understand and follow: <sep> Some specifics: <sep> 2.1. The introduction could be more concisely discussing the motivation, the main idea of the paper, as well as contributions. <sep> 2.2. Figure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. From the example it makes much more sense to label node 14 to me. <sep> 2.3. Page 6 mentions twice the ""ratio between APR and PR"", is this is this used/evaluated in the results? <sep> 2.4. The decision what is bold and what is not is not consistent throughout the table 2. <sep> 2.5. ""Thus, hybrid techniques, combining several approaches, outperform using only one approach have been proposed."" It is not clear what this refers to and where the hybrid techniques have been evaluated. <sep> Minor: <sep> The paper contains many minor writing issues, e.g. <sep> - missing spaces, e.g. ""distribution,and"" (page 2) <sep> - Table 1: incomplete sentence: ""∗∗ scores for smaller budget, since it was the"" <sep> - Table 2: unclear: ""accuracy without content"" <sep> The paper's incorrect claims (weakness 1) are highly concerning and strongly suggest rejecting the paper. Furthermore, the clarity of the paper should be improved to follow the author arguments and make the paper easier to read.","The paper proposes a method for performing active learning on graph convolutional networks. In particular, instead of performing uncertainty-based sampling based on an individual node level, the authors propose to look at regional based uncertainty. They propose an efficient algorithm based on page rank. Empirically, they compare their method to several other leading methods, comparing favorably. <sep> Reviewers found the work poorly organized and difficult to read. The idea to use region based estimates is intuitive but feels like nothing more than just that. It's not clear if there is a mathematical basis to justify such a method (e.g. an analysis of sample complexity as has been accomplished in other graph active learning problems, Dasarathy, Nowak, Zhu 2015). <sep> The idea requires further study and justification, and the paper needs an improved exposition. Finally, the authors were not anonymized on the PDF."
"This paper studies the problem of interpreting predictions of blackbox models. In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e. linear); thus, if they are trained on entire datasets they will underfit. <sep> The aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of black-boxes. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e. non-differentiable) decisions to select a subset of the dataset. <sep> This work is closely related to Ren et al. 2018 [1], which proposes to meta-learn how to weight samples in a batch so as to maximize performance on a validation set. By analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the black-box model predictions as a target. The data subsampling operation introduces the added complication of non-differentiability. The contribution of this paper is thus the use of RL for meta-learning how to subsample a larger dataset in order to maximize some validation loss. <sep> * Pros: <sep> * Considers an interesting dataset subsampling variant of the sample weighting meta-learning problem. <sep> * Novel application of meta-learning for improving locally-linear models. <sep> * Extensive quantitative evaluation shows that the method seems to perform better than baselines, though it might be that a differentiable approximation could do as well while being more sample efficient. <sep> * It is a nice result that the l1 penalty actually works well in reducing the number of samples chosen by the <sep> * I found the discussion and figures presented in 4.2 to be quite nice and informative. <sep> * Cons: <sep> * Given the lack of a differentiable approximation baseline, I am not entirely convinced that the use of RL is absolutely necessary/optimal. <sep> * I.e. if the weighting function is actually high-entropy, randomly sampling a (large) batch and weighting it might work just as well. <sep> * Though there is discussion of the complexity of the overall method it would be nice to see a discussion and figures related to the sample efficiency of REINFORCE? <sep> * This would be strongest if given with a comparison to differentiable alternatives (mentioned above) as well. <sep> * This would help elucidate whether RL is optimal in this setting: fitting a linear model on more data might be cheaper learning to subsample with REINFORCE. <sep> * While the sample weighting function is fast at inference time, most of the overhead comes at training time. This function needs be updated in settings where the underlying dataset changes. <sep> * This is a minor issue, but this pushes the burden of interpretability further up to the black-box sample weighting function. While this interpretability problem is less critical, it still exists. <sep> * Other comments/requests: <sep> * While the use of RL is certainly motivated in order to solve the problem in an unbiased way, it would be nice to see a comparison to a differentiable approximation as a baseline? A few ideas: <sep> * Randomly sample a (possibly large batch) and learn to weight it (closely related to the straight through estimator) <sep> * Randomly sample a batch and apply [1] <sep> * Would be nice to show the sizes of datasets and how many samples end up being used for different values of lambda. <sep> * Would be nice to understand which samples are chosen and why. This is probably tricky to analyze, but it would be interesting to see if certain samples are often chosen, or if the weighting distribution has an interesting shape (i.e. is low or high-entropy). <sep> I've given a weak accept, conditioned on being provided more evidence regarding 1) comparisons to simple differentiable alternatives, 2) sample efficiency of the RL method, and 3) basic analysis of the weighting function. <sep> [1] Learning to Reweight Examples for Robust Deep Learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. https://arxiv.org/abs/1803.09050","The paper aims to find locally interpretable models, such that the local models are fit (w.r.t. the ground truth) and faithful (w.r.t. the global underlying black box model). <sep> The contribution of the paper is that the local model is trained from a subset of points, selected via an optimized importance weight function. The difference compared to Ren et al. (cited) is that the IW function is non-differentiable and optimized using Reinforcement Learning. <sep> A first concern (Rev#1, Rev#2) regards the positioning of the paper w.r.t. RL, as the actual optimization method could be any black-box optimization method: one wants to find the IW that maximizes the faithfulness. The rebuttal makes a good job in explaining the impact of using a non-differentiable IW function. <sep> A second concern (Rev#2) regards the interpretability of the IW underlying the local interpretable model. <sep> There is no doubt that the paper was considerably improved during the rebuttal period. However, the improvements raise additional questions (e.g. about selecting the IW depending on the distance to the probes). I encourage the authors to continue on this promising line of search."
"The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. It is a sort of ""meta-study"" and comparison of different metrics proposed to identify cells with a preference for a specific target category. The claimed finding is that there are no cells that are ""sufficiently"" selective to be called object detectors. <sep> The paper is seemingly motivated by the authors' perceiving a contradiction: it is assumed that the power of neural networks is (among others) due to the distributed representation; whereas the presence of object detectors would, in the extreme case, mean that the representation is disentangled into a separate unit per category. It may be a matter of terminology, but this is where my disagreement with the authors start. I do not see a simplistic dichotomy, where one could or should determine which of the two interpretations is ""right"" or ""wrong"". In my view, which I believe is the mainstream interpretation, a distributed representation does not contradict the presence of specialised units. Some categories probably are easily identified by few distinctive features, so there will be more detector-like units; others are complex and hence more diffusely spread through the network; and of course there is no guarantee that the learned ""object detectors"" are tuned to exactly the target categories, after all it is the purpose of the network to gradually translate the data distribution to the label distribution - if the categories were directly apparent in the data, nearest-neighbour would be enough. So it is not only possible, but rather likely that the learned ""object detectors"" are to some degree driven by the statistics of the data, not the labels - e.g., there could be a highly selective ""bird"" unit which nevertheless has high false positive rate for any of the more specific bird species categories in the imageNet nomenclature. And vice versa, there could be a highly specific ""Ferrari"" detector that is so specialised that it has low recall for the ""sports car"" class (this case includes, among others, the case of viewpoint-specific detectors for certain categories). In the words of the paper, the ""selective units are sensitive to some feature that is frequently, but not exclusively associated with the class"" - I thought this is the standard majority view, not a surprising finding. In this context terminology matters: the study effectively tries to disprove that the network learns ""near-perfect single output-category detectors"", but who claims that it would do that? <sep> I agree with the authors that there is by now a zoo of selectivity metrics that are not always highly correlated. But is that a problem? We have a zoo of quality metrics for many machine learning problems - that is not necessarily a weakness, but simply reflects the obvious fact that a single number is not enough to characterise performance in a complex cognitive task. It is the job of the researcher/user to chose the metric that s most suitable for their specific question, and to correctly interpret its numerical value. <sep> Regarding the methodology, the paper did a lot of work to systematically crunch the numbers and analyse network units. It is a laudable effort that someone took on that job. A few technical decisions are unclear to me. Why analyse only some of the units? If one collects statistics over >2000 units of a fully connected layer, one might as well do the complete job and use all 4096 units. Similarly, why analyse only the correctly classified images? While it is clear that one must separately look at them, also the activations on incorrectly classified ones could provide valuable insights. E.g., do false positives of class X on average activate a certain ""class X detector""? Why chose only the class with highest mean activation for CCMAS? That might be unrepresentative, e.g., a neuron might, for that particular class, always have high activation due to some very common background context, and still be not selective at all. <sep> Regarding the results, I find them much less clear-cut than the paper claims. For example, I find it quite remarkable that some unit has 8% recall at perfect precision. After all, only approximately 0.1% of the images are in the correct category, so a unit that flags 8% of them without making a mistake is a pretty good detector for (part of) the target class, cf Fig. 3. Also regarding Fig. 2 / maximum informedness, the statistics actually do not look bad. Of course false alarm proportion remains high - but the chance level here is 99.9%, so even a 99% false alarm rate means that your unit can, on its own, reject 90% of the true negatives. I find the proposed ""minimum condition"" for an object detector (>50% recall at >50% precision) unrealistic: the top-1 accuracy of AlexNet is, to my knowledge, <63%. Even the complete network probably never reaches 50% recall for most classes. <sep> Especially the user study - which is again a commendable effort - in my view does not confirm the claims. According to that study, almost 60% 0f all fc8 units are ""object detectors"", with very high conherence between humans and selectivity metrics. <sep> Overall, while it is an interesting study, it remains unclear to me what I should learn from it. I don't see why different measures provide ""misleading conclusions"" that need to be rectified. Conclusions are the responsibility of the researcher interpreting the numbers, not of the formula to calculate some statistical performance metric. I am in a difficult situation here: the study is one of those things (like determining human performance on ImageNet, or re-coding some baseline where the original code is not available) where I find it valuable that someone did them in the community, but still I don't think they need a reviewed paper.  A note on the blog, or on arXiv, is enough.","This paper conducted a number of empirical studies to find whether units in object-classification CNN can be used as object detectors. The claimed conclusion is that there are no units that are sufficient powerful to be considered as object detectors. Three reviewers have split reviews. While reviewer #1 is positive about this work, the review is quite brief. In contrast, Reviewer #2 and #3 both rate weak reject, with similar major concerns. That is, the conclusion seems non-conclusive and not surprising as well. What would be the contribution of this type of conclusion to the *CONF* community? In particular, Reviewer #2 provided detailed and well elaborated comments. The authors made efforts to response to all reviewers' comments. However, the major concerns remain, and the rating were not changed. The ACs concur the major concerns and agree that the paper can not be accepted at its current state."
"This paper was extremely hard to read or comprehend. It's riddled with typos, inaccurate notations and undefined variables (see below for a sampling). The authors will need to significantly polish and improve the presentation of the paper. <sep> After a few forward and backward passes through the paper, I was able to gather the following high level ideas about the paper: <sep> (1) This paper is somewhat related to the Defferard et. al, 2016 in that the authors want to define a propagation filter for graph neural networks. <sep> 2) This proposed filter known as ""ballistic filter"" should have the property of allowing fast diffusion through the network. <sep> (3) The authors claim that the ballistic kernel diffuses @ O(k) as compared to O(\\sqrt k) when compared to traditional GCNs, where k is the number of propagation steps. <sep> (4) The authors additionally claim that their approach needs one-third the number of parameters. <sep> (5) The authors provide some plots to visualize the linear diffusion rate of their proposed filter. <sep> --- Issues and clarifications --- <sep> - Sec 3, Eq 1 seems to have been taken from Eq 1 in Defferard et. al, however there's no reference to it and the terms g, U, etc. are not defined. <sep> - Sec 4, Algo 1 contains the main core of the proposed algorithm, but it's only defined for the 2D grid case. The notation therein is extremely unclear. What is H_space, H_c? How does one sample \\hat{O}_coin.? The net result is that algorithm is undefined. Without a clear definition of the algorithm, it's completely unclear what the proposed method does. <sep> - Sec 4.2 is completely unparseable. What is problem setting? What is the metric? How have the baselines been implemented? How has data been split for training/testing? <sep> - Section 5 mentions that one-third params are used to get 97% but no details are provided as to how less params are consumed. <sep> - How is figure 7 generated? <sep> - Sec 8, feel totally unrelated to the paper. There are a whole bunch of random, unmotivated diffusion equations  Eq 6, mentions "".. \\hat{g}(f) decreases as f increases and thus can be seen as a low pass filter…"" . This is not true from the formula. <sep> --- A sampling of typos --- <sep> Sec 4.1, .. consisits … <sep> Sec 5 ""REVISIT"" -> ""REVISITING"" <sep> Figure 6, text, ""cassical"" <sep> Sec 6.2 title, ""SUMMAY"" <sep> Sec 8  ""aggreated"" <sep> Sec 8  t=\\-tau to -\\tau <sep> Several typos with Hardmard, Hadmard instead of Hadamard. <sep> Overall, the major criticisms of this paper: <sep> - The proposed algorithm is not clear. <sep> - The authors need much more experimentation to bolster their claims in the paper. It's completely unclear if fast diffusion even if it were possible will help GNNs perform better on a diverse set of tasks. <sep> - The paper needs a lot more polish and proof reading to make this paper presentable.","This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal. The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims. The reviewers acknowledge the authors' efforts to resolve the main issues but find these efforts insufficient. Thus, this paper cannot be accepted to *CONF*2020."
"The paper is concerned with the presence of isomorphism bias in commonly used graph learning benchmarks. In particular, the paper analyzes the amount of isomorphic graphs in 54 graph datasets and evaluates the performance of three graph classification methods under two isomorphism settings. <sep> Careful analyses of commonly used benchmarks can be important contributions that provide new insights into the performance of state-of-the-art models. The present paper's results on graph isomorphism properties can indeed be valuable for the ablation of models and testing their performance with regard to this property. I also found the relatively high label disagreements on some datasets (even under stronger isomorphism constraints) to be a surprising and useful result. <sep> However, the main assumption of the paper -- which equates the quality of a graph learning benchmark with the amount of isomorphic graphs that it contains, i.e., the lower the better -- seems questionable. <sep> The paper argues that isomorphic graphs are akin to duplicate images in computer vision and should be removed from a dataset. While completely identical graphs are certainly problematic, the case seems different for isomorphic graphs. In the latter, a learning method is required to identify the correct bijection form V_1 to V_2 which is a non-trivial task. Testing on isomorphic graphs evaluates the ability of a model to infer these equivalence classes from data which is an important property. Moreover, being able to capture the equivalence relation can be important for various graph learning tasks, e.g., to facilitate that two topologically equivalent graphs are be classified similarly.  Going back to the computer vision analogy: it seems a more adequate comparison for graph isomorphism would be translation and scale invariance which are certainly desirable properties for CV models. <sep> In addition, the dataset analysis could also be improved. For instance, the SYNTHETIC dataset includes continuous node attributes that are essential for classification and make the graphs non-isomorphic (when considering, for instance, each attribute vector as a unique node label). However, the attributes are not considered in the analysis what leads to a large number of isomorphic graphs. On a side note: the paper also incorrectly attributes the SYNTHETIC dataset to (Morris et al, 2016), but it is in fact from [1]. The synthetic dataset of Morris et al (SYNTHIE) does not consist of isomorphic graphs, while the SYNTHETIC dataset of [1] does so intentionally. <sep> The results of Section 5 seem also not very surprising: After removing node labels, it is expected that the number of isomorphic graphs increases since a discriminating feature has been removed. Moreover, when accounting for node labels, many standard benchmarks seem to consist of significantly less isomorphic and mismatched graphs (as can be seen in the appendix). <sep> Since graph isomorphism != graph identity, the assumption Y_iso \\sub Y_train in Property 6 seems also not appropriate. The results of Theorem 6.1 on the other hand seems straightforward and would hold for any classification task for which the true label for an equivalence class of instances is known.","Thanks to reviewers and authors for an interesting discussion. It seems the central question is whether learning to identify correct bijections should be part of graph classification problems, or whether this leads to bias and overfitting. Reviews are generally negative, putting this in the lower third of the submissions. The paper, however, inspired an interesting discussion, and I would encourage the authors to continue this line of work, addressing the question of bias and overfitting more directly, possibly going beyond dataset evaluation and, for example, thinking about how to evaluate whether training on non-isomorphic graphs leads to better off-training set generalization."
"PAPER SUMMARY: <sep> This paper proposes a fast inference method for Gaussian processes (GPs) that imposes a sparse decomposition on the VI approximation of the posterior GP (for computational efficiency) using the KNN set of each data point. This is further coupled with armortized inference for better scalability. <sep> NOVELTY & SIGNIFICANCE: <sep> This paper adopts a different approach of characterizing the VI approximation of a GP posterior than original VI approximation that was developed in Titsias (2009): Instead of characterizing the surrogate q(f_I) of p(f_I | Y) for a small collection of inducing inputs, the proposed method characterize q(f) directly where q(f) = int_f_I q(f_I) p(f | f_I)df_I. <sep> This is, however, a somewhat strange direction which, to me, seems to raise extra issues that could have been avoided if one follows the conventional VI approximation: <sep> (1) As the posterior surrogate is now directly over f instead of f_I, the number of variational parameters is now proportional to the data size which requires several (redundant) extra approximations including armortized inference & the lower-bound on the entropy term that admits a sparse decomposition. <sep> (2) This at least creates the armortized and entropy gaps that decrease the expressiveness of the original VI. While I understand that this is in exchange for the ability to encode local information (via KNN) within the surrogate posterior, it is not clear to me why do we need to incur all these computational issues to incorporate such local information. <sep> (3) For example, instead of forcing such local information in the posterior surrogate q(f), we could alternatively let it be reflected in the test conditional p(f_* | f_I, Y_n(*)) such that the test output depends on both the inducing output and a local partition of data (e.g., via K-mean), which has been previously explored in [*] and later incorporated in the conventional VI paradigm of Titsias (2009) without incurring extra intractability [**]. <sep> (4) This maintains the dense correlation between data points within the same neighborhood while allowing the VI surrogate to be more concisely specified and independent of the no. of training data points. Furthermore, it also helps avoid the data-bound overhead of computing a KNN per test point. <sep> [*] Local and global sparse Gaussian process approximations (AISTAT-07) <sep> [**] A distributed variational inference framework for unifying parallel sparse gaussian process regression models (ICML-16) <sep> To summarize, the practical significance of placing such a VI approximation directly on q(f) to encode such (indirectional) local information is, given the above, questionable to me. <sep> Please note that I am not disputing the potential use of this VI form here, which could have been the only way to encode a different (directional) type of information. <sep> For encoding KNN information, however, it only seems to create more troubles than it solves. <sep> Minor point: <sep> The above references, especially [*], should have been cited. <sep> TECHNICAL SOUNDNESS: <sep> [A] Optimization of the ELBO: <sep> (1) The ordering of data (i.e., the directional information) was mentioned repeatedly in the paper but its importance to the fast approximation was neither explained nor discussed. <sep> (2) The decomposition form of Eq. (6) also raises a question: How do we know that the term inside the log is positive? There seems to be missing information on the constraint of R. <sep> [B] Amortized Inference: <sep> (1) The choice of the GCN seems arbitrary here. I am in fact not sure why GCN is necessary for the inference network & furthermore, GCN also brings to the table another heuristic choice of A. <sep> (2) How do we set the adjacency graph A? <sep> (3) How do we know what is the right complexity for the GCN? <sep> [C] Complexity: <sep> The complexity analysis is too informal and lacking fine-grained information. <sep> Please include a detailed complexity analysis of the training and inference cost in terms of the input dimension, the no. of data points, the size of the neighborhood and the batch size. <sep> It is also necessary to factor in the KNN overhead (e.g., the cost of building the K-D tree for low-dimensional embedding of data & the approximation cost of projecting that information to high-dimensional data) <sep> EXPERIMENT: <sep> The experiment results only show marginal improvement over the baselines, and the size of the dataset for regression is too small. If I read correctly, both have fewer than 20000 data points. <sep> SVGP in particular has been tested on a much larger datasets (AIRLINE, UK Housing) comprising millions of data points -- comparison on such dataset should have been reported. <sep> Note that the largest dataset used to evaluate the efficiency of fast approximation of GP is on the scale of 6M data points [****]. On that note, eBird and precipitation should not even be considered mid-sized. <sep> To demonstrate the efficiency of local information encoding, comparison with [*] should be reported. There is another class of inducing-point methods that use expectation propagation that should have been discussed and/or compared with: <sep> [***] A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation (JMLR-18) <sep> [****] Distributed Gaussian Processes (ICML-15) <sep> CLARITY: <sep> The paper is clearly written. <sep> REVIEW SUMMARY: <sep> This paper adopts a VI approximation that deviates from the conventional form of (Titsias, 2009) to encode the KNN information, which causes extra computational issues (that incurs extra approximations). I find this deviation redundant seeing that the same information could have also been accounted for using the old VI form, which is a lot more computational efficient. I also find the experiment lacking as comparison with fast approximation method such as [*] that incorporate local information is not included. There are also a few technical ambiguities that need to be clarified. <sep> ------ Post-Rebuttal Update ------ <sep> Thank you for the rebuttal & I have read it in detail. However, it still does not address my concern, which I re-summarize here: <sep> I do not dispute the beneficial of exploiting neighborhood information but my point is we could still leverage the same amount of neighborhood information without going through the trouble of incurring extra steps of approximation due to approximating q(f) instead of q(f_I) -- I think I have elaborated this in points (1) - (4) in my original review -- which also creates the amortized gap. I am also not sure what model reuse means (in this context) and whether it is relevant since it appears somewhat noncentric to the objective of this paper. <sep> Also, apparently, the experiment is still lacking since to me, comparing with [*] is important in substantiating the contribution claim of this paper.","This paper presents a method for speeding up Gaussian process inference by leveraging locality information through k-nearest neighbours. <sep> The key idea is well-motivated intuitively, however the way in which it is implemented seems to introduce new complications. One such issue is KNN overhead in high dimensions, but R1 outlines other potential issues too. Moreover, the method's merit is not demonstrated in a convincing way through the experiments. The authors have provided a rebuttal for those issues, but it does not seem to solve the concerns entirely."
"Summary <sep> This paper proposes an algorithm to address the issue of nonlinear optimization in high dimensions and applies it to convolutional neural networks (VGG models) on CIFAR 10. <sep> They show 11% relative reduction in error for this particular task with this particular network. In addition, they prove additional theoretical results on the convergence of SGD using their method in the convex case as well as convergence of SGD to a stationary point in the nonconvex case when the homotopy parameter is fixed which is not done in practice. <sep> Given an optimization problem, their method first solves multiple independent lower-dimensional optimization problems each with a subset of the parameters and then optimizes a new objective function controlled by a monotonically decreasing parameter L that interpolates the original objective function and the previously-solved lower dimensional problems. L can be seen as a regularization parameter that is gradually decreased as we optimize the new optimization function. When L = 0, we recover the original optimization problem. <sep> The authors prove that (1) SGD with their procedure will find a stationary point under the Robbins-Monro conditions for a fixed L and (2) SGD with their procedure will converge for convex problems as L is decreased to 0. <sep> Decision and reasoning <sep> This paper should be rejected because (1) the proposed algorithm attempts to address the original issue of high dimensional nonlinear optimization of neural networks but violates the algorithm's assumption in practice, (2) the empirical evaluations are lacking - having only evaluated their method on a toy problem with up to only 6 dimensions and a relatively simple image classification task, <sep> and (3) the assumption of fixing the homotopy parameter in the theorem on the non-convex case directly violates the intention of the algorithm. <sep> Regarding (1): The proposed procedure requires initializing L at a large value and reducing L towards 0 in order to recover the original optimization problem. <sep> However, in practice for CIFAR 10, the authors initialize L to be 0.01 and gradually reduces it to 0.005 which is hardly the original intent of the algorithm. There is also no demonstration whether or not this gradual reduction in <sep> L actually has an effect on the optimization of the new objective function. For example, since the start and end values of L are similar, will we get similar results if we simply fix L to be 0.005 or 0.01? The authors also show that their method outperforms a quasi-newton method by combining the optimization with their procedure on a non-convex example by Chow et al. 2013. However, this example only goes up to n=6 dimensions, which is hardly comparable to the original problem of high dimensional non-convex optimization that this paper sought to address. <sep> Regarding (2): The authors evaluated their procedure on CIFAR10, a relatively simple image classification task that modern neural networks can solve easily and is not representative of the types of nonlinear optimization problems prevalent in deep learning. There's also an issue of using only VGG networks for their evaluations while VGGs are typically eschewed in favor of ResNets today. <sep> Given that the optimization is easier with residual connections, it may be the case that their procedure does not significantly improve the accuracy of ResNets. <sep> Regarding (3): By fixing L in Theorem 3.1, the authors essentially show that SGD <sep> converges to a stationary point for their new objective function which can be seen as a regularized version of the original objective function, which is not a strong result. Furthermore, fixing L goes against the original procedure's motivation of recovering the original optimization function as L decreases to 0. <sep> Additional comments and questions <sep> There are passages that are difficult to understand because not enough context is given. For example in the ""remark"" passage, it is not clear where the <sep> ""necessary condition"" comes from. In addition it seems like it doesn't even type-check since the first term is 2n dimensional while the second term is 4n dimensional. <sep> There are also many errors in the writing that hinder the presentation. A subset of them includes: <sep> - ""nerual netowrks on roboticsKonda et al."" -> ""neural networks on robotics Konda et al."" <sep> - ""based on homotopy continuation method"" -> ""based on the homotopy continuation methods"" <sep> - ""random chosen point"" -> ""randomly chosen point"" <sep> - ""we choose \\tilde{\\theta} = 0 in the dropout"" -> reword <sep> - Fourth term in Equation 3 should be \\theta_2 - \\tilde{\\theta_2} <sep> - ""By gradually increasing parameter L"" -> ""By gradually decreasing parameter L"" <sep> - ""where \\xi is a random variable due to random algorithms"" -> reword and possibly say the randomness is from SGD <sep> - After equation 6, should have b_i instead of \\beta_i <sep> - In equation 20, should be g(\\theta_*^0) instead of g(\\theta_*^1) <sep> - In theorem 3.2 you never explained what \\theta_*^{L_k} is <sep> - ""We compared the traditional optimization method (the quasi-Newton method)"" -> which quasi-Newton method? <sep> - Figures 2 and 3 label the x-axis with ""epochs"". However only 4 epochs were run, so I believe the x-axis should be ""iterations"" <sep> Besides improving the quality of writing in the paper, I would strongly suggest that the authors improve their empirical evaluation.  Possibilities include evaluating on <sep> CIFAR 100 or ImageNet, using a wider variety of networks including ResNets, <sep> evaluating on tasks other than image classification.","The work proposes to learn neural networks using a homotopy-based continuation method. Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results. With no response from the authors, I recommend rejecting the paper."
"This paper presents a new discriminator metric for adversarial attack's detection by deriving the different properties of l-th neuron network layer on different adv/benign samples. This method can achieve good AUC score comparing to other start-of-art detection methods and also achieve good robustness under corresponding adaptive attack. The framework is clear and the experiment is solid. <sep> However, I have several concerns: <sep> Major: <sep> 1. It seems that the whole process assumes that there is difference for the parameters in the environment of GGD with adv/benign samples, and the goal is to search for the major components of it and use a classifier to detect. To extract the approximation of parameters,  the authors use the ""response entries"" of l-th layer for several observations => this means the authors regard all the ""response entries"" of one layer as different samples on one certain GGD. This makes me feel a bit tricky, and it would be great if you the authors can provide some evidence or explanation here. <sep> 2. In the experiment's remark, the authors mentioned that the mean parameter of GGD is set to 0 and most of them are actually close to 0 (around 1e-2) so the assumption is right. However 1e-2 is not a value ""very close to zero"" and it would be great to show / explain the variance here. <sep> 3. I can't find the parameter of your evaluated attack method (like confidence, eps, etc.) Please also provide experimental details for reproducibility. <sep> Minor: <sep> Here are some reference error (e.g. P6, ""For each database, as described in Section ??""). Please fix that. <sep> Overall, this paper is a interesting based on the performance of detection. But the  assumptions made by the paper are a bit confusing and it would be good to clarify and provide clarification for them. Authors should explain the assumptions and give some extra experiment results if needed.","This paper presents a new metric for adversarial attack's detection. The reviewers find the idea interesting, but the some part has not been clearly explained, and there are questions on the reproducibility issue of the experiments."
"The paper attempts to establish the asymptotic accuracy of the ""RNN"" (but not the RNN models that are well-known in the literature - see the below comments) as a universal functional approximator. It considers a general state space model and uses feedforward neural nets (FNN) to learn the filtering and forecast distributions. Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter. <sep> The paper targets an important problem in statistical learning and offers some interesting insights. However, their so-called RNN-based filter formulation is not anywhere close to the usual RNN models, such as Elman's basic RNN model or LSTM, that are currently known in the literature. Hence the paper's title ""RNNs are universal filters"" and the way it is presenting are confusing. I think what exactly the paper is about is as follows. They consider a general state space model, then use FNNs to approximate the so-called transition equation and observation equation of that SS model. Then, the resulting RNN-based filter is shown to be able to approximate well the optimal filter of the original SS model. I didn't check the proof carefully but I guess it's intuitively straightforward given the available results from the approximation capacity of FNNs. <sep> Therefore, I suggest the authors re-write the paper carefully to reflect better the problem that it actually targets. The property of the RNN-based filter is interesting, but using it is, I believe, very difficult from a practical point of view. It's well-known that doing particle filters is computationally expensive, especially in high dimensions, and the RNN-based filter might have millions of parameters! Could the authors please give some comments/discussion about this issue? <sep> Some minor points: <sep> 1)  what exactly does ""synthesized"" mean? <sep> 2) Page 5: ""Note that the probability space ... with finite moment..."" doesn't read well. What is the moment of probability space?","Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem. Two reviewers, however, have doubts about the novelty and difficulty to get the result. Although I do not fully agree that Reviewer3 that the proof is just ""DNN can fit anything"" - it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term ""recurrent neural network"". The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for *CONF* in this form. However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice."
"The article introduces the novel MIST architecture which tries to solve the problem of multiple-instance classification and image generation from multiple objects. It employs two submodels, where the first generates a heatmap of intereting region and the second model is a task-specific model that works on image-patches, for example a classifier or an autoencoder. Both models are connected by a patch-extraction routine. The main contribution of this paper is to provide a way to propagate errors through this non-differentiable patch-extraction scheme. This is done by introducing slack-variables. <sep> ------------------ <sep> The paper is overall relatively easy to follow and the results are very good. However, it suffers from the fact that it does not differentiate between model-architecture and the overall approach. While the main contribution is described in Section 4, the paper spends a lot of space beforehand to introduce the task-dependent models as well as the heatmap architecture - things that i can imagine will vary a lot in different applications. The real important part is how to train the model and this is unfortunately only half described. A good deal of abstraction from the network architecture would have made the paper a lot better.  Further, I think that the loss-function for the classification task does not work in the general case. <sep> On my first read-through, i completely misunderstood Section 4. Here is an unsorted list of issues i had with this: <sep> - since E_K is not truly invertible, writing the approximate inverse as E_K^{-1} is misleading. <sep> - It might help to stress that you treat {x_k} as continuous and the sampling as differentiable. <sep> - In (7) it would be better to explicitly write E_K^{-1}(x_k) instead of introducing \\bar{h}. The line below is not clear. <sep> - It is also misleading, because the choice of \\bar{h}=E_K^{-1}(x_k) is not the minimizer of (5) given that all other variables are fixed. You can see this by observing that assuming that when {x_k}=E_K(H(I)) holds, we can choose all other pixels to be exactly the value returned by H(I). <sep> - I am not sure where the alternating part comes from because this usually involves taking your solution from (7) and feeding it into (6). <sep> - I am pretty sure that in (6)+(7), as well as lines 3+6 of the algorithm, you actually don't want to optimize for tau or eta from scratch but only perform a single SGD step. I think that is what you are doing, but right now it is written as ""find a complete new model for each batch"". <sep> - Since this is performed batch-wise: is x_k a variable kept between iterations or do you use H(I) for an initial estimate of x_k for the batch? <sep> Regarding the classification objective: <sep> -since (3) uses the MSE of the mean class-label and the mean-prediction, a dataset where all objects always appear with the exact same amount will not work since than for each image the mean label is identical. <sep> - Therefore, the MNIST-easy dataset should be unsolvable for the proposed architecture since every digit occurs exactly once.","Two reviewers are negative on this paper while the other one is slightly positive. Overall, this paper does not make the bar of *CONF*. A reject is recommended."
"Summary. <sep> The authors propose a new definition for robustness of random functions. This definition is ideal for analyzing the certified robustness under randomized smoothing techniques. They analyze and show that the Gaussian smoothing is near optimal for \\ell_2 smoothing as the mean maximum error is only off by a factor of log d where d is the dimension from the optimal mean maximum energy. This is the case even under a more strict definition of robustness defined as D_\\infty. Moreover, the authors show that indeed smoothing with an exponential family  is optimal under D_\\infty robustness metric with radius measured in \\ell_\\infty. <sep> I find the paper very interesting and the approach is novel and generic. I do not have any major criticism. <sep> Minor comments. <sep> 1) Equation 3 ""D(A(x'),A(x))"" >> ""D_\\infty(A(x'),A(x))"" <sep> 2) Page 6 third line below Theorem 16. Reference of Theorem 11 should be Corollary 11. <sep> 3) The authors should report the certified accuracy of the undefended baseline classifier over varying radius in Figures 1 and 2 and 3. <sep> 4) Running experiments on ImageNet following Cohen et al. should make the paper stronger. <sep> 4) Can the authors comment on is the certified accuracy for \\sigma=0.5 at radius = 0 is better than the unsmoothned classifier a sigma 1.0. I expect that the radius of certification is larger for larger sigma. <sep> 5) The authors should explain how does the new definition of robustness relate to the common robustness definitions as the one by Cohen et al.  More discussion is necessary for this and more justification. <sep> 6) Why is the D_MR defined as maximum over α? It seems it is only sufficient to define it as the ratio over α. It seems that this is only needed for Theorem 8 to hold. <sep> ------------------------------------------------------------------------ <sep> After further careful read of several relevant papers, e.g. Bun et. al 2016 and the work of Dwork ""Concentrated Differential Privacy"", I have several questions I would like to ask for some further clarifications. <sep> 1) Showing that a network is robust under D∞ robustness, implies very strong results. The type of results that are common in the literature. This is since D∞ robustness, implies ϵ DP networks (see Lemma 3.2 and proposition 3.3 of Bun et al.). Once ϵ DP is guaranteed identical results of Lecurer et al. can be derived immediately as this implies separation in expectation (Lecurer et al.) where one can study directly the deterministic classifier Eg(x) and not the random g studied in this work. <sep> 2) The authors rely on the lower bounds of Bun et al. to find the average maximum energy that preserves the D∞ robustness (Thm 15 and 16). Authors show that indeed exponential smoothing is optimal. This is significant but the analysis was intensively based on Bun et al. <sep> 3) The relaxation to DMR robustness results into improvement of the dependency on the dimension to d instead of d for under ℓ∞. This should not be surprising at all and in fact is identical to the results of Bun et al. Note that the zCDP proposed by Bun et al, is a relaxed version of DP where ϵ-DP for some radius r implies zCDP with radius r2. See proposition 3.3. Therefore, Theorem 6 and 17 are not surprising nor are they new. <sep> 4) My major concern was with the results relating to Gaussian smoothing. I do understand that since Gaussian smoothing only implies high probability result of DP which is often referred to as (ϵ,δ)-DP which happens to be a equivalent to zCDP proposed by Bun et. al. Therefore, I have no issues of using DMR to analyzing the robustness for Gaussian smoothing since it was always analyzed in the DP community with the ϵ,δ-DP and not the stronger ϵ-DP. However, the statement of the result (Theorem 12) confused me vastly. Let me clarify. <sep> Theorem 12 seems to be too good to be true. How is it possible that one can guarantee DMR robustness without any dimensionality dependence. Using Gaussian smoothing the DMR can depend on log⁡d. While log⁡d may seem small; improving this to a constant in dimension is still a very big gap from log⁡d. This may raise several questions whether one can actually find this optimal smoothing distribution. However, with a careful read of Theorem 12, the range of the input decreases as a function of d. That is for a given range of input (independent from d), the energy in fact is NOT constant but scales with d. In such a case, the Gaussian smoothing is now of order dlog⁡d. Now, the factor is still log⁡d, but now this is very different as indeed improving the Gaussian to d may not be of significant interest as the energy still depends in the optimal sense on d which does not allow it to scale for larger problems. Moreover, Cohen et al results show that with Gaussian smoothing the energy of the noise scales d since the noise energy ∥n∥=O(dσ) where σ is std of Gaussian. Therefore, it seems that there is nothing surprising about such a result at all. The statement of the Theorem is very misleading and confusing. <sep> Overall, I like this new approach of analyzing the random smoothed classifier; however, the poor presentation of the work and the mis-represented Theorems that seem to over claim are a major reason for my rating. In addition, the paper should be self-contained in which one should not need to read 2-3 other works to figure out the details in this work and the meaning of the several robustness metrics and their direct relations to DP and Lecuer et al. results. The statement of constant in dimension lower bound on the energy of the noise under D_MR was to me the major contribution; however, I found now that the statement is misleading and that in fact it is d reduces the contribution of the paper particularly after learning that such lower bounds are already derived in Bun et al.","After the rebuttal, the reviewers agree that this paper would benefit from further revisions to clarify issues regarding the motivation of the DP-based security definition, any relationship it may have to standard definitions of privacy, and the role of dimensionality in the theoretical guarantees."
"Note: I applied a higher standard to this paper given that it significantly exceeds the recommended page limit. Furthermore, important details are left out in the appendices, which make it difficult to read the main body of the paper in a self-contained fashion. Given that the main body was already over the recommended page limit, I did not read the appendices. <sep> This paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. This formulation is used to derive an adversarial training procedure that trains against the worst-case adversarial example among adversarial examples generated by a set of attacks. Experiments seek to demonstrate applicability of this framework to both attacks and defenses. <sep> As far as experiments are concerned, Section 4.1 presents results on MNIST, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705]. If models C and D are more difficult to attack, could better baselines be employed than attacking the ensemble A+B+C+D? For instance, would an adversary evading models C+D only perform better? It is difficult to draw insights that are generally applicable from a single ensemble. How was the ensemble chosen? Why would a defender add models which are known to be significantly less robust to the ensemble? <sep> When discussing universal perturbations, how are they generated? Given that the performance of the proposed approach significantly degrades average evasion across all images from all groups, what is the threat model for an adversary being interested in group-level success rather than average evasion across all images from all groups? How were the values of K chosen? This comment also  applies to experiments over data transformations. For these experiments, what was the value of K? <sep> As far as the defensive perspective is concerned, it is not clear whether the improvements observed are statistically significant. Were multiple runs averaged to produce Table 4? Given that without DPAR, the improvement is negligible, this is important to interpret results. It appears that most of the robustness gains in both the average and max settings stem from DPAR. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness. In particular, it is not clear whether DPAR is ""a beneficial supplement to adversarial training"" or a required supplement to adversarial training - per the formulation in this paper. <sep> There are issues with grammar throughout the document, which make it difficult to read. Some specific issues: <sep> 1 - Adversarial attack is a tautology (an attack is always adversarial) <sep> 7 - What does ""robust"" adversarial attack mean? <sep> 7 - What is CAAD-18? <sep> 7 - Define ASR_all: what does evade mean here? Is the attack targeted or untargeted? <sep> 7 - What is an ""advanced"" DNN?","This submission studies an interesting problem. However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited."
"===== Summary ===== <sep> The paper presents new theory to develop understanding about why adversarially robust neural networks show lower test performance compared to their standard counterparts despite being more robust to perturbations in the data. The main hypothesis is that the degradation in performance in adversarially robust networks is due to many samples being concentrated around the decision boundary, which makes the network less confident about its decisions. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. The paper then presents empirical demonstrations that aim to illustrate the theoretical findings. <sep> Contributions: <sep> 1. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. <sep> 2. Provide empirical evaluations that aim to illustrate the theoretical results. <sep> ===== Review ===== <sep> The problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks. Thus, I believe that the results could represent a significant contribution. However, due to the way that the information is presented, it is difficult to validate the correctness of the theory and the insights from the paper. Consequently, I consider that the paper should be rejected. <sep> ===== Detailed Comments ===== <sep> - First, and foremost, the paper should be proof-read for English grammar and writing style. In its current form, it is difficult to follow the main argument of many of the paragraphs. This is exceedingly important because the main subject of the paper is already difficult to digest as is. <sep> - In the related work section, a lot of previous work is referenced without any context about what the contribution of each of those papers is. Each of these papers should be mentioned along with their corresponding contributions. Otherwise, it is difficult to frame the paper within the context of the current literature. Moreover, not providing context makes it difficult to determine which parts of the paper are original and which are the result from previous work. <sep> - The first item in Section 1.1 is difficult to follow because there are many gaps in logic that are left to the reader to fill in. It is reasonable to expect the reader to fill in some of the details, but since the sole purpose of this section is to build intuition about what is about to be presented in the paper, then each step in the explanation should follow as seamlessly as possible. <sep> - The motivation for studying the margins between the training set and the decision boundary is not clear until Section 5.2.1 where it is mentioned that this is a widely used tool in learning theory. This should be presented earlier since not every reader will be completely familiar with learning theory. Moreover, it would also be useful to provide some intuition about how margins relate to the confidence of a classifier. <sep> - After presenting the main result of the paper — Theorem 4.1 — very little intuition is provided about each of the terms in the bounds. It would greatly increase the clarity of the result if each term was explained intuitively, so that the readers can gain the main insight of the paper before reading the proofs. This would also help motivate better the empirical evaluations in the following section. <sep> - The plots in FIgure 3 and Figure 4 are very difficult to understand and unintuitive. For Figure 3, the main reason the plots are difficult to read is because different colors are used for different networks. For Figure 4, it is difficult to understand what is happening because the tallest curves are plotted on top of the shortest ones. Hence, the information from the other curves is mostly lost. This seems like a very minor comment, but this graphs are not very complicated, so they should be easy to understand; yet it takes several minutes to take in what is happening in the graph. <sep> - For the proof of Lemma 4.1it is not clear how to get from Equation (7) to the main result of the lemma even after referencing Theorem 3 of Sokolic et. al. (2017) and Jia et. al. (2019). This could also be my lack of expertise on the topic; however, since the proof is already in the appendix, the proof should not be sparse in the amount of detail that it provides. <sep> ===== References ===== <sep> Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural Networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.2708039. <sep> Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal Deep Neural Networks. Technical report, 2019. URL http://arxiv.org/abs/1905.05929.","The paper investigates why adversarial training can sometimes degrade model performance on clean input examples. <sep> The reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations. On the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved. <sep> Overall, I think this paper explores a very interesting direction and such papers are valuable to the community. It's a borderline paper currently but I think it could turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue."
"This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. <sep> This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form. <sep> Detailed comments <sep> My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. <sep> Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \\tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \\tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. <sep> The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so? <sep> Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained? <sep> miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup? <sep> As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. <sep> Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method's limitations. <sep> The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened. <sep> Further questions and comments: <sep> - I am unable to parse Eq. 11 - what does the notation \\prod_i p_{\\tilde{x}_i, i} mean? <sep> - It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. <sep> -  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent? <sep> - Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \\hat{I}? <sep> - p^j_{ik} in Eq. 9 is undefined. <sep> - Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? <sep> References <sep> [1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018.","The reviewers were unanimous that this submission is not ready for publication at *CONF* in its current form. <sep> Concerns raised included that the method was not sufficiently general, including in choice of experiments reported, and the lack of discussion of some lines of significantly related work."
"This paper presents a classification model focused on interpretability. The model, Explaining model Decision through Unsupervised Concepts Extraction (EDUCE), is applied to a text classification task, while the authors argue in the appendix that this is also applicable to a wider problem, such as image classification. <sep> The model is composed of three parts: the first part is detecting salient spans of text relevant to the text classification problem, the second part assigning each salient span a concept label, and the third part which does the classification task based on the binary concept feature label. The models' loss is composed of two parts: (A) minimizing the cross-entropy of text classification loss and (B) minimizing the cross-entropy of concept classification loss. For (A), as the first and second part of the model introduce discrete choices, they use a RL with Monte-Carlo approximation of gradients. <sep> The system is evaluated under two measure: 1) classification accuracy and 2) concept accuracy. They define the concept accuracy as follows: after training, they train a classifier that takes output (in the form of <salient span, their concept label>) of the model from the test portion of the data. They split this output into train and test, and report the test accuracy. This aims to show how consistent is the labeling of the salient spans for different methods: if the concept label set correctly merged together semantically similar spans, this ""concept accuracy"" would be higher. This is a new metric they are proposing. While it is interesting, I would like to see *some* studies on how this correlates with human's judgements on how interpretable the model is. The paper is introducing a new measure *and* new model, and it's hard to be persuaded the model is doing well based on this new measure, when there is little ground to know what this measure really measures. <sep> Overall, I'm not impressed with the models' performances. The aspect rationale annotated beer sentiment dataset, presented by Lei et al (2016), has provided one of few opportunities to evaluate interpretability / rationale model quantitatively. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al's initial baseline or Bastings et al. While the paper argues this method isn't necessarily designed for this task unlike the other methods, I'm not sure this is necessarily the case. Bastings et al could be applied to other tasks that model is evaluated on, such as DBPedia and AGNews classification. The difference comes on how easy it is to interpret the methods, as these other rationale-based text processing methods would make use of captured words, while EDUCE would make use of detected ""concept"" clusters. Currently, the only real baselines are the ablations of its own model. <sep> Table 3 is quite interesting, different ""concepts"" capture different aspects fairly well. <sep> Not having a concept loss actually helps the classification accuracy. Would the concepts learned without concept loss qualitatively very different? This goes back to my original point that their new measure of ""concept accuracy"" is vague. <sep> Other comments and Q: <sep> - Figure (3), the visualization is a bit confusing cause it is unclear whether it is each span is a set of spans or a single span. Also, I would recommend making figures colorblind friendly, if possible. <sep> Q: what kind of classifier was used for the evaluation metric ""concept accuracy"" classifier? I don't think it's mentioned. <sep> Q: why are you sampling a test set for DBPedia experiments? Is it for efficiency reason? <sep> Q: how sensitive is model's performance to the hyper parameters, especially the number of concepts? <sep> Q: the current baseline classifier is a simple BiLSTM one, which definitely perform a lot worse than recent pre-trained LMs such as BERT. Would it be easy to use this method on top of richer representation such as pertained LM outputs? <sep> Q: how would this connects to saliency map literature in computer vision? I guess these would be mostly ""a posteriori"" explanations? Discussion would be helpful.","This paper introduces a method for building interpretable classifiers, along with a measure of ""concept accuracy"" to evaluate interpretability, and primarily applies this method to text models, but includes a proof of concept on images in the appendix. <sep> The main contributions are sensible enough, but the main problems the reviewers had were: <sep> A) The performance of the proposed method <sep> B) The lack of human evaluation of interpretability, and <sep> C) Lack of background and connections to other work. <sep> The authors improved the paper considerably during the rebuttal period, and might have addressed point C) satisfactorily, but only after several back and forths, and at this point it's too late to re-evaluate the paper. I expect that a more polished version of this paper would be acceptable in a future conference. <sep> I mostly ignored R1's review as they didn't seem to put much thought into their review and didn't respond to requests for clarifications."
"This paper proves generalization bounds for GANs. I think the paper can be improved significantly in several ways: <sep> 1- Writing: The first two sections are relatively well-written. The problem starts at section 3 and continues after that. Some of the things that can be improved: <sep> a) The discussion on the different definitions of generalizations is not really helpful in the current format. You might want to explain how these different definitions relate to each other. For example, if generalization in one of them implies generalization in the other one, etc. <sep> b)Theorem 2.3 is a general statement but it is followed by Corollary 3.3 which is a very specific generalization bound. There is no explanation how one can show the corollary.  Even worse is mixing these two in the proof of the theorem in the appendix.   Please consider improving the use of Theorems, Lemmas and Corollaries. <sep> c) Section 3 and 4 have bunch of theorems and collieries without much explanation. It is not clear that all of these are actually helpful for the main purpose of the paper. <sep> d) I don't completely understand the notation in Corollary 3.3. Eg. what is d_{f,\\ell}? <sep> 2) Related Work: I think authors need to do a more comprehensive literature review on generalization bounds. Since the generalization bounds presented here are built on the supervised learning bounds, authors discuss the generalization bounds in supervised learning. For example, authors heavily rely on Chen et al. (2019) for their generalization bounds while very similar results where shown before by [1] and [2]. <sep> [1] Neyshabur, Behnam, Ryota Tomioka, and Nathan Srebro. ""Norm-based capacity control in neural networks."" Conference on Learning Theory. 2015. <sep> [2] Golowich, Noah, Alexander Rakhlin, and Ohad Shamir. ""Size-independent sample complexity of neural networks."" Conference on Learning Theory. 2018. <sep> 3) Definition of generalization: I don't think the definition of generalization suggested in this work is much different than Arora et. al. since f really doesn't depend on samples from D_g and hence the empirical and true distributions are not very different. In fact, I think the definition provided by Arora et. al. 2017 is preferred because at the end of the day, we have to estimate the distribution D_g by generating some samples. <sep> 4) Generalization bound for fixed g: Unfortunately, the novelty of these generalization bounds are very limited as they are a direct application of known generalization bounds in the supervised settings. Therefore, the authors contributions are very limited here. <sep> 5) Generalization bounds for all generators: Again, here the novelty and final result is very limited since the bounds achieved by a union bound arguments and does not really go beyond that. <sep> 6) Experiments: Experiments can also be improved significantly. Currently, the correlation is reported for 5 trained networks and it is not clear to me that this result is statistically significant. Moreover, only one hyper-parameter is changed in the experiments which could be problematic. I suggest authors to change multiple hyper-parameters and train more networks to improve the evaluation. <sep> ****************************** <sep> After author rebuttals: <sep> I have read the authors response and looked at the revision. Unfortunately, many of my concerns are not addressed adequately so my score remains the same.","The authors received reviews from true experts and these experts felt the paper was not up to the standards of *CONF*. <sep> Reviewer 3 and Reviewer 1 disagree as to whether the new notion of generalization error is appropriate. I think both cases can be defended. I think the authors should aim to sharpen their argument in this regard.Several reviewers at one point remark that the results follow from standard techniques: shouldn't this be the case? I believe the actual criticism being made is that the value of these new results do not go above and beyond existing ones. There is also the matter of what value should be attributed to technical developments on their own. On this matter, the reviewers seem to agree that the derivations lean heavily on prior work."
"It is argued in this paper that GANs often suffer from mode collapse, which means they are prone to characterize only a single or a few modes of the data distribution. In order to address this problem, the paper proposed a framework called LDMGAN which constrains the generator to align distribution of generated samples with that of real samples in latent space by introducing a regularized AutoEncoder that maps the data distribution to prior distribution in encoded space. <sep> The major difference of this paper from many traditional GANs is to constrain the distributions of generated data same as distributions of true data in latent space instead of constrain the ability of discriminator. The authors detailed their motivation, the algorithm, and also reported a series of evaluation results on several datasets. <sep> Generally, this paper was well written. However, this paper has the following major concerns: <sep> （1） Though somewhat new, the novelty of this paper may be incremental to me. It looks like a combination of VEEGAN and AAE.  Though the authors mentioned that VEEGAN autoencoded the noise vectors rather than data items, and AAE exploited the adversarial learning in the encoded space rather than using an explicit divergence,   it appears not significant to me between the proposed model and these two models. At least the authors did not  address sufficiently how significant the proposed method would be. <sep> （2） The paper tested the proposed algorithm with a 2D Synthetic dataset. However, I found a lot of discrepancies in the results presented in Table 1 with other published works. The authors show 1 of mode captured on 2D Grid and 2D Ring using the VEEGAN method. However, the VEEGAN paper shows they get 24.6 and 8 on 2D Ring and 2D Grid respectively. Such discrepancies were also observed in Figure 3. These discrepancies must be explained. <sep> （3） In Figure 4, the authors showed the distribution of MODE scores for GAN and LDMGAN. From the figure, it seemed that LDMGAN improved the sample quality and diversity compared to GANs, but it is still prone to characterizing only a single or a few modes of the data distribution. In another word,  this may alleviate the problem but may not fully solve the problem.  Another minor point, the coordinate and legend are too small in this figure. It would be better if they become bigger. <sep> （4） The results of Table 4 is not convincing because the comparative methods are truly out-of-date. It would be more convincing if more latest methods can be compared with LDMGAN method. Those results reported are far lower than the state-of-the-art performance in these datasets. <sep> (5) There is a mistake in the second term of equation 11, it should be ⋯〖-D〗_KL (p^* (y)||p(y))  )","This paper proposes to mitigate mode collapse in GANs by encouraging distribution matching in the latent space. Reviewers 1 and 3 expressed concerns that the methodology is too incremental in the context of the existing literature (VEEGAN, VAE-GAN, AAE). This, combined with the lack of up-to-date baselines, makes it difficult to access the significance of the proposed modifications. The quality and precision of the writing can also be improved to meet the standards of publication at a top-tier conference."
"In this paper, the authors tackle the problem of multi-modal image-to-image translation by pre-training a style-based encoder. The style-based encoder is trained with a triplet loss that encourages similarity between images with similar styles and dissimilarity between images with different styles. The output of the encoder is a style embedding that helps differentiates different modes of image synthesis.  When training the generator for image synthesis, the input combines an image in the source and a style embedding, and the loss is essentially the sum of image conditional GAN loss and perceptual loss. Additionally, the authors propose a mapping function to sample styles from a unit Gaussian distribution. <sep> I think the idea of pre-training a style-based encoder is straightforward. I am mainly concerned about the performance of the presented approach. First, there are no many visual comparisons in the paper. The only visual comparison is in Figure 8, but results are only limited to faces. The visual results in Figure 5 do not look appealing to me. The change in the style mainly comes from the global change in color: no much change in the texture or local color. The ""night2day"" results look poor to me.  I am concerned about the diversity of the styles learned in the model. <sep> On the other hand, I am convinced that the proposed model is better than BicycleGAN, and the approach is somehow novel. The user study in Table 5 suggests that the proposed method is somehow better than BicyleGAN in visual quality on one task. My overall rating is borderline. <sep> Minor comments: <sep> - In the first sentence of Section 3.2, I do not think ""one-to-one correspondence"" is the right description. The encoder is not expected to be invertible. <sep> - In Equation (3), ""e_i"" is a little bit misleading. It does not mean the i-th element in {""e_j""}. You may want to replace ""e_i"" with ""s_i"" to avoid confusion. <sep> - The explanation of ours v1, v2, v3, v4 is not clear. It is also difficult to find its definition.","The submission describes a new two-stage training scheme for multi-modal image-to-image translation. The new scheme is compared to a single-stage end-to-end baseline, and the advantage of the new scheme is demonstrated empirically. All three reviewers appreciate the proposed contribution and the quality improvement it brings over the baseline. At the same time, the reviewers see the contribution as incremental and not sufficient for an *CONF* paper. The author response and paper adjustment have not changed the opinion of the reviewers, so the overall recommendation is to reject."
"This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores.    The sparsifying projections are posed as optimization problems, and algorithms for their computation, along with formula for their gradients are given.  Proof of the optimality of these algorithms relies significantly on prior work, so could not be checked deeply without bringing in additional sources. <sep> It is not clear that the motivation for these sparsifying objectives is sound.   The conventional softmax approach to attention weights should be capable of producing attention weights near zero, which would be effectively sparse, especially if the pre-activations, z_i, in equation (1), are allowed to have a large enough range.   It's not clear why weights should need to be zero exactly in the ignored regions, since being near zero should be sufficient to contribute almost nothing to the subsequent weighted sum.    So it is also not clear why the strict sparsity itself, as opposed to the effective sparsity of the softmax, should explain the differences in Figure 1, and in the results.  In particular it is unclear why the strict sparsity should prevent repetition; when looking at the weight distributions in the two cases, a more likely story seems to be that the weight distributions don't repeat as much from one word to the next in the second case, but there is no clear reason to attribute this to sparsity.   The pictures of the attention weights are lacking a color scale, so it is impossible to see how close to zero it comes in the unattended regions, although the gray color values chosen for these regions might be misleading. <sep> The TVmax approach, in addition to sparsity, also constrains the non-zero region to be contiguous.   To the extent that this improves performance, this presumably introduces an inductive bias that matches the data.   It is unclear why this fails to produce better objective scores than sparsemax, while producing better human ratings.   In any case it is not clear why this should necessarily be a good inductive bias for all images, although it is plausible that it helps in some cases. <sep> In many neural network problems, what makes a difference has more to do with the optimizability of the gradients, than the specific activations per se, and that might be the case here too, although the paper does not analyze this aspect of the proposed models. <sep> Overall the paper is flawed by the lack of clarity in the motivation for the proposed methods, and the lack of retrospective analysis and understanding of why the proposed methods should improve results.","This paper presents sparse attention mechanisms for image captioning. In addition to recent sparsemax based method, authors proposed to extend it by incorporating structural constraints in 2D images, which is called TVMAX. The proposed methods are shown to improve the quality of captioning, particularly in terms of fewer erroneous repetitions, and obtain better human evaluation scores. <sep> Through reviewer discussion, one reviewer updated the score to rejection. A major concern raised by the reviewers is that the motivation of introducing sparse attention is not clear, and the reason why it improves the quality (particularly, why it can reduce repetition) is not convincing. While we understand it is plausible for long sequences as in text domain, we are not convinced that it is really necessary for image captioning problems. Although authors seem to have some ideas, we cannot see how they will be reflected in the paper so I'd like to recommend rejection. <sep> I recommend authors to polish the paper with a clearer description of the motivation and high-level analysis of the method as well as testing on other visual tasks to show its generality."
"Summary <sep> This paper proposes a novel form of surprise-minimizing intrinsic reward signal that leads to interesting behavior in the absence of an external reward signal. The proposed approach encourages an agent to visit states with high probability / density under a parametric marginal state distribution that is learned as the agent interacts with its environment. The method (dubbed SMiRL) is evaluated in visual and proprioceptive high-dimensional ""entropic"" benchmarks (that progress without the agent doing anything in order to prevent trivial solutions such as standing and never moving), and compared against two surprise-maximizing intrinsic motivation methods (ICM and RND) as well as to a reward-maximizing oracle. The experiments demonstrate that SMiRL can lead to more sensible behavior compared to ICM and RND in the chosen environments, and eventually recover the performance of a purely reward-maximizing agent. Also, SMiRL can be used for imitation learning by pre-training the parametric state distribution with data from a teacher. Finally, SMiRL shows the potential of speeding up reinforcement learning by using intrinsic motivation as an additional reward signal added to the external task-defining reward. <sep> Quality <sep> As a practical paper, this work needs to be judged based on the quality of the experiments. I find the number of benchmarks and baselines sufficient. One major issue, that currently prevents me from voting for acceptance, is that experiments have not been conducted with enough seeds. I couldn't find any information in the paper regarding how many repetitions there are for each experiment. Also, most figures do not indicate any uncertainty measures (standard deviation, percentiles or the like)---some do, e.g. Figure (4), but it is not mentioned what type of uncertainty is depicted. One seed is certainly not enough to support the claims made by the authors---especially not the one that SMiRL can help improve RL in Figure (5). Figure (5a) clearly  does not draw a clear picture under one seed, and (5b) and (5c) require additional expert demonstrations. If experiments are repeated with more seeds and still support the claims, I am happy to increase my score to acceptance (presupposing that the discussion phase does not prevent acceptance for other reasons). <sep> Clarity <sep> The paper is clearly written and easy to follow. However, I do not like one aspect in the way the authors motivate their approach. The problem formulation starts with an MDP formulation. MDPs rely on a stationary reward signal and RL agents aim to optimize for future cumulative rewards based on these stationary reward signals. The authors propose to optimize for a non-stationary signal since the parametric state distribution changes over time. This itself is not uncommon and nothing controversial from a practical perspective. However, the statement that SMiRL agents seek to visit states that will change the parametric state distribution to obtain higher intrinsic reward in the future is controversial (see e.g. at the end of Section 2.1), because optimizing a non-stationary signal is outside the scope of the problem formulation. The reinforcement learning problem of maximizing intrinsic rewards does not know how the intrinsic reward signal is altered in the course of the future, i.e. how the parametric state distribution is updated. These statements should therefore be either adjusted accordingly, or the claims should be backed up theoretically rather than intuitively. On a minor note, I don't think that Figure (1) is necessary and the quote at the beginning of the paper might be better suited for a book chapter (but that is just my personal opinion). <sep> Originality <sep> I find the simple idea presented by the authors to minimize rather than maximize surprise quite original. However, I do not have much experience in the domain of intrinsic motivation and leave the judgement of originality to the other reviewers and the area chair. Also, some references might be missing regarding learning with intrinsic reward (e.g. empowerment). <sep> Significance <sep> The fact that a simple intrinsic reward, as presented by the authors, can lead to interesting behavior, as demonstrated by the experiments, is quite significant. Unfortunately, the experiments are not significant from a statistical perspective which is why I do not recommend acceptance at this stage (as mentioned above, depending on how the authors address this issue and the discussion period, I might change to acceptance). <sep> Update <sep> The authors have addressed my main concern regarding missing seeds. I therefore change to weak accept. But I am not happy how the authors responded to my concern regarding their motivation: <sep> 1.) The formulation that tries to justify their reasoning as given in the rebuttal was absent in the first version of this paper, but something along this line would have been necessary since the argument relies on a non-standard MDP formulation. <sep> 2.) I don't think the argumentation given in the rebuttal is correct. Imagine hypothetically the agent is in exactly the same augmented state s_t, a_t, \\theta_t at two different points in time, e.g. in the first episode and after multiple episodes. In both cases, the collected states seen so far are going to be different, hence the optimization objective for learning a marginal state distribution is different, hence the parameter updates are different, hence the transitions are not stationary. <sep> I do therefore encourage the authors to attenuate their wording.","This paper proposes augmentation of the state exploration strategy that is interesting and has a potential to lead to improvement. However, the current presentation makes it difficult to properly assess that. In particular, the way the authors convey both the underlying intuition and its implementation is fairly vague and does not build confidence in the grounding of the underlying methodology."
"Summarize what the paper claims to do/contribute. <sep> * The paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as ""domain-adaptive filter decomposition"". Each domain contributes its own share of features to be combined by a subsequent common layer. The method was benchmarked against competing methods on simple classification tasks and a hard semantic segmentation task. <sep> Clearly state your decision (accept or reject) with one or two key reasons for this choice. <sep> Weak Accept. <sep> * I think the paper was very well written, the explanations were clear and the technical contributions seem sound. <sep> * The experiments were satisfying for the most part. I would have wanted to see MNIST->SVHN for the unsupervised case as well, as this is a particularly hard one. <sep> Provide supporting arguments for the reasons for the decision. <sep> * Please do not use the Office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets. <sep> * For GTA->Cityscapes you are missing a few works eg, the CYCADA work. Also please use citations in the tables if you did not yourselves run experiments (as to make it clear that experimental protocols also might be slightly different etc). <sep> Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. <sep> * Tables 1 & 2: It'd be better to not refer to methods as A1,2,3 but rahter with some specific names or explicitly describe what these abbreviations mean in the caption of the table.","The paper proposes a domain-adaptive filter decomposition method via separating domain-specific and cross-domain features, towards learning invariant representations for unsupervised domain adaptation. <sep> Overall, this well-written paper is well motivated with a better technique for learning invariant representations using convolutional filters. Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain-invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results. <sep> Hence I recommend rejection."
"This paper investigates a list of methods to reduce the number of weights for deep RL architecture under the low-data regime. These methods include tensor regression, wavelet scattering, as well as second-order optimization (K-FAC). The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. <sep> However, I have some concerns on the novelty of this work and therefore I'm giving this paper a weak reject. Here are the reasons: <sep> To begin with, leveraging tensor structure of the neural nets to reduce number of parameters while maintaining similar level or getting even better results have been done before, for example: Tensorizing Neural Networks (Novikov et al, 2015), Learning compact recurrent neural networks with block-term tensor decomposition (Ye et al., 2018) etc. Although the use of tensor regression might be new, the core idea is still to leverage the low rank property of the tensor and obtain a compression of the weight tensors. Moreover, why use Tucker decomposition specifically for the tensor regression? It has been proposed that using tensor train (TT) decomposition can also get very good results (see Garipov et. al. Ultimate tensorization: compressing convolutional and FC layers alike). Is it possible to investigate the use of TT decomposition for the dense layer of the deep RL architecture? Therefore the novelty for this aspect seems a bit weak for me. <sep> The second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. For one particular game (demon_attack), this approach seems to outperform every other methods by a large margin. However the experiment shows that for the rest of the Atari games, there is a huge drop (45%) of performance. Therefore the significance of this approach is rather thin for me. Maybe some further investigation of the game demon_attack is needed to understand why using scattering in this game in particular gives such a huge performance boost. <sep> Thirdly, as an approximation of the second order optimization, K-FAC does not really concern with the main theme of the paper, which is an investigation of potential weights reduction methods. It is great that the authors applied this techniques and seems to have great results. However, as the authors pointed out, K-FAC has been wildly applied in the deep RL literature, and the authors did not propose new extension for the K-FAC method, therefore the contribution of this matter is also quite thin. <sep> Last but not least, the writing of the paper is a bit clumsy, and I was having a hard time to figure out what exactly is the proposed method. I think this paper might need some rework on the writing to describe the idea of the authors in a more clear way for the publication. Due to these reasons, I'm giving this paper a weak reject. <sep> Some writing comments and potential writing errors (did not affect the decision): <sep> Page 3, first line of ""Tensor regression layer"", the shape of the tensor X seems to be a typo. <sep> Also here, the definition of <X, Y>_N in the paper is to sum over the dimension of I_1…I_N, then the shape of <X, Y>_N should be K_1*…*K_x*L_1*…*L_y, without the I_N in the middle. <sep> Also in this section, the authors mentioned Tucker decomposition for the tensor regression. However the phrasing of this sentence needs a bit rework. The usage of ""For instance here"", gives the readers a feeling that Tucker is just one possible way of doing this decomposition, but not necessarily the actual decomposition for the reported experiments. <sep> In 2.3, there is a lack of definition for  \\Lambda_1 and \\Lambda_2. In addition, it would be better for the general readers to add a few definitions for the terminologies in this section. For example, ""circular convolution"", ""wavelet filter banks"" etc. I guess people with corresponding background will understand it with no problem, however I do find myself a bit lost in this section with these terminologies. <sep> 2.4 line 6, ""with A and. B smaller, architecture-dependent matrices"". I think it should be ""with A and B being…"" <sep> 3.1, line 5, ""This is all the more pressing that…."", I did not understand this sentence. <sep> In page 6, line 3, there is a lack of definition for ""compression rate"". Is it the compression rate w.r.t only the last dense layer, or w.r.t the whole network? <sep> Figure 4 is lacking y-axis and x-axis labels. <sep> 4.2, last bullet point, ""However, one must not forget that the conv layers one learns must be somehow be well adapted…"", I get what you are saying, but the sentence is a bit clumsy. <sep> Table 1 and 2, the row name ""Average"" is lacking definition. <sep> Overall it is a good attempt to reduce the number of weights in the deep RL architecture, but I do think the novelty of this work is a bit thin and the three contributions were not tied together with the main theme of the paper. Therefore, I'm giving this work a weak reject.","In this paper dense layers in deep neural networks representing policies are replaced by tensor regression layers, also by a scattering layer, and second-order optimization is considered. The paper does not have a single consistent message, and combines different techniques for unclear reason. Important related work is not cited. The presentation was found unclear by the reviewers."
"This work studies the head size <--> head number tradeoff in multihead attention. It argues and formally establishes that (1) the expressivity of an attention head is determined by its dimension and (b) fixing the head dimension, one gains additional expressive power by using more heads. In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. One can control the total amount of parameters by using smaller embedding sizes, making it comparable (in terms of #parameters) to standard multihead attention. Empirical results on language modeling and NLI tasks confirms the arguments. <sep> Pros: <sep> - The arguments on head size and head number tradeoff could be inspiring to future works. <sep> - A simple approach that proves strong in several NLP tasks. <sep> Cons: <sep> - The theoretical discussion imposes too strong assumptions that might make it less interesting in practice. <sep> - No NMT experiments. <sep> - The takeaway seems a bit trivial. <sep> Details: <sep> - Theorem 1 presents a rank-based view of each attention head's capacity, which is nice. Yet it is still unclear whether it is the case that the more expressive are the heads the better. For example, several recent works argues for specialized attention heads, i.e., each head has specific ""job,"" which may not require it being very expressive [1, 2]. Further, other works shows that a low-rank P matrix could be beneficial [3, 4, 5], which contradicts the argument in this work. It would be nice if the authors and discuss this in the revision <sep> (To be clear, I do believe this is still an open question, and do not think presenting a different view from previous works hurts the contribution of this work in any way.) <sep> - Theorem 2. I didn't carefully check the proof. Why is it required that the V matrices for each head have the same product. For both Thm.1 and 2, it would be nice to see some discussion on how they translate into the models in practice. <sep> - Can the authors compare the training/inference speed? It probably will be the same as standard transformers, but it would be nice to confirm. <sep> - Figure 1: the caption says trying out embedding sizes from 256 to 512. But it seems that only 4 values are tried. Can the authors comment on this? Also, it is a bit awkward to plot a line chart out of 4 points. Same for Figure 2. <sep> - It would be nice to see some NMT experiments. <sep> - The proposed method is so straightforward that I'm actually very surprised that this paper is the first trying this. The authors might need justify the technical contribution more. <sep> (I'm on the fence for this one, but the system doesn't allow me to. I'm happy to revise the score if the authors can address my concerns.) <sep> [1] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. https://arxiv.org/abs/1905.09418 <sep> [2] Are Sixteen Heads Really Better than One? https://arxiv.org/abs/1905.10650 <sep> [3] Generating Long Sequences with Sparse Transformers. https://arxiv.org/abs/1904.10509 <sep> [4] Generating Long Sequences with Sparse Transformers. https://arxiv.org/pdf/1904.10509.pdf. <sep> [5] Adaptively Sparse Transformers. https://arxiv.org/abs/1909.00015.","This paper studies tradeoffs in the design of attention-based architectures. It argues and formally establishes that the expressivity of an attention head is determined by its dimension and that fixing the head dimension, one gains additional expressive power by using more heads. <sep> Reviewers were generally positive about the question under study here, but raised important concerns about the significance of the results and the take-home message in the current manuscript. The AC shares these concerns, and recommends rejection, while encouraging the authors to address the concerns raised during this discussion."
"Summary. <sep> The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model's behavior. Three different datasets (i.e. image, gene expression, health-care) are chosen to evaluate the proposed model's effectiveness, while different regularizers (i.e. image prior, graph prior, and sparsity prior) are explored for the respective task. <sep> Strengths. <sep> 1. Incorporating human knowledge into the model has a growing interest in ML / CV communities. <sep> 2. Three datasets from different domains (i.e. image classification data, gene expression data, and health care data) are used to evaluate the effectiveness of the proposed approach. Data shows that the proposed approach shows better generalization performance (i.e. better performance in test dataset) than baselines. <sep> 3. The paper provides well-documented supplemental materials that contain details of the experimental setting and additional supporting figures. <sep> Weaknesses. <sep> 1. Task-specific heuristic human prior <sep> I agree (and personally like) the motivation that a method is needed to align a model's behavior with human knowledge or intuition -- model's behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. However, such an ability is achieved by simply adding task-specific heuristic functions as a penalty or a regularizer. Also, the introduced human priors are similar to general regularization conventions, i.e. a penalty of smoothness over adjacent pixels is commonly used in the CV community. I am concerned that only a limited set of expert-invented human priors can be used in this approach. <sep> Further, feature attribution methods aim to develop a richer notion of the contribution of a pixel to the output. However, the difficulty would be the lack of formal measures of how the network output is affected by spatially-extended features (rather than pixels). The explored priors (e.g. a total variation loss to make neighboring pixels have a similar impact on the final verdict) actually relieve this issue. <sep> 2. Incorporating humans into the modeling process? <sep> A key motivation behind this work is ""incorporating humans into the modeling process"". This would imply that (human-understandable) information needs first to be transferred from a model to humans. However, I am concerned about what information end-users are expected to obtain from the model. For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. Unless end-users cannot understand the model's behavior, how can we expect humans can provide knowledge to model? A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process. <sep> Minor comments. <sep> 1. Plots in Figure 3 are not intuitively understandable. <sep> 2. There is no section Conclusion. <sep> 3. A template for the reference section looks different from other *CONF* papers.","This work claims two primary contributions: first a new saliency method ""expected gradients"" is proposed, and second the authors propose the idea of attribution priors to improve model performance by integrating domain knowledge during training. Reviewers agreed that the expected gradients method is interesting and novel, and experiments such as Table 1 are a good starting point to demonstrate the effectiveness of the new method. However, the claimed ""novel framework, attribution priors"" has large overlap with prior work [1]. One suggestion for improving the paper is to revise the introduction and experiments to support the claim ""expected gradients improve model explainability and yield effective attribution priors"" rather than claiming to introduce attribution priors as a new framework. One possibility for strengthening this claim is to revisit experiments in [1] and related follow-up work to demonstrate that expected gradients yield improvements over existing saliency methods. Additionally, current experiments in Table 1 only consider integrated gradients as a baseline saliency method, there are many others worth considering, see for example the suite of methods explored in [2]. <sep> Finally, I would add that the current section on distribution shift provides an overly narrow perspective on model robustness by only considering robustness to additive Gaussian noise. It is known that it is easy to improve robustness to Gaussian noise by biasing the model towards low frequency statistics in the data, however this typically results in degraded robustness to other kinds of noise types. See for example [3], where it was observed that adversarial training degrades model robustness to low frequency noise and the fog corruption. If the authors wish to pursue using attribution priors for improving robustness to distribution shift, it is important that they evaluate on a more varied suite of corruptions/noise types [4]. Additionally, one should compare against strong baselines in this area [5]. <sep> 1. https://arxiv.org/abs/1703.03717 <sep> 2. https://arxiv.org/abs/1810.03292 <sep> 3. https://arxiv.org/abs/1906.08988 <sep> 4. https://arxiv.org/abs/1807.01697 <sep> 5. https://arxiv.org/abs/1811.12231"
"PAPER SUMMARY: This paper addresses the problem of encoding and decoding 3D chemical structures, with the ultimate goal of generating 3D crystal structures. The authors propose an auto-encoder framework for encoding the 3D locations of atoms in the crystal to a latent representation and then decoding that representation back into 3D structure. The paper's contributions can be summarized as follows: <sep> 1) A data representation that converts the 3D atom locations to a 3D voxel density map, so that they can be encoded by a standard 3D convolutional network. <sep> 2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. <sep> 3) The network is applied on unit cells of crystals or repeated unit cells from a dataset of crystal structures and the network is shown to be able to accurately reconstruct the 3D density maps, predict the number of atoms in the cell and perform fairly well in their classification into atomic types. <sep> I appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3D molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain.  If the presentation of results and experiments is improved, this could be a good application paper in material science with an interesting combination of techniques from machine learning and computer vision. It would be more appropriate to submit this paper to a domain-specific venue rather than to *CONF*. Therefore, I cannot justify its acceptance to *CONF* in its current format. <sep> Strengths <sep> ----------------------- <sep> 1. New domain: The paper addresses an interesting problem in a new domain. Previous work on generative models for molecules have concentrated on molecules with 1D structure (which can be represented as strings) or 2D structure (which can be represented as planar graphs). Generating 3D molecule structures is a challenging and interesting problem. <sep> 2. Efficient data representation: The authors bypass the difficulties associated with modeling sets of 3D points with arbitrary structure by proposing a canonical, voxel-based density representation. <sep> 3. Joint VAE + UNet training: The joint training of the encoder-decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction. <sep> 4. Qualitative results: There are nice visualizations of the reconstructed molecules and of the effect of the latent variable z. Figure 3 in particular does a great job at elucidating the outputs of the network and the reconstruction quality. <sep> Weaknesses <sep> ------------------------ <sep> 1. Limited methodological novelty: The methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications.  Their combination also seems straightforward, except for the joint training of the VAE and the segmentation network, which has not been tried before to the best of my knowledge. Networks have not been modified to exploit the intricacies of the new domain/task, such as symmetries, repetition of unit cells, large number of atoms. <sep> 2. Lack of comparison with other methods/baselines: There is no quantitative comparison with alternative baselines or methods or even discussion of such alternatives. It is understandable that the paper addresses a relatively new problem, however the method could be compared to CrystalGAN. Also, there is no justification of the advantages of using a voxel-based density map representation along with 3D convolutions vs, for instance, a graph representation along with graph convolutions (Xie & Grossman, [3]) or a point cloud GAN (Achlioptas). <sep> 3. No quantitative evaluation of the generative capabilities of the network: In the case of drug molecule generation, logP and QED scores  [4,5] are used to evaluate their drug-likeness. Without such scores in the crystal generation domain, it is not easy to judge how good a generated crystal is.  The need for some type of quantitative evaluation is especially important, since it is not as easy to qualitatively judge the quality of crystals obtained by sampling random latent vectors (Fig 5B) as in the case of generated images or text. The only such evaluation presented in the paper is related to the distribution of distances between atoms which seems to be close to their actual distribution in nature. <sep> 4. Presentation of quantitative results: The quantitative results are scattered in text throughout the results section without being summarized in a table. Results about the distance of generated atoms w.r.t to their true location, the predicted atom counts, predicted atom types etc. for the cases of single unit cells and repeated unit cells should be added to a table to complete a rigorous experimental evaluation. <sep> Additional Comments <sep> ------------------------- <sep> 1. Lack of novelty: The data representation, which is presented as one of the core contributions of the paper, is not new. Radial basis functions have been used to generate 3D density maps (e.g. [2]) and 3D Convolutional VAEs (Brock2016, [1]) have been employed on density maps (e.g. occupancy maps) on computer vision tasks. <sep> 2. Confusing description of the repeated unit cells case: It is not clear, especially for a general audience, how the repeated unit cells data are generated (in which directions are the unit cells repeated, how many times). It is also not clear how the training routine (conditioning) and the output post-processing (connected components + majority voting) is modified. <sep> 3. After Eq. (2), q is the --approximate-- posterior … <sep> Suggested References <sep> -------------------------- <sep> 1. VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition <sep> 2. VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation <sep> 3. Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs <sep> 4. Graph Convolutional Policy Network forGoal-Directed Molecular Graph Generation <sep> 5. Grammar Variational Autoencoder <sep> 6. 3D U-Net: Learning Dense VolumetricSegmentation from Sparse Annotation <sep> 7. PointConv: Deep Convolutional Networks on 3D Point Clouds","This paper presents an encoder-decoder based approach to construct a compressed latent space representation of each molecule. Then a second neural network segments the output and assigns an atomic number. Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations. <sep> The reviewers have several major concerns. Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. Secondly, there is no clear baseline to compare with. Finally, there is no clear quantitative results to measure the proposed method. The rebuttal did not well address these problems. <sep> Overall, this paper did not meet the standard of *CONF* and I choose to reject the paper."
"This paper addresses a limitation of BatchNorm: vulnerability to adversarial perturbations. The authors propose a possible explanation of this issue and correspondingly an alternative called RobustNorm to tackle this problem. Specifically, the authors observe that the statistics of BatchNorm for training and inference are different, resulting in different data distributions for training and inference. To solve this problem, the authors propose to use min-max rescaling instead of normalization. In addition, the running average is calculated with mean and the running mean of the denominator during inference. Experimental results show significant improvement of robustness and also comparable accuracy for clean data. <sep> The paper is well-written and the contributions are stated clearly. The explanation of vulnerability is reasonable. The proposed solution is simple but effective. <sep> However, I have several concerns: <sep> *The authors verify that the running average is the main culprit of vulnerability to adversarial attack, but provide no further investigation of why this happens. A possible solution is the drift in input distributions, but the manuscript does not state clearly how is the distribution changed. Further experiments would have made this claim more convincing. <sep> *The proposed method involves a hyper-parameter \\rho, but it may result in problematic issues. The variance of input is of the same order of magnitude as (max(x)-min(x))^2. If \\rho is set to other value, the magnitude of gradient will change drastically during back-propagation. Although \\rho can be set to 0.2, it still seems ad-hoc. Experiments on more datasets and the sensitivity of the proposed method to \\rho would have validated the claims of the authors.","This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing."
"The paper proposes an approach for learning graph convolutional networks for inferring labels on the nodes of a partially labeled graph  when only limited amount of labeled nodes are available. <sep> The proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem. <sep> The main idea here consists in relying on self training to get a better coverage of labeled nodes enabling learning with less deep models, this translates to a simple and intuitive algorithm. Using self training is not new in GCN but the way it is used here, computing adaptively a threshold for incorporating pseudo labels and using weights according to the confidence off predictions is new. <sep> Experimental results are reported on citation datasets and compared with many baselines show similar results as baselines when the coverage increases up to 50 labeled nodes /class, but the method brings significant improvements when the coverage is low (e.g. only few, <20, labels /class). <sep> Although the difference with previous approaches do not look like a huge step, the method seems to be quite justified empirically and achieve real good results wrt state of the art.","The paper is develops a self-training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes. The reviewers found the paper interesting. One reviewer notes the ability to better exploit available information and raised questions of computational costs. Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves. The final reviewer raised concerns on novelty and limited improvement in results. The authors provided detailed responses to these queries, providing additional results. <sep> The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time."
"This work aims to produce reinforcement learning methods that are 'distributionally robust'. They approach this by assuming the transition function may vary between elements of the domain distribution and extend some recent results (Blanchet & Murthy, 2019) to give some theoretical results (contraction and optimal deterministic policy) and an actor-critic algorithm. This is an important research area and the work takes a very nice approach. However, the clarity of the work and the empirical results could both use some work. <sep> There are a great many grammatical errors throughout the paper. The nature of the errors suggests a non-native speaker, which I really do not want to discourage, but this needs a proofread and edit. One exception, there is an actual typo just after assumption 1 ""dfnied"", but actually better grammatically to just remove the word entirely. <sep> In the intro, the citation of Mannor et al. 2004; 2007 for what is essentially a description of the gap between simulation to real transfer is a very strange choice. Is it possible this was a mistake? <sep> The description leading up to the main result could be made much clearer. I went through this section, the main results, and the proofs in the appendix. Things look good, although I do feel like (as just said) it could be made much better/clearer. One snag I hit was in the proof of Lemma 1, everything is good except I don't know why ""u_1 \\le u_2 + \\gamma \\| u_1 - u_2 \\|_\\infty"" must hold, but I might have simply missed something obvious so please help me out. <sep> The experimental results are very minimal and not very convincing. It is not immediately clear to me that Figures 1&2 actually show that the robustac algorithm is more robust than ac. The performance is worse than ac, which is completely understandable, but for most of the environments ac is still significantly better. <sep> Lastly, a small point to notice that this paper bleeds into the 9th page, and considering the amount of contributions (largely down to the two theoretical results and a very small experiment) I do not think the extra space is warranted. This could be compressed and actually come out stronger as a result of being forced to be clearer and more concise. <sep> Update: <sep> Thank you for your response, especially the fix to the proof. I will keep my score, but do believe that further experiments and small adjustments to the writing will see a future version of this accepted.","This paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance. The reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments. In its current form the paper is not ready for acceptance to *CONF*-2020."
"Summary: <sep> This paper presents an efficient stochastic neural network architecture by directly modeling activation uncertainty and adding a regularization term to encourage high activation variability by maximizing the entropy of stochastic neurons. Compared with other existing approaches, such as Bayesian neural networks and variational information bottleneck, the proposed architecture is simpler to implement and faster to train. The authors also achieve state of the art results in various fields, including network compression by pruning, adversarial defense and learning with label noise. <sep> Major comments: <sep> - Overall, I find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to VIBNet. <sep> - The derivation of the max-entropy term is somewhat unclear and I think the paper needs a major revision on this part. The authors suggest using a Gaussian with a finite mean and an infinite variance as the non-informative prior for the produced Gaussian random variable z (in Eq. 1), and then minimize the KL divergence between the produced Gaussian and the infinite-variance Gaussian. However, this may be questionable from a Bayesian viewpoint, in the sense that the infinite variance leads to an improper prior as the variance increases without bound and thus may produce an improper posterior distribution. This is not discussed and needs to be clarified in Sect. 2 (Max-entropy Regularization). <sep> - There are things unclear in the derivation (last line of Eq. 2), since log(\\sigma_2) trends to infinity. <sep> - In addition, the penalty terms for different types of tasks are directly given only based on some of the assumptions that the authors have made, there does not seem to be any theoretical justification for such choices. <sep> Minor comments: <sep> - Some of the notations used in this paper seem a bit confusing, which may hinder readability. For example, on page 3, in ""The non-informative prior is a Gaussian with arbitrary mean (\\mu_1) and infinite variance (\\sigma_1)"", I guess \\sigma means the standard deviation? I would like to recommend using N(\\mu, \\sigma^2) to denote a Gaussian distribution, where \\sigma means the standard deviation and \\sigma^2 the variance. <sep> - On page 3, in ""where \\sigma(h|\\theta) denotes the predicted standard deviation of hidden unit h given the neuron uncertainty prediction parameter \\theta"", there is no discussion on the neuron uncertainty prediction parameter. Does it means the predicted standard deviation is again parameterized by \\theta?",This paper proposes to use stacked layers of Gaussian latent variables with a maxent objective function as a regulariser. I agree with the reviewers that there is very little novelty and the experiments are not very convincing.
"This paper proposes a neural network architecture consisting of multiple independent recurrent modules that interact sparingly. These independent modules are not all used simultaneously, a subset of them is active at each time step. This subset of active modules is chosen through an attention mechanism. The idea behind this architecture is that it would allow the different modules to specialize in different mechanisms and that would allow compositionality. The empirical results suggest that the proposed approach is able to generalize better than traditional architectures (which all have the implicit assumption that all processes interact). <sep> This paper is well-written and it provides a very thorough empirical analysis of the proposed idea. Because it is not in my area of expertise I'm not confident that I can assess its novelty or its relationship to other existing approaches. <sep> In terms of presentation, I recommend the authors to enlarge some of the figures in the paper (e.g., I can't read the small box in Figure 1) and to not use citations as nouns (e.g., ""The mechanisms of this attention mechanism follow (Vaswani et al., 2017; Santoro et al., 2018), with the …""). I would also like to point out that although fairly different in how they tackle the problem, the work of Arjovsky et al. (2019) seems to be related to this one. <sep> Three questions I believe were not answered in the paper are: <sep> 1) How is the performance related to the total number of subsystems (and the number of *active* ones). I can only see results related to that in Table 1, but the variation in the number of modules is pretty small (4-6). The results also don't give any indication whether we want to have more modules active at each time step, if there's a sweet spot, etc. It is said that the method seems to be robust to this choice but this claim is made because it performs similarly for the values 5 and 6 if I recall correctly. <sep> 2) Is there any incentive in this architecture for a module to not simply ""give up""? I mean, the modules are not necessarily incentivized to be used as often as possible, so could it be the case that a module learns to set its weights to zero? <sep> 3) Would it make sense to present baseline results for an architecture that uses attention? It seems to me that LSTM was often the baseline of choice but RIMs have two important components: multiple LSTMs and an attention mechanism. Could the attention mechanism be explaining some of the results we are seeing? <sep> Finally, despite the very long appendix, I feel there are important details missing with respect to the empirical setup, at least in the Atari experiments which I'm more familiar with. Was stochasticity used, that is, sticky actions (Machado et al., 2018)? Moreover, for how long was PPO (and RIMs-PPO) trained in terms of number of frames? Finally, I'd recommend the authors to include a table with the actual average (and standard deviation) performance in each Atari games. It is really hard to know how well a method is doing by just squinting at learning curves. It is hard to know if the results are significant without a notion of variance. <sep> References: <sep> Martín Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz: Invariant Risk Minimization. CoRR abs/1907.02893 (2019) <sep> Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, Michael Bowling: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. J. Artif. Intell. Res. 61: 523-562 (2018) <sep> ------ <sep> >>> Update after rebuttal: I stand by my score after the rebuttal. <sep> Unfortunately I'm not an expert in this area and I don't feel confident in having a very strong opinion about this paper, willing to fight for its acceptance. I also agree with concerns raised by other reviewers. As I stated in the discussion with the authors, the clarifications and additional experiment does improve the paper a bit.","This paper has, at its core, a potential for constituting a valuable contribution. However, there was a shared belief among reviewers (that I also share) that the paper still has much room for improvement in terms of presentation and justification of the claims. I hope that the authors will be able to address the feedback they received to make this submission get where it should be."
"This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The tunability of the optimizer is a weighted sum of best performance at a given budget. The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer's learning rate is likely to be a very good choice; it doesn't guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations. <sep> Comments: <sep> The paper is easy to follow. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons: <sep> In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of ""sharpness"" of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions. Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails. <sep> In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. <sep> The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? The authors seems interchangeably using ""runs"" and ""iterations"", which makes the concept more confusable. <sep> The authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO. My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. <sep> My major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM. Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD. <sep> The author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement? <sep> [1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451 <sep> [2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489 <sep> [3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350 <sep> [4] Rethinking the Hyperparameters for Fine-tuning https://openreview.net/forum?id=B1g8VkHFPH","The paper proposed a new metric to define the quality of optimizers as a weighted average of the scores reached after a certain number of hyperparameters have been tested. <sep> While reviewers (and myself) understood the need to better be able to compare optimizers, they failed to be convinced by the proposed solutions. In particular (setting aside several complaints of the reviewers with which I disagree), by defining a very versatile metric, this paper lacks a strong conclusion as the ranking of optimizers would clearly depend on the instantiation of that metric. <sep> Although that is to be expected, by the very behaviour of these optimizers, it makes it unclear what the added value of the metric is. As one reviewer pointed out, all the points made could have been similarly made with other, more common plots. <sep> Ultimately, it wasn't clear to me what the paper was trying to achieve beyond defining a mathematical formula encompassing all ""standard"" evaluation metric, which I unfortunately see of limited value."
"[Overview] <sep> In this paper, the authors proposed a shuffle strategy for convolution layers in convolutional neural networks (CNNs). Specifically, the authors argued that the receptive field (RF) of each convolutional filter should be not constrained in the small patch. Instead, it should also cover other locations beyond the local patch and also the single channel. Based on this motivation, the authors proposed a spatial shuffling layer which is aimed at shuffling the original feature responses. In the experimental results, the authors evaluated the proposed ss convolutional layer on CIFAR-10 and ImageNet-1k and compared with various baseline architectures. Besides, the authors further did some ablated analysis and visualizations for the proposed ss convolutional layer. <sep> [Pros] <sep> 1. The authors proposed a new strategy for convolutional layers. The idea is borrowed from the biological domain, and then transformed to a spatial shuffling layer which can shuffle the feature response at each convolutional layer. <sep> 2. The authors performed experiments on both small-scale dataset (CIFAR-10) and large-scale data (CIFAR-100) for evaluations. <sep> 3. The authors further added some ablated analysis on the proposed model. Specifically, the authors visualize the receptive field which can be used in ss layer compared with the original convolutional layer, which indicates that ss layer can incorporate the global context at the very beginning. <sep> [Cons] <sep> 1. The motivation behind the proposed ss layer is not explained very well. Though the authors mentioned that it is biologically inspired, I would not buy that since it is still a unclear phenomenon, and even it is true, using a randomized shuffling seems not align with the observations to some extent. <sep> 2. The paper is poorly written in general. The motivation behind the proposed method, and the presentation of method section part are cluttered much. In the model analysis in experiment section, the presentation and explanations are also vague and not clear to me. <sep> 3. The proposed model seems increase the baseline models' performance very marginally on all architectures. It is hard to say that it is because the shuffling layer enable the neurons to incorporate the global context information. Instead, it might just because the randomization which would increase the generalization ability of the trained model. <sep> 4. Finally, the comparison with previous models, such as SENet, ShuffleNet, etc are not systematically. I would like to see a more comprehensive summarization of the differences between the proposed ss layer and other architectures, because all of them are trying to incorporate more contextual information from other channels or locations. <sep> [Summary] <sep> Overall, I think the proposed ss layer is still a reasonable way to incorporate the contextual information in CNNs. However, the poor presentation and the weak experimental results and analysis make the paper overall a one under the bar of the venue. I would suggest the authors revise the paper with more well-motivated formula and more solid experiments and analysis in the next submission.","The paper is well-motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top-down mechanisms. However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets."
"This paper extends the unpooled mention pair model of annotation (MPA) (Paun et al., 2018b) with the hierarchical priors (e.g., mean and variance) on the ability of the annotators. The proposed method was evaluated on Phrase Detectives 2 corpus, which was annotated by players in a game with a purpose setting for coreference resolution. To control the sparsity of the dataset, the authors split annotations from larger player workloads into smaller batches, and assumed that each batch was produced by a different player. The experimental results show that, when the data is sparse, the proposed method (CommunityMPA) worked better than MPA on Phrase Detectives 2 corpus in terms of mention-pair accuracy, silver-chain quality, and the performance of the state-of-the-art method trained on the aggregated mention pairs. This paper also includes a discussion about the inferred community profiles. The comparison with the traditional approaches that consider communities showed that the proposed method is comparable to the traditional approaches. <sep> I am wondering of the connection between community and sparsity. This study assumes that knowledge of communities (spammers, adversarial, biased, average or high-quality players) would allow to regularize the ability of the annotators towards. In P2, this paper wrote, ""This partially pooled structure can prove effective in conditions of sparsity where there are not enough observations to accurately estimate the ability of the annotators in isolation."" I have two questions here: why is it effective to consider community for reducing the problem of the sparsity? If the knowledge of communities is useful, why did the advantage of the proposed method disappear in Figures 1, 2, and 3 when we used all data? <sep> In addition, I'm not convinced with the idea of ""breaking the larger player workloads into smaller batches"" for simulating the sparsity and communities. This treatment introduces quite a few shadow users whose capabilities are exactly the same, and deviates from the reality of the user community. Does this treatment favor the proposed method over MPA more than necessary? I am also wondering why evaluating portions of the dataset where annotations were made by 'sparse' users would not work to highlight the effectiveness of the proposed method for sparse users. <sep> The impact of this paper would be greater if the experimental results could support the importance of modeling user community on the real data. The authors may justify the simulation procedure of the sparsity because 98.67% of Phrase Detectives 2 corpus was annotated by those who produced more than 40 annotations. However, I also think that it is important to show how the proposed method is effective on the real data with sparse annotators. Currently, Table 1 showed no improvement over the conventional methods. <sep> Minor comment <sep> In Section 2.1: It was difficult to separate which part is the base model (MPA) and the novel proposal without reading Paun et al. (2018b).","Thanks to the reviewers and the authors for an interesting discussion. The reviewers are mixed, learning toward positive, but a few shortcomings were left unaddressed: (i) Turning the task into a mention-pair classification problem ignores the mention detection step, and synergies from joint modeling are lost. (ii) Lee et al. (2018) has been surpassed by some margin by BERT and spanBERT, models ignored in this paper. (iii) Several approaches to aggregating structured annotations have already been introduced, e.g., for sequence labelling tasks. [0] Overall, the limited novelty, the missing baselines, and the missing related work lead me to not favor acceptance at this point. <sep> [0] https://www.aclweb.org/anthology/P17-1028/"
"In this paper, the authors investigate the use of ellipsoidal trust region constraints for second order optimization. The authors first show that adaptive gradient methods can be viewed as first-order trust region methods with ellipsoid constraints. The authors then show that the preconditioning matrix of RMSProp and Adam can be used as norm inducing matrices for second order trust region methods. This ellipsoidal trust region method is empirically compared with first order gradient methods and spherical second order trust region methods. <sep> Overall the paper is nicely written and very easy to read. The ideas are interesting. However, I have a number of concerns/questions about the work, that I list below. <sep> 1. Why is the preconditioning matrix of RMSProp/Adam a reasonable norm inducing matrix? One can show that the empirical Fisher is not an accurate curvature matrix in general, and so there is no reason to believe this would in fact enforce the proper ellipsoidal trust region for the method? See for example: https://arxiv.org/pdf/1905.12558.pdf. <sep> 2. I am also not convinced that Figure 2 actually shows that the curvature matrix is diagonally dominant. How do I interpret a value of 40 or 50 for this metric, and why does it imply that it is diagonally dominant? <sep> 3. The experiments also do not look very convincing to me. How sensitive is the algorithm to the hyperparameters like lambda1 and lambda2? I am also a bit confused about why different batch sizes were used for the first order gradient methods and the second order TR methods? The method overall doesn't seem to be able to match first order gradient methods, and it is not clear whether this is because of using the RMSProp/Adam preconditioner as a curvature matrix. <sep> Given these concerns, I consider this paper to be borderline. I am happy to have a discussion with the authors and the other reviewers and change my score however. <sep> ========================================= <sep> Edit after rebuttal: <sep> I thank the reviewers for their response. While the updated paper has certainly improved, I think the paper still requires a much more thorough experimental evaluation. I am sticking to my score.","This paper interprets adaptive gradient methods as trust region methods, and then extends the trust regions to axis-aligned ellipsoids determined by the approximate curvature. It's fairly natural to try to extend the algorithms in this way, but the paper doesn't show much evidence that this is actually effective. (The experiments show an improvement only in terms of iterations, which doesn't account for the computational cost or the increased batch size; there doesn't seem to be an improvement in terms of epochs.) I suspect the second-order version might also lose some of the online convex optimization guarantees of the original methods, raising the question of whether the trust-region interpretation really captures the benefits of the original methods. The reviewers recommend rejection (even after discussion) because they are unsatisfied with the experiments; I agree with their assessment."
"The paper suggests to use temperature scaling in adversarial attack design for improving transferability under black-box attack setting. Based on this, the paper proposes several new attacks: D-FGSM, D-MIFGSM, and their ensemble versions. Experimental results found that the proposed methods improves transferability from VGG networks, compared to the non-distillated counterparts. <sep> In overall, I liked its novel motivation and simplicity of the method, but it seems to me the manuscript should be improved to meet the *CONF* standard. Firstly, the presentation of the method is not that clear to me. The mathematical notations are quite confusing for me as most of them are used without any definitions. I am still not convinced that the arguments in Section 3.1 and 3.2 are indeed relevant to the actual practice of black-box adversarial attacks, which usually includes extremely non-smooth boundaries with multiple gradient steps. Even though the experiments show effectiveness partially on VGGNets, but the overall improvements are not sufficient for me to claim the general effectiveness of the method unless the paper could provide additional results on broader range of architectures and  threat models. <sep> - I feel Section 2.3 is too subjective with vague statements. The following statement was particularly unclear to me: ""The first problem with gradient based methods is that they lose their effectiveness after a certain number of iterations."": Does the term ""effectiveness"" indicate some relative effectiveness compared to other methods, e.g. optimization-based attacks? Is this really a general phenomenon in gradient-based attacks? Also, please elaborate more on ""So, insufficient information acquisition for different categories and premature stop of gradient update are the reasons ..."" <sep> - Regarding that the softmax is the problem, one could try to directly minimize the logit layers skipping the softmax, i.e., gradient on logits? This is actually one of common techniques and there are many simple tricks in the context of adversarial attack, so the paper may include comparisons with such of tricks as well. <sep> - It is important to specify the exact threat model used throughout the experiments, e.g. perturbation constraints and attack details. Demonstrating the effectiveness on a variety of threat models could also strengthen the manuscript. <sep> - Table 1 and 2 may include other baseline (black-box attack) methods for comparison. This would much help to understand the method better.","This paper proposes an attack method to improve the transferability of adversarial examples under black-box attack settings. <sep> Despite the simplicity of the proposed idea, reviewers and AC commonly think that the paper is far from being ready to publish in various aspects: (a) the presentation/writing quality, (b) in-depth analysis and (c) experimental results. <sep> Hence, I recommend rejection."
"This paper presents a method for doing RL from demonstrations in continuous control tasks. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Results on 5 sparse reward mujoco tasks show that it out-performs other related methods. <sep> The motivation for the paper is difficult to follow. They claim that using demonstration data in a supervised manner ""cannot generalize supervision signal over those states unseen in the demonstrations,"" but most of these approaches are using neural networks and definitely are generalizing those signals to other states. Whether they're generalizing accurately or not is a different question. In contrast, they say that reward shaping approaches do not suffer that problem because they evaluate trajectories rather than states, but there will still be a problem of generalizing to new trajectories. <sep> The abstract is even more confusing as it tries to jump straight into the issues with these approaches without any explanation. I don't think there's enough space in the abstract to go into that level of detail. <sep> The description of DQfD and DDPGfD in the related work is not accurate. They're described as ""treating demonstration data as self-generated data,"" but in fact they both add supervised losses to more closely match the demonstrated data. https://ieeexplore.ieee.org/document/8794074 is another method built on DDPG that has both a critic and actor loss like yours and would make a useful comparison. <sep> The related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. <sep> The end of the related work section is not very clear, you say these methods are problematic because ""the adopted shaping reward yields no direct dependence on the current policy"" but there's no explanation or motivation for why that would be a problem. <sep> Assumption 1 seems like a very strong assumption that would not be true for many human experts. <sep> For the experiments, I wonder about the impact of only using sparse reward tasks. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. How do the methods compare on the unmodified tasks? There was nothing specific in your algorithm that meant it should specifically address sparse reward tasks. What about tasks that are naturally sparse reward? <sep> Overall, the algorithm is interesting and the results are nice. The motivation and related work need to be made clearer to situate this work with the other related works. And the experiments should go beyond these tasks that have been modified to have sparse rewards. <sep> The revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept.","The paper proposes to combine RL and Imitation Learning. It defines a regularized reward function that minimizes the KL distance between the policy and the expert action. The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert's distribution is multiplied to the regularized term. <sep> Several issues have been brought up by the reviewers, including: <sep> * Comparison with pre-deep learning literature on the combination of RL and imitation learning <sep> * Similarity to regularized MDP framework <sep> * Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim <sep> * Difficulty of learning the indicator function of the support of the expert's data distribution <sep> Some of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all. The reviewer believes that learning such a function requires ""learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments."" <sep> Another issue is related to the policy invariance under the optimal expert policy. In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1. <sep> Overall, it seems that even though this might become a good paper, it requires some improvements. I encourage the authors to address the reviewers' comments as much as possible."
"This paper introduces a new step-size adaptation algorithm called AdaX. AdaX <sep> builds on the ideas of the Adam algorithm to address instability and non-convergence issues. <sep> Convergence of AdaX is proven in both convex and non-convex settings. The paper also provides an empirical comparison of AdaX against its predecessors <sep> (SGD, RMSProp, Adam, AMSGrad) on a variety of tasks. <sep> I recommend the paper be rejected. I believe the convergence results could be a significant contribution, but the quality of the paper is hampered by its experimental design. The paper felt generally unpolished, containing frequent grammatical errors, imprecise language, and uncited statements. <sep> My main issue with the paper is the experimental design. I am not convinced that we can draw valid conclusions from the experimental results for the following reasons: <sep> - The experiments are lacking important details. How many independent runs of the experiment were the experimental results averaged over? All of the experiments have random initial conditions <sep> (e.g. initialization of the network), and should be ran multiple times, not just once. <sep> There's no error bars in any of the plots, so it's unclear whether AdaX really does provide a statistically significant improvement over the baselines. <sep> Similarly, the data in all the tables is quite similar, so without indicating the spread of these estimates its impossible to tell whether these results are significant or not. <sep> - How were the hyperparameters and step-size schedules chosen? The performance of Adam, AMSGrad, and <sep> RMSProp are quite sensitive to their hyperparameters, and the optimal hyperparameters are problem-dependent. <sep> Some of the experiments just use the default hyperparameters; this is insufficient when trying to directly compare the performance of these methods, as their performance can vary greatly with different values of these parameters. I'm not convinced that we should be drawing conclusions about the relative performance of these algorithms from any of the experiments for this reason. <sep> Of course, meaningful empirical results are not necessarily characterized by statistically outperforming the baselines. <sep> Well designed experiments can highlight important ways in which the performances differ, providing the community with a deeper understanding of the methods investigated. I would argue that the experiments in the paper do not achieve this either; the experiments do not provide any new intuition or understanding of the methods, showing only the relative performances in terms of learning curves on a somewhat random collection of supervised learning problems. Why were these specific problems chosen? What makes these problems ideal for showcasing the performance of AdaX? If AdaX is an improvement over Adam, why? What exactly is happening with it's effective step-sizes that leads to the better performance? Can you show how their step-sizes differ over time? <sep> Statements that need citation or revision: <sep> - ""Adaptive optimization algorithms such as RMSProp and Adam... as well as weak performance compared to the first order gradient methods such as SGD"" (Abstract). This needs a citation. <sep> Similarly, ""AdaX outperforms various tasks of computer vision and natural language processing and can catch up with SGD""; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly outperforms Adam in deep neural networks. <sep> -  ""In the era of deep learning, SGD ... remains the most effective algorithm in training deep neural networks"" (Introduction). What are you referring to here? Vanilla SGD? Or are you including Adam etc here? <sep> As above, this should have a citation. Adam's popularity is largely due to its effectiveness in training deep neural networks. <sep> - ""However, Adam has worse performance (i.e. generalization ability in testing stage) compared with SGD"" <sep> (Introduction). Citation needed. <sep> - In the last paragraph of the Introduction, you introduced AdaX twice: ""To address the above issues, we propose a new adaptive optimization method, termed AdaX, which guarantees convergence..."", and, ""To address the above problems, we introduce a novel AdaX algorithm and theoreetically prove that it converges...""","This paper analyzes the non-convergence issue in Adam in a simple non-convex case. The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non-convex settings. The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms. I agree with the reviewers' evaluation and thus recommend reject."
"Overview: In this paper, a meta-learning approach is proposed to perform link prediction across multi-graphs with scarce data. To do so, each graph is treated as a link prediction ""task"". Different from the tasks in conventional meta-learning, the graphs here are generally non i.i.d. Based on the variational graph auto-coders, the method learns two important components: the global parameters used for GNNs and a graph signature function used to modulate the parameters of inference model based on the history of observed training graphs. <sep> The idea of formulating link prediction as a few-shot learning problem and solving it via multi-graph meta-learning is novel. The approach basically takes advantage of meta-learning and is expected to generalize well across graphs. The numerical study is extensive in discussing the properties of meta-graph and showing more attractive empirical performance than the existing approaches. <sep> Comments: <sep> 1. The explanation of Figure 1 needs to be improved. In its current form, the figure does not illustrate the idea of this paper very clearly. It would be better to provide more explanations on the graphical model for meta-graph and the meta-graph architecture in the context. <sep> 2. The description of Algorithm 1 is confusing in some places. For example, the operation in Line 6 is commented as *compute graph signature*. However, what's actually going on is to set the signature function to stop gradient computation which seems not matching the comment. <sep> 3. In section 4.2, the 5-gradient update AUC is reported to show the fast adaptation of meta-graph. However, in my opinion, it could be much more informative to show the accuracies of different gradient steps, like what has done in the experimental study of MAML. I don't think the current experimental results can well explain the property of fast adaptation. <sep> === update after author response === <sep> Thank you for your response.   I find my main concerns properly addressed in the feedback.","This paper presents a new link prediction framework in the case of small amount labels using meta learning methods. The reviewers think the problem is important, and the proposed approach is a modification of meta learning to this case. However, the method is not compared to other knowledge graph completion methods such as TransE, RotaE, Neural Tensor Factorization in benchmark dataset such as Fb15k and freebase. Adding these comparisons can make the paper more convincing."
"The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. <sep> The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. <sep> In particular, the output distribution is approximated by dividing the output range of each neuron into K intervals. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. <sep> The whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution. <sep> The paper also presents a good experimental campaign that shows good performances. <sep> The paper is really well written and enjoyable, clear in its description and in the objectives it aims to achieve. To improve the readability a bit further, I would suggest trying to move equation 1 after or close to its reference, or at least to describe before it what KL is. <sep> Moreover, a similar problem is present for TI, which is introduced in Algorithm 1 and described later. I would add, for example in the input section of the algorithm, a sentence stating that TI is used in getCandidate and described later. <sep> The first sentence on page 5 seems to be incomplete as it is written. I would suggest rephrasing the sentence. <sep> In Algorithm 2, about the two ""for"" cycles for i in 1,m and foreach k in 2,K, I suggest unifying them and use the same cycle (only for reasons of readability). Moreover, I think that using braces instead of parentheses would be more correct in these cycles. <sep> It is interesting to note that in the first line for VGG19, last column of Table 3, the accuracy is lower with KL < 0.001 than with KL < 0.005. I would expect monotonicity here. Do the authors have any idea of the reasons for this? <sep> Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems. <sep> Typos: <sep> On page 6, penultimate paragraph, the word ""the"" is repeated twice in the parentheses. <sep> On page 8, at the beginning of the first sentence after Table 3, there is a comma that seems to be useless after the word ""that"". <sep> On page 8, ""When the termination crite gets stricter"", crite should be corrected in criterion. <sep> There is a typo in the README of the github project linked in the paper: ""coveraeg data"" instead of ""coverage data"".","This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training. The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations. The algorithm has two-phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold. <sep> This approach is incremental in nature -- the resulting multi-objective optimisation problem is not a significant improvement over BOT. After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications. Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant. Hence, I will recommend the rejection of this paper. To merit acceptance to *CONF* the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach."
"This paper considers the effect of network width of the neural network and its ability to capture various intricate features of the data. In particular, the central claim of this paper is what the title claims ""Wider networks learn features that are better"". They make this claim using the visualization technique called ""activation atlasses"". They find that wider networks learn features in the hidden neurons that are more ""interpretable"" in this visualization framework. Additionally, they also notice that fine-tuning a _linear model_ using the learned features for the wider networks provide better accuracy for new (but related) tasks over the shallower counterparts. For most experiments of this paper, ""shallow network"" refers to a width of 64 and ""wide network"" refers to a width of 2048. The main datasets used for the experiments are MNIST, CIFAR 10/100 and a ""translated"" version of MNIST images. <sep> Overall the paper is written well and the ideas and results are communicated crisply. I have a few comments. First, regarding the related work, I think that the reader would be served better if the authors also list the recent works related to effect of network width on convergence and generalization (e.g., [1] and references that cite this). The reason I say this is so that the reader should not (wrongly) interpret that this is the first work that finds ""favorable"" properties of wider networks (the paper does not make this claim, but it is easy for a reader to interpret it). Second, I find it slightly concerning that a lot of findings have been extrapolated from just one architecture. In particular, I find the experiments in section 5 to be the most informative (and also objective), since it is a single number which is easy to think about. To be clear, I like the visualization experiments and it gives credibility to the claim about interpretability. Given that there are many levers in a neural net (batch norm, architectural choices, hyper-params etc.) one could fiddle with, to make the claim made in the introduction one needs a more extensive set of experiments. I acknowledge that the authors say they haven't explored the possibility of fine-tuning the hyper-params for instance, but I think considering some of these choices is really helpful. This will help _isolate_ the effect of width independent of the architecture choice. <sep> Given the above observations, my current decision of this paper is that it doesn't meet the bar. I find the results promising but the paper is not yet ready. <sep> [1] - https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf","This paper investigated the effect of network width on learned features using activation atlases. From the current view of deep learning, the novelty of the paper is limited. <sep> As all reviews rejected the paper and the authors gave up rebuttal, I choose to reject the paper."
"Summary: This paper proposes a new mechanism for computing the attention scores efficiently in Transformers to avoid the quadratic computational cost in conventional Transformers (i.e., when computing the QK^T for self-attention). With a content-based ""routing"" technique (""group/cluster attention"" may be a more accurate term to differentiate with the dynamics of Capsule Networks), the time steps of each layer form different clusters, and the self-attention mechanism is only performed within each such cluster. The authors verify their approach on very large-scale and challenging sequence benchmarks such as WikiText-103 (WT103) and enwik-8. <sep> Pros: <sep> + The math notations are generally clear (e.g., dimensionality) and the paper is well-organized. <sep> + The paper addresses a very important (efficiency) question in Transformers, and gave a proper review to prior related works (such as Child et al.). <sep> + The experiments used to evaluate the model are all large-scale, high-dimensional and challenging tasks. The routing Transformer seems to be able to do very well on the WikiText-103 word-level language modeling task. <sep> + I especially like the analysis of the local vs. routing attention via the JSD score. It's a very interesting and useful way to compare the attention maps without visualizations (which many other works use). <sep> -------------------------------- <sep> Cons/Questions: <sep> 1. Please number your formulas and equations. <sep> 2. Based on your definition of X' (at the end of page 3, Xi′=∑j<inAijVj) and the subsequent Eq. (2), it seems that you did not add layer normalization and residual connection to the self-attention block. However, almost all prior works that use the Transformer architecture, such as Vaswani et al. [1], Child et al. [2] and Dai et al. [3] has two residual blocks: 1) X' = layernorm(SelfAttention(X) + X); and 2) X'' = layernorm(mlp(X') + X'). Is there any reason that you abandoned this design and used Eq. (2)? Could that have impacted the model performance? In addition, it seems you didn't scale QK^T by 1/\\sqrt{d}, as did in prior works? <sep> 3. How are the k-means centroid vectors (\\mu_1, ..., \\mu_k) initialized? Picking the initial centers for k-means clustering has been long known as an interesting/challenging problem, especially in a high-dimensional space (which tasks like language modeling deal with). This challenge spawned works such as K-means++ [4]. I suggest the authors discussing their initialization scheme as well as the initialization's impact on the effectiveness of their approach. <sep> 4. In Section 4.2, you said ""FG⊤ where F and G are binary matrices denoting cluster memberships of queries and keys respectively"". However, your previous description seems to suggest that the clusters are formed based on the R matrix (with shape n x d), which contains both Q and K. Does this mean F = G^T? Or could position i in query and position i in key be in different clusters? (I think ltr(FG⊤∗A) would only make sense if F=G^T.) This part is confusing in the paper; please clarify. <sep> 5. The inequality at the bottom of page 5 (**feel free to clarify if I missed some assumptions from the paper**): given that ∥a−b∥2=(a−b)⊤(a−b)=a⊤a+b⊤b−2a⊤b, it's unclear in this case (a=Qi−Kj and b=Qj−Ki) whether 2a⊤b is positive or negative (even if Qi,Ki are unit vectors). Therefore, I'm not sure why it is a >= relation here. <sep> If this is incorrect indeed, then the subsequent analysis the authors made on \\epsilon would be a bit problematic, too. (Additionally, the authors seem to suggest that Qi⊤Kj=Qj⊤Ki in the subsequent analysis, which should not be the case, unless W_Q = W_K.) Also, just curious, how did you regularize WR to be orthonormal while training? <sep> 6. The authors acknowledge at the end of Section 4 that their method of assigning clusters ""does not guarantee that each point belongs to a single cluster"". This means that even with balanced cluster sizes, each cluster will have more than n/k routing vectors (i.e., time steps). This makes the O(nkd+n2d/k) bound the authors mentioned potentially inaccurate. On the other hand, without forcing such cluster balance, in the worst case, the complexity is still O(n^2d), correct? <sep> 7. The paper proposes to use n clustering centers, which makes sense from the perspective of minimizing total computations. However, since the cluster centroids are learned parameters, does this mean the trained routing Transformer cannot easily generalize to different sequence lengths easily? (e.g., train on sequence length 256 but test on sequence length 2000, or simply 15?) Moreover, on longer sequences, won't the sorting/clustering be increasingly inefficient? <sep> 8. What is the motivation of using half of the heads for local attention and the other half for routing attention (cf. the intro paragraph of Section 5)? Why not just use all of the heads with routing attention? Without ablations on this, it's hard to tell which part is contributing how much exactly. <sep> 9. Could you also show the # of parameters used by your model in Table 1-3 (as did in prior works)? <sep> 10. Do you observe better performance with deeper layers (which seems to be the case for the sparse Transformer [2])? <sep> 11. Except the computational concern, I still don't quite exactly see the motivation behind clustering attention. For language modeling, for instance, the time steps should be mutually dependent, and it's more like a connected, chain-like structure (where each word depends strongly on its neighbors). However, the cluster attention seems to group the words together, and there is no cross-cluster attention (i.e., the graph is broken into smaller components). In addition, the authors said the centroid parameters (\\mu_1, ..., \\mu_k) are shared across sequences. So in a certain sense, are they like the ""anchors"" in the (transformed) word embedding space? Are we then, in language modeling tasks, performing attentions among words only around these ""anchors""? <sep> 12. While the theoretical complexity may be lower, how does the wall-clock time of an L-layer routing Transformer compare to an L-layer conventional Transformer (on the same sequence length, batch size)? In particular, although the routing mechanism avoids computing A, having to deal with each of the n clusters means that one will need to process the cluster-based attentions sequentially (and, as mentioned, you have 8 heads for local attention and 8 for routing attention, which I think are also processed in two steps?). Would this make the model actually slower on GPUs? <sep> ===================== <sep> Minor issues that didn't impact the score: <sep> 13. In the equation at the bottom of page 3 (the definition of X_i'), shouldn't it be (j <= i) instead of (j < i), in the case of autoregressive sequence modeling? E.g., in language modeling, you do use the current word when predicting the next word. <sep> 14. At the end of Section 5.1, you said: ""while having fewer [...] attention heads"". I believe the released Transformer-XL model also used 16 heads? (https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/run_wt103_large.sh) <sep> 15. How does the number of cluster centers influence the performance of the model? I think this would be an interesting ablation study to make. <sep> ===================== <sep> While I'm impressed with the result the routing Transformer achieved on WikiText-103, I am not fully convinced the effectiveness of the approach (e.g., on the other tasks, the routing Transformer seems to be slightly worse than the SOTA transformer; this is less surprising, as the authors didn't change the underlying design of the Transformers). I think the paper can be improved by including more elements, such as ablative experiments and runtime benchmarking. Also, there seem to be some problems with the derivations that the authors made in the current version of the paper. <sep> [1] https://arxiv.org/abs/1706.03762 <sep> [2] https://arxiv.org/abs/1904.10509 <sep> [3] https://arxiv.org/abs/1901.02860 <sep> [4] http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf","This paper proposes a new model, the Routing Transformer, which endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention from O(n^2) to O(n^1.5). The model attained very good performance on WikiText-103 (in terms of perplexity) and similar performance to baselines (published numbers) in two other tasks. <sep> Even though the problem addressed (reducing the quadratic complexity of self-attention) is extremely relevant and the proposed approach is very intuitive and interesting, the reviewers raised some concerns, notably: <sep> - How efficient is the proposed approach in practice. Even though the theoretical complexity is reduced, more modules were introduced (e.g., forced clustering, mix of local heads and clustering heads, sorting, etc.) <sep> - Why is W_R fixed random? Since W_R is orthogonal, it's just a random (generalized) ""rotation"" (performed on the word embedding space). Does this really provide sensible ""routing""? <sep> - The experimental section can be improved to better understand the impact of the proposed method. Adding ablations, as suggested by the reviewers, would be an important part of this work. <sep> - Not clear why the work needs to be motivated through NMF, since the proposed method uses k-means. <sep> Unfortunately several points raised by the reviewers (except R2) were not addressed in the author rebuttal, and therefore it is not clear if some of the raised issues are fixable in camera ready time, which prevents me from recommend this paper to be accepted. <sep> However, I *do* think the proposed approach is very interesting and has great potential, once these points are clarified. The gains obtained in WikiText-103 are promising. Therefore, I strongly encourage the authors to resubmit this paper taking into account the suggestions made by the reviewers."
"The authors present a framework to perform meta-learning on the loss used for training. They introduce the Baikal loss, obtained using the MNIST dataset, and <sep> BaikalCMA where the coefficients have been tuned. The evaluation of these loss functions is performed on the MNIST and CIFAR-10, and according to the results they converge faster, towards lower test error and need fewer samples to obtain results similar to the cross-entropy loss. <sep> The claims are clearly stated and the framework is detailed, the experiments cover all the potential benefits of the Baikal loss. However it seems that some potentially critical points have been omitted. The cross-entropy loss is well known to be beneficial in dataset with severe class imbalance. The two datasets used for evaluation are perfectly balanced, it might beneficial to see how it performs in the unbalanced case. <sep> I have a couple of concerns about the method. First about step ""(1) loss function discovery"": The initial population starts with trees of depth at most <sep> 2, and the final solution(Baikal) has either 2 or 3 (depending on which definition of depth is chosen). It is unclear that the genetic optimization is superior to simply choosing random loss functions. I think it would be relevant to add a figure that shows how the fitness of the leader of each generation evolves over time. <sep> The second step ""(2) coefficient optimization"", while objectively generating a loss function that was superior on the metrics evaluated, raised some questions. In equation (2) the factor ""1.5352"" seems to be equivalent to adding a constant to the loss, which should not impact optimization. Also the factor <sep> ""2.7279"" seems to be equivalent to a change in learning rate. This may be an indication that the learning rate search was not done thoroughly. It would be beneficial to clarify when it is happening: a) For each individual of the population during step (1), b) before performing CMA, c) after CMA. Also: Was learning rate search was performed on the network trained with Cross-Entropy? It was not entirely clear from the experiment details in Appendix A.2.1. <sep> About the Baikal loss itself, I fear that it could produce models that have very poor calibration, it might be nice to evaluate that (even if it is only in the appendix). <sep> While the paper does a great job at presenting the problem and its applications and propose a framework that generated a loss that can transfer to other datasets without any tuning required. I think it lacks a more thorough evaluation and description of the dynamics observed during the genetic evolution, and the performance of the Baikal loss on other datasets (my quick experients with it on ImageNet diverged I did not have the time necessary to tune the hyper-parameters). <sep> Minor remarks: <sep> There might be a slight omission in section 3.1: according to Figure 1, exp(x) <sep> is one of the potential unary operators explored by the GLO framework. However it is not present it the list of operators. Could you clarify this? <sep> To the best of my knowledge, in the machine learning literature, it seems that the letter x is used to denote the prediction and y for the ground truth. The fact that this paper used the opposite convention confused me the first time I <sep> read it.","This paper proposes a GA-based method for optimizing the loss function a model is trained on to produce better models (in terms of final performance). The general consensus from the reviewers is that the paper, while interesting, dedicates too much of its content to analyzing one such discovered loss (the Baikal loss), and that the experimental setting (MNIST and Cifar10) is too basic to be conclusive. It seems this paper can be so significantly improved with some further and larger scale experiments that it would be wrong to prematurely recommend acceptance. My recommendation is that the authors consider the reviewer feedback, run the suggested further experiments, and are hopefully in the position to submit a significantly stronger version of this paper to a future conference."
"This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. The authors generalize existing offline RL approaches to an actor-critic algorithm that regularizes the learned policy such that it can stay close to the behavior policy. Based on prior work, the authors state that there are two variants of regularizations to the behavior policy, value penalty (vp) and policy regularization (pr) and three choices of divergence functions along with their sample estimate that measures the distance between the learned policy and the behavior policy. The paper empirically investigates the effectiveness of each regularization scheme as well as each divergence function and conclude that vp is slightly more effective than pr while all divergence functions have similar performances. <sep> Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. My score would be increased given some technical insights or some promising results in a relatively novel offline RL algorithm in the author's response. <sep> Specifically, the paper does a thorough ablation study on BEAR, BCQ, and KL-control within the BRAC framework. While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence. <sep> Though BRAC summarizes offline RL methods in a neat way, it would be more technically sound if a general theoretical analysis/insights of offline RL algorithms can be offered in the paper, e.g. showing the reason that vp is outperforming pr through convergence analysis in the tabular case. <sep> Minor comment: <sep> Error bars should be added to the all the bar plots. <sep> **UPDATE** <sep> After reading the rebuttal, I think my concerns are addressed and thus I updated my rating.","This paper is an empirical studies of methods to stabilize offline (ie, batch) RL methods where the dataset is available up front and not collected during learning. This can be an important setting in e.g. safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified. Since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized. This paper studies various methods to perform such regularization. <sep> The reviewers are all very happy about the thoroughness of the empirical work. The work only studies existing methods (and combination thereof), so the novelty is limited by design. The paper was also considered well written and easy to follow. The results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline (although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these). Bigger differences were observed between ""value penalties"" versus ""policy regularization"". This seems to correspond to theoretical observations by Neu et al (https://arxiv.org/abs/1705.07798, 2017), which is not cited in the manuscript. Although unpublished, I think that work is highly relevant for the current manuscript, and I'd strongly recommend the authors to consider its content. Some minor comments about the paper are given below. <sep> On the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points. Due to the high selectivity of *CONF*, I unfortunately have to recommend rejection for this manuscript. <sep> I have some minor comments about the contents of the paper: <sep> - The manuscript contains the line: ""Under this definition, such a behavior policy πb is always well-defined even <sep> if the dataset was collected by multiple, distinct behavior policies"". Wouldn't simply defining the behavior as a mixture of the underlying behavior policies (when known) work equally well? <sep> - The paper mentions several earlier works that regularize policies update using the KL from a reference policy (or to a reference policy). The paper of Peters is cited in this context, although there the constraint is actually on the KL divergence between state-action distributions, resulting in a different type of regularization."
"This paper proposes a multi-task dynamical system for sequence generation. The model learns a number of parameters that represents the latent code z. The learned model can generate the customized individual data sequence and provide the smooth interpolation in the sequence space. The experiments on the synthetic data and the ocap dataset show the customization is beneficial for prediction tasks and enables for style transfer and morphing within generated sequences. <sep> The mathematical definition and the proof in this paper are well justified. However, I had a hard time understanding the contribution of this paper. <sep> - The main motivation of this paper is to treat each sequence as a task in the training set (customization of the individual data sequence). What are the advantages of this setting for sequence generation? <sep> - Separate parameterization of the latent variable z for different tasks seems to be a key idea in this paper. What is the main benefit of parameterizing each latent variable? Does it improve diversity? <sep> - If not, what is the key idea that the proposed model can generate diverse sequences compare to other generative models (e.g., GAN or VAE-based models). <sep> - The authors claim that the proposed approach provides grater data efficiency. How is it compare to other generative models? <sep> - Overall, it is not clear the benefit of the proposed model over existing algorithms. This paper needs to provide comparisons with any other existing models, especially on Mocap data, <sep> - What are the differences between pooled models and single-task models? <sep> ---- <sep> I increase the rating after rebuttal, since the idea is interesting and the paper has been improved. However, I still think the experiments and the comparisons are unconvincing.","This work proposes a dynamical systems model to allow the user to better control sequence generation via the latent z. Reviewers all agreed the that the proposed method is quite interesting. However, reviewers also felt that current evaluations were weak and were ultimately unconvinced by the author rebuttal. I recommend the authors resubmit with a stronger set of experiments as suggested by Reviewers 2 and 3."
"The paper makes the observation that likelihood models trained with batch norm assign much lower likelihoods to ""training batches"" of OoD data (batch norm statistics computed over over minibatch) than evaluation batches of OoD data (batch norm statistics over entire training set). <sep> One issue with comparing this method to most other OoD detection works is that it considers OoD detection on *batches* of (all OoD data) or (all in-distribution data). As soon as the problem is changed to ""classify between OoD batches"" and not single samples, there are a large number of possible statistical tests one can perform to perform OoD (T-test between likelihoods of each batch) and the problem becomes *much* easier. In some ways, this makes things more well-defined (hard to compare distributions when one of them is just a single sample from an arbitrary distribution). <sep> However, that brings me to a big concern I have with the evaluation protocol. The batch size used for train/evaluation is rather large (64, this detail is hidden in the Appendix and I would have appreciated the number put in the main experiments section). If you take a likelihood model and evaluate on 64 samples from SVHN, you are all but guaranteed to sample a sample with *exceedingly* low likelihood, which dominates the mean statistic, making it possible to separate SVHN batch from CIFAR10 batches. I suspect that OoD datasets have plenty of these ""extremely low likelihood"" examples that will drag the mean likelihood down a lot. <sep> This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. In evaluation mode, likelihoods for each OoD sample are evaluated independently, which results in a similar observation to prior work showing that CIFAR10 likelihoods are inaccurate for SVHN. In other words, I think there is a mistake made here: it is the phenomenon that *batch likelihoods*, not *batch norm*, that is responsible for this method working well. One experiment that is missing from your paper (and would prove my hypothesis wrong) would be if you adapted the OoD criteria to compare the *mean* likelihoods in the evaluation mode, and show that for OoD datasets, the difference between batches still remains small. <sep> Nits: <sep> - ""...such as learning a mixture of Gaussians"", I believe this toy example was on univariate gaussians, not mixtures. <sep> - Choi et al. 2018 and should also be included in the citation that ""CIFAR10 gives higher likelihood estimates to SVHN than CIFAR10 ones"" (this was a concurrent discovery between the two papers) <sep> - Choi et al. 2018 is not the right citation for ""we evaluate the area under the ROC curve (AUC) and average precision (AP)"" for each binary classification task, a more appropriate one would be Hendryks and Gimpel 2017. <sep> - No doubt the authors realized already but Page 7 has some ""related work still needs some work, but there...."" which should be deleted. <sep> - It took me awhile to find the batch size used in training and evaluation mode (64), which was on page 16.","The authors observe that batch normalization using the statistics computed from a *test* batch significantly improves out-of-distribution detection with generative models. Essentially, normalizing an OOD test batch using the test batch statistics decreases the likelihood of that batch and thus improves detection of OOD examples. The reviewers seemed concerned with this setting and they felt that it gives a significant advantage over existing methods since they typically deal with single test example. The reviewers thus wanted empirical comparisons to methods designed for this setting, i.e. traditional statistical tests for comparing distributions. Despite some positive discussion, this paper unfortunately falls below the bar for acceptance. The authors added significant experiments and hopefully adding these and additional analysis providing some insight into how the batchnorm is helping would make for a stronger submission to a future conference."
"This paper presents a new reversible flow-based graph generative model wherein the whole graph i.e., representative attributes such as node features and adjacency tensor is modeled using seperate streams of invertible flow model. This allows training of generative model using exact likelihood maximization over the underlying graph dataset.The model avoids encoding any domain specific heuristics and thus can be applied to any structured graph data. The paper focusses it applicability for molecular graphs. Given that this approach avoids sequential generation of graph, it is faster by an order of magnitude than prior models for molecular generation. Empirical experiments on couple of molecular graph data suggets that GraphNVP approach performs as well as prior approach but albeit without any rule checker. <sep> My major concern with such invertible models is ""scalability"". Given that flow-based model are required to retain the original dimension its applicability is limited to low dimensional feature vectors. In the case of GraphNVP, this means limited number of node labels as well as edge labels. Additionally, since it limits adjacency tensor, this would lead to modeling graphs with few nodes. However, if integrated with encoder-decder model some of these limitation can be overcome. Given this major weakness and with limited novelty (i.e., extending to adjacency tensor), I am inclined to reject this paper. I shall improve my rating if GraphNVP is applied to general graph structures - synthetic / real. <sep> Few more limitations: <sep> 1. Although paper claims one-shot generation of graphs, in reality it seems otherwise. Since every layer processes only on single node, overall it operates sequentially from one node to another. <sep> 2. Moreover, this same sequential processing yet again limits it applicability to small graphs. <sep> 3. As in MolGAN, the direct generation of adjacency tensor leads to training with fixed size graphs i.e., through the addition of virtual nodes.  It is not possible to train model with variable number of nodes. <sep> 4. As pointed by authors, their model is not node permutation invariant. <sep> Clarification: <sep> 1. Are the function 's' and 't' fixed across time ? For QM9 with max of 9 atoms and 27 layers, each atom attribute is processed multiple times. Are they processed using same functionality of s and t ? <sep> 2. Is it possible to model permutation invariance by augmenting the training data using multiple permutation of nodes such as BFS, DFS, degree, k-node (see GRAN) ? <sep> 3. I understand GraphNVP can reconstruct perfectly. But I fail to note the actual significance of such metric. If it reconstruct 100% or not how does it matter ? What matters is unniqueness, validity and novelty. <sep> 4. Can you please compare inference time ? <sep> 5. How difficult is it to integrate validity checker with your generation process ? Can we have some comparison using it ? <sep> Minor: <sep> 1. In eq (2) please use different notation for layer 'l' and node 'l'. <sep> 2. Page 4, penultimate line: So as functions s and t -? To model functions s and t","The authors propose an invertible flow-based model for molecular graph generation. The reviewers like the idea but have several concerns: in particular, overfitting in the model, need for more experiments and missing related work. It is important for authors to address them in a future submission"
"This paper presents a M-product based temporal GCNs to handle dynamic graphs. Experiments on four real datasets are performed to verify the effectiveness of the proposed model. <sep> Overall, I think this paper make a few contributions to advocate tensor M-product. However, there are several big issues as listed below. Given the current status, I could not accept the paper. <sep> Pros: <sep> 1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way. <sep> 2, The experimental setup is reasonable. Datasets are collected from practical problems and of moderately large scale. <sep> 3, The paper is clearly written and easy to follow. <sep> Cons & Questions: <sep> 1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time. For example, the M-transform is just applying 1x1 convolution to multi-channel image. Slice-wise matrix multiplication is also common in practice. <sep> 2, Moreover, I think there are several challenges in the M-product formulation which prevent the technique from being practical. <sep> (1) Sharing M such that frontal slices of the transformed signal are the same, i.e., each row of M share the same vector, limits the model capacity significantly. If there is no sharing mechanism, then the model learned on one sequence of graphs could not be applied to another sequence of graphs given two sequences have different lengths. <sep> (2) If you learn M from data, how could you ensure that M is invertible? In the paragraph before section 4.1, an edge classification formulation is proposed where the inverse M-transform is abandoned. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation? <sep> 3, A few temporal GCN baselines are neither compared or discussed, e.g., [1]. <sep> 4, Could you explain why all the other GCN variants performs significantly worse with a symmetrized adjacency matrix compared to using the asymmetric one? <sep> [1] Li, Y., Yu, R., Shahabi, C. and Liu, Y., 2017. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926. <sep> ====================================================================================================== <sep> After I read authors' reply and other reviewers' comments, I would like to keep my original rating as the issues have not been properly addressed. I agree with the Reviewer #4 that the theoretical results are a bit artificial and trivial.","The paper proposes a tensor-based extension to graph convolutional networks for prediction over dynamic graphs. <sep> The proposed model is reasonable and achieves promising empirical results. After discussion, it is agreed that while the problem of handling dynamic graphs is interesting and challenging, the proposed tensor method lacks novelty, the theoretical analysis is artificial, and the empirical study does not cover enough benchmarks. <sep> The current version of the paper is not ready for publication. Addressing the issues above could lead to a strong publication in the future."
"The paper proposed ``TriMap''---a novel dimensionality reduction technique that learns to preserve relative distances among points in a triplet. The paper has done extensive experiments and presents the results in very nice visualizations. The paper is clearly written. <sep> Major merits of this paper are: <sep> 1. The proposed method seems effective. <sep> On some datasets (e.g., S-curve), the learned low-dimensional embeddings indeed look very good. And the proposed method has much less runtime than other baselines as shown in table-1. <sep> 2. The paper is well written. <sep> However, I am still leaning towards rejecting this submission because the proposed method is lack of necessary justification. <sep> 1. Many important technical decisions on this method seem arbitrary, including the parametrization of functions (e.g., s, \\omega, \\zeta, etc) and values of hyperparameters (e.g., \\gamma and \\delta). For function forms, the authors should justify why particular parametrization has been chosen; for the hyperparameters, the authors should clearly explain how they are picked---maybe using domain knowledge or tuned on data? <sep> 2. The argument for ``global score'' is not clear enough. There are at least two points that need clarification. <sep> First, ``non-local'' is not equal to ``global''. The proposed method indeed considers non-local (or non-near) points while learning embeddings, which helps preserve non-local information. But I am not convinced that the preserved information is actually global. Maybe what helps is to first define ``global'' in a dimensionality reduction context. <sep> Second, the definition of ``global score'' depends on another baseline method (i.e. PCA), which seems odd. A principled evaluation (or a score) should be method-independent. What seems right to me is to compute global score (i.e. how much global information has been preserved) by comparing to some statistics in the (high-dim) x space, not to another method. <sep> Moreover, the authors had a strong claim that the proposed ``GS is the only DR performance measure that can reflect this property''---it doesn't sound right and why one can't just use another score which is monotonic wrt the proposed score? The authors mentioned that ``PCA has the lowest possible MRE''---but this is only right up to the use of a linear transformation and F-norm, so this shouldn't be a justification for my questions above. <sep> 3. What is the reason for the successful runtime? <sep> The authors didn't clarify why the proposed method is theoretically faster than the baselines. <sep> What I noted is: the authors chose a subset \\mathcal{T} for the TriMap method and used \\mathcal{T} throughout the paper---is it a typo or is a subset always chosen? If the latter holds, then how was it chosen and how large is it compared to the training data used by other methods? In the end, is the proposed method faster because it uses less data? <sep> Besides the weakness above, I also suggest the authors evaluate their method with some extrinsic evaluations. What's currently used is only intrinsic---the embeddings are trained to preserve relative distances and are evaluated on a trade-off between local accuracy and the defined global score. It is fine because extensive visualizations are provided and readers can subjectively judge the quality of the learned embeddings. However, the experimental section can be stronger if the authors can show the learned embeddings are better at helping some downstream tasks than other baselines (by preserving non-local information?).","This paper proposes a new dimensionality reduction technique that tries to preserve the global structure of the data as measured by the relative distances between triplets. As Reviewer 1 noted, the construction of the TriMap algorithm is fairly heuristic, making it difficult to determine how TriMap ought to behave ""better"" than existing dimensionality reduction approaches other than through qualitative assessment. Here, I share Reviewer 2's concern that the qualitative behavior of TriMap is difficult to distinguish from existing methods in many of the figures."
"This paper describes a connection between flatness of minima and generalization in deep neural networks. The authors define a concept called ""feature-robustness"" and show that it is related to flatness. This is derived through a straightforward observation that perturbations in feature space can be recast as perturbations of the model in parameter space. This allows the authors to define a (layerwise) flatness measure for minima in deep networks (this layerwise flatness measure is also invariant to rescalings of the layers in neural networks with positively homogenous activations). The authors combine their notion of feature robustness with epsilon representativeness of a function to connect flatness to generalization. They present a few empirical evaluations on CIFAR10 and MNIST. <sep> I believe this paper is able to once again confirm the relationship between flatness and generalization in an empirical manner with their layerwise measure of flatness. I am not so convinced about the theoretical justification that they claim to provide and thus do not recommend acceptance. <sep> Theory - The key theorem relating generalization and flatness is Theorem 10 which says that if a compositional model is feature robust and the output of the first component is an epsilon-representative for the second component, then the compositional model will generalize. While this is interesting, it is not clear to me that this guarantees generalization for deep neural networks. This result only talks about feature robustness and representativeness for a particular layer. If a deep network has many layers, will the feature robustness layers closer to the input guarantee feature robustness at deeper layers? That might require a further unit operator norm constraint on the layer operator, which is a restriction on the types of weights that can be used. If a sample is epsilon representative at one layer, what is required for the next layer to be epsilon representative for the rest of the deep network? This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. <sep> Another idea that I think arises from Theorem 10 is that the flatness of loss landscapes is important when you have learning problems where the hypothesis class is compositional. While flatness is only spoken of in the case of deep neural networks, can we identify the same phenomenon in other problems? I would encourage the authors to try and identify another model in which the flatness-generalization relationship exists (even empirical evidence would suffice for now). This would strengthen the case for studying flatness and biasing optimization towards flatter solutions in the case of deep networks. <sep> Experiments - This section seems to be pretty rudimentary, I would like to see more results on different kinds of network architectures (VGG? Inception? AlexNet?), more datasets (KMNIST? Fashion MNIST? SVHN?), and possibly more repetitions. At one point the authors mention that they declare a minimum has been reached if the training loss is < 0.07. Atleast on CIFAR10 and MNIST it is possible to achieve training loss <1e-4 so am not sure if the networks that the authors are testing are minima at all (It is important for them to be minima since the flatness measure is only defined at minima). Can the authors also identify more situations other than large batch vs small batch training that would lead them to obtain flatter/sharper minima? <sep> The authors also claim that measuring generalization using test error is flawed, but do not provide details about their method of measuring generalization. I would want to see these details and a more thorough discussion of why measuring generalization through test error is flawed. <sep> While this is an interesting paper, I do not believe it is ready for acceptance at *CONF* 2020.","The authors propose a notion of feature robustness, provide a straightforward decomposition of risk in terms of this robustness measure, and then provide some empirical evidence for their perspective. Across the board, the reviewers raised issues with missing related work, which the authors then addressed. I will point out that some things the authors say about PAC-Bayes are false. E.g., in the rebuttal the authors say that PAC-Bayes is limited to 0-1 error. It is generally trivial to obtain bounds for bounded loss. For unbounded loss functions, there are bounds based on, e.g., sub gaussian assumptions. <sep> Despite improvements in connections with related work, reviewers continued to find the theoretical contributions to be marginal. Even the empirical contributions were found to be marginal."
"[Summary] <sep> This paper proposes an interactive agent that tries to infer the underlying causal structure by interacting with the environment; the authors called it ""causal induction.""  The inferred graph will later help the agent complete goal-directed tasks referred to as a ""causal inference"" stage. Notably, the agent directly learns from visual inputs. <sep> Both the induction and inference phases heavily rely upon the attention mechanism, which ensures that the agent only focuses on the relevant components of the causal graph. During the induction phase, the agent incrementally updates the predicted causal graph through each interaction using an attention-based edge decoder. During the inference phase, the attention bottleneck also showed to improve the agent's generalization ability. <sep> They have shown that the proposed model outperforms several baselines in a synthetic environment that uses switches to control lights. They have also demonstrated the model's generalization ability by operating on unseen causal graphs and new task goals. <sep> [Major Comments] <sep> My primary concern about this work is the scope of its applicability. <sep> For the causal induction phase, the proposed method makes a strong assumption that it can access the ground truth causal relationship during training. The authors can directly read this information from the synthetic environments used in this paper, yet, in more complex real-world situations, we might not know the underlying causal structure for supervised training the induction model. <sep> For learning the goal-conditioned policies, the authors also assume that they have access to the ground truth causal graph. They use this information to generate the expert demonstrations, which, I presume, are deterministic and unimodal (correct me if I'm wrong). In the real world, a human may be able to infer the underlying causal structure from the observations by interacting with the environment and provide the demonstration data. However, the demonstration may be noisy or form a multi-modal distribution. While I agree that learning from demonstration is an effective way of guiding the learning of the policy, I'm not sure if the method can generalize to more realistic scenarios. <sep> For inferring the causal graph, the authors also assume that they know the ""cause"" set and the ""effect"" set, which is already a DAG by construction. Instead of inferring the direction of the edge, they are solving an easier problem of deciding whether a directed edge between a ""cause"" node and an ""effect"" node exists or not. The assumption on the graph structure also limits the method's applicability, as, in the real world, the direction of the edge is not always known in advance. Smoking may cause lung cancer, but it is possible that lung cancer may make people smoke more. <sep> I feel this paper makes strong assumptions on both the induction and inference stages, as well as the structure of the causal graph, which greatly limits the applicability of the approach. <sep> [Detailed Comments] <sep> I also have a few questions regarding the details of this paper. <sep> In Section 3.1, the authors said that ""N is the number of actions in the environments,"" which is a bit confusing. Before this point, the authors did not discuss the relationship between the size of the graph and the size of the action set. Only until Section 3.2 did I realize that N is the number of both ""cause"" set and ""effect"" set. It would be better to discuss the size of the graph and the action set at an earlier position. <sep> How is the expert planner implemented? Is the expert's policy deterministic and unimodal? I feel this is an important detail to include. <sep> It is related to the previous question. What will happen if we learn the goal-conditional policy from scratch? How much does imitation learning help with policy learning? Again, the assumption that we know the ground truth causal graph may not be feasible in the real world. <sep> In Section 3.3, the authors said that ""the expert's action is added to the memory of the policy."" However, the authors also noted that ""the policy has no memory"" in Section 3.2, which seems to contradict each other. Does the ""memory"" mean replay buffer in Section 3.3? <sep> Does the number of switches fixed across all environments and always the same as the number of lights? Also, are all the lights mounted in the same location? I'm wondering if the model is invariant to the order of the cause nodes and effects nodes in the graph, and can it generalize to larger environments, more lights, and different room configurations. <sep> In Figure 4, TCIN seems to have the best performance in the ""Masterswitch"" environment when there are 500 seen causal structures. What might be the reason?","The submission presents an approach to uncovering causal relations in an environment via interaction. The topic is interesting and the work is timely. However, the experimental setting is quite simplistic and the approach makes strong assumptions that limit its applicability. The reviewers are split. R2 raised their rating from 3 to 6 following the authors' responses and revision, but R1 maintained their rating of 3 and posted a response that justifies this position. In light of the limitations of the work, the AC recommends against accepting the submission."
"Summary: <sep> This paper presents a data augmentation procedure for semi-supervised learning on graph structured data. In general it is not clear how augmented observations can be incorporated into an existing graph while still preserving the graph structure. Instead of trying to incorporate an augmented dataset into the graph, this paper uses a seperate fully-connected network (FCN) that shares weights with a graph neural net (GNN). The final method also incorporates various components from the literature, such as Mixup and Sharpening. While the literature appears to have been adequately cited, the position of GraphMix relative to previous work could be better dilineated. In my opinion, the main contribution of this paper is the suggestion of using weight-sharing to sidestep the issue of incorporating augmented datapoints into graph-structured data. I am not sufficiently versed in the literature to assess whether the idea is sufficiently novel to warrant acceptance. The idea is simple and appears to improve the performance of Graph Convolutional Nets (GCNs), so I am leaning towards acceptance. <sep> Rating Justification: <sep> Efficient data-augmentation procedures are an important area of research. The relative simplicity and generality of GraphMix is appealing. I would give the paper a higher score if the authors showed that GraphMix(Graph U-net) was an improvement over Graph U-net, or if it was made more clear that some substantial benefit is derived from using Mixup features. <sep> Additional Comments: <sep> 1. Based on the ablation study it seems that Mixup actually plays a very minor role in the overall success of the procedure. I would be curious to see the t-SNE visualization of the GraphMix(GCN) \\ Mixup features in order to determine how much of the cluster separation is due to Mixup specifically. I understand that previous work has suggested that Mixup features are superior for discrimination than normal features, but in this work specifically the evidence for this assertion is fairly weak. <sep> 2. A clearer distinction between GraphMix(GCN) and GCN (with predicted targets) would be very helpful, especially since GCN (with predicted targets) actually performs the best on the standard PubMed splits. Why were the results for GCN (with predicted targets) not included in Table 2?","The authors integrate an interpolation based regularization to develop a graph neural network for semi-supervised learning. While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period. The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions."
"This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse. It proposed a new metric to measure the diversity of the generative model's ""worst"" outputs based on the sample clustering patterns. Furthermore, it proposed two blackbox approaches to increasing the model diversity through resampling the latent z. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches. <sep> In terms of experiment setup, the authors chooses face generation as the area to investigate and measures the diversity by detecting the generated face identity. With the proposed methods, the authors showed that most STOA methods have a wide gap between the top p faces of the most popular face identities and randomly sampled faces. It further showed that the proposed blackbox approaches increases the proposed diversity metric without sacrificing image quality. <sep> The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations. While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result. For those reasons, I propose to REJECT this paper. <sep> Missing key experiments that will provide more motivation that 1. the new metric reflects human perception of diversity 2. the new metric works better than existing ones: <sep> 1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity. this is important since all your experiments rely on that assumption. <sep> 2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth. <sep> Missing assumptions about blackbox calibration approaches: <sep> 1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G? <sep> 2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator? A website that just exposes the image generation API may not allow you to ping their service 100k times to improve the generation diversity. If you are allowed to do that, it may be reasonable to assume that you can contact the API provider to get access to the rest of the model. <sep> Minor improvements that did not have a huge impact on the score <sep> 1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy. <sep> 2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity. <sep> 3. You may want to consider stating the work as ""a pilot study"" (sec 6.) earlier in the abstract or in the introduction, so that the reader knows what to expect.","This paper studies the problem of mode collapse in GANs. The authors present new metrics to judge the model's diversity of the generated faces. The authors present two black-box approaches to increasing the model diversity. The benefit of using a black box approach is that the method does not require access to the weights of the model and hence it is more easily usable than white-box approaches. However, there are significant evaluation problems and lack of theoretical and empirical motivation on why the methods proposed by the paper are good. The reviewers have not changed their score after having read the response and there is still some gaps in evaluation which can be improved in the paper. Thus, I'm recommending a Rejection."
"[Overview] <sep> In this paper, the authors proposed a new method called knowledge acquisition (KA) for distilling the learned knowledge from the teacher model to the student model. Unlike the conventional KL-divergence based knowledge distillation method, the authors take the reverse version which learns the student to increase the precision. The paper gave a thorgough analysis on the proposed KA strategy and compared with other strategies like KL and JS divergence. On the sequence generation task (translation), the authors showed that the proposed KA strategy achieved better performance compared with KD based methods when distilling the knowledge from a teacher model to a student model. <sep> [Pros] <sep> 1. The authors proposed a new strategy to perform the knowledge distillation from a teacher model to student model for sequence generator. To improve the precision of the student model, the authors proposed to invert the formula of KL divergence, i.e., the position of prediction probability from teacher and student models. <sep> 2. The authors presented a thorough analysis on the proposed KA strategy and compared it with KL strategy in terms of the precision and recall for the student models. I think it is very readable and understandable. This analysis align with those put on generative adversarial network. <sep> 3. The authors performed the experiments on the translation tasks showing that the proposed KA strategy outperforms both KL and JS strategy in terms of the generation performance. Also, the authors ablated the number of top reference tokens from the teacher model and showed that using a reasonable number of top tokens is important to help alleviate the noised in the teacher model. <sep> [Cons] <sep> The main concern about the proposed method is whether it can be used as a generic strategy for transferring the knowledge from the teacher model to student model. <sep> 1. First, I have a doubt on the stability of the proposed strategy. In my opinion, the improvements on the sequence generation tasks are mainly due to the tuned hyper parameters for the training, especially the lambda in Eq(12), which is tuned at the validation set. It controls how much to modulate the prediction distribution of student model toward that of teacher model. However, a less tuned lambda would cause either over curve fitting or under curve fitting. As a result, the authors should: 1) first show how the performance would be affected by varying the lambda in the formula; 2) from the reading, I did not see whether the lambda was tuned as well for KD or (KD + KA) / 2. If not, then for fair comparison, the authors should tune the lambda for the KD and (KD + KA) / 2 strategy as well. At some point, I would think the combination of KD and KA would be better than either of them. <sep> 2. Second, some experimental results are somewhat counter-intuitive to me. These are two folds: a) In Figure 4(b), we can see that the KA strategy has learned to generate more new tokens compared with KD. This is a bit strange to me because, KA will focus on the precision instead of recall. To me, pushing the precision will high likely sacrifice the recall and thus the number of novel tokens generated by the model. b) similarly, in Figure 5, it is shown that KA has generally higher entropy than KD. This is also a bit counter intuitive. In Eq(6), it is obvious that the proposed strategy has a entropy term which will be reduced when we want to reduce the KL divergence during the training time. From Figure 5, KA and KD start from the same point (I guess it is because the same pre-trained student model) is used. However, for KA, the entropy start to increase and then converge to a stable number which is consistently higher than KD strategy. <sep> 3. Third, Figure 4(a) also indicates some thing. When only the top few tokens are used to transfer the knowledge from teacher model to student model, KA focus on the precision of a small subspace, which tends to have few modes. However, when the number of tokens is increased, the mode number would also increase drastically. i guess that's why the both strategies finally become very close to each other, and the minor gap between them is probably due to the benefit from hyper-parameter fine-tuning. <sep> Besides the above comments. there are some minor points which are missed in the paper: <sep> 1. As pointed above, it is not clear whether the same tuning is also applied to KD and (KD + KA) / 2. the authors should mention this in the experiment section. <sep> 2. It is also not clear how many tokens is used for reporting the numbers in Table 2. Is it the whole vocabulary side? If this is the case, the gap between KA and KD on validation set are pretty close while more significant on test set. <sep> 3. In Eq (11), should there be a minus sign before the expectation? <sep> 4. Also, is there any more comment on why it is hard to train the student model joint from scratch? what will happen in this case? <sep> [Summary] <sep> In this paper, the authors proposed a new strategy called Knowledge Acquisition which is used for distilling the knowledge learned from  teacher model to the student model. Different from KD strategy, it inverted the position of probability distributions for teacher and student models. By this way, the KA strategy learns a student model which can achieves higher precision. The proposed strategy is evaluated on sequence generation, particularly translation task.  However, as pointed above, in my opinion, there are some counter-intutive observations in the experimental results. It would be good if the authors can address these concerns in the rebuttal.","This paper shows a nice idea to transfer knowledge from larger sequence models to small models. However, all the reivewers find that the contribution is too limited and the experiments are insufficient. All the reviewers agree to reject."
"This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al. in 1990. The authors claim that this allows one to better incorporate structural information into learning and easier interpretability for the learned representations. The proposed approach is motivated by a theoretical analysis showing that using TPR in this context acts as a sort of pre-conditioner and stabilizes learning. Experiments on entailment tasks (given two statement, decide whether the first implies the second) are provided to validate the approach. <sep> I find the paper not easy to follow, with a non-negligible amount of typos in the notations and results. The advantage in terms of accuracy of the proposed approach seems marginal in the experiment, and the analysis of the interpretability of the learned representations could be improved: loosely speaking, particular examples of interpretability are given but sometimes without contexts or baselines to compare to (see the two last comments below). I know ""interpretability"" is a difficult property to assess but I think there may be more principled ways to showcase the approach. <sep> I think this paper is not yet ready for publication: the proposed model is interesting and relevant but its validity could be better assessed and the paper needs some thorough proof-reading. <sep> * Questions / Comments * <sep> - page 1: the authors write U^TR = I, but I believe this is only possible if the TPR dimension d is bigger than the number of roles N. Is this always the case? This should be clarified. <sep> - related to the previous point:  if U^TR = I then shouldn't U^Tb_{t-1} simply be f_{t-1} in Eq. 1? <sep> - before Eq.1, f should be from R^d\\times R^d' to R^d, not from R^d \\times R^d <sep> - In Eq. 1, b_{t-1} and x_t are not of the same dimension, so the cannot be multiplied by the same matrix U (this is why the matrices V_x and V_b are introduced later on). <sep> - there seems to be a problem with Eq. (7): db_t/d_{b_{t-1}} appears on both sides of the equality... <sep> - Modification 1: what is \\tilde{vb_t}? I don't remember seeing this notation introduced before. <sep> - Table 1: constants should not be included in big O notation! To compare constants, one should give the exact number of operations needed for inference. <sep> - POS tagging: Aren't there many other reasons that could lead to this correlation (beside the informal argument that ""TPR captures structured information"")? Maybe the authors should compare with something else, for example the PMI between values of hidden neurons in a learned RNN and POS tags. Out of context, the numbers in Table 5 are not informative. <sep> - Polysemy: Only a very specific cherry picked example is given here. A more principled or in depth analysis of this phenomenon is needed to make a stronger case. <sep> * Typos * <sep> - "" The number of parameter matrices *is* the same as that of..."" <sep> - page 4 ""stables"" -> ""stabilizes"" (but rephrasing the sentence altogether would be better). <sep> - page 5: BiDAF misses the capital letters (""bidaf""). <sep> - ""dev set"" -> ""validation set"" or ""development set"". <sep> - page 8: ""provides research*ers with* an intuitive...""?","This paper has been reviewed by three reviewers and received scores such as 3/3/6. The reviewers took into account the rebuttal in their final verdict. The major criticism concerned the somewhat ad-hoc notion of interpretability, the analysis of vanishing/exploding gradients in TPRU is experimental lacking theory. Finally, all reviewers noted the paper is difficult to read and contains grammar issues etc. which does not help. On balance, we regret that this paper cannot be accepted to *CONF*2020."
"The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. GAN stabilization is a well-motivated problem and limiting the dramatic changes of discriminator loss clearly helps achieving this goal. Experiments show that in few settings the proposed method successfully addresses this issue. However, the theoretical insights have multiple flaws and incorrect proofs, while the choice of experiments raises few questions. Finally, the resolution limitations suggest the overall contribution is rather incremental. <sep> The paper thus requires further work in terms of design, theory and experiments; for instance the choice of Sinkhorn distance as a boundary-heuristic might be too limiting. Although the main bounding strategy seems to be a promising idea, this work does not meet the quality requirements of *CONF*. <sep> Pros: <sep> A. Conceptual simplicity of the general method of bounding discriminator's objective. <sep> B. The method works well for a few variants of Wasserstein GAN on medium resolutions (64x64, 128x128). <sep> Issues: <sep> 1. Resolution limitation. The upper-bound cannot be reasonably computed for higher resolutions in mini-batch training. <sep> 2. It is unclear how costly the method is in terms of resources and training time. <sep> 3. Experiments were done with CelebA/CelebA-hq with DCGAN and BigGAN architectures at 64x64 resolution and BigGAN architecture for 128x128 resolution. However, they lack comparison with vanilla BigGAN (trained with hinge loss) and the explanation of the architectual differences. Authors only mention the lack of self-attention, yet the used architecture has much less parameters than the original BigGAN; it is also unclear how BigGAN for 64x64 resolution should look like. In fact, the choice of BigGAN for 64x64 resolution is questionable, as this model has been designed for datasets of much higher complexity. <sep> 4. In two out of three experiments, WBGAN-GP does not beat WGAN-GP. Given BigGAN's results at 64x64, it might be the case that bounding strategy just helps in the case of heavily-overparametrized model. <sep> 5. Definiton (8) is unclear: firstly, the objective L_theta is parametrized by parameters of discriminator (theta), over which the maximum on the rhs is taken (this follows in Appendices). Secondly, it has non-deterministic term d^lambda(..) which depends on random empirical distributions \\hat(P_r), \\hat(P_g) (in Proposition 8. which follows, authors take expectation over a Wasserstein distance between these distributions). <sep> 6. Remark 2's proof is incorrect. At the beginning of p.13 authors take D_theta(x) = sign(P_r(x) - P_g(x)), which is not a Lipschitz function. Also P_r and P_g may not be discrete so the entire expression does not make sense. <sep> 7. Remark 2 is not true non-deterministic formulation of (8). It is possible that P_g != P_r and \\hat(P_g) = \\hat(P_r); in such case sinkhorn distance is 0 and the remaining terms of rhs of (8) cancell out (e.g. P_r is a Dirac delta at 0 and P_g = Bernoulli(0.1), then it is possible that \\hat(P_r) = \\hat(P_g) = delta(0)). Perhaps re-shaping definition (8) so that it is deterministic would help achieving correctness of Remark 2. <sep> 8. Remark 3. is unclear: in what sense expression (8) can be optimized by gradient descent? <sep> 9. The paper seems rougly written (see below) and the text needs some polishing. <sep> Typos/ requires clarification: <sep> [p2.] Backgrounds -> background <sep> [p.3] corresponds a \\lambda -> corresponds to \\lambda <sep> [p.3] 'The proposed bounded strategy' -> bounding <sep> [p.3] 'We name it general in two folds' (?) <sep> [p.7] 'High-resolution' - 128x128 is not high. <sep> [p.8] Discussions -> Discussion","The paper presents a framework named Wasserstein-bounded GANs which generalizes WGAN. The paper shows that WBGAN can improve stability. <sep> The reviewers raised several questions about the method and the experiments, but these were not addressed. <sep> I encourage the authors to revise the draft and resubmit to a different venue."
"The authors present an approach to improve performance for retro-synthesis of chemical targets in a seq2seq setting using transformers. The authors have encouraging results and the paper was fairly easy to read and follow. However there are a variety of concerns that the authors need to address: <sep> The technical contributions in this paper are somewhat thin. The main contributions are data augmentation techniques, pre-training and a mixture model that seems to improve performance on the USPTO-50K dataset. The novelty is quite low and it's not clear if this will transfer to another domain. The impact is also low as the pre-training techniques using bond breaking and template-based are specific to this problem task. Additionally, mixture models for encouraging diversity is a simple instance of ensembling. <sep> Using deep learning to this application area is also not novel. This paper largely builds upon previous work with Transformers from Schwaller et. al and Karpov et. al. <sep> Other clarifications/issues: <sep> - The experimental results are based only on the USPTO dataset. It's unclear how significant the results are. The authors can consider using diverse datasets or applying their techniques to another application domain to bolster their claims. <sep> - Table 3, lists the average number of unique reactions classes. The authors say "" … we predict the reaction class for every output of our models… "" . It's not clear how it makes sense to calculate diversity when there's no ground truth available for determining if the predicted output is a valid synthesis for the target. To say this another way, what good is diversity if the prediction is incorrect? <sep> - Table 3, lists human eval results. The details here seem quite vague. How does a human determine something to be more diverse? What is the rubric they use? How qualified is the human in being able to judge this task? <sep> - Figure 6 does not have a color scale.","The authors present a new approach to improve performance for retro-synthesis using a seq2seq model, achieving significant improvement over the baseline. There are a number of lingering questions regarding the significance and impact of this work. Hence, my recommendation is to reject."
"This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff. Experiments on Maze and Door show the advantages of residual policy learning over some baselines. <sep> 1. The Bayesian Reinforcement Learning problem this work considered is important. However, using experts immediately make the problem much easier. The original Bayesian Reinforcement Learning problem is then reduced to making decision with experts. Under this setting, there are many existing work with respect to exploration-exploitation tradeoff (OFU, Thompson Sampling) with theoretical guarantees. I did not see why using this residual policy learning (although as mentioned residual/boosting is useful under other settings) is reasonable here. There is not theoretical support showing that residual learning enjoys guaranteed performance. The motivation of introducing this heuristic is not clear. <sep> 2. The comparisons with UPMLE and BPO seems not convincing. Both BPO and UPMLE do not use experts, and ensemble of experts outperforms them as shown in the experiments. And the ensemble baseline here is kind of weak (why sensing with probability 0.5 at each timestep?) Always 0.5 does not make sense (exploration should decrease as uncertainty reduced). Other exploration methods should be compared, to empirically show the advantages/necessities of residual policy learning. <sep> Overall, I consider the proposed BRPO a simple extension of BPO, with a heuristic of learning ensemble policy to make decisions. BRPO is lack of theoretical support, and it is not clear why residual policy learning here is necessary and what exactly the advantage is over other exploration methods. Comparisons with simple baseline like exploration with constant probability is not enough to justify the proposed method. <sep> =====Update===== <sep> Thanks for the rebuttal. The comparison with PSRL improves the paper. However, I still think this paper needs more improvement as follows. <sep> Theorem 1 looks hasty to me. Batch policy optimization Alg is going to solve n_{sample} MDPs, which are generated from P_0. But Eq. (6) or Theorem 1 does not contain information about P_0, implying that P_0 has no impact, which is questionable (an uniform P_0 that can generate different MDPs and a deterministic P_0 can only generate one MDP should be very different). I suggest the authors do more detailed analysis. <sep> On the other hand, I expected whether this special ""residual action"" heuristic has any guarantees in RL? Can decomposing action into a_r + a_e provide us a better exploration method (than others like PSRL, OFU...)? Since this is the main idea of this paper as an extension of BPO, I think this point is important. The experiments shows that it can work in some cases, but I do not see an explanation (the ""residual learning"" paragraph is high level and I do not get an insight from that.).","This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions."
"This paper proposed a BO-based black-box attack generation method. In general, it is very well written and easy to follow. The main contribution is to combine BO with dimension reduction, which leads to the effectiveness in generating black-box adversarial examples in the regime of limited queries.  However, I still have some concerns about this paper. <sep> 1) The benefits of BO? It seems that the step of dimension reduction is crucial to make BO scablable to high-dimensional problems. I wonder if the gradient estimation-based attack methods can apply the similar trick and yield the similar performance. That is, one can solve problem min_{\\delta} attack_loss( x, y, g(\\delta) ) by estimating gradients via finite-difference of function values, where g(\\cdot) is the dimension-reduction operator, and \\delta is the low-dimensional perturbation. Such a baseline is not clear in the paper, and the comparison with (Tu et al., 2019) is not provided in the paper. <sep> 2) Moreover, in experiments, it seems that only query-efficiency was reported. What about distortion-efficiency for BO-based attack? For ℓ∞-attacks, the other ℓp norms can be used as distortion metrics. I wonder what perturbation does the BO method converge to. It was shown in (https://arxiv.org/pdf/1907.11684.pdf, Table 1) that BO usually leads to larger \\ell_1 and \\ell_2 distortion. <sep> 3) It might be useful to show the convergence of BO in terms of objective value versus iterations/queries. This may give a clearer picture on how BO works in the attack generation setting. <sep> 4) Minor comment: In related work ""Bayesian optimization has played a supporting role in several methods, <sep> including Tu et al. (2019), where ...."" However,  Tu et al. (2019) does not seem using BO and ADMM. <sep> ############ Post-feedback ########## <sep> Thanks for the clarification and the additional experiments. I am satisfied with the response, and have increased my score to 6.","The paper proposes a Bayesian optimization approach to creating adversarial examples. The general idea has been in the air for some years, and over the last year especially there have been a number of approaches using BayesOpt for this purpose. Reviewers raised concerns about differences between this approach and related work, and practical challenges in general for using BayesOpt in this domain (regarding dimensionality, etc.). The authors provided thoughtful responses, although some of these concerns still remain. The authors are encouraged to address all comments carefully in future revisions, which a sufficiently substantial that the paper would benefit from additional review."
"This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. It shows that overall, reducing the precision of the neural network in DRL algorithms from 32 bits to 16 or 8 bits doesn't have much effect on the quality of the learned policy. It also shows how this quantization leads to a reduced memory cost and faster training and inference times. <sep> I don't think this paper contributes with many novel results in the field, with most results being known or expected. The result that is interesting, in my opinion, is not properly explored.  The paper is well-written but it is a bit repetitive. It seems to me that the first 3 pages could be compressed in 1, as the same information is introduced over and over again. <sep> With respect to the results being known, quantization is known to succeed in supervised learning tasks. In a deep reinforcement learning algorithm, when you apply post-training quantization in a deep reinforcement learning algorithm, mainly when that algorithm uses a value function (e.g., A2C or DQN), the problem is reduced to a regression problem. It is no different than a supervised learning problem. One has the original network's prediction and they need to match that prediction. The complexities introduced in the reinforcement learning problem (bootstrapping, exploration, stability) don't exist anymore as they arise during training. Thus, it doesn't seem to me that these results are novel or surprising. In a sense it is neat to see that eventual errors do not compound, but that's it. If I were to write this paper I would make this set of experiments much shorter just as a sanity check. One thing that I feel is missing is a notion of the impact of the quantization not in the rewards accumulated but in the policy/value function. How often does the quantized agent take a different action than the original agent, for example? Does it happen often but only when it doesn't matter, or is it rare? <sep> The quantization during training is potentially interesting. It was not properly explored though. I wonder if the quantization during training has a regularization effect, which is known to improve agent's performance in reinforcement learning (e.g., Cobbe et al., 2018, Farebrother et al., 2018). Does the agent generalize better when using a network with fewer bits of precision? How does this change impact training? These are all questions that could potentially make the results in this paper novel (i.e., quantization as a form of regularization), but as it is now, the results are not that surprising. <sep> Importantly, there are important details missing in the paper that make it hard for me to evaluate the validity of the results presented. Are the results reported over multiple runs? What is the version of the Atari games used, is it the one with stochasticity? How much variance do we have if we replicate this process over different networks that perform well? These are questions I would like to see answered because they also inform us about the impact of the proposed idea. For example, if by repeating this experiment multiple times one observe a high variance, it might mean that different models might be impacted in different ways. <sep> The results in the ""real-world"" (Pong is not real-world) are not that surprising as well. Basically they show that if one uses a network with lower precision training and inference are faster, which, again, is not surprising. <sep> There's also an important distinction in the results that is not discussed in the paper: DQN estimates a value function while methods such as PPO directly estimate a policy. The reason DQN might have a wider distribution is exactly because it is estimating a different objective. These are important details that should be acknowledged and discussed in the paper. In my opinion, for this paper be relevant, it should have a very thorough evaluation of these different dimensions of reinforcement learning algorithms, with explicit discussions about it. Variance, the impact of quantization during learning, the distinction between parametrizing policies versus value functions, etc. <sep> Finally, there are some aspects of the presentation of this paper that could also be improved. Aside from typos, below are some other comments on the presentation. <sep> - There's no such thing as Atari environment, it is either Arcade Learning Environment (Bellemare et al., 2013) or Atari games. <sep> - I'd introduce/explain quantization in the beginning of the second paragraph of the Introduction for those not familiar with the term. <sep> - No references are provided for the environments used. You should refer to Bellemare et al.'s (2013) work as well as Brockman et al.'s (2016). <sep> - Is it really necessary to explain Fp16 quantization as it is done now, with even a picture of two bytes? I'd expect most readers are familiar with how numbers are represented in a computer. <sep> - The equation for Uniform Affine Quantization is pretty much the same as the one in the Section Quantization Aware Training. All these ""repetitions"", or discussions that are common-knowledge give the impression that the paper is trying to fill all the pages without necessarily having enough content. <sep> - The references are not standardized (e.g., sometimes names are shortened, sometimes they are not) and the paper ""Efficient inference engine on compressed deep neural network"" is cited twice. <sep> References: <sep> Marc G. Bellemare, Yavar Naddaf, Joel Veness, Michael Bowling: The Arcade Learning Environment: An Evaluation Platform for General Agents. J. Artif. Intell. Res. 47: 253-279 (2013) <sep> Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba: OpenAI Gym. CoRR abs/1606.01540 (2016) <sep> Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, John Schulman: Quantifying Generalization in Reinforcement Learning. CoRR abs/1812.02341 (2018) <sep> Jesse Farebrother, Marlos C. Machado, Michael Bowling: Generalization and Regularization in DQN. CoRR abs/1810.00123 (2018) <sep> ------ <sep> >>> Update after rebuttal: I stand by my score after the rebuttal. <sep> The rebuttal did acknowledge some points I made to me the paper took a gradient update towards the right direction. I don't think the paper is quite there yet though. It is repetitive, spending too much time with basic concepts, and it still ignores small details that matter (e.g., calling it Atari Arcade Learning). I strongly recommend the authors to follow my recommendations closely and then submit the paper again to a next conference. The discussion about generalization is potentially interesting, going beyond the regularization for exploration aspect. A better discussion about quantization during learning is also essential. The first three pages could probably be compressed by half.","The paper investigates quantization for speeding up RL. While the reviewers agree that the idea is a good one (it should definitely help), they also have a number of concerns about the paper and presentation. In particular, the reviewers feel that the authors should have provided more insight into the challenges of quantization in RL and the tradeoffs involved. After having read the rebuttals, the reviewers believe that the authors are on the right track, but that the paper is still not ready for publication. If the authors take the reviewer comments and concerns seriously and update their paper accordingly, the reviewers believe that this could eventually result in a strong paper."
"This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. This is done by leveraging a self-attention mechanism to decide which slots should be updated in a given transition, leaving the others untouched. The model learns to encode in a slot-wise and is trained on single step transitions. <sep> This work tackles an important problem, and is very well motivated and presented in a very clear fashion. It reuses some known ideas and components, but combines them in a nice way. I especially liked the use of attention to select what to update, which is a good prior to have. <sep> However, the results presented unfortunately seem to fall a bit short in this current version, and some decisions might have had too much of an effect on some of these shortcomings. Given some improvements, this work might become quite promising, but for the time being I am leaning against publication. <sep> 1. Most modeling decisions are clear and well-motivated, however the choice to make the transition model f_trans always be applied only ""slot-wise"" might be too restrictive. Indeed, for a given action, this means that 2 slots have to independently learn the effect of that action (e.g. as shown in the example in Figure 2. right), and that some interactions are ~impossible to learn (e.g. in Sokoban, pushing a box requires knowing about the location of both the agent and the box). This could have been alleviated if the transition had access to ""interactions outcomes"" (e.g. if using a GraphNet, or in your model, if \\tilde{s} was provided to the transition function f_theta). Other works (including Zambaldi et al 2018, which is cited several times), handle this appropriately. Did you try to provide \\tilde{s} to the transformation operator (e.g. in Figure 7 left)? <sep> 2. Adding a direct comparison to pure non-slotted versions of the model/baselines would have been quite useful, as currently it is unclear why certain things are failing. <sep> 3. Similarly, finding what was the output of the CNN encoder for pixel inputs was a bit too difficult. It is explained in the Appendix that one maps into 4x4 feature maps, but that might be too large for the current environments? Indeed for Sokoban, this means that any grid is partially supported by several ""slots"", which may hurt the results more than they should (especially combined with the slot-wise transition constraints expressed above). <sep> 4. The early state of the current results are quite visible in all examples of the ""Separate training"" model predictions (Figure 3, 5, 8, 9, 10 and 11). None of these actually show this model performing a ""correct"" prediction for t+1? They only predict no changes, or nonsensical interpolations… This is not sufficient to try to make an argument about the ""joint training"" helping, and most discussions about ""what information they contain"" is strenuous at best. <sep> 5. The paper keeps mentioning that it ""implicitly imposes transitions to be sparse"", however it is never explained how that would come about? I understand that the softmax in the self-attention may tend to become ""peaky"" and hence only affect a few slots (and the results do seem to confirm this observation), but I was expecting an explicit loss to enforce this fact. The current emphasis seems a bit ill-funded, so I would present more evidence to it or downplay it. <sep> 6. Some of the results shown seem hard to interpret or provide only weak evidence for the proposed model: <sep> a. Figure 2. Left does seem to indicate a benefit in using the self-attention module, but it is hard to know how much of an effect the gap between the orange and red curves actually imply. This figure is overall a bit too small to interpret, and it might be better to split the 2 conditions into sub-plots. The names of the curves in the legend do not correspond to anything described in the text/caption (but I could understand them…). I was expecting more discussion of the results in Figure 2, for example at the end of Section 4.1. <sep> b. As explained above, Figure 3 only shows that the Joint Training can perform a 1-step prediction, which is ok but is the bare minimum. Does it handle multi-step rollouts? <sep> c. Figure 4 does not seem to provide any significant results or insights. I would interpret them as showing no significant difference between the curves, and they are too small to extract any information out of them. I would remove this figure fully. <sep> d. Figure 5 is unclear about what f_k=0 really is, and once again just shows that Separate training does nothing. I expected it to be exactly reconstructing s_t (given that the others are trying to predict s_t+1)? But this is only the case for the joint training, and without knowing what x_t actually was, it is hard to trust. The fact that f_k changes what it does as it is being increased makes the whole point hard to interpret. <sep> 7. Figure 6 was quite interesting, and I feel like this could be pushed forward in a quite interesting manner. The choice of ""maximizing the number of entities selected"" was fair as a first try, but I could imagine it failing to generalize to more complex environments, or to be easily exploited if one ever decides to pass gradients back into the representation from the policy. It was unfortunate that the legends do not correspond to anything expressed in the main text or caption (e.g. why do you not reuse the ""valid_move"", …, ""blocked_push"" names that you thoroughly introduce?). What is ""random_XX""? Could you comment on why the curves seem to have a large increase in variance along the 80000-100000 updates region? <sep> Details: <sep> 8. Figure 2 (right) was good to explain how the model worked, but I would actually change its location and try to move it into Figure 1. Similarly, Figure 7 in the Appendix seemed rather necessary to understand the model, and belongs in the main text in my opinion. <sep> 9. When presenting the self-attention block, it would be good to directly state that these are MLPs receiving [s_t, a_t] (as done in the Appendix). <sep> 10. Is sigma^2 in the decoder fixed? To which value? <sep> 11. When presenting the ""sparse"" and ""full"" settings, having access to Figure 7 and the rest of the Appendix might be beneficial, it would be good to point forward to it.","This paper introduces a model that learns a slot-based representation and its transition model to predict the representation changes over time. While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments. It certainly is missing multiple important relevant works, thereby overclaiming at a few places. The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission. We believe this paper is not at the stage to be published at this point."
"This paper proposes to learn entity representations by matching entities to the context it occurs in. It also shows that using these representations is very effective for a wide variety of down-stream entity-centric tasks such as entity typing, linking, and answering entity centric trivia questions. They train the model using a corpus of entity linked Wikipedia contexts (sentences unto length 128 tokens). The context is encoded with a BERT model and the CLS representation is used as the representation of context. After obtaining the representation, they train the entity embedding (present in the sentence) to be similar to the context embedding. They test their embeddings on few down-stream entity-centric tasks — linking, typing and trivia question answering. <sep> Strengths: <sep> 1. They try the entity representations on a wide variety of entity-centric tasks and get reasonable results. <sep> Weaknesses: <sep> 1. The biggest weakness of the paper is wrt novelty. Masking out entities and training to context is not a new idea. As pointed by the paper, Yamada et al., 2017 have a very similar objective and it is not very clear from the paper what is the additional contribution that this paper makes. Is using pretrained LMs the major difference? If not, it would have been nice to see Yamada et al's results with BERT. Over all, this paper needs to make its own contribution clear compared to Yamada et al., 2017. <sep> 2. The paper needs to be written more clearly at several places. Few examples are, even though in entity linking results (Table 1) the model achieves 83.0 with other papers achieving 90.9. I didnt see a discussion on how to close the gap. Even in the coNLL benchmark, the initial results of the paper is significantly behind. Claims like ""CoNLL -Aida is known to be restricted and idiotic-synctatic domain"" should be backed by detailed analysis or atleast a citation. Even after finetuning on the CoNLL benchmark, the result is 2.2 points behind state of the art and no discussions have been provided. As a result, I think the entity linking section needs major re-writing and explanation of the results. <sep> 3. The paper makes an interesting observation that masking of entities is better for typing tasks and it affects linking performance, because spelling features are really important for linking. It would be interesting to see a discussion on what could be done to remedy this. Because if we have to retrain entity embeddings for different tasks, then it goes against the hypothesis of the paper which is to use entity representations for a wide variety of down-stream tasks. <sep> 4. I found it confusing to read the setup in sec 5.4. especially where it says we represent each category with three random exemplars. Initially I thought 3 randomly sampled entities formed a category, which didnt make sense, but from figure 3, I think I understood that you first pick a category from Typenet and Wikipedia and then 3 entities are sampled from there. Is that correct? Regarding the results, can the poor results of Yamada et al., can be understood by the fact that it was trained using smaller number of categories? Also, why are the numbers wrt All entities left blank in Table 4. Given that your model is similar, I am assuming its easy to retrain Yamada et al and test it on the all entities benchmark?","The paper describes an approach for learning context dependent entity representations that encodes fine-grained entity types. The paper includes some good empirical results and observations, but the proposed approach is very simple but lacks technical novelty needed to top ML conference; the clarify of the presentation can also be improved."
"Summary <sep> --- <sep> (motivation) <sep> CNN image classifiers tend to overfit to distractor patterns. <sep> Perhaps these patterns are spatially local, such that in most images signal is at one location while the noise models tend to overfit to is somewhere else. <sep> If so, then generalization should improve if models are given additional supervision (i.e., a mask identifying salient regions) that specifies where the signal is and is not. <sep> This paper designs the Actdiff loss to realize this intuition. <sep> (approach) <sep> Actdiff: <sep> 1) The Actdiff loss requires a mask that highlights areas of the input image which have signal and not distractor regions. It extracts features from the original input image and its masked version then encourages the two features to be similar at every layer of the CNN using an L2 loss. <sep> Actdiff is compared to 5 other methods including a reconstruction loss and Gradmask (previous work). <sep> (evaluation - synthetic dataset) <sep> A synthetic dataset is constructed for a simple binary classification task based on the presence of simple shapes. <sep> Two patterns can predict the correct class at train time, but one of the patterns is removed at test time so additional information (masks in this case) <sep> is required to specify which pattern the classifier should use. <sep> All the losses except Actdiff achieve at best 50% accuracy on the val set, but Actdiff gets 80% or more accuracy on 3 of 4 tested model variations. <sep> This shows that Actdiff can effectively introduce the relevant masking information. <sep> (evaluation - Medical Segmentation Decathalon) <sep> This dataset provides 3 segmentation tasks (Liver, Cardiac, Pancreas), including ground truth masks for those images. <sep> All 6 methods outperform all the others at least some of the time. <sep> The conclusion is that adding mask information using Actdiff doesn't improve segmentation performance. <sep> (evaluation - Multi-Site dataset) <sep> A final task tries to construct another synthetic dataset out of real X-ray images collected at two different places. <sep> The train set is largely from one place and the test set is mostly from the other place, and masks are constructed so Actdiff can try to eliminate this bias. <sep> Actdiff causes a negligible increase in performance. <sep> The paper concludes that signals CNNs tend to fit to in this type of data are not very spatially distinct. <sep> Strengths <sep> --- <sep> There is some novelty in the approach. The actdiff loss makes sense and masking + activation mapping have not been tried together before to my knowledge. <sep> Experiments follow a logical progression, starting by verifying the idea on a synthetic dataset, then moving to real data, and then evaluating on a half-synthetic dataset designed to debug the approach. <sep> Experiments average over many random initializations. <sep> The paper embraces its negative result. <sep> Weaknesses <sep> --- <sep> The approach is not very compelling to me: <sep> * Implicit in this paper is that any information outside the mask is a distractor and any information inside is not a distractor. Why should the particular masks chosen for the experiments have this property? How can an expert know which features a model will find useful? <sep> The paper's novelty is somewhat limited. The idea of regularizing using saliency maps has been explored and even applied to medical data like the MSD used here in Gradmask (one of the strong baselines this paper compares to). Activation matching is also common (e.g. [1]), though it has not been combined with masking before. <sep> While the weights applied to the various losses are provided, it's not clear how they were tuned. In this case there may be lots of competing losses, so it's important to tune the weights somehow to ensure the tradeoff between losses is optimal. <sep> The results (and the conclusions) suggest Actdiff is not very effective at increasing generalization. Table 2 reports test results on all datasets. In that table, each loss outperforms all the other losses in at least two cases (a case is a model-dataset pair). <sep> [1]: Gatys, Leon A. et al. ""A Neural Algorithm of Artistic Style."" ArXiv abs/1508.06576 (2015): n. pag. <sep> Missing experiments: <sep> * In the Multi-Site experiment, compare to what happens when the circular mask is applied to all images and not just those from one site. This is a necessary control to be sure that any benefits from masking are due to domain transfer and not other regularization effects. Either conclusion could be useful, but it would be nice to know. <sep> Presentation weaknesses: <sep> * In the synthetic dataset the model cannot tell the difference between correct and incorrect signals at train time. Therefore, I think there's no way for some of the baselines (plain classifier, autoencoder) to generalize correctly. Is that right? If so, it should be clear that comparisons to these baselines are not fair when discussing the synthetic evaluation in section 4. <sep> Missing details / points of clarification: <sep> * What is the Conv AE? I assume it is a CNN based autoencoder of some sort. A detailed description of the non-standard architectures would be useful for reproducibility, though probably only in the appendix. <sep> * What are the lambda hyperparameters? I assume these are weights on the corresponding loss terms, but this is never made explicit. <sep> * How does f(.) relate to the function o_l(.)? Is o_l(.) an intermediate step in f(.)? <sep> * The MSD dataset is not clearly described. Is this a classification dataset where classes are different diseases? What do the ground truth masks capture? <sep> * I think only Conv AE and UNet contain reconstruction losses. This presentation is a bit confusing since reconstruction loss was presented as another loss and it shows up in the tables implicitly based on the architecture being compared. <sep> Suggestions <sep> --- <sep> * This paper would be a bit more convincing if it started with a concrete example of the problem illustrated on some dataset (e.g., maybe an example from Gradmask). That may also help drive intuitions later on in the paper. <sep> Preliminary Evaluation <sep> --- <sep> Clarity: The paper is fairly clear. <sep> Quality: Quality is mixed. Lots of relevant experiments are reported but they don't support clear conclusions and I'm not sure how well the models were tuned. <sep> Originality: There is some novelty, but it is limited as discussed above. <sep> Significance: I see limited significance. <sep> For me this paper requires special scruitiny because it presents a negative result. Here are some factors that come to mind when thinking about whether to publish a negative result: <sep> * Is the approach compelling? - This approach is not very compelling (e.g., comments about limited novelty and lack of concrete examples to boost intuition). <sep> * Are the experiments thorough? - The experiments could be significantly more thorough (e.g., comments about tuning lambda). <sep> * Will readers learn something useful? - This paper may help researchers trying to leverage similar intuitions, but it won't be very useful outside this audience. <sep> * Does the paper present experiments that promote deeper understanding of why the approach failed? - This paper makes significant reasonable steps in that direction with sections 4 and 6, but I was still a bit dissapointed with the conclusions of these sections. <sep> * Does the paper discuss alternative approaches that were investigated? - Many alterantive approaches were considered and their performance reported. <sep> Overall I think this paper is close but fails to meet the bar because it does a bit worse than expected on most criteria above.","This paper studies the effect of training image classifier with masked images to exclude distraction regions in the image and avoid formation of spurious correlation between them and predicted labels. The paper proposes actdiff regularizer and demonstrates that it prevents such overfitting phenomenon on synthetic data. However, there was no success on real data. This is important as it shows that the improvement reported in some saliency-map based approaches in the literature may be due to other regularization effects such as cutout. <sep> This was a unique submission in my batch, as it embraces its negative results. Among our internal discussions, all reviewers that and we all believe that negative results are important and should be encouraged. However, in order for the negative results to be sufficiently insightful for the entire community, they need to be examined under well-organized experiments. This is the aspect that the reviewers think the paper needs to improve on. In particular, R2 believes the paper could consider a larger set of possible regularizations as well as a broader range of applications. The insights in such setting may then lead to solid insights on why the current approaches are not very helpful, and in which direction the follow-up researches should focus on."
"The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. It builds upon previous work [Jakab 2018, Lorenz 2019] who proposed to train by reconstructing the original image from one version of the image with perturbed appearance and another with perturbed pose. It extends this approach by introducing an additional separation of foreground and background in the image. <sep> Strengths: <sep> + Nicely motivates the approach of separating foreground and background <sep> + Fewer landmarks are needed than in previous work <sep> + Approach seems beneficial for video prediction <sep> + Clear and well written <sep> + Detailed description of architecture and training <sep> Weaknesses: <sep> - The changes and improvements feel somewhat incremental <sep> - Some uncertainty about the solidity of the evaluation/comparability with baselines <sep> Results on CelebA somewhat weak <sep> Overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement. I'm thus generally supportive of the paper. <sep> One question I'd like to see addressed in the response is about the evaluation: as you state, the details about cropping are not known for previous work, introducing some uncertainty into the external comparisons. It would be great if you could provide some more details about the steps you took to verify that your internal baseline is indeed comparable to previous work (e.g. Lorenz 2019). For instance, on CelebA your baseline seems to fall short of Lorenz 2019, which may suggest that your substantial looking improvement on BBC Pose is indeed due to more favorable cropping. <sep> The second concern is about the results on CelebA presented in Fig. 6 in the Appendix: It looks like the background net reconstructs almost the entire image. I would have expected that hair style, shape of ears or existence of a beard would equally warrant landmarks and appearance. It seems a bit odd that the foreground is so focused on the central part of the face. Do you have an explanation for that? <sep> Minor comments: <sep> - Test in Fig. 1 too small and not readable when printed <sep> - Fig. 1 seems to be missing an arrow pointing from image to appearance encoder <sep> - Why are there more than 8 points in Fig. 2 for ""Ours8"" (and more than 12 for ""Ours12"")?","The paper proposes an approach for unsupervised learning of keypoint landmarks from images and videos by decomposing them into the foreground and static background. The technical approach builds upon related prior works such as Lorenz et al. 2019 and Jakab et al. 2018 by extending them with foreground/background separation. The proposed method works well for static background achieving strong pose prediction results. The weaknesses of the paper are that (1) the proposed method is a fairly reasonable but incremental extension of existing techniques; (2) it relies on a strong assumption on the property of static backgrounds; (3) video prediction results are of limited significance and scope. In particular, the proposed method may work for simple data like KTH but is very limited for modeling videos as it is not well-suited to handle moving backgrounds, interactions between objects (e.g., robot arm in the foreground and objects in the background), and stochasticity."
"This article studies the similarities between the learned representations for different tasks when trained using reinforcement learning algorithms. The ultimate question that this study tries to answer is an interesting one. Namely, how much can representations learned by training on one task be beneficial for learning other tasks? A high interdependence between the representations can lead to a more successful transfer of knowledge between tasks. <sep> The authors are further interested in studying the properties that influence this relationship, which depends on the elements of training as well as attributes of the tasks themselves. <sep> However, I believe that the results are not strong enough to support the claims that are stated in the paper and the limited scope of the environments tested does not make a convincing case that the results will be generalizable much beyond these scenarios. Therefore, in the current state, I think this paper should be rejected. <sep> Firstly, the paper states that: <sep> > "".. if the distance between models trained on different tasks is the same as that between models trained on the same task, representations are task-agnostic."" <sep> This seems to be a key argument in the paper. However, I believe that this argument is based on the assumption that representations learned for a single task are indeed highly similar. I think this assumption requires some sort of support as reinforcement learning algorithms are known to be highly inconsistent even in reaching similar solutions. Therefore, one cannot take for granted that the learned representations would be similar in any way. <sep> Second, the authors claim in Section 5 that the random splits have little impact on the learned representation, but in Section 6 claim that the spatially disjoint splits have a noticeable impact on the representations. Without any measure of the impact, looking at figures 1b and 3a, I'm not convinced that this distinction is so obvious. The situation is worse when looking at the figures in the appendix, namely figures A3 and A5. If someone were to swap these two figures, my untrained eye would not be able to tell the difference. <sep> I suggest that the authors spend more time explaining their reasoning as to why these results are significant enough to support the claims. Also, the text should be improved if it is to be accepted. There exist many problems ranging from small typos (simlate -> simulate, reuse-ability -> reusability, and ""?."" -> ""?"") to sentences that need to be reworked. Some figure legends/captions can also be improved to include more information, such as explaining what exactly the shaded regions represent (I'm guessing one standard deviation from the mean over some unknown replicates), or in Fig 3b making clear whether ""A -> B"" is done with ""New Policy"" or ""Fine-tuned Policy"". I am also curious as to why only one of these scenarios was experimented with in sections 6 and 7.","The general consensus amongst the reviewers is that this paper is not quite ready for publication, and needs to dig a little deeper in some areas. Some reviewers thought the contributions are unclear, or unsupported. I hope these reviews will help you as you work towards finding a home for this work."
"This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. The authors propose an algorithm, ""discriminative jackknife"", based on the standard jackknife confidence interval estimate which they augment by a ""local uncertainty estimate"" based on the variability of the n leave-one-out fitted versions of the underlying algorithm (n = # data points). The whole study is concluded with toy and real-world examples showing the proposed algorithm is competitive with existing methods while also achieving the desired coverage. <sep> I am currently leaning towards recommending rejection of the paper. The main reasons for this are: (i) A potential failure to cite and acknowledge the prior contribution of [1] which seems to have non-trivial overlap with this paper (on arxiv since end of July which is more than 30 days before the *CONF* submission deadline and thus should be treated as prior work); (ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non-asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). Finally, I would like to say that I am holding the paper to a higher standard due to its 10 page length as instructed by the guidelines. <sep> Major comments: <sep> - Can you please explain the relation of this work to [1]? It seems that [1] already proposes use of the higher order expansions, provides an efficient implementation based on forward mode autodiff (do you plan to release code?), and moreover provides non-asymptotic bounds which are not present in your work?! <sep> - On a related note, Giordano et al. provide a careful analysis and discussion of the assumptions in their sect.4. Can you please clarify which of the assumptions you also make, and why you don't need the others (if any)? <sep> - Throughout the paper (e.g., in and around eq.1), you seem to assume that there exists a unique minimiser of the objective which generally won't be true (especially in your application to deep neural networks). In [1], the technical assumptions ensure this is true but I don't see how this is handled in your case?! Can you please help me understand how to interpret your work in case there are multiple (possibly local) minima, and whether this has any effect on the results in thm.2? <sep> - I think it would be beneficial to the reader if you could please provide a discussion and/or formula for how large n has to be for Theorem 2 to apply. <sep> - On p.8, you say ""To ensure a fair comparison, the hyper-parameters of the model f(x; θ) were the same for all baselines."" I am not sure I agree this is a fair comparison. Commonly, one has the opportunity to select hyperparameters for their algorithm (e.g., using a validation set, cross-validation, etc.) so it seems it would have been fair to run each algorithm with its best hyperparameters. Can you please provide a discussion of how this would affect the reported results? <sep> Minor comments: <sep> - On p.3, you say ""We do not pose any assumptions on how the loss function in (1) is optimized."" Do you mean to say that you do not assume anything **but** that the chosen optimiser reaches a (global?!) minimum of the loss function? It seems like you would want to exclude pathological optimisers (e.g., one that always outputs zero) but also more realistically think about the known pathologies of commonly used optimisers (see, e.g., [2]). <sep> - Can you please clarify if and how the use of the algorithm from (Agarwal et al., 2016) for approximation of the Hessian products affects accuracy of your confidence intervals? <sep> - In fig.3, it seems like some of the methods are not properly tuned. For example, MC-dropout should not have zero uncertainty around zero (did you by any chance set bias variance to zero?!), and BNN-SGLD does not seem to have converged (can you please provide plots providing some evidence that the MCMC sampler has mixed + information about how the hyperparameters were selected?). <sep> - I am somewhat confused by the statement ""The only hyper-parameter involved in our method is the number of HOIFs m — this was tuned by optimizing the evaluation metrics ..."" on p.8. Wouldn't thm.2 suggest that you should use as high m as possible? <sep> - Also on p.8, can you please clarify how you selected the threshold for the evaluation of ""discriminative power""? In particular, thm.2 seems to suggest that what one would desire is that the ranking based on width of predictive intervals is equivalent to the ranking based on the actual prediction error. This would suggest, for example, comparing these two rankings using Kendall's tau coefficient (perhaps a binned modification where one would count only the number of times true error puts a point into a different bin than the width of its confidence interval). Note that I am not suggesting the above method is perfect either (it completely ignores the actual sizes of the intervals), but I'm currently having trouble interpreting the results you report, so it would be very helpful to understand how you selected this particular measure of ""discriminative power"" and why alternatives like the example above were discarded please. <sep> References: <sep> [1] Ryan Giordano, Michael I. Jordan, Tamara Broderick. A Higher-Order Swiss Army Infinitesimal Jackknife. https://arxiv.org/abs/1907.12116 <sep> [2] Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht. The Marginal Value of Adaptive Gradient Methods in Machine Learning. https://arxiv.org/abs/1705.08292","In this work, the authors develop a method for providing frequentist confidence intervals for a range of deep learning models with coverage guarantees. While deep learning models are being used pervasively, providing reasonable uncertainty estimates from these models remains challenging and an important open problem. Here, the authors argue that frequentist statistics can provide confidence intervals along with rigorous guarantees on their quality. They develop a jack-knife based procedure for deep learning. The reviews for this paper were all borderline, with two weak accepts and two weak rejects (one reviewer was added to provide an additional viewpoint). The reviewers all thought that the proposed methodology seemed sensible and well motivated. Among the cited issues, major topics of discussion were the close relation to related work (some of which is very recent, Giordano et al.) and that the reviewers felt the baselines were too weak (or weakly tuned). The reviewers ultimately did not seem convinced enough by the author rebuttal to raise their scores during discussion and there was no reviewer really willing to champion the paper for acceptance. Unfortunately, this paper falls below the bar for acceptance. It seems clear that there is compelling work here and addressing the reviewer comments (relation to related work, i.e. Robbins, Giordano and stronger baselines) would make the paper much stronger for a future submission."
"The paper proposes an approach to train NMT models on extremely large parallel corpora. Because of the dataset size, training several epochs on the full dataset with a single model is too expensive. As a result, the dataset is split into several chunks, on which different models are trained independently. The different models are combined to form an ensemble model. Different strategies are proposed to split the dataset effectively. The resulting model achieves a performance of 32.3, outperforming the previous SOTA by 3.2 BLEU. <sep> In Section 4.3, I'm curious about why only 6 layers are used in the encoder and decoder? Large scale pretraining like in BERT usually benefits from very large datasets, but also very large models. 6 layers seems very small given the size of the training set. Table 2 reports results with ""Small"" and ""Large"" models. What does this correspond to? Only Section 4.3 discusses the size of the model, but it does not mention different architectural choices. <sep> In Section 5.1 the paper mentions ""the single-model achieves a BLEU score of 29.7, already outperforming the current best system"", but in Tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7. <sep> I feel that the results are a bit disappointing given the scale of the experiments. Table 2 suggests that a single model, even trained with 40B sentence pairs, does not outperform a single model trained with 20M sentence pairs as in ""He et al., 2019"", while being significantly more expensive to train. Also, the comparisons in Table 1 are done between single and ensemble models, which is not a fair comparison. The model with a BLEU score of 32.0 uses an ensemble of 10 models. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? <sep> Also, did you try the approach of ""Hassan et al (2018)"" suggested in Section 4.1, where only in-domain sentences are selected? It is true that  ""every time we encounter a new domain, we have to retrain the model"", but I think this is still a more viable approach than pretraining on the full 40B sentences. Why not trying to train on the top-100M sentence pairs that are the most in-domain? <sep> In the back-translation (BT) experiments, did you select 100M monolingual sentences randomly? If that is the case, this is expected to see a drop in performance, BT is usually a very effective, but not so much when the monolingual data is noisy or out of domain. Although it is critical to work with cleaned data in NLP (especially in the context of generation), dataset cleaning is not really addressed in the paper. <sep> Overall, the experimental setup is impressive, but the improvements in terms of BLEU are relatively small, and the technical contributions seem quite thin to me for a ML conference. Moreover, the dataset used in the paper is not available to the research community, which prevents reproducibility. Also, as mentioned above, I think several important experiments are missing.","The authors address the problem of training an NMT model on a really massive parallel data set of 40 billion Chinese-English sentence pairs, an order of magnitude bigger than other cz-en experiments. To address noise and training time problems they propose pretraining + a couple of different ways of creating a fine-tuning data set. Two of the reviewers assert that the technical contribution is thin, and the results are SOTA but not really as good as you might hope with this amount of data. This combined with the fact that the data set is not released, makes me think that this paper is not a good fit with *CONF* and would more appropriate for an application focussed conference. The authors engaged strongly with the reviewers, adding more backtranslation results. The reviewers took their responses into account but did not change their scores."
"This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). DAT is referred to in this paper as a scheme that only input feature vectors are mixed, while MixUp also incorporates their corresponding labels. The authors provide a theoretical discussion that both DAT and UMixUp converges to be equivalent to each other when the number of training samples becomes infinity. Experimental results on Cifar 10, Cifar 100, MNIST, and Fashion MNIST show quantitative comparisons among the baseline, MixUp, and UMixUp. <sep> According to the author guideline, <sep> > There will be a strict upper limit of 10 pages for the main text. Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages. <sep> The authors use nine pages. Therefore, the review should be more careful about its quality. <sep> Currently, I have three major concerns that keep me from judging this paper acceptable in *CONF* 2020. <sep> First, the authors failed to cite two closely related papers below: <sep> - Tokozume et al., LEARNING FROM BETWEEN-CLASS EXAMPLES FOR DEEP SOUND RECOGNITION. *CONF*, 2018. <sep> - Tokozume et al., Between-class Learning for Image Classification. CVPR, 2018. <sep> The first one is published in the previous *CONF* and mixing two samples belonging to different classes. The second one is an application to image classification using ImageNet dataset, which is larger than the dataset used in this paper.  What's more important is that both papers propose that the mixing ratio of two samples is not linearly but depending on the strength of their signals. Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method. <sep> Secondly, the theoretical discussion is not so fascinating. Actually, both MixUp and UMixUp are shown to converge to DAT when the number of training samples tends to infinity. Data augmentation is, however, performed to remedy the lack of training samples in general. The discussion that the number of training samples is assumed to be large is the opposite situation. <sep> Thirdly, the experimental results show that the performance gain by UMixUp is relatively small in comparison to that of the original MixUp. There are no ablation studies using different values for alpha and beta, which are parameters for the policy of UMixUp. The authors reported that these values are defined using a heuristic search. Thus, we cannot see if the performance is sensitive to the parameter selection. <sep> I lean to reject this paper because of these concerns. I'm looking forward to seeing the revised version in another conference.","This paper builds a connection between MixUp and adversarial training. It introduces untied MixUp (UMixUp), which generalizes the methods of MixUp. Then, it also shows that DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios. Though it has some valuable theoretical contributions, I agree with the reviewers that it's important to include results on adversarial robustness, where both adversarial training and MixUp are playing an important role."
"This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. The motivation is that the discriminator is usually too powerful, and so the gradient the generator receives is often too small in magnitude. By conditioning on the attention map, the generator could leverage information about the regions in the image that the discriminator attends to and use it to generate a new image that better fools the discriminator. <sep> My main concern is about whether the proposed extension achieves the desired goal. The intuitive motivation provided in the paper aims to add a cooperative component to the two-player game, but the min-max objective corresponds to a zero-sum adversarial game. As a result, when training the discriminator, the discriminator is encouraged to reveal as little information as possible via the attention map, so that the loss maximized. This appears to be the opposite of the desired behavior, so the objective needs to be reformulated. <sep> Also, it is unclear how inference is performed: at test time, the attention map is unknown and so some placeholder must be used in its place. The paper should clarify what is done at test time, and clearly state the shortcomings as a result of this, i.e. different procedures are used for training and testing, which is not principled. I imagine the generator could rely too much on the attention map as a result - how this is alleviated/prevented should be explained. <sep> Figure 2: Only the qualitative results for unsupervised image-to-image translation are available; qualitative results for supervised image-to-image translation should also be provided. <sep> While the quantitative improvement over existing methods is somewhat insignificant, I appreciate the authors discussing their hypotheses why this might be the case. It would be more useful to empirically validate these hypotheses as well. For example, for the claim that ""maybe the attention map only focuses on a few domain specific classes so the generator works too hard on those classes and ignores others"", it might be good to compute the average per-class attention map intensity to show that some classes appear rarely in the attention map. <sep> The evaluation protocol should be explained in greater detail (perhaps in the appendix); the segmentation model (which I assume is FCN) should be described and each of the evaluation metrics (per-pixel acc., per-class acc. and IoU) should be described for the benefit of researchers outside the area. <sep> pg. 4: ""in their implementation contains several Resblock (He et al., 2016), which makes it infeasible in our framework"". Why is it infeasible? <sep> pg. 6: What are the architectures used by the baselines? Are they comparable to the architecture the proposed method used? <sep> Minor Issues: <sep> pg. 3: ""differences between P_x and G_Y \\cdot P_y, P_y and G_X \\cdot P_x are minimized"" - confusing; should rephrase as ""the difference between P_x and G_Y \\cdot P_y and the difference between P_y and G_X \\cdot P_x are minimized"". Also should replace \\cdot with \\circ. <sep> pg. 4: ""like random noisy"" -> ""like random noise"" <sep> pg. 4, last paragraph: ""Our trainable attention module follows the same structure of the attention block in RAM (Wang et al., 2017). They built a very deep network with several such blocks, each containing two branches: mask branch and trunk branch. Mask branch cascades the input features through a bottom-up top- down architecture that mimics human attention. Trunk branch is applied as feature processing."" - this is very confusing; it would be easier to refer readers to the appendix. <sep> pg. 5 - ""Attention mask can potentially break good property of the raw input."" - what does this mean? <sep> pg. 6 - ""as showed in Table 4.1"" -> ""as shown in Table 4.1""","The paper proposes to augment the conditional GAN discriminator with an attention mechanism, with the aim to help the generator, in the context of image to image translation. The reviewers raise several issues in their reviews. One theoretical concern has to do with how the training of the attention mechanism (which seems to be collaborative) would interact with the minimax, zero-sum nature of a GAN objective; another with the discrepancy in how the attention map is used during training and testing. The experimental results were not significant enough, and the reviewers also recommend additional experiment results to clearly demonstrate the benefit of the method."
"This manuscript develops a metric-learning with non-Euclidean error terms for robustness and applies in to data reduction to learn diagnostic models of Alzheimer's disease from brain images. The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer's disease. <sep> My two main issues with the manuscript is that the theoretical part is written very imprecisely and that the experiments are not convincing due to the lack of good baselines and of statistical power. <sep> With regards to the theoretical contributions, a fraction of the results in the present manuscript are trivial consequences or Wang2014, and yet it comes with errors in the statements. For instance, in equation (9), the present manuscript writes greater or equal, while I believe that it should be strictly greater, as in Wang. Theorem 1 and 4 seem almost the same thing, though with a contradiction between the two. Other statements are inaccurate: the authors claim some results on reaching global optima, while I believe that they can only claim that they reach stationary points. Theorem 2 and 5 seem to be the same thing. <sep> Concerning the iterated reweighted approach, I believe that this is non smooth only for g(x)=0, which is not covered by the theorems of Wang 2014. Is this algorithm needed? Note that Wang apply their algorithm with an l1 norm, ie non-smooth in zero, and do not report problems with out. The manuscript mentions that ""to inverted matrices that divide 0s, which routinely lead to inferior learning performance."". I am not exactly sure what that means and I would need to understand better the problem. Also, the theoretical contribution that with the added the delta the algortihm converges, seems quite trivial: it seems to me that it is the eta trick. <sep> Minor comments: in algorithm 3, it would be useful to write the full expression of the equations, rather than just reference the numbers. Also, the computational cost of the eigenvectors at each iteration seems quite prohibitive. <sep> How was the value r=3 selected? <sep> Figure 2 seem to choose that approaches have not converged: they final value is larger than intermediate values? <sep> How were the p-values between cross-validation assessment of estimators computed? If it was done using standard paired t-test, this is incorrect are the folds are not independent. <sep> With regards to the experiments, I worry that the model is not compared against simple baselines, such as a PCA.","This paper proposes to overcome the issue of inconsistent availability of longitundinal data via the combination of leveraging principal components analysis and locality preserving projections. All three reviewers express significant reservations regarding the technical writing in the paper. As it stands, this paper is not ready for publication."
"The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. The novelty of the idea is mostly to integrate these codes with transformers. The authors present experiments on two tasks showing that keeping overall model size fixed (i..e, number of parameters), transformers with shorter (but denser) binary codes achieve better performances than standard transformers using one-hot encodings. The gain mostly comes from using larger embedding sizes and larger intermediate layers. <sep> While the technical contribution is limited, because most of the principles are already known or straightforward, the main contribution of the paper is to show that random hash functions are sufficient to create significantly shorter codes and maintain good performances. Such codes provide more freedom in terms of where to put model capacity (larger embeddings, more transformer layers, etc.) which may be useful in applications where most of the model parameters are in embedding matrices. <sep> The value of such a paper resides mostly in the experimental study. On the bright side, the experiments present in sufficient details the impact of the various hyper parameters and the new trade-offs model size/performance that can be achieved. On the other hand, the experiments are carried out on non-standard tasks without previously published baselines, and it is unclear why. Since the method is applicable to any problem involving natural language data (and more generally categorical values, such as knowledge base completion), I would have expected experiments on tasks with a well-defined state-of-the-art. This makes the experiments in the paper look more like ""proofs of concept"", and they are less convincing than they should be. <sep> detailed comments: <sep> - The paper really is *not* about bloom filters (Bloom filters are data structures that represent sets and efficiently answer membership queries). It is about using codes of identifiers of lower dimension than one-hot encoding. This idea has been used in multi class classification setting (i.e., for the output layer) since (at least) Dietterich & Bakiri (1995) ""Solving Multiclass Learning Problems via Error-Correcting Output Codes"" (with more insights on what makes a good code for prediction problems). The authors borrow from Bloom filter the way to create the codes using random hash functions, but the analogy stops here. <sep> - following the comment above, and assuming I understood correctly: There is an originality on the paper compared to other works that use binary codes/bloom filters: In the current paper, the authors actually predict the result of individual hashing functions. This is different from predicting the binary encoding that results from using the ""or"" of one-hot encodings generated by several hash functions, as would be done in approaches (truely) based on Bloom filters. For instance, if there are m hash functions taking values in {1, ..., P}, an approach based on Bloom filters would predict a binary output of dimension P, while here there are m multiclass problems with P classes (IIUC). This difference from previous work may be significant in practice. <sep> - I found the description of the link prediction task (section 3.1) rather cryptic: <sep> * ""from each page, we take a random contiguous segment of entities"". If I understand clearly, the text is filtered out and only links are kept (?). Links are replaced by the entity id they point to. What happens to the entity the page is about? Is it added at the start? <sep> * 'For evaluation, we hold out one random entity from a random segment on a test page. "": what does ""holding out"" mean? From my understanding, it means replaced by the [MASK] token, but it could also mean removed altogether from the input sequence. <sep> * the task is to predict masked links in sequences of links with the surrounding text filtered out. Does that correspond to any real-life prediction problem (i don't see which one)? Is this ""task"" intended to serve as unsupervised pre-training of embeddings? If yes, maybe the authors might say so and give example applications. <sep> - For the natural language task: <sep> * "", then apply a Bloom filter to reduce the vocabulary size."" -> From my understanding, there is no bloom filter here. If I understand, what is done is to represent unigrams and bigrams by their bloom digest to reduce the input dimension. <sep> - ""We hold out 10% random entity pages for testing. "" -> is there a validation set? How do you choose the hyper parameters? <sep> minor comments: <sep> - ""Since the Wikipedia site usually removes duplicates of links on each page, the distribution of pages is rather long tail."" -> I wouldn't be surprised if the distribution was long tailed even without this specific policy <sep> - I found the formalization/notation more confusing than helping because it is not really thorough (there is no distinction between sets and sequences, ""1[\\eta_j(t_i)] ... where 1[.] is the indicator function"" -> what is the ""indicator function"" of a number?)","This paper presents to integrate the codes based on multiple hashing functions with Transformer networks to reduce vocabulary sizes in input and output spaces. Compared to non-hashed models, it enables training more complex and powerful models with the same number of overall parameters, thus leads to better performance. <sep> Although the technical contribution is limited considering hash-based approach itself is rather well-known and straightforward, all reviewers agree that some findings in the experiments are interesting. On the cons side, two reviewers were concerned about unclear presentation regarding the details of the method. More importantly, the proposed method is only evaluated on non-standard tasks without comparison to other previous methods. Considering that the main contribution of the paper is in empirical side, I agree it is necessary to evaluate the method on more standard benchmarking tasks in NLP where there should be many other state-of-the-art methods of model compression. For these reasons, I'd like to recommend rejection."
"Summary: This paper proposes to use a combination of a pretrained GAN and an untrained deep decoder as the image prior for image restoration problem. The combined model jointly infers the latent code for the trained GAN and the parameters in the untrained deep decoder. It also jointly infers the mixing coefficient alpha and beta during test time for each image, thus learning how much we should rely on GAN. The proposed hybrid model is helpful on compressed sensing experiments on the CelebA dataset; however, it is only marginally better than deep decoder on image super resolution and out-of-distribution compressed sensing. <sep> Detailed comments: <sep> - The writing is clear and I was able to understand the model part of the paper. The algorithm box is helpful. However, I would still appreciate if the authors can provide an overall model figure in the model section to help understanding. <sep> - Jointly learning the mixing coefficient is an interesting part of the model. <sep> - The motivation in the abstract and intro could be strengthened. A smaller version of Figure 1 can be probably moved to the beginning of the paper to illustrate the problem of GAN. But even with the help of Figure 1, it is still unclear what is the fundamental problem for GAN. Simply combining a GAN with an untrained decoder model doesn't help elucidate the source of the problem. <sep> - The proposed Hybrid model seems to help on compressed sensing experiments on CelebA. However, it doesn't help much on out-of-distribution experiments. Moreover, in the super-resolution task, as shown in Figure 5, the improvement over deep decoder is also not significant. <sep> - The out-of-distribution experiments seems lack of thorough study. In particular, the paper only studies the transfer between CelebA -> Caltech-UCSD Bird dataset. It would be better if the paper can study a variety of other image datasets as well. Also some visualization on the Bird dataset should also be included. <sep> - Effect of n_pre needs to be further investigated. Why not directly train both models together? It would be good if the authors could comment on how sensitive the n_pre is and what is the intuition. <sep> - For figures, I would recommend rename ""Hybrid"" to ""Hybrid (Ours)"" to highlight the paper's contribution, and use a brighter color. <sep> - Figure 5 should be renamed as a Table. <sep> - Hyperparameter details should be moved to the Experiment section. <sep> Conclusion: <sep> The paper proposes a simple combination of a trained GAN and an untrained decoder model for the task of image restoration. Although the method is clear and straightforward, in the experiments, the influence of the new model component seems marginal. Moreover, the motivation is not strong enough. Therefore, I recommend weak reject.",The paper introduces a method for removing what they call representation error and apply the method to super resolution and compressive sensing. <sep> The reviewers have provided constructive feedback. The reviewers like aspects of the paper but are also concerned with various shortcomings. The consensus is that the paper is not ready for publication as it stands. <sep> Rejection is therefore recommended with strong encouragement to keep working on the method and submit elsewhere.
"The paper presents a new approach, SMOE scale, to extract saliency maps from a neural network. The approach is deemed as efficient, because it does not require one or multiple backward passes, as opposed to other approaches based on gradients. The main idea is to process the output activation tensors of a few selected layers in a deep network using an operator combining mean and std.dev. called SMOE scale. The result of this operator can be combined through different scales to obtain a global saliency map, or visualized in such a way that shows the consistency of saliency maps at different scales. Experiments show some improvement against traditional gradient-based approaches. <sep> This paper is interesting in that it provides a different way to extract saliency maps from a network. The visualization at multiple scales is also nice and while I do not perfectly agree with the HSV encoding in Fig.2, I do see the potential. Being efficient is also a nice to have. There are three issues, however, which limits the novelty of the paper. First, the SMOE metric does not seem to bring much improvement compared to simple metrics. Second, the few comparisons made against other methods do not reveal a significant improvement. Third, at core, the paper suggests that the ""high efficiency"" of this approach is one of its main advantages, a statement I do not forcibly agree with. More details follow. <sep> For the first element, we have to consider the paper as the combination of two things. 1) the use of activation maps as source of salient information, and 2) the way we should process these activation maps. 1) is relatively straightforward, so the core of the contribution should lie in 2). However, while the SMOE scale method definition (eq.2) is sound, it does not bring valuable improvement compared to other ""trivial"" metrics, like standard deviation. For instance, Fig.4 caption tells that ""SMOE Scale differentiates itself the most early on in the network"", but it is actually only for the very first scale layer. At every other scale, standard deviation (for instance) is at least as good. Same thing can be said about Table 4 in appendix, and also about Table 2 (and the scores of Trunc. Normal Entropy). Overall, while SMOE is indeed novel, it is not highly convincing. <sep> On a side note about the SMOE description, I did not find the list of ""conditions and assumptions"" at the beginning of Sec. 2.1. It looks more like an after-thought over which the proposed method coincidentally fits. Moreover, point 3 is kind of conflicting in its formulation. <sep> For the second element, the improvements in KAR and ROAR scores are quite minimal. It does seem to have an edge on KAR score, but not by a huge amount. Additionally, the methods compared are relatively old. To give just two examples of missing new techniques, Smooth Grad-CAM++ (Omeiza et al.) or even Grad-CAM++ (Chattopadhay et al.) would presumably obtain better performances. Moreover, Smooth Grad-CAM++ allows to target a particular feature map or even a specific neuron, which makes it even more relevant to this work. <sep> Finally, a note about efficiency. Generally speaking, I agree that it is always good to be more efficient. However, I fail to see the high importance given to efficiency for this particular problem. Sure, gradient-based approaches are probably not suitable to online, in-network applications, but is it an important requisite? Computing saliency maps on a subset of the dataset once a few epochs already gives a good idea of what the network is doing. In any case, since these saliancy maps are intended for human use, I am not convinced about the importance of computing them for each training example at each epoch. Overall, in my opinion, being efficient at generating saliency maps is a nice to have, but not much more. <sep> Some general comments: <sep> - In Sec. 2.2, the last ""con"" seems a bit out of place. This could be applied to pretty much anything. <sep> - Sec. 2.3 is interesting, but the explanations are convoluted. In essence, what should be said is 1) value encompasses the importance of a pixel, 2) saturation presents the max-min of the distribution and 3) hue shows the position in the network. Also, superimposing these HSV maps over gray scale version of the image like is Fig.2 is difficult to analyze because the ""gray"" of the image can be confused with the saturation channel. <sep> - On p.2, ""this is proceeded"" -> ""this is preceded""? <sep> In summary, this paper presents a nice way of generating saliency maps from activations inside a network. However, the comparison to other approaches does not show a clear advantage, and the related work (and experiments) lacks recent techniques. I would thus ask this general question: what is the ""selling point"" of this method? The current focus on efficiency does not convince me. That being said, there are no clear flaws in the paper, so I am open to reconsider my assessment if improvements are made to the paper (better descriptions, comparison with more recent techniques, experimental justification for SMOE instead of simpler approaches, etc.). <sep> On a final note, there is also the Score-CAM approach (Wang et al.) that looks similar (in the idea of using activation maps). I did not consider it in this review since it was published a few weeks ago on Arxiv, but it could be interesting to discuss it nevertheless.","The paper presents an efficient approach to computer saliency measures by exploiting saliency map order equivalence (SMOE), and visualization of individual layer contribution by a layer ordered visualization of information. <sep> The authors did a good job at addressing most issues raised in the reviews. In the end, two major concerns remained not fully addressed: one is the motivation of efficiency, and the other is how much better SMOE is compared with existing statistics. I think these two issue also determines how significance the work is. <sep> After discussion, we agree that while the revised draft pans out to be a much more improved one, the work itself is nothing groundbreaking. Given many other excellent papers on related topics, the paper cannot make the cut for *CONF*."
"This paper proposed peer loss function for learning with noisy labels, combining two areas learning with noisy labels and peer prediction together. The novelty and the significance are both borderline (or below). There are 4 major issues I have found so far. <sep> References: Looking at section 1.1 the related work, the references are a bit too old. While I am not sure about the area of peer prediction, in the area of learning with noisy labels (in a general sense), there were often 10 to 15 papers from every NeurIPS, ICML, *CONF* and CVPR in recent years. The authors didn't survey the literature after 2016 at all... Nowadays most papers focus on sample selection/reweighting and label correction rather than loss correction in this area, but there are still many recent papers on designing more robust losses, see https://arxiv.org/abs/1805.07836 (NeurIPS 2018 spotlight), https://openreview.net/forum?id=rklB76EKPr and references therein. Note also that some label-noise related papers may not have the term label noise or noisy labels in the title, for example, https://openreview.net/forum?id=B1xWcj0qYm (*CONF* 2019). <sep> Motivation: The motivating claim ""existing approaches require practitioners to specify noise rates"" is wrong... Many loss correction methods can estimate the transition matrix T (which is indispensable in any loss correction) without knowing the noise rate, when there are anchor points or even no anchor points in the noisy training data. See https://arxiv.org/abs/1906.00189 (NeurIPS 2019) and references therein. See also the public comment posted by Nontawat when a special symmetric condition is assumed on the surrogate loss function. <sep> Novelty: The paper introduced peer prediction, an area in computational economics and algorithmic game theory, to learning with noisy labels. This should be novel (to the best of my knowledge) and I like it! However, the obtained loss is very similar to the general loss correction approach, see https://arxiv.org/abs/1609.03683 (CVPR 2017 oral). This fact undermines the novelty of the paper, significantly. The authors should clarity the connection to and the difference from the loss correction approach. <sep> Significance: The proposed method focuses on binary classification, otherwise the paper will be much more significant! Note that the backward and forward corrections can both be applied to multi-class classification. Moreover, similar to many theory papers, the experiments are too simple, where single-hidden-layer neural networks were trained on 10 UCI benchmark datasets. I have to say this may not be enough for *CONF* that should be a more deep learning conference.","Thank you very much for the detailed feedback to the reviewers, which helped us better understand your paper. <sep> Thanks also for revising the manuscript significantly; many parts were indeed revised. <sep> However, due to the major revision, we find more points to be further discussed, which requires another round of reviews/rebuttals. <sep> For this reason, we decided not to accept this paper. <sep> We hope that the reviewers' comments are useful for improving the paper for potential future publication."
"This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. The intuition of this method is that question generation requires to generate a sentence which contains the information about the answer while being conditioned on the given paragraph. In particular, the paper compares its approach to Devlin's presentation (https://nlp.stanford.edu/seminar/details/jdevlin.pdf according to the references; is it not a published work?) which uses next sentence generation for pretraining, that is less related to the downstream question generation task. <sep> The proposed pretrained model is then used to finetune on a standard question generation task, which is then used to generate synthetic QA pairs for data augmentation in QA. The proposed method is evaluated on four datasets (SQuAD v1, SQuAD v2, KorQuAD, QUASAR-T) and have shown substantial performance gain. <sep> The strength of this paper is clear to me: the idea in the paper is new and interesting, and they provide a good intuition of why the proposed learning objective is helpful compared to the previous methods. <sep> However, I have several concerns as follows. <sep> First, the performance improvements are marginal (less than 1.0 on three datasets, except QUASAR-T, which is actually interesting given it is under transfer learning setting). <sep> Second, there is a very related work, ""Dong et al. Unified Language Model Pre-training for Natural Language Understanding and Generation NeurIPS 2019"" which also proposes a new pretraining approach for question generation and data augmentation for question answering. Not only this submission did not discuss this paper (it was posted in May 2019, which is substantially ahead of *CONF* deadline), the result is substantially worse in both BLEU score of question generation and EM/F1 of end QA performance after data augmentation on SQuAD v2. <sep> Third, the paper does not discuss any work in question generation overall, although the task of question generation was studied for decades, including question generation as data augmentation for question answering. I believe this paper should discuss previous work in question generation and compare the performance with them. <sep> Some of the work on question generation. <sep> - Heilman & Smith. Good Question! Statistical Ranking for Question Generation. NAACL 2010. <sep> - Wang et al. Learning to ask questions in open-domain conversational systems with typed decoders. ACL 2018. <sep> - Zhao et al. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Network. EMNLP 2018. <sep> - Kim et al. Improving Neural Question Generation Using Answer Separation. AAAI 2019. <sep> Some of the work on question generation for question answering. <sep> - Du et al. Learning to ask: Neural question generation for reading comprehension. ACL 2017. <sep> - Song et al. A Unified Query-based Generative Model for Question Generation and Question Answering. AAAI 2018. <sep> - Tang et al. Learning to Collaborate for Question Answering and Asking. NAACL 2018. <sep> - Sachan & Xing, Self-training for jointly learning to ask and answer questions. NAACL 2018. <sep> - Tang et al. Question Answering and Question Generation as Dual Tasks. AAAI 2018. <sep> - Lewis et al. Unsupervised Question Answering by Cloze Translation. ACL 2019. <sep> Fourth, it looks like the paper uses the development set for both development and the final evaluation. The paper should either use a portion of the train set for development and treat the original development set as the test set, or just show the final model's performance on the hidden test set by submitting to the leaderboard. (Especially, for Table 3, did authors reimplement BERT+NS and evaluate on the dev set? Because Devlin's presentation only shows the result on the test set from the official leaderboard.) <sep> (This is not a weakness of the paper, but more a subjective opinion about the paper's claim) The paper claims answer-containing sentence generation is close to question generation. However, unlike question generation where core information about the answer is given as input text, answer containing sentence generation should generate the information about the answer itself with no given information. Consider the first example in Table 5. Question generation requires to read ""The racial makeup of Fresno was 245,306 (49.6%)"" and generate question ""What percentage of the Fresno population is White?"" However, in the proposed pretraining technique, the generation model is supposed to generate ""The racial makeup of Fresno was 245,306 (49.6%)"" with no information. In fact, I think the fact that answer candidate is given makes it closer to question generation task, rather than ""generating answer-containing sentence"" is the key. <sep> (By the way, I am giving 3 as a rating although my actual rating is closer to 4 or 5, because 4 or 5 is blocked in the review system. I am happy to increase the score after rebuttals.) <sep> -------------------------------------------------------------------------------------------------------------------------------------------- <sep> Now, here are clarification questions. <sep> Regarding Section 2.1 <sep> 1) Did you attempt to predict 'number of answer candidates' by regression or classification? Which objective function did you use? Based on the current description, it looks like it's neither regression nor classification which makes me confused. <sep> 2) I believe the second last formula in this section should say ""#{...}=k"" instead of ""#{...}""? <sep> 3) Have you compared with thresholding instead of predicting the number of answer candidates? <sep> 4) What is the objective of training the span selection model? There are multiple candidate spans given a single sentence, and hopefully you do not want to discourage candidates except only one. <sep> 5) How was this model trained (both predicting the number and predicting the span)? Was it trained on QA datasets such as SQuAD? Then, did you train this model & go through preprocessing separately for each dataset? (Except QUASAR-I which uses synthetic data from SQuAD v1.) <sep> 6) I believe that the most popular approach in question generation literature is selecting named entities & noun phrases using off-the-shelf tools, and wonder if authors have compared their method with this baseline. <sep> Regarding Section 2.2 <sep> 1) Although this section is the most important section in the paper, the description is a bit confusing to me. My understanding is that, if the initial paragraph has three sentences, <Sentence 1>, ""The racial makeup of Fresno was 245,306 (49.6%)."", and <Sentence 3>, and the answer prediction model (described in Section 2.1) chooses ""49.6%"", the question generation model is given ""<Sentence 1> 49.6% <Sentence 3>"", and the model is supposed to generate  ""The racial makeup of Fresno was 245,306 (49.6%)."". Is it correct? <sep> 2) Did you insert any special token to indicate whether it is the sentence or the target answer candidate, or which sentence is before or after the target sentence? <sep> 3) Have you tried the formulation of the masked language model, instead of the proposed model? <sep> Regarding Section 3 <sep> Is preprocessing the same for all Golub et al 2017, NS and AS? <sep> Regarding Section 4 <sep> 1) I am curious why the improvement in BLEU-4 score (in Table 2) is significant but the improvement in the end task (in Table 3) is marginal. Do you have any intuition on it? <sep> 2) For Figure 5(a), do you have results with BERT without any data augmentation as well? I wonder if using too small number of SQuAD annotations makes question generation inaccurate, which makes synthetic data be bad quality and hurts the performance compared to not using any synthetic data. <sep> 3) How many synthetic QA pairs are used for Table 3? Curious since the model gets around 92.5 with the largest amount of data shown in Figure 5(b) but the number reported in Table 3 is 92.8. <sep> **** Update on Nov 10 **** Increasing the score to 6.","Thanks for an interesting discussion. The paper introduces a sound question generation technique for QA. Reviewers are moderately positive, with low confidence. Some issues remain unresolved, though: While the UniLM comparison is currently not apples-to-apples, for example, nothing prevents the authors from using their method to pretrain UniLM. Currently, QA results are low-ish, and it is hard to accept a paper based solely on BLEU scores (questionable metric) for question generation (the task is but a means to an end). Moreover, the authors do not really discuss how their method relates to previous work (see Review 2 and the related work cited there; there's more, e.g., [0]). I also find it a little problematic that the paper completely ignores all work prior to 2017: The NLP community started organizing workshops on question generation in 2010. [1]"
"This paper trains agents which are able to verify hypotheses, such as ""the blue switch causes the door to open"". It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. The paper shows that agents trained using this procedure are able to not only verify the types of hypotheses seen during pre-training, but also learn to verify more complex hypotheses. In contrast, an agent which is trained directly on the hypothesis verification task is unable to learn to do it. <sep> Overall, I enjoyed reading this paper and thought that it provided an interesting take on the question of how to train agents that can appropriately gather information about their environments. However, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak. I thus am giving a score of ""weak reject"", though it is possible I could increase my score is some of my concerns can be addressed. <sep> First, I was very surprised to see that the paper included no discussion at all about either causal reasoning or partial observability. The whole notion of verifying hypotheses—particularly those in the triplet form as presented in the paper—is equivalent to the idea of performing inference about the structure of a causal graph with three variables. The choice of which interventions to perform in order to make these inferences is a well-studied problem [1] and has been recently explored in the context of RL as well [2]. The novelty here seems to be in embedding the problem of causal reasoning in harder credit assignment problems (i.e. longer time horizon), though see [3]. Similarly, the setup of the MDP in the paper is actually a POMDP, where the state includes the truth value of the hypothesis but where observations do not include this information. Yet, there is no mention of POMDPs or discussion of the literature on partial observability in the paper. <sep> Second, I felt that the setup was overly complex in places making it difficult to draw conclusions, that there were a lack of comparisons, and that the analysis was not as in depth as it could have been. For example, why is it necessary to represent the hypothesis with natural language? Why not use a symbolic representation? It seems like including the pseudo-natural language adds unnecessary complexity and makes it difficult to distentangle what about the problem is hard (Understanding the hypothesis? Choosing the right interventions? Parsing the observations correctly?). The utility of having it be closer to language is that you might see generalization between related hypotheses, but this isn't really something that is actually tested for since all hypotheses are trained on either during pretraining or finetuning. <sep> I also feel like the choice of pretraining reward feels somewhat arbitrary, and it would have been nice to see comparisons to other alternatives (and even better, to other forms of intrinsic motivation). For example, here are a few alternate ways of rewarding the agent that seem intuitively like they could also work: <sep> Reward the agent for changing the state of any of the objects in the environment <sep> Reward the agent for changing the state of any object referenced in the hypothesis <sep> Reward the agent for observing a state of the world it has not seen before (i.e. count-based exploration) <sep> In other words, how important is the fact that the reward is given based on the pre and postconditions? <sep> I thought the paper would benefit from more detailed analyses to tease apart the behavior of the agent. For example, I am curious how many errors are a result of errors in the predictor versus poor exploration behavior by the policy. Could you report (1) how frequently the policy's behavior results in the right observations necessary to make a decision, and (2) results with a policy which uses an oracle predictor (i.e. which will always report the correct answer, if there was enough data in the last N frames to detect that answer)? <sep> On the more practical side, I also thought the quality of the evaluations was not very thorough. For instance, it looks like the pretraining proceeds for 1e8 steps and finetuning for 5e7 steps, based on the plots (these values should be stated more explicitly in the paper). However, this is a bit of an unfair comparison for the ""RL Baseline"", as it only is trained for 5e7 steps while the other agents are trained for 1.5e8 steps. I would like to see a comparison where the RL Baseline agent is trained for 1.5e8 steps as well. Similarly, on the bottom of page 6 the paper says ""we show the max out of five for each of the methods shown"". However, only reporting the max value is considered bad practice and can result in misleading comparisons (see Joelle Pineau's talk on ""Reproducible, Reusable, and Robust Reinforcement Learning"" at NeurIPS 2018). I'd like to see the data in all figures and tables reported with means or medians across seeds, rather than best seeds. <sep> A few minor comments: <sep> - Please state in the main text which RL algorithm you use. <sep> - Can you clarify whether Figure 2 show the proxy rewards or the true rewards? <sep> - For R_pre and R_ppost, what values do you use for C and N? <sep> [1] Pearl, J. (2000). Causality: models, reasoning and inference (Vol. 29). Cambridge: MIT press. <sep> [2] Dasgupta, I., Wang, J., Chiappa, S., Mitrovic, J., Ortega, P., Raposo, D., ... & Kurth-Nelson, Z. (2019). Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162. <sep> [3] Denil, M., Agrawal, P., Kulkarni, T. D., Erez, T., Battaglia, P., & de Freitas, N. (2016). Learning to perform physics experiments via deep reinforcement learning. arXiv preprint arXiv:1611.01843. <sep> -- <sep> Update after rebuttal: <sep> Thank you very much for your response. However, I do not feel that all of my concerns have been addressed and thus will keep my score as it is. In particular, I still feel the paper lacks sufficient discussion of the literature on causal reasoning. I also do not think it is sufficient to add an appendix with the results across multiple seeds: these results should be in the main paper. I'm not sure I follow the justification that max seeds make sense because ""the reward distribution is quite binary in nature""---the plots shown in Figure 3 and 4, for example, span a range of values from 0 to 1. I find the plots that have both variance and max seed very hard to interpret---in some cases the mean is so much lower than the max seed that the variance region doesn't overlap at all. More broadly, it might be easier to compare using bar plots showing final performance, rather than training curves <sep> I appreciate the additional results, especially with different pretraining schemes---thanks for adding these! I have a bit of hard time interpreting the results though since there are no direct comparisons with the triplet pretraining scheme; it would be helpful if these results could be included in these figures too.","The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre-condition, action sequence, and post-condition variables. Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets. <sep> Strengths: Reviewers generally agreed it's an important problem and interesting approach <sep> Weaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds). Two reviewers also found the use of natural language to unnecessarily complicate their setup. Overall, clarity seemed to be an issue. Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability). <sep> The authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period. However, these efforts ultimately didn't satisfy the reviewers enough to change their scores. Although I find that the additional experiments and revisions have significantly strengthened the paper, I don't believe it's currently ready for publication at *CONF*. I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to."
"Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. A decoder based on a CNN, a Conditional RealNVP, or a Conditional Pyramid PixelCNN is used to decode high-dimensional images from these projected Fisher score.  Experiments with different autoregressive and decoder architectures are conducted on MNIST and CelebA datasets are conducted. <sep> Pros: <sep> This paper is well-written overall and the method is clearly presented. <sep> Cons: <sep> 1) It is well-known that the latent activations of deep autoregressive models don't contain much semantically meaningful information. It is very obvious that either a CNN decoder, a conditional RealNVP decoder, or a conditional Pyramid PixelCNN decoder conditioned on projected Fisher scores will produce better images because the Fisher scores simply contain much more information about the images than the latent activations. When the α is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small α's. These results are not surprising. Detailed explanations should be added here. <sep> 2) The comparisons to baselines are unfair. As mentioned in 1), it's obvious that Fisher scores contain more information than latent activations for deep autoregressive models and are better suited for manipulations. Fair comparisons should be performed against other latent variable models such as flow models and VAEs with more interesting tasks, which will make the paper much stronger. <sep> 3) In Figure 3, how is the reconstruction error calculated? It's squared error per pixel per image? <sep> 4) On pp. 8, for semantic manipulations, some quantitative evaluations will strengthen this part. <sep> In summary, this paper proposes a novel method based on projected Fisher scores for performing semantically meaningful image manipulations under the framework of deep autoregressive models. However, the experiments are not well-designed and the results are unconvincing. I like the idea proposed in the paper and strongly encourage the authors to seriously address the raised questions regarding experiments and comparisons. <sep> ------------------ <sep> After Rebuttal: <sep> I took back what I said. It's not that obvious that the ""latent activations of deep autoregressive models don't contain much semantically meaningful information"". But the latent activations are indeed a weak baseline considering that PixelCNN is so powerful a generator. If the autoregressive generator is powerful enough, the latent activations can theoretically encode nothing.  I have spent a lot of time reviewing this paper and related papers, the technical explanation about the hidden activation calculation of PixelCNN  used in this paper is unclear and lacking (please use equations not just words). <sep> Related paper:  The PixelVAE paper ( https://openreview.net/pdf?id=BJKYvt5lg ) explains that PixelCNN doesn't learn a good hidden representation for downstream tasks <sep> Another paper combining VAE and PixelCNN also mentions this point: <sep> ECML 2018: http://www.ecmlpkdd2018.org/wp-content/uploads/2018/09/455.pdf <sep> Please also check the related arguments about PixcelCNN (and the ""Unconditional Decoder"" results) in Variational Lossy Autoencoder (https://arxiv.org/pdf/1611.02731.pdf ) <sep> As I mentioned in the response to the authors' rebuttal, training a separate powerful conditional generative model from some useful condition information (Fisher scores) is feasible to capture the global information in the condition, which is obvious to me. This separate powerful decoder has nothing to do with PixelCNN, which is the major reason that I vote reject.","The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low-dimensional space. <sep> The reviewers have several concerns about this paper: <sep> * Novelty <sep> * Random projection doesn't learn useful representation <sep> * Weak evaluations <sep> Since two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage."
"Summary: This paper proposes a clustering attention-based approach to handle the problem of unsmoothness while modeling spatio-temporal data, which may be divided into several regions with unsmooth boundaries. With the help of a graph attention mechanism between vertices (which correspond to different regions), the CGT model is able to model the (originally unsmooth) cross-region interactions just like how Transformers are applied in NLP tasks (where words are discrete). Experiments seem to suggest a big improvement when compared to baselines. <sep> Pros: <sep> +This should be one of the first works that apply a graph transformer alike method in this domain, and specifically on the unsmoothness problem. <sep> + Since the dataset is not publically available, there aren't many prior works to compare the CGT to. However, at least compared to the one prior work [1] that the authors point to in Section 4, the RMSE results achieved CGT does seem to be significantly better. <sep> ======================================== <sep> However, I still have some questions/concerns on the paper, detailed below. <sep> 1) The current organization of the paper, as well as its clarity, can (and should) be significantly improved. I didn't completely understand the approach on my first two passes, and I **had** to read the code published by the authors. Here are some issues that I found: <sep> - For one, Figure 2 is not quite helpful as it's too messy with font size too small. A similar problem is with Figure 4 which, without further clarification (e.g., of what ""atrous aggregation"" exactly mean), is very hard to interpret. <sep> - The notations are very inconsistent and messy: <sep> i) In Eq. (1), you should use a symbol different from X to refer to the ""predictions"". Since you are applying f(⋅) on Xt−Tx+1:t, you should not get the ""exact same"" target sequence. That's your target. Maybe use y^, which you used in Eq. (8). <sep> ii) In Figure 3, what is the orange line? In addition, I only saw two blue lines in the figure, but the legend seems to suggest there are four of them... <sep> iii) The notations used in Figure 4 are somewhat confusing. For example, what does ""f->1"" mean? (I later found through Eq. (2) that it means transform to 1 dimension; but the small plots in Figure 4 suggest f is a ""magnitude"" of the feature.) In addition, there are two H1 in Figure 4 with clearly different definitions. <sep> iv) The authors used Gθk(xi) in Eq. (3) without defining it. The definition actually came much later in the text in Eq. (6). I suggest moving the usage of the clustering assignment (i.e., Eq. (3)) to after Eq. (6). <sep> v) What does [⋅||⋅] mean (cf. Eq. (4))? (The code seems to suggest it's concatenation?) <sep> vi) The authors first used hxi in Eq. (3) to denote the output of the CAB module. Then letter h is then re-used in Eq. (4) and (5) with completely different meanings. For instance, the Wkhi in Eq. (4) correspond to line 48 of the code ""model.py"". (By the way, nowhere around Eq. (4) did the authors explain how hi is produced, such as taking the mean over the batch dimension, etc.). <sep> vii) In Section 2.6, you denote the ""optimal vertex cluster scheme"" with letter C, which is used in Eq. (2). Similar for parameter ak and atrous offset a. <sep> - This not a very big problem (as it seems somewhat inevitable), but I think there are too many acronyms in the paper. <sep> I think it'd be great if the authors can take care of these issues, as clarity in math and descriptions are critical to the presentation of such an involuted method. It would also be useful to clearly define the dimensionality of all the variables (e.g., you defined V in Section 2.1, but never used it again in later subsections). <sep> 2) Regarding the usage of the multi-view position encoding, the authors claimed that it ""provides unique identifiers for all time-steps in temporal sequences"". However, if you consider x=7 and x=14, then PEi(7)=PEi(14) for all i=1,2,3,4 with PE5(7)≈PE5(14). Doesn't this invalidate the authors' claim? Also, doesn't this mean that the proposed MVPE only works on sequences with length <= 7? (In comparison, the design of positional encoding in the original Transformer doesn't have this problem.) <sep> (You didn't show how you implemented and initialized the position encoding in the uploaded code, so I may be missing some assumptions here.) <sep> 3) In line 48 of the code (https://github.com/CGT-*CONF*2020/CGT-*CONF*2020/blob/master/model.py#L48), why did you take the mean over the batch dimension? Shouldn't different samples in a minibatch be very different? Does a (potentially completely independent) sample in a batch affect another sample? A similar problem occurs for Eq. (9): Why do you require clusterings of two different samples b1,b2 to be similar? (Where these samples can come from quite different times and years of the data?) <sep> 4) In the experiments, you ""sampled 10 input time-steps"" due to computational resources. Typically, in Transformer-based NLP tasks the sequence lengths can be over 500, with much higher dimensionality (e.g., 512); but you are only using sequence length 10 and dimensions <= 16 (in your code, you used ""self.dec_io_list = [[5,8,16,16],[16,16,16,16],[16,8,8,8]]""). What is the bottleneck for the computation of your approach? (I noticed there are more than 1K vertices in city A, which may be a costly factor indeed.) How much memory/compute does the CGT method consume? How does using a longer sequence affect the performance of CGT? <sep> 5) You performed an ablation study on MVPE. Did you simply remove MVPE, or did you use the conventional PE from the original Transformer paper (Vaswani et al. 2017)? (If the latter, I'm very surprised that MVPE is so much better than PE. In that case, you may want to try MVPE on NLP tasks to see if it also improves SOTA.) <sep> 6) How did you measure unsmoothness in Figure 6? It doesn't seem like a quantifiable property to me. You should discuss this in the experiment section. <sep> ----------------------------------- <sep> Minor questions/issues that did not affect the score: <sep> 7) There are some strange phrases/sentences in the paper. For example, the first sentence of the 2nd paragraph of Section 1: ""we will show **throughout the paper** that urban spatiotemporal prediction task suffers from..."" <sep> 8) Why use an encoder-decoder architecture at all? Why can't we train the model like in language modeling tasks, where we want to predict the next token? In other words, you can simply use a decoder-side CGT, and mask the temporal self-attention as in the Transformers. <sep> ----------------------------------- <sep> In general, I think this paper proposed a valuable approach that seems to work very well on the spatio-temporal dataset they used (which unfortunately is private). However, as I pointed out above, I still have numerous issues with the paper's organization and clarity, as well as some doubts over the methodology and the experiment. I'm happy to consider adjusting my score if the authors can resolve my concerns satisfactorily. <sep> [1] http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf","This paper proposes an approach to handle the problem of unsmoothness while modeling spatio-temporal urban data. However all reviewers have pointed major issues with the presentation of the work, and whether the method's complexity is justified."
"This work proposes an outlier detection method based on WAE framework. WAE is trained to ensure that 1) latent distribution follows a prior distribution 2) weighted reconstruction error is low where prior PDF is used to weight the reconstruction error. <sep> Positives <sep> ------------ <sep> 1.I liked the intuition behind the proposed method and I felt its worth exploring. Paper points out that in previous works, there is no mechanism to prevent outliers from getting mapped to high probability areas in the model [21]. Authors claim that their method will over-come this issue. I believe this is the core contribution of the paper. <sep> 2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former ""encourages the latent representations as a whole to match the prior "". <sep> 3.I agree training with a distributional divergence loss along with a weighed reconstruction loss will be helpful for learning a robust representation (as outliers in the training dataset would be assigned a lower weight). <sep> 4.Authors have compared the performance of their method on OOD dataset where they have  compared against three other baseline methods, where they obtain better performance in majority of cases. <sep> Negatives <sep> -------------- <sep> A. Outlier detection and anomaly/novelty detection are two very different problems.  Outliers are 'bad eggs' coming from the same class as normal data. On the other hand, anomaly/novelty are unexpected data possibly coming from other classes. This is the taxonomy followed by majority of works. In my understanding this work is about 'outlier detection'. I hope authors will use the term 'outlier detection' consistency through the paper. <sep> B. Authors have not done a good survey on existing outlier detection methods.  Eg: <sep> I. Chong You, Rene Vidal, Provable Self-Representation Based Outlier Detection in a Union of Subspaces, CVPR 17 <sep> II. Yan Xia, Xudong Cao, Fang Wen, Gang Hua, Jian Sun, Learning Discriminative Reconstructions for Unsupervised Outlier Removal, ICCV 15 <sep> III. Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli, Adversarially Learned One-Class Classifier for Novelty Detection, CVPR 18. (they have experiments on outlier detection) <sep> C. Although existing methods have not explicitly stated the problem identified in (1) above, their proposals are indirectly solving this problem. Therefore, authors should have compared with papers listed in (B) to demonstrate the effectiveness of their method for a meaningful comparison. <sep> D. This is the first time I'm seeing the OOD dataset used in the paper. Have other works published their results on this dataset? Can they be included in your paper?. If not, consider reporting results on standard datasets used in papers (B). I believe reporting results on at least two datasets is necessary to demonstrate the generalizability of the method. <sep> Other comments <sep> ------------------------ <sep> a. What is the dimensionality used in the latent space? I believe a larger latent space may be required in modeling more complex data such as images. Is the weighting mechanism effective when a very large latent space is used due to the curse of dimensionality. <sep> b. I don't think the synthetic dataset experiment is giving any interesting insights. This space is better used if an additional dataset is used instead. <sep> In conclusion, I like the idea presented in this paper; however, I believe experimental results needs to be improved significantly to demonstrate the effectiveness of the proposed method. I cannot recommend to accept the paper in its present condition. <sep> Post Rebuttal: <sep> Authors have partially addressed my concerns. In light of new experiments provided, I'm changing my decision to weak accept.","This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space. The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions. The reviewers find the idea promising. <sep> They have also raised several questions. It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA. But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage."
"See post-rebuttal updates below! <sep> Summary <sep> --- <sep> (motivation) <sep> There are lots of heat map/saliency/visual explanation approaches that try to deep image classifiers more interpretable. <sep> It's hard to tell which ones are good, so we need better ways of evaluating explanations. <sep> This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence. <sep> (approach - correctness) <sep> An explanation is correct if it highlights enough of an image for a classifier to tell the correct class with only the highlight parts of the image. <sep> The default way to evaluate on only highlighted portions is to set the non-highlighted bckground to black/grey. <sep> Instead, this method finds images with the same ground truth class which the classifier scored the lowest of all such images, forming a low-confidence baseline. <sep> It copies the background from one of these images instead of using a black/grey background to try and put the masked image back into the distribution of images from the ground truth class. <sep> This style of masking is used to compute correctness. <sep> (approach - consistency) <sep> An explanation is consistent if it is invariance w.r.t. a number of mostly semantically invariant transformations. <sep> These include small affine transformations, horizontal flips, vertical flips, and adding noise. <sep> (approach - confidence) <sep> An explanation is confident if the masked images it produces still have high condidence under the classifier. <sep> Masked images are produced as for correctness, by copying a distractor from the same class into the background. <sep> (experiments) <sep> The experiments compare existing explanations (LIME, Grad-CAM, Integrated Gradients, SmoothGrad) using the proposed metrics. <sep> 1. Correctness: Classifiers have higher accuracy on explanation-masked images than on images they were least confident on (the ones used to fill in the background). <sep> 2. Grad-CAM is most correct, followed by SmoothGrad, Integrated Gradients, and LIME. <sep> 3. Consistency: Grad-CAM explanations are most resilient to the proposed transformations with Integrated Gradients, SmoothGrad, and LIME being successively less invariant. <sep> 4. Confidence: Explanation-masked images have higher scores for their ground truth class than the low-confidence baseline images. <sep> 5. Hyperparameter variations in the correcness/confidence metrics mostly preserve the ranking of methods, though the absolute values of performance do change substantially. <sep> (conclusion) <sep> The paper concludes that Grad-CAM is usually the best of the methods tested according to the new metrics and that LIME is the worst. <sep> Strengths <sep> --- <sep> I really like the related work section. It could be a valuable resource going forward. <sep> I like the research direction of this paper very much. I think that enumerating a suite of complementary benchmarks is a good way to measure explanation quality because we can only come up with benchmarks that capture a small part of what we want so far. <sep> Weaknesses <sep> --- <sep> I see some major conceptual flaws with these metrics: <sep> * In section 3.1 it seems like the first reasons that normal masking failed is not solved by the proposed approach. The generated images are still out of distribution because the ""foreground"" and the ""background"" don't match. <sep> * I'm concerned about the low-confidence distractor images used in the background. They are from the same ground truth class as the high confidence images they are pasted into the background of, correct? The correctness metric is supposed to capture whether or not an explanation highlights all the class-relevant content in an image and no more. However, information that the explanation did not highlight (the background) can inform the classifier of the ground truth class because the background came from an image of that class (even if a low confidence one). This is especially true because the relevant objects might be in differrent positions in the two images. Thus it could be that the explanation did not highlight informative content but the classifier still gets the corresponding masked image correct because of the background. How often does this happen? <sep> * Consistency is supposed to measure ""the ability of the explainer to capture the relevant components"" under semantically invariant transformations. <sep> The reported metric is mimized when the explanation is the same before and after a variety of transformations. <sep> If this were the case then at least one of them must be wrong in the sense that it would not have captured some relevant components <sep> (unless perhaps it just highlighted everything and was thus useless). <sep> Because of the transformation (e.g. 15 degree rotation) the relevant components would have been at a different position, but the best explanation according to this metric would have been at the same position. Thus this metric seems to reward explanations for not capturing relevant components. <sep> Parts I Didn't Understand: <sep> * In section 3.1, I don't understand the second reason that masking failed. In what sense is masking made meaningless? How is that sense different from the out of distribution concern from the first point? <sep> Missing Details / Presentation Weaknesses: <sep> * Missing reference to [1] which provides more metrics. <sep> * The meaning of confidence is different than it normally is and this may be confusing. <sep> Neural networks should be well calibrated, not necessarily confident (in the commonly used sense of [3]). <sep> Minor flaws: <sep> * Masking by replacing the background with grey (i.e., the bias of the first conv layer) rather than black is more common (e.g., [2] and Grad-CAM). A grey background negates the bias. It's not clear that the background should cancel the bias, but it would be nice to compare to both grey and black masking in Table 7. <sep> [1]: Adebayo, Julius et al. ""Sanity Checks for Saliency Maps."" NeurIPS (2018). <sep> [2]: Zeiler, Matthew D. and Rob Fergus. ""Visualizing and Understanding Convolutional Networks."" ECCV (2013). <sep> [3]: Guo, Chuan et al. ""On Calibration of Modern Neural Networks."" ICML (2017). <sep> Final Evaluation <sep> --- <sep> This paper relies solely on theoretical arguments to show its metrics capture meaningful information. Empirically, it only shows that the proposed metrics can differentiate between some popular explanations. It does not empirically show that the differentiation is meaningful (e.g., by measuring agreement with human judgement). This by itself isn't a problem. However, above I detailed significant flaws in the theoretical justification for the metrics, so I can't recommend these metrics (this paper) on either a theoretical or an empirical basis. <sep> Quality: Per above, I do not think the arguments/evidence in the paper support its conclusions. <sep> Clarity: The paper could be clearer, but can be understood without too much effort. <sep> Originality: These metrics are new enough, being novel variations on prior approaches. <sep> Significance: If I was convinced the metrics made sense then I would guess this paper would be very impactful. As is, I don't think it will have much impact. <sep> The quality of the paper is my reason for the low rating. I'm interested to see whether what others think to make sure I've understood the paper correctly and analyzed it accurately. If my understanding is incorrect I could definitely raise my rating. <sep> Post-Rebuttal Evaluation <sep> --- <sep> After reading the other reviews and the author responses and taking a brief look at the updated paper I still think this paper should be rejected. <sep> The authors' response to my comments clarified my understanding of the consistency metric. Now I understand it and think it is a useful metric. <sep> However, I did not find clarification about the confidence or correctness metrics, though I agree they are not redundant. They still don't really quite make sense to me. This puts me in about the same position as R3, who also doubts those metrics. In the end, this leaves my initial evaluation essentially unchanged. I still recommend rejection because the paper relies on a theoretical understanding of what makes confidence and correctness metrics useful and that understanding is not provided.","The paper proposes metrics for comparing explainability metrics. <sep> Both reviewers and authors have engaged in a thorough discussion of the paper and feedback. The reviewers, although appreciating aspects of the paper, all see major issues with the paper. <sep> All reviewers recommend reject."
"This paper proposes a method to train style-conditional policies. The method has three components, a dynamics model, a labeling procedure and its approximation, and a policy. The model is trained on real trajectories, while the policy is trained on both real and simulated data coming from the model. The policy is trained to both imitate and be sensitive to the style labelling of states. <sep> Such a method yields policies that can be executed with styles chosen externally, e.g. by a user. <sep> This paper proposes a novel method that is an interesting take on imitation learning, but it is hard to judge how relevant this method is, as the paper has several inconsistencies and weaknesses that need to be resolved before it is accepted. <sep> Inconsistencies: the reported NLL results do not match the trajectories shown in Figures. Some equations do not seem to reflect what is being optimized. <sep> Weaknesses: Many quantities that should be reported are not: quality of models, diversity of policies, etc. The source of datasets should be clarified. (see detailed comments) <sep> It's interesting that this method can leverage these very sparse or poor quality annotations, but it would be helpful to get a sense of how good the annotations provided here are. What's the threshold were annotation's quality is too low to be helpful? In the general case I suspect this threshold to be higher than what is hinted at in this paper, especially given Table 2. What happens in more realistic settings where e.g. users may specify hundreds of different labels? <sep> Detailed comments: <sep> - how dependent on having a variety of policies is this method? It's not clear how diverse the set of policies coming from the basketball dataset nor the Cheetah policies is, nor how this diversity affects learning. An experiment with explicitly different levels of diversity would strengthen understanding of this method. For Cheetah, it would be easy to report p(y) as a function of the target forward speed, that would give readers a sense of diversity for each label. <sep> - For something like cheetah, with the labels that you propose being a very simple function of the state space, this somewhat resembles DIAYN [1] but with some grounding. Would it make sense to compare to a similar baseline? <sep> - (Table 1) Why report (only) the median over 5 seeds? Papers usually report means. Plus, an indication of variance would be nice. <sep> - It's not clear what the basketball dataset is. Where does it come from? Does it come with a simulator? If not, how do you evaluate style consistency or run step 11-12 of Alg. 2? Do you have a train/valid/test split to choose hyperparameters? (the appendix only suggests a train/test split) <sep> - Why not cite MuJoCo? [2] <sep> - For Figure 2 & 3, I suggest lowering the transparency/alpha value of the trajectories, as there is a lot of overlap. <sep> - Table 1 is somewhat confusing. In (4) and the paragraph thereafter, you define \\mathcal{L}^{style} as an error rate, i.e. when \\lambda(\\tau) \\neq y, but in Table 1 you seem to report instead accuracy? (i.e. when \\lambda(\\tau) = y) If so, then you are reporting percentages, so \\times 10^2 rather than 10^-2. <sep> - You never report how well C_\\psi is doing, and it's not clear to me why C_\\psi is needed at all. When optimizing (8) are you directly treating (8)'s inner terms as negative rewards which is differentiated wrt \\pi's parameters? If so, you are doing a form of DDPG, but there is a problem: after (4) you mention that L^style is not differentiable, meaning C_\\psi doesn't provide gradient information to \\pi. I assume that you acutally optimize (8) with L^label? Whether that's what you're doing or not, it should be clarified. If you are truly using L^style, then it's not clear why C_\\psi is needed, as there is no differentiability anyways, and simply using \\lambda directly will provide more signal. <sep> - I'm somewhat perplexed by the values of Table 4. A log-likelihood of -190 represents a probability of 10^-83 (a _negative_ LL of -190 is, on the other hand, impossible by virtue of logs and probabilities, but I assume it is a ""typo""). What is this the probability of? Entire trajectories? If so it would make more sense to report the _average_ log-likelihood, i.e. per timestep, because at this point in the paper, readers have no sense of how long trajectories are, and thus what these likelihoods represent. (for example, a NLL of 190 could be an average likelihood of 0.5 for 275 steps, or of 0.1 for 83 steps, or of .8 for 850 steps, which are all very different results! According to the appendix Table 5, the basketball trajectories are 25 steps long, which would mean that the imitation objective is not respected at all, exp(-190/25)=.0005, which would mean that pi(a|s) is .0005 on average.) <sep> - Again Table 4, if the ""-"" is indeed a typo, then CTVAE-style is actually performing better than the baselines, especially for Cheetah (normally one wants negative log-likelihood to be as close to 0 as possible). Same for Table 10, if the NLL column is truly actually log-likelihood, then the ""style+"" objective really degrades imitation quality rather than improves it. <sep> [1] Diversity is All You Need: Learning Skills without a Reward Function, Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine <sep> [2] MuJoCo: A physics engine for model-based control, Emanuel Todorov, Tom Erez Yuval Tassa","The reviewers generally reached a consensus that the work is not quite ready for acceptance in its current form. The central concerns were about the potentially limited novelty of the method, and the fact that it was not quite clear how good the annotations needed to be (or how robust the method would be to imperfect annotations). This, combined with an evaluation scenario that is non-standard and requires some guesswork to understand its difficulty, leaves one with the impression that it is not quite clear from the experiments whether the method really works well. I would recommend for the authors to improve the evaluation in the next submission."
"A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. In the experimental analyses, the proposed method outperforms some of the previous work in terms of the compression ratio and accuracy for training ResNet-34 and MobileNetV2. <sep> The proposed method and initial results are promising. However, the paper and the work should be improved for a clear acceptance: <sep> - Some parts of the method need to be explained more clearly: <sep> – In the statement ""Due to the choice of 1 × 1 × C blocks, the PCA transform essentially becomes a 1 × 1 tensor convolution kernel"", what do you mean by ""the PCA transform becomes a convolution kernel.""? <sep> - Could you please further explain how you compute PCA using batch data, how you update online and how you employ that in convolution weights together with BN? Please also explain the following in detail: <sep> (I) ""To avoid this, we calculate the covariance matrix layer by layer, gradually applying the quantization."" What is the quantization method you applied, and how did you apply it gradually? <sep> (II) ""The PCA matrix is calculated after quantization of the weights is performed, and is itself quantized to 8 bits."" How did you quantize the weights, how did you calculate PCA using quantized weights and how did you quantize them to 8 bits? <sep> - Could you please explain the following settings, more precisely: direct quantization of the activations; quantization of PCA coefficients; direct quantization followed by VLC; and full encoder chain comprising PCA, quantization, and VLC? Please note that there are various methods and algorithms which can be used for these quantization steps. Therefore, please explain your proposed or employed quantization methods more clearly and precisely. <sep> –  Please clarify the statement ""This projection helps to concentrate most of the data in part of the channels, and then have the ability to write less data that layers."". <sep> - Did you apply your methods to larger networks such as larger ResNets, VGG like architectures, Inception etc? <sep> - I also suggest you to perform experiments on different smaller and larger datasets, such as Cifar 10/100, face recognition datasets etc., to examine generalization of the proposed methods at least among different datasets.","The paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations. It has shown the proposed method can bring good memory usage while maintaining the the accuracy. <sep> The main concern on this paper is the limited novelty. The lossy transform coding is borrowed from other domains and only the use of it on CNN intermediate activation is new, which seems insufficient."
"Summary: <sep> This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks.  The paper focuses on the two major adversaries: \\ell_2 adversaries and \\ell_\\infty adversaries. Theorem 1 quantifies the tradeoff between the choice of smoothing distribution which (1) has clean accuracy close to the original classifier and (2) promotes the smoothness of smoothed classifier (and hence adversarial accuracy).  For the \\ell_2 adversary, the paper argues that Gaussian distribution is not the right choice, because the distribution is concentrated on the spherical shell around the x. Instead, the authors propose using a new family of distributions, with the norm square  (p_{|z|_2^2}) following the scaled \\chi^2 distribution with degree d-k (Eq. 8). This allows an extra degree of freedom, and setting k=0 recovers the Gaussian distribution. For \\ell_\\infty perturbations, the paper suggests another family of distributions combining the \\ell_2 and \\ell_\\infty norm (Eq. 9), and argues that it outperforms the natural choice of \\ell_\\infty norm-based distributions (Eq. 10). <sep> I think the paper should be rejected because (1) For \\ell_2 perturbations, there is no major difference between this new family of distributions (d-k \\chi^2) and a Gaussian with different variance. (2) For \\ell_\\infty distributions, the motivation of mixed norm distributions (Eq. 9) over \\ell_\\infty based distributions (Eq. 10) is not very clear. (3) The experimental evidence is also weak (see below). <sep> Main arguments: <sep> 1. The distribution of the norm \\|z\\|_2 in Eq. (8) would be concentrated on a thin spherical shell of radius about \\sqrt{d-k}\\sigma. As the Gaussian distribution with standard deviation \\sigma' is supported on a shell of radius about \\sqrt{d} \\sigma', for each (k,\\sigma) in the family of Eq. 8, there is an equivalent Gaussian with appropriate \\sigma' (Theorem 3 now just compares the radius of the spherical shell). Therefore, I don't see the benefit of this extra degree of freedom of k: the noise distribution is again a ""soap bubble"" of a different radius. Thus, a grid search over \\sigma' for a Gaussian should be the same as a grid search over (k,\\sigma) in Eq. 8. <sep> Even the experimental experiments are a marginal improvement over Cohen et al.  I don't see why the value of (k,\\sigma) was not provided in Table 1 and only \\sigma was provided. Also, the table of Cohen et al. was only calculated for specific values of \\sigma for Gaussian distributions (0.12, 0.25, 0.5, 1.00). For a fair comparison, comparable values of \\sigma's must be calculated, and then the best choice should be selected. <sep> 2. In the light of previous arguments, I don't think the choice of Eq. (9) or Eq. (10) is well motivated. <sep> Why not smooth it with a cube of appropriate radius? Also, not enough experimental details are provided for Table 3.  Salman et al. (2019) reports the accuracy of 68.2% for \\ell_infty perturbations (Table 3, Salman et al. (2019)), whereas the value reported in your Table 3 for at the same radius is 58%. Is it a typo? In any case, the values reported for the proposed model in Table 3 are only a marginal improvement over Figure 1 (left) in Salman et al. (2019), just going by the trivial \\ell_2 to \\ell_\\infty certificate. <sep> Other areas for improvement: <sep> 1. The paper contains numerous grammatical errors, confusing statements, and nonstandard phrases.  For example: (i) more less robust, (ii) black start, (iii) pointy points, etc.  I suggest that the authors spend more time clarifying their manuscript. <sep> 2. The paragraph starting with ""Trade-off between Accuracy and Robustness"": I think this paragraph should be reworded for clarification.  It is not robustness but rather the lack thereof -- say, sensitivity. <sep> 3. On p.5, why was the toy classifier sphere-based? The toughest classifier for Gaussian smoothing (the one achieving the lower bound for Gaussian smoothing) is actually a linear classifier.","The authors extend the framework of randomized smoothing to handle non-Gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks. They show that the resulting framework can obtain state-of-the-art certified robustness results improving upon prior work. <sep> While the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper: <sep> 1) A theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal. However, the reasoning of the authors on the ""fundamental trade-off"" is specific to the particular framework they consider, and is not really a fundamental trade-off. <sep> 2) The justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work. Thus, the significance of this contribution is not clear. <sep> Some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points. <sep> Thus, the paper cannot be accepted in its current form."
"This paper proposes a method to approximate the ""fairness-accuracy landscape"" of classifiers based on neural networks. <sep> The key idea is to set up a multi-dimensional objective, where one dimension is about prediction accuracy and another about fairness. The fairness component relies on a  definition of fairness based on causal inference, relying on the idea that a sensitive attribute should not causally affect model predictions. <sep> I found the causal idea intriguing, since it makes sense that we don't want a sensitive attribute to have a causal effect. <sep> However, there may be several problems with this approach: <sep> 1) For a causal estimate to be valid we need several assumptions. For example, we need A (the sensitive attribute) <sep> to be independent of potential outcomes conditional on X --- the so-called ""unconfoundedness assumption"" in causal inference. <sep> We also need ""positivity"", i.e., that 0< P(A=1|X) <1. <sep> These assumptions are not discussed in the paper. Furthermore, the particular context of the paper, where the treatment is actually an immutable characteristic, makes such discussion much more subtle. <sep> What will we do, for instance, if there are no A=1 in the sample when X = ...? <sep> 2) The authors seem to assume that the propensity score model is well specified. This can be tested, e.g., using [1]. <sep> What do we do when this fails? <sep> 3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified. <sep> In particular, its relation to ""fairness"" is never fleshed out, but just assumed to be so. <sep> This can be problematic when, say, we are missing certain important X that are important for A. <sep> Then, there will be a measurable causal effect of A on h(). <sep> Some other problems: <sep> - What is the reason for focusing on 'neural classifiers'? There is nothing specific in the method or analysis that relates to neural networks, except for the use of the causal estimand in a 'hidden layer'. <sep> - In the Introduction, the authors could cite the works of Amartya Sea, etc., on fairness. <sep> Certainly the study of fairness problems did not start in 2016. <sep> - What exactly is a ""sensitive attribute""? If we don't want to bias our predictions, then why include it in the analysis? <sep> - It is unclear what is new and what is related work in page 3. <sep> - Sec. 4: The claim that ""causal inference is about situations where manipulation is impossible"" discards voluminous work in causal inference through randomized experiments. In fact, many scientists would agree that causal inference is impossible without manipulation. <sep> - As mentioned above, why this particular estimand leads to more ""fairness"" is never explained. <sep> - Do we need a square or abs value in Eq (5)? <sep> - The experimental section is weak and does not illustrate the claims well. <sep> It would be important to explain the choice of the particular causal estimand, the choice of the hidden layer to put the estimand in, to explore the choice of the objective, and so on. Currently, none of these choices/design aspects are being investigated. <sep> [1] ""A specification test for the propensity score using its distribution conditional on participation"" (Shaikh et al, 2009)","This manuscript investigates and characterizes the tradeoff between fairness and accuracy in neural network models. The primary empirical contribution is to investigate this tradeoff for a variety of datasets. <sep> The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty of the results. IN particular, it is not clear that the idea of a fairness/performance tradeoff is a new one. In reviews and discussion, the reviewers also noted issues with clarity of the presentation. In the opinion of the AC, the manuscript is not appropriate for publication in its current state."
"This paper presents a method intended to allow practitioners to *use* explanations provided by various methods. Concretely, the authors propose contextual decomposition explanation penalization (CDEP), which aims to use explanation methods to allow users to dissuade the model from learning unwanted correlations. <sep> The proposed method is somewhat similar to prior work by Ross et al., in that the idea is to include an explicit term in the objective that encourages the model to align with prior knowledge. In particular, the authors assume supervision --- effectively labeled features, from what I gather --- provided by users and define an objective that penalizes divergence from this. The object that is penalized is \\Beta(xi,s), which is the importance score for feature s in instance i; for this they use a decontextualized representation of the feature (this is the contextual decomposition aspect). Although the authors highlight that any differentiable scoring function could be used, I think the use of this decontextualized variant as is done here is nice because it avoids issues with feature interactions in the hidden space that might result in misleading 'attribution' w.r.t. the original inputs. <sep> The main advantage of this effort compared to work that directly penalizes the gradients (as in Ross et al.) is that the method does not rely on second gradients (gradients of gradients), which is computationally problematic. Overall, this is a nice contribution that offers a new mechanism for exploiting human provided annotations. I do have some specific comments below. <sep> - I am not sure I agree with the premise as stated here. Namely, the authors write ""For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective"" -- I would argue that an explanation may be useful in and of itself by highlighting how a model came to a prediction. I am not convinced that it need necessarily lead to, e.g., improving model performance. I think the authors are perhaps arguing that explanations might be used to interactively improve the underlying model, which is an interesting and sensible direction. <sep> - This work, which aims to harness user supervision on explanations to improve model performance, seems closely related to work on ""annotator rationales"" (Zaidan 2007 being the first work on this), but no mention is made of this. ""Do Human Rationales Improve Machine Explanations?"" by Strout et al. (2019) also seems relevant as a more recent instance in this line of work. I do not think such approaches are necessarily directly comparable, but some discussion of how this effort is situatied with respect to this line of work would be appreciated. <sep> - The experiment with MNIST colors was neat. <sep> - The authors compare their approach to Ross and colleagues in Table 1 but see quite poor results for the latter approach. Is this a result of the smaller batch size / learning rate adjustment? It seems that some tuning of this approach is warranted. <sep> - Figure 3 is nice but not terribly surprising: The image shows that the objective indeed works as expected; but if this were not the case, then it would suggest basically a failure of optimization (i.e., the objective dictates that the image should look like this *by construction*). Still, it's a good sanity check.","The paper contains interesting ideas for giving simple explanations to a NN; however, the reviewers do not feel the contribution is sufficiently novel to merit acceptance."
"###### Overall Recommendation <sep> I vote for the ""Weak Accept"" decision for this paper. <sep> ### Summary <sep> This paper introduces a novel model termed Random Distance Prediction model, it can predict data distances in randomly projected space. The distance in projected space is used as the supervisory signal to learn representations without any manually labeled data, avoiding the concentration and inefficiency problem when dealing with high-dimensional tabular data. Their main contribution is extending the random distance in projected spaces to approximate the original distance information of the hight-dimensional tabular data effectively. Overall, the idea in this paper is interesting and effective, the experiment results in two typical unsupervised tasks (anomaly detection and clustering) also look very promising. However, the writing sometimes has unclear descriptions, given these clarifications in an author's response, I would be willing to increase the score. <sep> ### Strengths <sep> 1. The illustration of the authors' idea is clear and concise. <sep> 2. The theoretical analysis of the proposed method is solid and systematic, the validity of the subparts have been proven previously. <sep> 3. The experiment part is well organized. The RDP model are compared with several state-of-the-art unsupervised learning methods in 19 real-world datasets of various domains. The experimental setup is solid with realistic considerations, the results are very convincing and promising. <sep> 4. This paper provides sufficient detail for reproducing the results. <sep> ### Weaknesses <sep> Lack of systematic description of the authors' major contribution. The proposed model looks more like a combination of previous conclusions, which makes readers feel the core parts of this paper build heavily on previous work. <sep> ### Questions <sep> 1. What is the relation between ""random distance prediction loss"" and ""task-dependent auxiliary loss""? <sep> 2. Are there any solutions and references about how to choose the task-dependent loss L_aux? <sep> 3. Why you shade the second part of the loss function in Figure 1; <sep> 4. How long is it for training the proposed model and getting the experiment results? Does the RDP model still outperform the other algorithms? <sep> ### Suggestions to improve the paper <sep> 1.  It would be better to reorganize Section 1 and Section 2, please describe the contribution in a more systematic way. <sep> 2. Add details for the architecture of the model, please give more descriptions about Figure 1. <sep> 3. It might also help to add an algorithm comparison box for the test time for the proposed method. <sep> ### Minor Edit Suggestions <sep> 1. It would be better to give more descriptions about Figure 1; the lower right part in Figure 1 is not explained in the caption; the shadow part in Figure 1 is not precise. <sep> 2. Figure 1 was bad organized, please make the legend readable size. <sep> 3. I don't think there exist the proofs of Eqns. (2)-(4) in the reference paper (Vempala, 1998), which was written in Page 4. The number should be revised.","The reviewers agree that this is an interesting paper but it required major modifications. After rebuttal, thee paper is much improved but unfortunately not above the bar yet. We encourage the authors to iterate on this work again."
"This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. As is well-known in the literature, this is equivalent to minimizing the expected remaining entropy. What the authors have done differently is to consider the use of neural nets (as opposed to the widely-used Gaussian process) as the learning models in this sensor placement problem, specifically to (a) approximate the expectation using a set of samples generated from a generator neural net and to (b) estimate the probability term in the entropy by a deterministic/inspector neural net. The authors have performed some simple synthetic experiments to elucidate the behavior and performance of their proposed strategy. <sep> Conventionally, the sensor placement strategy is tasked to gather the most informative observations (given a limited sensing budget) for maximimally improving the model(s) of choice (in the context of this paper, the neural networks) so as to maximize the information gain. The authors seem to have adopted a different paradigm in this paper: Large training datasets are needed for the prior training of both neural nets (in the order of thousands as reported in the experiments). This seems to be defeating the original aim/objective of sensor placement, as described above. Consequently, it is not clear to me whether their proposed strategy would be general enough for use in sensor placement for a wide variety of environmental monitoring applications. Random sampling and GP-based sensor placement strategies do not face such a severe practical limitation. <sep> The paper is also missing several important technical details and clarity of presentation is poor. For example, <sep> (a) The configurations and training procedure of generator NN G and deterministic NN D for the experiments are not sufficiently described for each experiment. <sep> (b) What do the authors do with the new observations obtained from placing the sensors in the last experiment? Do they adopt an open-loop sensor placement strategy? <sep> (c) The setup for the last experiment is not clear. Is it still the same object classification task? Is the GP receiving an exclusive set of 4D features that are different from the other two methods? I get the impression that the classifiers are trained a priori. For the GP classifier, isn't it the case that one should gather the most informative observations to maximally improve its classification accuracy? <sep> Though I like the authors' motivation of the setup of the x-ray baggage scanning system in security screening, what has really been done in their experiments appears to be still quite far from this real-world setup. Furthermore, their proposed strategy has been used to gather only 1 to 4 observations. More extensive empirical evaluation with real-world datasets (inspired by realistic problem motivation) is necessary. <sep> Fig. 2: I find it surprising that with a single observation, it is possible to generate the instance/imagined spectrum in orange that resembles that of the true spectrum. Similarly, with 3 observations, all 10 instances/imagined spectrums can exhibit the first spike (without observations on it). Can the authors explain this phenomena? <sep> Minor issues <sep> The authors need to put a space in front of all opening round bracket. Other formatting issues exist. <sep> Equation 3: v_k is missing from the conditioned part on the lefthand side of the equation. <sep> Page 3: to estimates? <sep> Page 3: evidence lower bond? <sep> Figure 2 appears on page 3 and is only referenced on page 4. <sep> Algorithm 1: The use of subscript j in x^m_j to represent an unobserved location confuses with that of subscript k in x^m_k to denote the time step. <sep> Figure 3 captions: adapted on the measurements? <sep> Figure 4 captions: corner feature occurred?","This paper proposes a sensor placement strategy based on maximising the information gain. Instead of using Gaussian process, the authors apply neural nets as function approximators. A limited empirical evaluation is performed to assess the performance of the proposed strategy. <sep> The reviewers have raised several major issues, including the lack of novelty, clarity, and missing critical details in the exposition. The authors didn't address any of the raised concerns in the rebuttal. I will hence recommend rejection of this paper."
"The paper presents a visually-guided interpretation of activations of the convolution layers in the generator of StyleGAN on four semantic abstractions (Layout, Scene Category, Scene Attributes and Color), which are referred to as the ""Variation Factors"" and validates/corroborates these interpretations quantitatively using a re-scoring function. The claim of the paper is that there is a hierarchical encoding in the layers of the StyleGAN generator with respect to the aforementioned ""Variation Factors"".  Figure 3(a) illustrates how these ""Variation Factors"" emerge in the layers of the StyleGAN generator. <sep> The basic GAN architecture used in this work is that of StyleGAN. However, details on the architecture of this particular GAN are missing, including in the Appendix. How many Convolution layers are present in its generator? Not everyone is aware of StyleGAN architecture -- A better illustration of their architecture in the main paper and its correspondence with the layer levels (bottom, lower, middle, top) is desired, mainly because the paper is built upon this. The dataset used to train the StyleGAN model is not clear either. In Appendix, Table 1 tabulates the training details, but nowhere is it clearly mentioned if the N=500,000 latent codes are sampled from a GAN model that was trained on a mixture of datasets (i.e., bedroom, living rooms, kitchen etc.) or individual datasets. As well, the unit of training time in Table 1 of the Appendix seems to be M; is it Million or Minutes? Both of them seem unrealistic units for training a GAN. <sep> Since the training dataset is not clear, my understanding of the method is that a range of datasets are used to produce the results, especially for the effect where the transition of Semantic Category results is studied. As a first step, StyleGAN model trained on ""bedroom"" scenes from LSUN dataset is used to randomly sample codes from the learned distribution, which are further passed through the generator to obtain the respective image mappings. Off-the-shelf image classifiers are employed on each of the images to classify them to one of the four ""Visual concepts"", which is nothing but the aforementioned four semantic abstractions. Here, I would like to encourage the authors to use a consistent terminology -- The four semantic abstractions have been referred to as ""Variation Factors"" (page 3), ""Candidate concepts"" (page 6), ""Visual concepts""(page 12), ""Semantics"" (page 8) interchangeably throughout the literature, which is confusing.  Then, 2000 top positive examples and 2000 top negative examples identified by the image classifiers are used to train a linear SVM, i.e., a binary-SVM all the four scene abstractions (""Varying Factors""), and the separation boundary is obtained. I assume the separation boundary is only obtained once, and not after every layer of the generator. Otherwise, it would not make much sense. <sep> With the separation boundary (in the form of a normal vector) known for each of the four scene semantics, different feature activations are obtained by moving the latent code towards/away from the separation boundary. A scoring function is obtained to quantify (Equation 1) how the corresponding images vary in a particular semantic aspect when the latent code is moved from the separation boundary.  As per the last line of Paragraph 2 on page 4, a ranking of such scores using this function is used to understand the most relevant latent semantics. Does this mean that initially, a large set of semantics is used to observe whether the output of GAN is manipulated by probing each of them? Or are only four scene semantics chosen to begin with? And what happens when there is a tie? And, what value of K makes this metric more accurate? Any lower bound? Please explain. More questions on the effect of lamda later below. <sep> In the next step, the authors sample a latent code from the learned distribution and pass it through every layer of the GAN generator. The output code y is varied along the boundary of the SVM classifier. This is repeated at every layer of the GAN generator and the same lamda is used to perturb the resulting output code from the separation boundary. The results are visualized in Fig 3(c). The claim here is that with the same perturbation of the resulting codes (lambda=2) at the output of different GAN layers, the change in the visualized output demonstrates what kind of, if any, semantic is being captured by different layers of GAN. This is also claimed to have been validated through the ""re-scoring"" function. I am not very clear on this. <sep> I request the following experiment: <sep> 1) Within just a single layer (be it bottom, lower, middle or top), how does the output change when the output code of that layer is perturbed in all directions? This is to see the effect (by visualizing) of the range of lamda values on the output at all the layers. Do you discover any changes weakening your claim? <sep> 2) I would like to see the visualizations of the latent codes at the separation boundaries, just to see how well the binary-SVM performs and whether or not, non-binary information is lost/unaccounted for. <sep> On Page-5, see the fourth line from the bottom (going up): how do we know the desired output apriori? Are the four semantic abstractions decided based on the desired output? This takes us back to a question I asked earlier. <sep> Moreover, Layout variation is just view-point variation. So I think it will be appropriate to call it ""view-Point Variation"" rather than ""Layout Variation"". This is because Layout is associated with spatial arrangement of objects in a scene, with functionality goals. <sep> One last question I have is: What is so special about StyleGAN that it was used as the guiding architecture in this work? How generalizable is this approach to other kinds of GANs other than PGGAN and BigGAN (or rather, why is this approach relatable to StyleGAN, PGGAN and BigGAN alone)? <sep> The paper has grammatical errors  (sentences are not well written), typos (ex; ""manipulabe"" on page-8 which should be ""manipulatable"") and is not polished. I also suggest the authors to change the title of the paper, which right now, is a bit odd; if you decide to keep it, there should not be a ""the"" before ""Deep"" in the title. <sep> All in all, the paper is interesting but lacks persuasiveness. <sep> I may jump my score if the authors address all the aforementioned questions and concerns convincingly, and work on the presentation.","The paper proposes to study what information is encoded in different layers of StyleGAN. The authors do so by training classifiers for different layers of latent codes and investigating whether changing the latent code changes the generated output in the expected fashion. <sep> The paper received borderline reviews with two weak accepts and one weak reject. Initially, the reviewers were more negative (with one reject, one weak reject, and one weak accept). After the rebuttal, the authors addressed most of the reviewer questions/concerns. <sep> Overall, the reviewers thought the results were interesting and appreciated the care the authors took in their investigations. The main concern of the reviewers is that the analysis is limited to only StyleGAN. It would be more interesting and informative if the authors applied their methodology to different GANs. Then they can analyze whether the methodology and findings holds for other types of GANs as well. R1 notes that given the wide interest in StyleGAN-like models, the work maybe of interest to the community despite the limited investigation. The reviewers also point out the writing can be improved to be more precise. <sep> The AC agrees that the paper is mostly well written and well presented. However, there are limitations in what is achieved in the paper and it would be of limited interest to the community. The AC recommends that the authors consider improving their work, potentially broadening their investigation to other GAN architectures, and resubmit to an appropriate venue."
"** post rebuttal start ** <sep> After reading reviews and authors' response, I decided not to change my score. <sep> I recommend to strengthen their theoretical justification or make their method scalable to improve their work. <sep> Detailed comments: <sep> 2. ""Moreover, their results on some of the gray-scale experiments are significantly worse compared to ours."" <sep> -> If you are talking about the comparison in MNIST-variants, please note that experimental results on MNIST cannot be seriously taken unless there is a strong theoretical background; especially, MNIST-variants are too small to talk about the scalability of the method. It is hard to convince readers only with results in MNIST-variants, unless the method has a strong theoretical justification. <sep> However, if your claim is true for general gray-scale images, e.g., preprocessing CIFAR to be in gray scale, then you may add supporting experiments about it. <sep> 4. Again, if the method is only applicable to MNIST-variants due to its computational complexity while it has no strong theoretical justification, I can't find benefits from it. <sep> ** post rebuttal end ** <sep> - Summary: <sep> This paper proposes to improve confident-classifiers for OOD detection by introducing an explicit ""reject"" class. Although this auxiliary reject class strategy has been explored in the literature and empirically observed that it is not better than the conventional confidence-based detection, the authors provide both theoretical and empirical justification that introducing an auxiliary reject class is indeed more effective. <sep> - Decision and supporting arguments: <sep> Weak reject. <sep> 1. Though the analysis is interesting, it is not applicable to both benchmark datasets and real-world cases. Including the benchmark datasets they experimented, the input to the model is in general bounded, e.g., natural images are in RGB format, which is typically normalized to be bounded in [0,1]. Therefore, the polytopes would not be stretched to the infinity in most cases. <sep> On the other hand, note that softmax classifiers produce a high confidence if the input vector and the weight vector of a certain class are in the same direction (of course feature/weight norm also matters, but let's skip it for simplicity). Therefore, if there is an auxiliary reject class, only data in the same direction will be detected as OOD; in other words, OOD is ""modeled"" to be in the same direction with the weight vector of the auxiliary reject class. However, the conventional confidence-based detection does not model OOD explicitly. Since OOD is widely distributed over the data space by definition, modeling such a wide distribution would be difficult. Thus, the conventional approach makes more sense to me. <sep> 2. The experiment is conducted only on MNIST variations, so it is unclear whether their claim is true on large-scale datasets and real-world scenario. <sep> Why don't you provide some experimental results on other datasets commonly used in other OOD detection papers, such as CIFAR, SVHN, TinyImageNet, and so on? <sep> - Comments: <sep> 1. In section 4, the authors conjectured the reason why the performance of reject class in Lee et al. (2018a) was worse is that the generated OOD samples do not follow the in-distribution boundaries well. I think Appendix E in the Lee et al.'s paper corresponds to this reasoning, but Lee et al. actually didn't generate OOD samples but simply optimized the confidence loss with a ""seen OOD."" Lee et al. didn't experiment on MNIST variations but many natural image datasets. So, it is possible that the auxiliary reject class strategy is only effective in MNIST variations. I suggest the authors to do more experiments on larger datasets to avoid this criticism.","The paper improves the previous method for detecting out-of-distribution (OOD) samples. <sep> Some theoretical analysis/motivation is interesting as pointed out by a reviewer. I think the paper is well written in overall and has some potential. <sep> However, as all reviewers pointed out, I think experimental results are quite below the borderline to be accepted (considering the *CONF* audience), i.e., the authors should consider non-MNIST-like and more realistic datasets. This indicates the limitation on the scalability of the proposed method. <sep> Hence, I recommend rejection."
"Summary: <sep> The authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. They claim that the method brings several advantages such as: <sep> -Stabilize end-to-end mtl, <sep> -Leads to coordination b/w master policies, <sep> -Allows fine-tuning of options <sep> -Learn termination policies naturally <sep> The proposed approach derives the reward Eq.6 by extending the graph to options for Levine's tutorial. Eq.6 is simply the extension of the reward of maximum entropy RL to the options framework. The ideas presented in the paper are interesting, but I have concerns about the scalability of such an approach. Please see the detailed comments below.  Additionally, please note that although I have marked a weak reject, I am open to adjusting my score if the rebuttal addresses enough issues. <sep> Detailed Comments: <sep> A primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn't this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? If there are n tasks, do you need to use n different networks? <sep> The authors assume that all options are present everywhere i.e. I ⊆ S. I think the work could benefit from removing this assumption. <sep> The authors mention that unlike (Frans et al., 2018), they learn both intra-option and termination policies: there is definitely more work that aims to learn both the skill and its termination. It would be more complete to cite additional references here that learn both of these or rephrase this sentence. <sep> It does not seem clear why ""term 1 of the regularization is only nonzero whenever we terminate an option and the master policy needs to select another to execute."" This doesn't seem true as this is a ratio of the two probabilities and not just the instantiation of the random variable. <sep> The results in moving bandits alone are very convincing. However, in Taxi (2b) distral+action seems to be as good as/even better MSOL. In directional Taxi (2c) Distral(+action) manages to reach the same final performance (if we care about that), can you please comment on this. <sep> Some parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? if yes, then it is obvious that their method could improve over learning on 12 tasks with one set of network. Please clarify. <sep> One major concern is that the only high dimensional experiment is a swimmer and it is not immediately clear how much do we gain there. Distral is relatively closer in performance to both MSOL and MSOL frozen. I would recommend evaluation in a variety of high-dimensional domains such as other instances in Mujoco, and visual domains. In particular, the proposed ideas would make a stronger case if the baselines included other multitask hierarchical agents such as [4] for example. A discussion including some of the missing relevant related multi-task literature would also be helpful [1,2,4,5,6]. <sep> [1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438. <sep> [2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI, <sep> [3] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML <sep> [4] Tessler, Chen, et al. ""A deep hierarchical approach to lifelong learning in minecraft."" Thirty-First AAAI Conference on Artificial Intelligence. 2017. <sep> [5] Ammar, Haitham Bou, et al. ""Online multi-task learning for policy gradient methods."" International Conference on Machine Learning. 2014. <sep> [6] Mankowitz, Daniel J., Timothy A. Mann, and Shie Mannor. ""Adaptive skills adaptive partitions (ASAP)."" Advances in Neural Information Processing Systems. 2016.","Apologies for only receiving two reviews. R2 gave a WR and R3 gave an A. Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal. Thoughts: <sep> - Paper is on interesting topic. <sep> - AC agrees with R2's concern about the evaluation not using more complex environments like Mujoco. Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works. <sep> - AC agrees with authors that the DISTRAL approach forms a strong baseline. <sep> - Nevertheless, the experiments aren't super compelling either. <sep> - AC has some concerns about scaling issues w.r.t. model size & #tasks. <sep> The paper is very borderline, but the AC sides with R2's concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation. With this, it would make a compelling paper."
"This paper proposes an alternative way of reusing pretrained BERT for downstream tasks rather than the traditional method of fine-tuning the embeddings equivalent to the CLS token. <sep> For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. Authors provide two ways to provide this disentagling using LSTM or transformer blocks. with several design choices such as: *  a regularization term to encourages the roles matrix to be orthogonal and hence each role carry independent information *  design the roles and symbols matrices so that the number of symbols is greater than the number of roles <sep> In evaluation authors design several experiments to show that: <sep> * Does transferring disentangled role & symbol embeddings improve transfer learning <sep> * the effectiveness of the TPR layer on performance? <sep> * Transfer beyond Glue tasks? <sep> While those experiments provide empirical gains of the design choices, authors don't show enough study to attribute those  empirical gains to the presented design choices: <sep> One large claim in the paper is that empirical gains in the ability of transfer between similar tasks MNLI and GLUE is because of disentangling the semantics from the role representations. We don't know if the TPR layer really manages to do that, this could have been easily verified using for example clustering word senses of the same word. <sep> The empirical gains in transfer learning can be simply attributed to: <sep> - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn't. This aligns with some recent findings that BERT is undertrained (Liu et al. 2019) https://arxiv.org/abs/1907.11692 <sep> - Variance in the results (authors report only results of one single run not mean and std of several runs). <sep> - More budget given to hyper-parameter search for the models proposed in the paper.  Hyper param budget isn't also reported in the paper. <sep> - other factors, not the ones associated with the claims in the paper: for example what authors claim is an ablation study was comparing several different models together. It would have been more interesting to see for example the effect of making the # symbols = # roles or removing the orthogonality loss from the roles matrix. <sep> Conclusion: The paper introduces large claims and empirical results that correlate with, however the provided experiments are not done with enough control to attribute gains to the design choices provided in the paper.",The paper introduces additional layers on top BERT type models for disentangling of semantic and positional information. The paper demonstrates (small) performance gains in transfer learning compared to pure BERT baseline. <sep> Both reviewers and authors have engaged in a constructive discussion of the merits of the proposed method. Although the reviewers appreciate the ideas and parts of the paper the consensus among the reviewers is that the evaluation of the method is not clearcut enough to warrant publication. <sep> Rejection is therefore recommended. Given the good ideas presented in the paper and the promising results the authors are encouraged to take the feedback into account and submit to the next ML conference.
"*Summary* <sep> This paper compares network pruning masks learned via different iterative pruning methods. Experiments on LeNet + MNIST show (a) different methods can achieve similar accuracy, (b) pruned sub-networks may differ significantly despite identical initialization, (c) weight reinitialization between pruning iterations yields more structured convolutional layer pruning than not reinitializing, and (d) pruning methods may differ in the stability of weights over pruning iterations. <sep> *Rating* <sep> There are interesting bits of data in this paper, but the overall story is somewhat muddled and some inferences seem to be insufficiently supported by data (1-2 below). In addition, the text would benefit from better organization and presentation (3-4 below) and replications on other datasets and architectures (5 below). As a result, my rating is currently weak reject. <sep> (1) *Overlap in pruned sub-networks*: In the middle of Sec. 4, Fig 3-5 examine the similarity of pruning masks between methods. It seems clear from several of the plots that multiple methods produce identical layer-wise masks, e.g. Fig 3(a), while others show a wide variance. The overlap in lines makes this difficult to assess at times: perhaps a table would communicate it better? Also, are Fig 3-4 depicting the Jaccard distance between masks of unpruned or pruned weights? Is the ordering of training samples fixed in addition to network initialization? Is reinitialization used between iterations? Also, Fig 5 seems to contradict the conclusion that methods tend to learn different masks, since the structures are noticeably similar. <sep> (2) *Weight stability during pruning*: It is difficult to discern a conclusion in Sec 5. First, a clarification on the figures: are lines for pruned weights terminated where they are pruned? If so, this would be helpful to state. The 4th paragraph claims, ""we empirically find a correlation between weight stability and performance"", but this is not at all obvious from Figures 6-7. I'm not sure what a more stable evolution looks like. Hybrid is shown to be accurate in Fig 1, but the conv. weights in 6(a) are a spaghetti tangle and the FC weights in 7(a) are constantly increasing in magnitude. Perhaps a mathematical formulation for stability (perhaps based on average standard deviation of each weight's values over training) with a table of values for each method/layer would help to clarify. <sep> (3) *Organization*: Since the paper has many intertwined observations, a better organization would be helpful. Consider mirroring the structure of Sec 1.1 in a combined Sec. 4-5 with clear paragraph headers summarizing each conclusion. <sep> (4) *Presentation*: Figure is too small throughout to read from a printed copy (or even on a screen without significant zooming). Several results could be presented with less ambiguity in tabular form, as noted above. <sep> (5) *Replications*: The paper presents results only a single set of experiments using the MNIST dataset with the LeNet architecture. While this isn't a fatal issue, it is a significant weakness. <sep> *Notes* <sep> Fig 1 and 2: What spacing is used for the x- and y- axes? <sep> Fig 8: Perhaps scale vertically by the standard deviation of the weights?","This is an observational work with experiments for comparing iterative pruning methods. <sep> I agree with the main concerns of all reviewers: <sep> (a) Experimental setups are of too small-scale or with easy datasets, so hard to believe they would generalize for other settings, e.g., large-scale residual networks. This aspect is very important as this is an observational paper. <sep> (b) The main take-home contribution/message is weak considering the high-standard of *CONF*. <sep> Hence, I recommend rejection. <sep> I would encourage the authors to consider the above concerns as it could yield a valuable contribution."
"This paper investigates the asymptotic spectral density of a random feature model F(Wx + B).  This is an extension of existing result that analyzed a model without the bias term, i.e., F(WX). This extension requires a modification of the proof technique. In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators. <sep> Pros: <sep> - This paper investigates an interesting problem and it successfully extends the existing work. The theoretical curve well matches the simulated curve. <sep> - The finding that mixture of nonlinearities gives better expected training error is interesting. <sep> Cons: <sep> - The extension to the model with bias seems a bit incremental. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis. <sep> - The effect of mixture of activation functions is investigated in the ""training error,"" but I don't see much significance on investigating the training error thoroughly. Instead, people are interested in the test error. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. Hence, more expositions about why the training error is investigated should be provided. <sep> More minor comment: <sep> - I guess the definition of Etrain  (Eq.(17)) requires an expectation with respect to the training data. <sep> - Assumptions of the activation function f should be provided; is it just assumed to be differentiable?, ReLU is included? <sep> - The definition of G(\\gamma) in page 6 had better to be consistent to that in previous pages.","In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. <sep> Unfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold."
"This paper studies reinforcement learning algorithms in a specific subset of multi-agent environments that are 'dominance solvable'. This means that, given an initial set of strategies in the game, if we iteratively remove 'dominated strategies' (those whose utility is strictly less than another strategy independent of the strategies used by other agents), then only one strategy remains for each player. The remaining strategy is called the iterated dominance solution. The paper proves the convergence of certain RL algorithms (REINFORCE in the 2-action case, and importance weighted monte-carlo policy iteration in the multi-action case) for normal-form games. The paper demonstrates the utility of this via mechanism design: in a principal-agent problem where one can design the rewarding scheme given by a 'principal agent' to various (RL) sub-agents, rewarding schemes motivated by iterated dominance guarantees the best solution for the principal agent, whereas schemes motivated by Nash equilibria do not. <sep> The paper is quite well-written and understandable. To my knowledge, the idea is novel and has not yet been explored in the RL literature (UPDATE: based on Reviewer #1's review, this may not be the case. I'll wait to hear the author response to this). I did not check the proofs thoroughly. However, the experiments in the principal-agent problem make sense, and it's interesting to see that iterated dominance reward schemes results in good performance for the principal agent. I appreciate that, while the main results in the paper are limited to normal-form games (which are quite restricted), there are empirical results in the appendix showing the extension to Markov games with multiple timesteps, suggesting that the applicability of iterated dominance reward schemes extend beyond the simple two-action case, where no temporally extended decisions need to be made. Even so, the Markov game considered is fairly simplistic. <sep> My personal curiosity about this paper revolves around scaling to real-world applications. This is not really discussed in the paper; the conclusion talks about directions for future work, for example expanding the number of RL algorithms where convergence can be proven, or producing complexity bounds for convergence. What I want to know is: what sorts of games can we compute the iterated dominance reward schemes for? How can this be applied when the space of policies becomes too large to be enumerated (and thus determining whether a policy is strictly dominated becomes impossible)? I don't expect this paper to solve these issues, but it would be nice to have a discussion of them. <sep> Overall, I'd say this paper is interesting to the multi-agent RL community and I could imagine others building off of this work, so I err on the side of acceptance. <sep> Small fixes: <sep> - Our proof of Theorem 3.1 -> Theorem 3.2 <sep> - I'd recommend extending the captions of figures 6-8 and 9-11 in the Appendix. <sep> - Close bracket in Section 6.3 title","The paper proofs that reinforcement learning (using two different algorithms) converge to iterative dominance solutions for a class of multi-player games (dominance solvable games). <sep> There was a lively discussion around the paper. However, two of the reviewers remain unconvinced of the novelty of the approach, pointing to [1] and [2], with [1] only pertaining to supermodular games. The exact contribution over such existing results is currently not addressed in the manuscript. There were also concerns about the scaling and applicability of the results, as dominance solvable games are limited. <sep> [1] http://www.parisschoolofeconomics.eu/docs/guesnerie-roger/milgromroberts90.pdf <sep> [2] Friedman, James W., and Claudio Mezzetti. ""Learning in games by random sampling."" Journal of Economic Theory 98.1 (2001): 55-84."
"In this paper the authors present a new way to use autoregressive modeling to generate images pixel by pixel where each pixel is generated by modeling the difference between the current pixel value and  the preexistent ones. In order to achieve that, the authors propose a copy and adjustment mechanism that select an existing pixel, and then adjust its sub-pixel (channel values) to generate the new pixel. The proposed model is demonstrated with a suite of experiments in classic image generation benchmark. The authors also demonstrate the use of their technique in Image to Image translation. <sep> Overall, although the paper explain clearly the intuition and the motivation of the proposed technique, I think that the paper in its present state have low novelty, weak related work analysis review and insufficient experiments to support a publication at *CONF*. <sep> **Novelty, contribution and related work** <sep> The authors should highlight better their main contribution novelty of the proposed method compared to their baseline. <sep> **Result and conducted experiments** <sep> the correctness of the proposed approach is not proved by the conducted experiment  in fact: <sep> The experiments do not provide the details of the used architecture compared to your baseline. <sep> In Table 1 you report the results using your technique on several computer vision tasks (generation, colorization and super-resolution) but you're not comparing with the SoA of each of these tasks. <sep> The  results reported in Tables 1 and 2 are not convincing  when compared to existing approaches (using only CIFAR10 in Table2). <sep> There are so many missing details specially to validate Image-To-image translation <sep> Figure 3 is confusing and  not clear <sep> **Minor comments** <sep> In  references section : (Kingma & Dhariwal, 2018) is not in a proper format (nips 2018) <sep> Bad quality of illustrations and images <sep> Be coherent with the position of captions (figure 3)",All reviewers rated this submission as a weak reject and there was no author response. <sep> The AC recommends rejection.
"* Summary <sep> This paper introduces architecture modifications for self-attention to stabilize transformers in reinforcement learning. <sep> The new architecture, Gated Transformer-XL, replaces the order of the layer norm blocks to preserve an identity mapping. <sep> Multiple existing gating layers are proposed to replace the residual connections of transformer. <sep> The new architecture is compared against MERLIN and LSTM on DMlab-30, and further ablation studies are done on Numpad and Memory Maze. <sep> It is noted that they use the recent V-MPO objective to train LSTM and transformer. <sep> Their results show that transformers are able to learn in memory-intensive environments, with some gating combinations surpassing LSTM. <sep> * Decision <sep> This paper presents promising empirical results, however the experiments are limited, making it difficult to place in the broader work. <sep> In addition, the contribution is incremental and not well-motivated. <sep> I would recommend a weak rejection. <sep> Still, I think the paper is well written and could be improved upon. <sep> * Reasons <sep> While the empirical results are impressive, they are not put into context. <sep> DMlab-30 is still a relatively new environment suite and it is difficult to place the result of this paper in the context of broader work. <sep> In addition, the comparison is against LSTM on a new objective. <sep> While Transformer is able to beat LSTM on the same objective, it is unclear whether that is a success of the objective or the architecture. <sep> In Numpad, the transformer architecture shows an improvement over LSTM, but no comparisons are made to any other memory-based agents. <sep> The hyperparameter studies on Memory Maze also show improvements in memory-related tasks, but do not help in understanding of the proposed work. <sep> The choice of architecture modification is also not well motivated. <sep> As the paper mentions, initializing near identity has been shown to be important in the supervised learning literature. <sep> For the topic of this paper however, I do not think this adequetly explains the instability of transformers in reinforcement learning. <sep> In the related work section for example, the paper notes that gating mechanisms have been used to handle the vanishing gradients problem. <sep> The paper also notes that vanishing gradients is not an issue in transformers. <sep> Hence, it is unclear why gating would stabilize transformers for reinforcement learning. <sep> The paper is overall well written and the ideas developed are clear. <sep> Unfortunately, the impressive results on DMlab are not sufficient for both the lack of deeper empirical study and better theoretical motivation for the architecture modifications.","This paper proposes architectural modifications to transformers, which are promising for sequential tasks requiring memory but can be unstable to optimize, and applies the resulting method to the RL setting, evaluated in the DMLab-30 benchmark. <sep> While I thought the approach was interesting and the results promising, the reviewers unanimously felt that the experimental evaluation could be more thorough, and were concerned with the motivation behind of some of the proposed changes."
"Main contribution of the paper <sep> - The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info. <sep> - The method obtained improvements over various networks (SuffleNet v2, MobileNet v2, ResNet 18) on ImageNet. <sep> Methods <sep> - Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention. <sep> - The GAP of the features gives the channel attention on each stage, and the method applies the dynamic selection of the kernels. <sep> - The number of channels in skip-connection shluld be the same because it should be elementwise multiplied with the channel attention acquired from GAP. <sep> - The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops. <sep> Questions <sep> - According to Figure 4, it seems that the proposed add-on requires many parameters because it would include a FC layer for each block. But we cannot find the number of parameters in this paper. <sep> - The parameter gt is defined as 6. The experiment shows the ablation to the case gt =1, but what if we set the parameters to other numbers? <sep> Strong points <sep> - The proposed model achieved improvement with fewer Flops on large scale image classification dataset. <sep> - The method shows effectiveness when it is attached to various classification networks. <sep> Concerns <sep> - The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze-and-excitation network (SEnet, Hu et.al.). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. <sep> The author should clarify the difference and the strong points of the proposed block compared to SEnet. <sep> - Also, the reviewer cannot guarantee that the networks trained by the proposed method can transfer the knowledge to other tasks such as detection. <sep> The reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre-trained backbone for the other tasks. <sep> Regarding this, the baseline methods (MobileNet V2, Shufflenet v2, ResNet)  are widely used as a pre-trained backbone for object detection, and the papers mention the CoCo object detection results using the pre-trained backbones from their method. The reviewer thinks that the experiment regarding this should be included. <sep> - The other thing is that the parameter increases. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'. <sep> Conclusion <sep> - The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. <sep> - However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection). <sep> Inquiries <sep> - Clarifying the difference between SEnet. <sep> - Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone). <sep> - Discussing the number of the parameter as well.","The paper proposed the use of dynamic convolutional kernels as a way to reduce inference computation cost, which is a linear combination of static kernels and fused after training for inference to reduce computation cost. The authors evaluated the proposed methods on a variety models and shown good FLOPS reduction while maintaining accuracy. <sep> The main concern for this paper is the limited novelty. There have been many works use dynamic convolutions as pointed out by all the reviewers. The most similar ones are SENet and soft conditional computation. Although the authors claim that soft conditional computation ""focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations"", the methods are pretty the same and moreover in the abstract of soft conditional computation they have ""CondConv improves the performance and inference cost trade-off""."
"The paper studies the problem of advantage estimation for actor-critic RL algorithms. The key observation is that the advantage can be computed using 1-step returns, 2-step returns, etc. The paper suggests that, instead of choosing a fixed n, we should aggregate these advantageous together. If the maximum is taken, the resulting policy will be exploratory (i.e., have a ""promotion focus""); if the minimum advantage is taken, the resulting policy will be risk sensitive (i.e., have a ""regulatory focus""). <sep> The paper presents results on a number of tasks. A few toy examples show instances where the proposed method works better. Experiments on sparse reward environments show that taking the max advantage provides for exploration that outperforms a baseline (EX2). On a walking talk show that the min-approach can outperform state-of-the-art on-policy RL (PPO); the paper suggests this is because the min-approach is implicitly risk sensitive. Experiments on some of the Mujoco control tasks and some of the Atari games show that taking the advantage with the maximum absolute value performs well, as compared to PPO. <sep> Overall, I think the main contribution of this paper is the finding that there exist smarter ways of computing the advantage. A second contribution is connecting the ideas of risk-sensitive and risk-seeking control with ideas from psychology (regulatory fit theory). <sep> I am leaning towards rejecting this paper. On the one hand, the paper is well written, well motivated, and the experiments quite thorough. On the other hand, I don't think the paper is particularly useful, either from a theory or algorithmic perspective. It is not surprising that if we add an additional hyperparater to an existing RL algorithm and tune that hyperparameter, we'll do better than the existing RL algorithm. The ablation experiments in Fig 6 and Fig 7 suggest that how the advantage estimates are combined, and how they are ""mixed"" with a standard advantage estimate, matter a lot. Further, the fact that using a smaller number of advantage estimates worked better (point #2 on pg 5, Effect of Ensemble Size in Appendix A) suggests that the ensemble size is an important hyperparameter, and that risk-seeking / risk-aversion (i.e., regulatory vs promotion focus) cannot alone explain why the proposed method works. I think that, for this idea to be useful, it must be equipped with a fixed value for these hyperparameters that works well across a wide range of tasks (e.g., a learning rate of 3e-4 works well for Adam on most tasks), or an automated strategy for choosing this hyperparameter (e.g., the automatic entropy tuning in SAC). I would consider increasing my review if either of these were included. <sep> A second concern is that, despite the close connections with risk-sensitive and risk-seeking control (discussed in Section 2), none of these prior works are compared against. Many of these prior works include a temperature parameter for trading off risk-seeking vs risk-aversion (e.g., \\beta in Eq 11 of [Mihatsch 2002]), which is arguably a more transparent (to the user) and easier to analyze (for the RL researcher) than the order statistics used in this paper. Given the wealth of prior work on risk-sensitive and risk-seeking control, I think it's important to know whether these prior methods already solve the problem at hand (choosing between risk-seeking and risk-aversion). I would consider increasing my review if one of these methods were included as a baseline. <sep> Minor comments <sep> * ""that humans own"" -> ""that humans' own"" <sep> * The max strategy seems quite closely related to UCB-based exploration. I'd be curious to learn about some discussion on the similarities/differences <sep> * Fig 6 -- Can you add error bars to show the variance across random seeds? <sep> * Appendix D -- How does this approach and the results shown in Fig 9 differ from standard GAE? <sep> ----------------------- UPDATE AFTER (NO) AUTHOR RESPONSE ---------------------------- <sep> The authors did not post a response, so I will maintain my vote to ""weak reject"" this paper. I would encourage the authors to incorporate the feedback in all reviews and submit the paper to a future conference.","The authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in RL that can manage the ""regulatory focus"" of an agent. They hypothesize that this can affect performance in a problem-specific way, especially when trading off between broad exploration and risk. The reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results. Unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, I recommend to reject."
"###  Summary <sep> The paper proposes a method for regularizing neural networks to mitigate certain known biases from the representations learned by CNNs. The authors look at the setting in which the distribution of biases in the train and test set remains the same, but the distribution of targets given biases changes. <sep> To remove the bias, they propose hand-designing a set of models that rely exclusively on these biases to make predictions. They then train a new unbiased model by making sure that this new model uses sufficiently different information than the biased models to make predictions (By adding a regularization term) <sep> ### Decision and reasons <sep> I vote for a weak accept. <sep> Strengths: <sep> 1: The paper is well written with a clearly defined problem-setting. The proposed method is sound and interesting, and the empirical results, thorough. <sep> Weaknesses: <sep> 2: The solution just pushes the problem of 'learning a model that only uses the true signal to make predictions' to 'learning a set of models that only use the noise to make predictions.' It's not clear why the latter is easier than the former. <sep> 3: The paper does not have any baselines that directly try to remove the bias (Instead of using the two-step process). As a result, it's hard to judge how meaningful the improvements are. <sep> ### Supporting arguments for the reasons for the decision. <sep> Strengths: <sep> 1: The paper does a really good job of defining the problem-setting. Contrasting cross-bias with cross-domain and in-distribution makes the goal of the paper very clear. The notation used to formalize the problem setting in Section 2.1 is also clear and concise. Moreover, the experiments on the toy dataset help clarify the proposed solution whereas experiments on Biased MNIST and Imagenet show that it successfully mitigates the bias. Finally, the authors show the importance of each component of the proposed solution by factor analysis. <sep> Weaknesses: <sep> 2: In the most general case, it is not obvious why it is easier to define and learn a set of models that only use noise to make predictions (which is the required first step for their proposed solution) as opposed to learning a model that only uses signal (which is the goal of the problem). These two problems seem equally hard. The paper builds on the premise that in some cases former is easier (i.e. in some cases, it is easier to learn a set of models that use only noise as opposed to learning a debiased model directly). The authors give two such examples (They only explore the first experimentally.) <sep> 1. Learning a model that relies on local texture. They achieve this by limiting the receptive field of the features. <sep> 2. Learning a model that relies on static images to make predictions about actions in videos. <sep> I feel that the two given examples are very narrow. It would be nice if the authors could identify a broader class of problems for which G is given or can easily be defined. Moreover, even in these two examples, I'm not convinced that the proposed biased models only use B for making predictions. For example, for some classification tasks, the local texture could be part of the signal and not just the bias. Similarly, static images from a video do contain important information for making the prediction. <sep> 3: The authors only compare their method to a baseline that does nothing to debias the representations. Even though this is an important comparison (as it shows that the proposed method can debias the representations), it does not tell the reader how effective the proposed method is compared to other possible solutions. The results would be more meaningful if the authors could include at-least a simple baseline that tries to remove the bias in other ways (For example, they could use the style-transfer baseline used by Geirhos et al., 2019). <sep> I vote for accepting the paper as a poster. It introduces an interesting approach for debiasing representations. However, due to its narrow scope and missing baselines, I would not recommend the paper for an oral presentation. <sep> ### Questions <sep> 1. What are some broader class of problems for which defining G is easier than directly regularizing for debiased representations? <sep> 2. How well do Bagnets alone perform on the benchmarks in Table 3? I would expect to see that Bagnets alone do worse than vanilla Resnets on Unbiased and IN-A. Is that so? <sep> 3. How well do other methods do in these domains? (Such as methods that directly debias the training data against texture by applying style-transfer). <sep> ### Update after Author's response <sep> The author's response has clarified the motivation behind the proposed approach to an extent. They have also added a comparison with a method that directly promotes learning the shape as opposed to the texture ( by training on stylized Imagenet) <sep> I agree with R1 on all accounts (i.e. it is very hard to define the family of biased feature extractors, the proposed approach is ad-hoc, the authors need to compare to texture-shape disentanglement methods, etc), however at the same time, I can see that proposed approach can act as a useful heuristic for regularizing neural networks to pay attention to certain kind of information. <sep> An interesting use-case of the proposed method (which the authors indirectly mentioned in their response to my review) is in a multi-modal setting. It's not trivial to enforce deep learning systems to utilize all data modalities in a multi-modal setting. By defining G to be models trained on individual modalities, it would be possible to nudge our models to pay attention to the information in all modalities. <sep> Some important results in deep learning have been ad-hoc (For example skip connections in deep networks, ReLUs) and have nonetheless progressed the field. This work is not as widely applicable as skip connections or ReLUs, but it is, nonetheless, providing a heuristic for solving an important problem.","This paper provides and analyzes an interesting approach to ""de-biasing"" a predictor from its training set. The work is valuable, however unfortunately just below the borderline for this year. I urge the authors to continue their investigations, for instance further addressing the reviewer comments below (some of which are marked as coming after the end of the feedback period)."
"This paper wants to study the problem of ""learning with rejection under adversarial attacks"". It first naively extends the learning with rejection framework for handling adversarial examples. It then considers the classical cost-sensitive learning by transfer the multi-class problem into binary classification problem through one-vs-all and using the technique they proposed to reject predictions on non-important labels, and name such technique as ""learning with protection"". Finally, they do some experimental studies. <sep> The paper does not show any connection between ""learning with rejection"" and ""adversarial learning"". The method it proposes is also a naïve extension of existing methods. Both the problem setting and the technique does not have novelty. The paper fails to realize that the motivated application is actually called ""cost-sensitive learning"" and has been studied long time before. The paper also has problems in writing. Finally, there is no comparison with any baseline. Only empirical results of the proposed methods are shown. Due to all these reasons, there is still a long way to go before the paper can be published. I will rate it a clear rejection. <sep> More specially, <sep> The definition of ""suspicious example"" in Sec.3.1 has no relationship with adversary examples. Does the paper focus on adversary examples? If the definition has no relationship, it is classical learning with rejection. <sep> In the last equation of Page 3, there is no definition of \\tilde L. Actually, according to Figure 1, x's is more close to the decision boundary, it is an example more hard to classify, which could also be ""suspicious"". <sep> In the definition of ""suspicious example"" at the beginning of Sec.3.1, is both x and x' defined as suspicious examples in this way? <sep> In the last equation of page 2, there is a rejection function, so minimizing this loss is a ""separation-based approach"". However, at the end of Sec.2 the paper states they ""follow a confidence-based approach"". Any comment on the inconsistency? <sep> The motivated problem is not new. It is called cost-sensitive learning in machine learning and can date back to 2001: <sep> Charles Elkan. The Foundations of Cost-Sensitive Learning. IJCAI 2001: 973-978. <sep> Where they study the same problem when misclassifying one class of data may cost a lot than misclassifying another class of data. The current paper has not discussed any related work of cost-sensitive learning although they want to study a problem in its field. <sep> The paper should be also improved in writing in the following aspects. <sep> There is a lot of inaccurate statements in the paper. For example,  ""In Sections 3 and 4, we propose and describe our algorithm"", what is the difference between propose and describe? ""an estimator \\hat h might return result that differ greatly from h^* in a case with finite samples"". Actually there are rigorous theoretical results describing how the number of finite samples will impact the estimator \\hat h on unseen data. For example, <sep> Peter L. Bartlett, Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. JMLR, 2002. <sep> So inaccurate/unclear statements that will mislead readers should be avoided. <sep> In writing, the paper also lacks the necessary references in many places. For example, ""Learning with rejection is a classification scenario where the learner is given the option to reject an instance instead of predicting its label."", ""…classifies adversary attacks to two types of attacks, white-box attack and black-box attack."", ""Methods for protecting against these adversarial examples are also being proposed."". Necessary references are needed for these places. <sep> The organization is also problematic. For example, in the second half of Sec.2 introducing two kinds of learning with rejection models, it should be included in a ""related work"" part. <sep> ------------------------------------------ <sep> Thank you for the rebuttal. I raised my score a little bit. But I still think this paper has not been ready to be published yet.","The paper addresses the setting of learning with rejection while incorporating the ideas from learning with adversarial examples to tackle adversarial attacks. While the reviewers acknowledged the importance to study learning with rejection in this setting, they raised several concerns: (1) lack of technical contribution -- see R1's and R2's related references, see R3's suggestion on designing c(x); (2) insufficient empirical evidence -- see R3's comment about the sensitivity experiment on the strength of the attack, see R1's suggestion to compare with a baseline that learns the rejection function such as SelectiveNet; (3) clarity of presentation -- see R2's suggestions how to improve clarity. <sep> Among these, (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. <sep> AC can confirm that all three reviewers have read the author responses and have revised the final ratings. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper."
"The paper argues for encoding external knowledge in the (linguistic) embedding layer of a multimodal neural network, as a set of hard constraints. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. A technique which involves distillation is used to satisfy those constraints during learning. <sep> The question of how to encode external knowledge in neural networks is a crucial one, and the limitations of end-to-end learning with supervised data is well-made. Overall I feel that this is a potentially interesting paper, addressing an important question in a novel way, but I found the current version a highly-frustrating read (and I read the paper carefully a number of times); in fact, so frustrating that it is hard for me to recommend acceptance in its current form. More detailed comments below. <sep> Major comments <sep> -- <sep> The main problem I have with the paper lies with the first part of section 3, which is a key section describing the main method by which the constraints are satisfied during learning. This is very confusing. The need for the two-step procedure, in particular, and the importance of distillation needs much more explanation, and not relegated to the Appendix (which reviewers are not required to read - see call for papers). I'm not suggesting that the whole of the appendix needs moving to the body of the paper, but I would suggest perhaps 1/2 a page. <sep> A related comment is the use of the distillation technique. This looks crucial, but I don't believe distillation is mentioned at all until the end of the related work section, and even there it comes as a bit of a surprise since there's no mention anywhere of this technique in the introduction. <sep> I would say a little more about the distinction between the embedding space and parameter space, since you say that the external knowledge is encoded in the former and not the latter, and this is important to the overall method. Since embeddings are typically learned (or at least fine-tuned) it's not clear where the boundary is here. Another comment is that embedding space in this paper means the linguistic embedding space. Since this is *CONF* and not, eg, ACL, I would make clear what you mean by embedding space. <sep> I don't understand the diagram in Fig. 3 of the architecture, nor the explanation. What's an operation here? Is it *, or *6? I don't get why 3 is embedded by itself in the diagram, and then combined with the remainder using the MLP. Why not just run the RNN over the sequence? <sep> Why are the training instances {3,+1...} and {4,*2,...} equivalent. I stared at this a while, and still have no idea. Also, how are these ""known to be equivalent"" - what's the procedure? <sep> Minor comments including typos etc. <sep> -- <sep> The paper has the potential to be really nicely written and well-presented. Currently it reads like it was thrown together just before the deadline (which only adds to the overall frustration as a reader). <sep> In fig. 1 the second equivalent question example is interesting, since strictly speaking ""box"" and ""rectangular container"" are not synonyms (e.g. boxes can be round). Since strict synonymy is hard to find, does that matter? (I realise the dataset already exists and was presented elsewhere, but this might be worth a footnote). <sep> missing (additional) right bracket after Herbert (2016) <sep> Not sure footnote 1 needs to be a footnote. It's already been said, I think, but if it does need repeating it probably deserves to be in the body of the text. <sep> between pairs questions see Fig.3 -> figure 2? <sep> see Fig.1 -> Tab. 1? (on p.5) <sep> footnote 1 missing a right bracket usually involve -> involves <sep> +9]) - extraneous bracket <sep> Fig. 4.1 -> Fig. 3? (p.6) <sep> p.7 wastes a lot of space. In order to bring some of the appendix into the main body, I would do away with the very large bulleted list. (I don't mean lose the content - just present it more efficiently) <sep> Remember than <sep> Finally in Fig. 4.2 - some other figure due of the long chains","The paper proposes a technique for incorporating prior knowledge as relations between training instances. <sep> The reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work. Some reviewers also found the clarity lacking, but were satisfied with the revision. One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic. <sep> Following the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue."
"============ comments after rebuttal <sep> I would like to thank the authors for addressing some of my concerns. I believe the new results under Q2 and Q3 are useful additions to strengthen the paper. <sep> As for the authors' comments for Q1, I'd like to point out that a ""larger"" search space is not necessarily more difficult (a more meaningful metric would be the average accuracy of random architectures). It is still possible that the current search space is putting the proposed method at advantage, especially if having dense connections is a useful prior. <sep> Given the above, I would like to increase my score from 1 to 3 (weak reject). <sep> ============ previous comments <sep> Neural architecture search can be formulated as learning a distribution of promising architectures (the sampling policy). Such a distribution is usually represented in a fully factorized fashion (e.g., as a set of multinational distributions as in DARTS). This paper proposes to model the architecture distribution using a VAE instead, where the encoder and decoder are implemented using LSTMs. The authors argue that the increased flexibility of the sampling policy leads to improved performance on CIFAR-10, NASBench and ImageNet. <sep> The idea of representing the architecture distribution using VAEs is very natural, which in principle could offer better coverage over interesting regions in the search space as compared to traditional factorized distribution representation (which has a single mode only). <sep> While the method itself is interesting, I do not think it has been properly backed up by controlled experiments. This is largely due to the fact that the authors are comparing their method against baselines in fundamentally different search spaces. For instance: <sep> * For CIFAR-10 experiments, the authors mentioned in the appendix: ""Different from DARTS, in our search space, one node could have more than two predecessors in one cell"". This makes the search space very different from the existing ones as used by NASNet/AmoebaNet/DARTS/SNAS, and it hence remains unclear to what degree the resulting architecture has benefited from the increased in-degrees per node. Note the searched densely connected cells in Figure 4 & 5 in Appendix A.4 are clearly not part of the search space for many of the baselines. <sep> * For ImageNet experiments, the authors are using a ShuffleNet-like search space which has fundamentally different building blocks than other architecture search baselines (commonly built on top of inverted bottleneck layers). It is unclear to what degree the 77.4 top-1 accuracy @ 365 MFlops results have benefited from this different search space. <sep> Without fair comparisons in a controlled setup, it is impossible for readers to draw any solid conclusion about the true empirical advantages of the method. I'm therefore unable to recommend acceptance for the paper at the moment, but am willing to raise my score if the authors can properly address those issues in the rebuttal. <sep> Additional question: How can we isolate it to tell whether the gains come from the LSTMs or the VAEs? Is there any intuition why incorporating a generative sampler based on VAEs is potentially superior to method like ENAS (which involves LSTMs decoders only)?","This paper proposes to represent the distribution w.r.t. which neural architecture search (NAS) samples architectures through a variational autoencoder, rather than through a fully factorized distribution (as previous work did). <sep> In the discussion, a few things improved (causing one reviewer to increase his/her score from 1 to 3), but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines. There was unanimous agreement for rejection. I agree with this judgement and thus recommend rejection."
"Note: the style-formatting of this paper has been heavily tweaked, and so the evaluation should be calibrated for a 9-page paper. <sep> This paper proposes an approach for diverse self-imitation for hard exploration problems.  The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself.  By encouraging diversity in the pool of trajectories for self-imitation, the idea is to encourage faster learner -- this basic concept is also used in approaches like prioritized experience replay, albeit at the entire trajectory level rather than individual state/action level. <sep> The authors view this approach as a generalization of Go-Explore, since it does not rely on having a reset mechanism.  However, I think this discussion has a lot of subtle nuances pertaining to the stochasticity of the environment (which the authors acknowledge).  For instance, if the environment is deterministic, then why not just do something like Go-Explore, since state-reset is just memorizing a deterministic action sequence? <sep> The empirical results are very strong, achieving state-of-the-art results for any approach not reliant on a reset mechanism.  All the primary experiments appear to be for deterministic environments.  The results on stochastic environments (in the Appendix) seem pretty weak (but please correct me if I'm mistaken here).  So one major question is whether Go-Explore is a scientifically appropriate benchmark to compare with for this setting. <sep> In summary, I'm willing to be convinced that this is an interesting and scientifically novel result.  I have some concerns as expressed above. <sep> **** After Author Response **** <sep> Thanks for the response.  I'm willing to raise my score to weak accept. <sep> I think the authors did a reasonable job addressing my specific questions.  Some further reflection revealed to me that there is a huge opportunity to scientifically investigate how stochasticity impacts the proposed algorithm.  For instance, one could conduct a systematic study (say of the Apple domain) where one varies the degree of stochasticity and measures how the performance the proposed algorithm changes, perhaps relative to Go-Explore on the purely deterministic version of the environment.  It seems a bit of a cop-out to say that Go-Explore is not applicable, and misses out a huge opportunity for real scientific understanding.","This paper addresses the problem of exploration in challenging RL environments using self-imitation learning. The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories. To achieve this, the authors introduce a policy conditioned on trajectories. The proposed approach is evaluated on various domains including Atari Montezuma's Revenge and MuJoCo. <sep> Given that the evaluation is purely empirical, the major concern is in the design of experiments. The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (e.g. Go-Explore). With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go-Explore. Although this paper tackles an important problem (hard-exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper."
"This paper tackles the issue of identifying the causal reasoning behind why partial models in MBRL settings fail to make correct predictions under a new policy. The novel contribution was a framework for learning better partial models based on models learning an interventional conditional, rather than an observational conditional. The paper tried to provide both theoretical and experimental reasoning for this framework. <sep> I vote to (weak) reject the paper due to the major issues with section 2. Furthermore, the paper hard or at times almost impossible to understand as too many assumptions are made and too little is explained. <sep> Recommendations <sep> Because your graphs are not MDPs, you are not framing your example as an RL problem. This is causing a number of issues with notation and lack of clarity in the argument you're making. <sep> 1. It is unclear to me that the FuzzyBear example is correctly constructed as a RL example, reasons being that: <sep> - Figure 1 (a) & (b) do not correspond to an MDP as two different states, teddy vs grizzly, are both designated as s_1 and similarly, the two possible actions, hug or run, are both designated as a_1 and thus are not distinct. <sep> - Note your terminal state for the episodes <sep> - Have a reward for (s0, a0) as every s-a pair should have a reward <sep> - It would be helpful to note that the environments in Figure 1 are stochastic <sep> 2. Clarify notation. There are a number of assumptions about what background knowledg the reader should have. Given the bridging of disciplines in the paper, it would be useful to provide more detail on notation in Section 3. <sep> 3. Add a section on reinforcement learning in Section 3. If it's the last subsection in section 3, you could describe the relationship between the various causal reasoning and RL principles. This would further clarify how you're bridging these subtopics. <sep> 4. For sentence, <sep> ""Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1)."" <sep> As you don't cover the meaning of the do() operator until a later paragraph, provide a quick description of it as it is not common knowledge to a general AI audience, e.g., where do() indicates that the action was taken. <sep> 5. Correct the following sentences, <sep> ""Mathematically, the model with learn the following conditional probability:"" <sep> ""In Section 3, we review relevant concepts from causal reasoning based on which we propose solutions that address the problem."" <sep> 6. I recommend putting the interventional conditional equation, p(r|s0, do(a0), do(a1)) = 􏰀s1 p(s1|s0, a0)p(r|s1, a1), on its own line as the reader is doing a comparison of it with the previous equation, p(r|s0, a0, a1), given on page 2. <sep> 7. Strengthen your abstract by aligning more with claims you make in your conclusion. <sep> 8. The experiments in Figure 5 are averaged over 5 seeds. This is not enough to be statistically significant - furthermore, there are no error bars in the Figure. <sep> Question(s): <sep> 1. You've indicated two policies for Figure 1 (a): <sep> - pi1: the agent knows it is encountering a teddy bear, so it will hug <sep> - pi2: the agent knows it is encountering a grizzly bear, so it will run <sep> Is this the ""change in the behavior policy"" that you're referring to? If so, make this clearer, this currently requires a lot of work by the reader to make sense of it. <sep> 2. What are the partially observable parts of the environments in Figure 1 (a) & (b)? Make this clear.","The authors show that in a reinforcement learning setting, partial models can be causally incorrect, leading to improper evaluation of policies that are different from those used to collect the data for the model. They then propose a backdoor correction to this problem that allows the model to generalize properly by separating the effects of the stochasticity of the environment and the policy. The reviewers had substantial concerns about both issues of clarity and the clear, but largely undiscussed, connection to off-policy policy evaluation (OPPE). <sep> In response, the authors made a significant number of changes for the sake of clarity, as well as further explained the differences between their approach and the OPPE setting. First, OPPE is not typically model-based. Second, while an importance sampling solution would be technically possible, by re-training the model based on importance-weighted experiences, this would need to be done for every evaluation policy considered, whereas the authors' solution uses a fundamentally different approach of causal reasoning so that a causally correct model can be learned once and work for all policies. <sep> After much discussion, the reviewers could not come to a consensus about the validity of these arguments. Futhermore, there were lingering questions about writing clarity. Thus, in the future, it appears the paper could be significantly improved if the authors cite more of the off policy evaluation literature, in addition to their added textual clairifications of the relation of their work to that body of work. Overall, my recommendation at this time is to reject this paper."
"The goal of this work is to best understand the performance and benchmarking of continual learning algorithms when applied to sequential data processing problems like language or sequence data sets. The contributions of the paper are 3 fold - new benchmarks for CL with sequential data for RNN processing, new architecture introduced for more effective processing and a thorough empirical evaluation. <sep> Introduction: <sep> I think a little more insight into why the sequential data processing CL scenario is any different than the vision scenario would be quite helpful. Specifically, it would be quite impactful to tell us more about what the additional challenges with RNNs for CL vs feedforward for CL are in the intro. <sep> The paper is written as if the benchmark is the main contribution and the architecture improvement is just a delta on top of this, but it gets confusing when the methods section starts off with just directly stating the new architecture. <sep> The algorithm seems like a straightforward combination of recurrent progressive nets and gated autoencoders for CL. Can the authors provide more justification if that is the contribution or there is more to the insight than has been previously suggested in prior work? <sep> Figure 1 has a very uninformative caption. It also doesn't show how modules feed into one another properly. <sep> The motivation for why one needs GIM after one already has A-LSTM or A-LMN is not very clear? <sep> Overall the contribution does seem a bit incremental based on prior work and the description lacks enough detail to properly indicate why this is a very important contribution? <sep> Experiments: <sep> What does it mean to be application agnostic but restricted to particular datasets and losses? This doesn't quite parse to me. <sep> The description of the tasks is very informal and hard to follow. It's not clear what exactly the tasks and datasets look like <sep> ""using morehidden units can bridge this gap"" -> why not just do it? Its a benchmark after all. <sep> Overall the task descriptions should be in a separate section where the setup is described in a lot of detail and motivated properly. <sep> The results in the experiments section are very hard to parse. The captions need much more detail for eg Table 2. <sep> Could we also possibly have more baselines from continual learning? For instance EWC (Kirkpatrick) or generative replay might be competitive baselines. <sep> Overall I think that the GIM and A-LMN and A-LSTM methods are reasonable although somewhat incremental. But the proposed benchmarks are pretty unclear and the results are a bit hard to really interpret well. It would also be important to run comparisons with more baselines and to provide more ablation/analysis experiments to really see the benefit of GIM/A-LMN or A-LSTM. I also think that the task descriptions should be much earlier in the paper and desribed in much more rigorous detail.","This manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling. The paper consists of a definition of a problem setting, tasks in that problem setting, baselines (not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue), and a novel architecture. <sep> Reviews focused on the gravity of the contribution. R1 and R2, in particular, argued that the paper is written as though the problem/benchmark definition is the main contribution. R2 mentions that in spite of this, the methods section jumps directly into the candidate architecture. As mentioned above, several reviewers also took issue with the fact that existing CL techniques are not employed as baselines. The authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper. <sep> As many of the reviewers' comments remain unaddressed and the authors' updates did not materialize, I recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission."
"This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. The authors also add tiling trick to make the attack even more efficient. The experimental results show that the proposed method achieves state-of-the-art attack efficiency in black-box setting. <sep> The paper indeed presented slightly better results than the current state-of-the-art black-box attacks. It is clearly written and easy to follow, however, the paper itself does not bring much insightful information. <sep> The major components of the proposed method are two things: using better evolution strategies and using tiling trick. The tiling trick is not something new, it is introduced in (Ilyas et al., 2018) and also discussed in (Moon et al., 2019). The authors further empirically studied the best choice of tiling size. I appreciated that, but will not count it as a major contribution. In terms of better evolution strategies, the authors show that (1+1) and CMA-EA can achieve better attack result but it lacks intuition/explanations why these helps, what is the difference. It would be best if the authors could provide some theories to show the advantages of the proposed method, if not, at least the authors should give more intuition/explanation/demonstrative experiments to show the advantages. <sep> Detailed comments: <sep> - In section 3.2, is the form of the discretized problem a standard way to transform from continuous to discrete one? What is the intuition of using a and b? Have you considered using only one variable to do it? <sep> - In section 3.3.2 what do you mean by ""with or without softmax, the optimum is at infinity""? I hope the authors could further explain it. <sep> - In eq (2), do you mean  max_{\\tau} L(f(x + \\epsilon tanh(\\tau)), y) ? <sep> - In section 3.3.1, the authors said (1+1)-ES and CMA-ES can be seen as an instantiation of NES. Can the authors further elaborate on this? <sep> - Can the authors provide algorithm for DiagonalCMA? <sep> - It is better to put the evolution strategy algorithms in the main paper and discuss it. <sep> - Can the authors also comment/compare the results with the following relevant paper? <sep> Li, Yandong, et al. ""NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks."" ICML 2019. <sep> Chen, Jinghui, Jinfeng Yi, and Quanquan Gu. ""A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks."" arXiv preprint arXiv:1811.10828 (2018). <sep> -  In Table 1, why for Parsimonious and Bandit methods, # of tiles parts are missing? I think both of the baselines use tilting trick? And they should also run using the optimal tiling size? The result seems directly copied from the Parsimonious paper? It makes more sense to rerun it in your setting and environment cause the sampled data points may not be the same. Since CMA costs significantly more time, it makes a fair comparison to also report the attack time needed for each method. <sep> - In Table 3, why did not compare with Bandit and Parsimonious attacks? <sep> ====================== <sep> after the rebuttal <sep> I thank the authors for their response but I still feel that there is a lot more to improve for this paper in terms of intuition and experiments. Therefore I decided to keep my score unchanged.","This paper proposes a new black-box adversarial attack based on tiling and evolution strategies. While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments. The paper does not gather sufficient support from the reviewers even after author response. I encourage the authors to improve this paper and resubmit to future conference."
"Gradient sparsification is an important technique to reduce the communication overhead in distributed training. In this paper, the authors proposed a training method called global momentum compression (GMC) for distributed momentum SGD with sparse gradient. Following existing gradient sparsification techniques such DGC, GMC is also built up on the memory gradient approach; the major distinction between GMC and existing techniques is that GMC keeps track of global gradient to maintain the memory gradient, while the existing technique keeps track of worker-local gradients for memory gradient. The primary contributions in the paper are as the following: <sep> 1. The authors propose GMC, a training method for distributed momentum SGD with sparse gradient communication. It uses global gradient (but still achieve sparse communication) to maintain the gradient memory while existing approaches such as DGC use worker-local gradient to do so. <sep> 2. The authors prove the convergence rate of GMC for 1. strongly convex and smooth functions 2. convex functions and 3. Non-convex Lipschitz smooth functions. This is the first work on proving the convergence rate of distributed momentum SGD using sparse communication techniques based on memory gradient. <sep> 3. Empirically, the authors show that GMC can empirically attain the same model accuracy as conventional distributed momentum SGD with ~100x reduction in communication overhead. It can also match the performance of DGC at the same communication compression rate. <sep> I think in general the ideas and efforts of the authors in proving the convergence rate of distributed momentum SGD with *gradient sparsification* is interesting and important. However, I have the some questions and concerns on validating the claims in the paper. I currently give weak reject but I am happy to raise the score if the authors can clarify or improve in their rebuttal / future drafts. The primary questions and concerns (critical to the rating) are: <sep> 1. One important claimed advantage of GMC over existing method is that it uses global gradient for memory gradient, while existing methods such as DGC uses local-work gradient to do so. But I did not find convincing support of this advantage in the paper: Empirically, in the experiment results, I don't think GMC demonstrate better performance than DGC in a statistical meaningful way; instead they are basically demonstrating matching performance. Theoretically, I am not sure if only the global gradient enables the proof of convergence rate while the worker-local gradient cannot. My preliminary feeling is that by bounding the gradient variance, it should also be possible to prove a rate for DGC using worker-local gradient; this is because the difference between the global gradient and the local gradient might be bounded via the gradient variance. <sep> 2. In the experiments, the authors focus on momentum SGD for image classification tasks. To better support the versatility and efficacy of GMC, it would be interesting to include some experiments for other domains (e.g. using the STOA transformer style models for NLP tasks). In these models, momentum-like components are also used in the optimizer (e.g. Adam for fairseq for machine translations), it will be interesting to see if the efficacy of GMC also empirically transfer to these settings. <sep> Minor questions (influencing the rating in a secondary way) <sep> 1. Regarding the assumptions in the paper, I think assumption 2 need some validation / support to show that it is a proper one. My preliminary feeling is that assumption 2 is intuitive as the sparsification procedures only zero out small values so that the error introduced in the gradient is small and bounded. But it should be more convincing to empirically show the magnitude of u comparing to the magnitude of gradient g in Equ. 8. <sep> 2. I notice that the experiments uses conventional momentum SGD for a few epochs as warm up, is there any specific reasoning on using this warmup approach instead of the sparsity level warmup as used in DGC? <sep> 3. In the experiments, GMC does not use the factor masking trick while DGC uses. If it is for demonstrating the benefits of global gradient for gradient memory, I think it is more proper to also include the results of DGC without factor masking? In this way, this question can be directly answered in an ablation study way by eliminating the possible contribution of using/not using factor masking. <sep> NITS to improve the paper (not related to the rating): <sep> 1. The last contribution bullet forgets to mention that it is about comparing to DGC. <sep> 2. In algorithm 1, it is clearer to mention how the mask m is generated (e.g. based on magnitude). <sep> 3. In the second paragraph in section 3.2, the vector inner product is not properly written between coefficients and w. <sep> 4. Above theorem 1, in the text, the condition for the discussion on the two cases are confusing. <sep> 5. In the definition of CR in the first paragraph of section 5, why the summation starts from 5. The text describes as warm up with 5 *epochs* while in the equation it is saying warm up with 4 *steps*.","The author propose a method called global momentum compression for sparse communication setting, and provided some theoretical results on the convergence rate. The convergence result is interesting, but the underlying assumptions used in the analysis appear very strong. Moreover, the proposed algorithm has limited novelty as it is only a minor modification. Another main concern is that the proposed algorithm shows little performance improvement in the experiments. Moreover, more related algorithms should be included in the experimental comparison."
"This paper proposes a novel extension to SGD/incremental gradient methods called CRAIG. The algorithm selects a subset of datapoints to approximate the training loss at the beginning of each epoch in order to reduce the total amount of time necessary to solve the empirical risk minimization problem. In particular, the algorithm formulates a submodular optimization problem based on the intuition that the gradient of the problem on the selected subset approximates the gradient of the true training loss up to some tolerance. Each datapoint in the subset is a medoid and assigned a weight corresponding to the number of datapoints in the full set that are assigned to that particular datapoint. A greedy algorithm is employed to approximately solve the subproblem. Theory is proven based on based on an incremental subgradient method with errors. Experiments demonstrate significant savings in time for training both logistic regression and small neural networks. <sep> Strengths: <sep> The proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss. Based on the experiments provided in the paper, it does appear to yield a significant speedup in training time. It is interesting to observe how the order of the datapoints matter significantly for training, and that CRAIG is also able to naturally define a good ordering of the datapoints for SG training. This is strong algorithmic work. <sep> Weaknesses: <sep> Some questions I had about the work: <sep> - How well does one have to approximate dij in order for the method to be effective? The authors provide an approach to approximate this for both logistic regression and neural networks. How does one guarantee that one is obtaining the maximum over xinX for neural networks via backpropagating only on the last layer? Does taking this maximum matter? <sep> - How does one choose ϵ? Is this related to how dij's are approximated? <sep> - If one were to consider an algorithm that samples points from this new distribution over the data given by CRAIG, if one were to include the weight γj into the algorithm, would the sample gradient be unbiased? What if one were to simply use γj to weight that particular sample in the new distribution? <sep> - In machine learning, the empirical risk (finite-sum) minimization problem is an approximation to the true expected risk minimization problem. What is the effect of CRAIG on the expected risk? Is there any deterioration in generalization performance? <sep> - In page 4, what does the minS⊆V refer to? Should the equation be interpreted as with the set S fixed or not? <sep> - Theorems 1 and 2 are stated a bit non-rigorously. Are these theorems for fixed k? What does it mean for these bounds that k→∞? <sep> - In Theorems 1 and 2, what is the bound on the steplength in order to obtain the convergence result for τ=0? <sep> - In Theorem 1 for 0<τ<1, why does one obtain a result where ∥xk–x∗∥2≤2ϵR/μ, why is the distance to the solution bounded by a constant? What if one were to initialize x0 to be such that ∥x0–x∗∥2>2ϵR/μ? (Similar for Theorem 2.) <sep> - In the experiments, how is the steplength and other hyperparameters tuned? Are multiple trials run? <sep> - Is ϵ used to determine the subset or is it based on a predetermined subset size? <sep> - How do the final test losses compare between CRAIG and the original algorithms? <sep> - How do the relative distances (rather than the absolute distance) to the solution behave? <sep> - How does CRAIG perform over multiple epochs? How does the algorithm transition when the subset is changed (as in neural networks)? <sep> - Why does CovType appear more stable with the shuffled version over the other datasets? Is the stability related to the distribution of the weights γj? <sep> Some grammatical errors/typos/formatting issues: <sep> - Equation (9) needs more space between the ∀x,i,j and the rest of the equation. <sep> - What is Δ on page 5? Is it supposed to be F? <sep> - On page 8, And should not be capitalized. <sep> - Page 14, prove not proof <sep> - Page 14, subtracting not subtracking <sep> - Page 16, cycle not cycke <sep> Overall, although I like the ideas in the paper, the paper still needs some significant amount of refining in terms of both writing and theory, as well as some additional experiments to be convincing. If the comments I made above were addressed, I would be open to changing my decision.","This paper investigates the practical and theoretical consequences of speeding up training using incremental gradient methods (such as stochastic descent) by calculating the gradients with respect to a specifically chosen sparse subset of data. <sep> The reviewers were quite split on the paper. <sep> On the one hand, there was a general excitement about the direction of the paper. The idea of speeding up gradient descent is of course hugely relevant to the current machine learning landscape. The approach was also considered novel, and the paper well-written. <sep> However, the reviewers also pointed out multiple shortcomings. The experimental section was deemed to lack clarity and baselines. The results on standard dataset were very different from expected, causing worry about the reliability, although this has partially been addressed in additional experiments. The applicability to deep learning and large dataset, as well as the significance of time saved by using this method, were other worries. <sep> Unfortunately, I have to agree with the majority of the reviewers that the idea is fascinating, but that more work is required for acceptance to *CONF*."
"Thank you for your rebuttal. The paper improved after the rebuttal but I still think point 5 in the rebuttal is problematic since using d'=d, may not guarantee that we have a proper metric as claimed by the authors. I m updating  my score as a weak reject for this paper. <sep> Summary of the paper: <sep> The paper proposes to train implicit  model such as gan and an explicit model (Energy Based ) jointly . The GAN is trained using WGAN-GP objective or the original JS objective (we have a discriminator D and Generator G). The energy based model (E) is trained using Stein Divergence with a fixed kernel k or a learned critic who's parameters are denoted pi in the paper. Note that the critic of the stein divergence is vector valued. This paper propose to add a regularization loss on the stein divergence between the generator G (implicit model ) and the explicit model (E). This gives a training objective minG,EW(Pr,G)+λ1S(Pr,PE)+λ2S(PG,PE) <sep> In the paper the stein critic is shared between the two stein divergence which means that the authors are rather considering : S(λ1Pr+λ2PG,PE) <sep> Paper shows the effect of this additional coupling between the two models as a regularization on the Discriminator D and on the critic of the stein divergence. <sep> Then  the effect of the regularization is also show in terms of convergence in the optimization on a bilinear game, and in the convex concave case. <sep> Experiments are given showing the benefits of the joint training. <sep> Review: <sep> The paper has a lot of typos and needs a lot of proofreading and is not in shape for being reviewed. <sep> There are too many concerns with this papers: <sep> 1- The first one was mentioned above if the critic is shared then you better be considering :  S(λ1Pr+λ2PG,PE) <sep> 2- In equation 4,  the problem is minGmaxD it is swapped. <sep> 3- There a lot of gaps in the proofs of Theorems 1 and 2. The transition from equation 14 to the infP... is not explained and seems flawed. In theorem 2 , the proof is too short and swapping of min and E is not backed rigoursly. <sep> 4- Again in Equation 8, it is not clear how the Stein terms were computed , the appendix does not give the derivations either. <sep> 5 - Authors say that the Stein critic have similar architecture to the GAN critic , which indicates an error in the implementation in the neural case for stein critic. Stein critic has to be vector valued, after checking the code of this paper on GitHub, stein critic maps to a real value in the code , which is flawed. The critic of stein needs to map the image to an image , which actually quite expensive. <sep> Typos: <sep> abstract : without explicitly defines -> defining multimodal data . has been -> have without explicit defines -> defining",The paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using Stein's method. There are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers.
"The paper revisits limitations of relation-embedding models by taking losses into account in the derivation of these limitations. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. <sep> There seems to be merit in distinguishing the loss when studying relation encoding but I think the paper's analysis lacks proper rigor as-is. A loss minimization won't make equalities in (3) and (5) hold exactly, which the analysis do not account for. A rewriting of the essential elements of the different proofs could make the arguments clearer. <sep> Paper writing: <sep> * The manuscript should be improved with a thorough revision of the style and grammar. Example of mistakes include: extraneous or missing articles, incorrect verbs or tenses. <sep> * The 10-pages length is not beneficial, the recommended 8-pages could hold the same overall content. <sep> * The option list on page 8 is very difficult to read and should be put in a table, e.g. in appendix. <sep> * Parentheses are missing around many citations and equation references <sep> Theory: <sep> Equation (2) and (4) do not seem to bring much compared to the conditions in Table 1. Eq. (3) and (5) show ""a"" loss function rather than ""the"" loss function since multiple choices are possible. \\gamma_1 should be set to 0 when it is 0 rather than staying in the equations. <sep> * Minimizing the objective (3) and (5) will still not make the conditions in Table 1 hold exactly, because of slack variables. <sep> Experiments: <sep> - Can the authors provide examples of relations learned with RPTransComplEx# that go address the limitations L1...L6, validating experimentally the theoretical claims and showing that the gain with RPTransComplEx5 correspond to having learned these relations?","The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by choosing more appropriate loss functions. The submission then proposes TransComplEx to further improve results. This paper received four reviews, with three recommending rejection, and one recommending weak acceptance. A main concern was in the clarity of motivating the different models. Another was in the relatively low performance of RotatE compared with [1], which was raised by multiple reviewers. The authors provided extensive responses to the concerns raised by the reviewers. However, at least the implementation of RotatE remains of concern, with the response of the authors indicating ""Please note that we couldn't use exactly the same setting of RotatE due to limitations in our infrastructure."" On the balance, a majority of reviewers felt that the paper was not suitable for publication in its current form."
"In this paper, authors propose a way to speed up the computation of GNN. More specifically, the hierarchically aggregate computation graphs are proposed to aggregate the intermediate node and utilize this to speed up a GNN computation. Authors proof that the computation based on HAGs are equivalent to the vanilla computation of GNN (Theorem1). Moreover, for sequential aggregation, it can find a HAG that is at least (1-1/e)-approximation of the globally optimal HAGs (Theorem 3). These theoretical results are nice. Through experiments, the authors demonstrate that the proposed method can get faster computation than vanilla algorithms. <sep> The paper is clearly written and easy to follow. However, there are some missing piece needed to be addressed. <sep> I put 6 (weak accept), since we cannot put 5. However, current my intention about the score is slightly above 5. <sep> Detailed comments: <sep> 1. Experiments are only done for computational time comparison. In particular, for the sequential one, prediction accuracy can be changed due to the aggregation algorithm. Thus, it needs to report the prediction accuracy. <sep> 2. In GraphSAGE, what is the sampling rate? It would be nice to have the trade-off between the sampling rate and the speedup. I guess if we sample small number of points in GraphSAGE, the performance can be degraded. In contrast, the proposed algorithm can get similar performance with larger sampling rate? Related to the question 1, the performance comparison is needed. <sep> 3. Equations are used without not explaining the meaning. For instance AGGREGATE function (1), there is no definition how to aggregate.","This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency. It achieves good speedup and also provide theoretical analysis. There has been several concerns from the reviewers; authors' response addressed them partially. Despite this, due to the large number of strong papers, we cannot accept the paper at this time. We encourage the authors to further improve the work for a future version."
"This paper examines generalization performance of various neural network architectures in terms of a sensitivity metric that approximates how the error responds to perturbations of the input. A crude argument is presented for how the proposed sensitivity metric captures the variance term in the standard bias-variance decomposition of the loss. A number of experimental results are presented that show strong correlation between the sensitivity metric and the empirical test loss. <sep> Understanding the distinguishing characteristics of networks that generalize well versus networks that generalize poorly is a central challenge in modern deep learning research, so the topic and analyses presented in this paper are salient and will be of interest to most of the community. The experimental results are intriguing and the presentation is clear and easy to read. While some may object to the egregious simplifications utilized in ""deriving"" the sensitivity metric, I believe this kind of analysis should be welcomed if it produces new insights and helps explain otherwise opaque empirical phenomena. All told, if taken in isolation from prior work, I think the insights and empirical results presented in this paper are quite interesting and certainly sufficient for acceptance to *CONF*. <sep> However, there is significant overlap with prior work that severely detracts from the novelty of the results presented here, and I think the community is already familiar with the paper's main conclusions. From the empirical viewpoint, [1] performs a very similar (and actually quite a bit more thorough) analysis, and reaches very similar conclusions. The authors do cite [1], but unless I missed something, their main argument for uniqueness is basically ""in experiments, we prefer S to the Jacobian, because in order to compute S it is enough to look at the network as a black box that given an input, generates an output, without requiring further knowledge of the model."" While this may be useful from the practical standpoint for some non-differentiable models, I'm not convinced that this distinction is really significant in terms of building insights or new understanding. <sep> One additional way this paper is distinct from [1] is that it includes a theoretical ""derivation"" for the sensitivity metric. While I found the argument interesting, from the theoretical perspective, [2] gives much more rigorous and insightful arguments that help explain the observed phenomena. <sep> Overall, I'm just not convinced this paper is novel enough to merit publication. But perhaps I've overlooked something, in which case I hope the author's response can highlight their unique contributions relative to prior work. <sep> [1] Novak, Roman, et al. ""Sensitivity and generalization in neural networks: an empirical study."" arXiv preprint arXiv:1802.08760 (2018). <sep> [2] Arora, Sanjeev, et al. ""Stronger Generalization Bounds for Deep Nets via a Compression Approach."" International Conference on Machine Learning. 2018.","The paper proposes a definition of the sensitivity of the output to random perturbations of the input and its link to generalization. <sep> While both reviewers appreciated the timeliness of this research, they were taken aback by the striking similarity with the work of Novak et al. I encourage the authors to resubmit to a later conference with a lengthier analysis of the differences between the two frameworks, as they started to do in their rebuttal."
"In the paper, the authors present a new algorithm for training neural networks used in an automated theorem prover using theorems with or without proofs as training data. The algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the Proximal Policy Optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs. The authors also propose a new dataset for theorems and proofs for a simple equational theory of arithmetic, which is again suitable for improving (via learning) and testing the ability of the prover for finding long proofs. The proposed prover is tested against existing theorem provers, and for the authors' dataset, it outperforms those provers. <sep> I found it difficult to make up my mind on this paper. On the one hand, the paper tackles an interesting problem of improving an automated theorem prover via learning, in particular, its ability for finding long nontrivial proofs. Also, I liked a qualitative analysis of the failure of the curriculum learning for tackling hard tasks in the paper. On the other hand, I couldn't quite make me excited with the dataset used to test the prover in the paper. The dataset seems to consist of easy variable-free equational formulas about arithmetic that can be proved by evaluation. Of course, I may be completely wrong about the value of the dataset. Also, if the dataset includes variables and other propositional logic formulas, such as disjunction, negation and conjunction, so that the prover can be applied to any formulas from Peano arithmetic via Skolemization, I would be much more supportive for the paper. Another thing that demotivated me is that I couldn't find the discussion about the subtleties in using curriculum learning and PPO for the theorem-proving task in the paper. What are the possible design choices? Why does the authors' choice work better than others? <sep> I added a few minor comments below. <sep> * abstract, p1: ""significantly outperforms previous learning-based"". When I read the experimental result section, I couldn't quite get this sense of huge improvement of the proposed approach over the existing provers. Specifically, from Table 4, I can see FLoP performs better than rlCoP, but I wasn't sure that the improvement was that significant (especially because rlCoP might not have given a chance to be tuned to the type of questions used to train FLoP -- I may be wrong here). I suggest you to add some further explanation so that a reader can share your sentiment and excitement on the improvement brought by your technique. <sep> * p2: The related work section is great. I learned a lot by reading it. Thanks. <sep> * p4: I think that you used the latex citation command incorrectly in ""learning Resnick ... Chen (2018)"" <sep> and ""features Kaliszyk ... Kaliszyk et al. (2015a; 2018)"". <sep> * p6: discount factor) parameters related ===> discount factor), parameters related <sep> * p8: a a well ==> a well","This paper proposes a curriculum-based reinforcement learning approach to improve theorem proving towards longer proofs. While the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough. In particular, the authors are encouraged to apply their methods to more complex domains beyond Robinson arithmetic. It would also be helpful to get a more in depth analysis of the role of the curriculum. The discussion period did not lead to improvements in the reviewers' scores, hence I recommend that this paper is rejected at this time."
"A Simple method to detect adversarial examples, but needs more work. <sep> #Summary: <sep> The paper proposed a method that utilizes the model's explainability to detect adversarial images whose explanations that are not consistent with the predicted class.  The explainability is generated by SHAP, which uses Shapley values to identify relative contributions of each input to a class decision. It designs two detection methods: EXAID familiar, which is aimed to detect the known attacks and EXAID unknown, which is against unknown attacks. Both of the two methods are evaluated on perturbed test data which are generated by FGSM, PGD and CW attack with perturbations of different magnitudes. Qualitative results also show that the proposed method can effectively detect adversaries, especially when the perturbation is relatively small. <sep> #Strength <sep> The method is easy to implement and using the idea of interpretation for detecting adversarial examples seems interesting. <sep> Good results are demonstrated compared with other comparators. <sep> #Weakness <sep> The idea of this paper is based on the interpretation method of DNN. However, it has been shown that these interpretation methods are not reliable and easy to be manipulated [1][2]. Therefore, although the method is simple to design, it also brings other security concerns. <sep> Unfortunately, the paper does not address these issues. In addition, the comparators listed in the experiments are not state-of-art or common baselines. It is either not clear why authors modified the existing method and develop their own ""unsupervised"" version. <sep> In the experiments, many details are omitted. For example, how is the ""noise level"" defined? Are they based on L1, L2 or L-inf perturbation? For PGD attack, how many iterations does the generation run and what is the step size? How many effective adversarial examples are generated for training and testing? And all the experiments are conducted in a relatively small dataset, it is also suggested to do experiments on large datasets, e.g. Imagenet. <sep> In the evaluation part, it looks strange to me why the EXAID familiar performs worse than EXAID unknown in evaluating FGSM attack on SVHN since the EXAID familiar is trained using FGSM attack. <sep> #Presentation <sep> I think the authors used a wrong template to generate the article. The font looks strange and the headnote indicates it is prepared for *CONF*2020. The paper contains many typos and even the title contains a misspelling. Poor coverage of citations. There are more works for detecting adversarial examples that are published, e.g. [3][4][5]. On the other hand, the paper does not have the literature review for work related to the model interpretation. <sep> Overall, I think the paper is not good enough for publication at *CONF*. <sep> [1] Dombrowski, Ann-Kathrin, et al. ""Explanations can be manipulated and geometry is to blame."" arXiv preprint arXiv:1906.07983 (2019). <sep> [2] Ghorbani, Amirata, Abubakar Abid, and James Zou. ""Interpretation of neural networks is fragile."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019. <sep> [3] Meng, Dongyu, and Hao Chen. ""Magnet: a two-pronged defense against adversarial examples."" In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135-147. ACM, 2017. <sep> [4] Liao, Fangzhou, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. ""Defense against adversarial attacks using high-level representation guided denoiser."" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1778-1787. 2018. <sep> [5] Ma, Shiqing, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang. ""NIC: Detecting Adversarial Samples with Neural Network Invariant Checking."" In NDSS. 2019.","This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity-map-like explanations are used to justify and validate decisions. Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. No rebuttal was provided."
"Very good paper that studies the error rate of low-bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. The theoretical claims are strongly supported by experiments, and the experimental analysis covers state-of-the-art architectures and demonstrates competitive results. The paper in addition also analyzes the inference cost of their approach (in addition to the accuracy results), and shows positive results on ResNet and MobileNet architectures. <sep> The paper primarily shows that the mean squared error of the final output of a quantized network has the additive property of being equal to the sum of squared errors of the outputs obtained by quantizing each layer individually. Although there is no reason why this should be case, experimental results from the authors on AlexNet and VGG-16 validate this. Based on this assumption, the authors then use a Lagrangian based constrained optimization to minimize the sum of squared errors of outputs when individual weights/activations are quantized, with the constraint being the total bit budget for weights and activations. The authors show that this can be optimized under the Pareto condition easily. <sep> The experimental section is quite detailed and covers the popular architectures instead of toy ones. The accuracy results compared to other 2-bit and 4-bit approaches are competitive. It's also nice to see analysis of inference cost where unequal bitrate allocation performs better than other methods. <sep> The authors show that given the constrained optimization, layers that have a large number of weights receive lower bitrates and vice-versa. While it makes sense that this would contribute to stronger inference speedup compared to methods with either equal bitrate allocation across layers or those that allocate higher bitrate to layers with large number of weights, it's not entirely clear why the optimization would produce this allocation in the first place. Do the authors mean to conclude that layers with large number of weights hold a lot of redundancy and don't have a significant impact on the overall accuracy of the model? This needs to be clarified further.","This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited."
"The authors derive the influence function of models that are first pre-trained and then fine-tuned. This extends influence functions beyond the standard supervised setting that they have been primarily considered in. To do so, the authors make two methodological contributions: 1) working through the calculus for the pre-training setting and deriving a corresponding efficient algorithm, and 2) adding L2 regularization to approximate the effect of fine-tuning for a limited number of gradient steps. <sep> I believe that these are useful technical contributions that will help to broaden the applicability of influence functions beyond the standard supervised setting. For that reason, I recommend a weak accept. I have some questions and reservations about the current paper: <sep> 1) Does pretraining actually help in the MNIST/CIFAR settings considered? These seem to be non-standard pretraining settings. More generally, can we relate influence to some objective measure that we care about (say test accuracy), for example by showing that removing the top X% of influential pretraining data hurts test accuracy as much as predicted? Minor: section 4.2 also seems non-standard. Are the exact same bird vs. frog examples being used for both pretraining and finetuning? <sep> 2) In what situations might we want to examine the influence of pretraining data, and can we design experiments that show those situations? For example, perhaps we're wondering if different types of sentences in the one-billion-word dataset might be more or less useful. Can we verify those claims using these multi-stage influence functions? It is otherwise difficult to assess the utility of the qualitative results (e.g., Figure 3 and Appendix C). <sep> 3) It'd be helpful to get a better understanding of the technical contributions of this paper. Specifically, <sep> a. What is the impact of α in equation 12 and how does it interact with the number of fine-tuning steps taken? <sep> b. If the Hessian has negative eigenvalues, we can still take H−1b by solving CG with H2, but what does this correspond to? Is the influence equation well defined (or the Taylor approximation justified) if H is not positive definite?","This paper extends the idea of influence functions (aka the implicit function theorem) to multi-stage training pipelines, and also adds an L2 penalty to approximate the effect of training for a limited number of iterations. <sep> I think this paper is borderline. I also think that R3 had the best take and questions on this paper. <sep> Pros: <sep> - The main idea makes sense, and could be used to understand real training pipelines better. <sep> - The experiments, while mostly small-scale, answer most of the immediate questions about this model. <sep> Cons: <sep> - The paper still isn't all that polished. E.g. on page 4: ""Algorithm 1 shows how to compute the influence score in (11). The pseudocode for computing the influence function in (11) is shown in Algorithm 1"" <sep> - I wish the image dataset experiments had been done with larger images and models. <sep> Ultimately, the straightforwardness of the extension and the relative niche applications mean that although the main idea is sound, the quality and the overall impact of this paper don't quite meet the bar."
"The paper aims to study the topology of loss surfaces of neural networks using tools from algebraic topology. From what I understood, the idea is to effectively (1) take a grid over the parameters of a function (say a parameters of a neural net), (2) evaluate the function at those points, (3) compute sub-levelset persistent homology and (4) study the resulting barcode (for 0/1-dim features) (i.e., the mentioned ""canonical form"" invariants). Some experiments are presented on extremely simple toy data. <sep> Overall, the paper is very hard to read, as different concepts and terminology appear all over the place without a precise definition (see comments below). Given the problems in the writing of the paper, my assessment is that this idea boils down to computing persistent homology of the sub-levelset filtration of the loss surface sampled at fixed parameter realizations. I do not think that this will be feasible to do, even for small-scale real-world neural networks, simply due to the difficulty of finding a suitable grid, let alone the vast number of function evaluations involved. <sep> The paper is also unclear in many parts. A selection is listed below: <sep> (1) What do you mean by gradient flow? One can define a gradient flow in a linear space X and for a function F: X->R, e.g., as  a smooth curve R->X, such that x'(t) = -\\nabla F(x(t)); is that what is meant? <sep> (2) What do you mean by ""TDA package""? There are many TDA packages these days (maybe the CRAN TDA package?) <sep> (3) ""It was tested in dimensions up to 16 ..."" What is meant by dimension here? The dimensionality of the parameter space? <sep> (4) The author's talk about the ""minima's barcode"" - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it's not associated to a minima. <sep> (5) Is Theorem 2.3. not just a restatement of a theorem from Barannikov '94? At least the proof in the appendix seems to be . <sep> (6) Right before Theorem 2.3., what does the notation F_sC_* mean? This needs to be introduced somewhere. <sep> From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. The paper devotes quite some time to the introduction of these concepts, but not in a very clear or understandable manner. The experiments are effectively done on toy data, which is fine, but the paper stops at that point. I do not buy the argument that ""it is possible to apply it [the method] to large-scale modern neural networks"". Without a clear strategy to extend this, or at least some preliminary ""larger""-scale results, the paper does not meet the *CONF* threshold. The more theoretical part is too convoluted and, from my perspective, just a restatement of earlier results.","The main concern raised by the reviewers is that the paper is difficult to read and potentially unclear. Therefore, the area chair read the paper, and also found it fairly dense and challenging to read. While there may be important discoveries in the paper, the paper in its current form makes it too difficult to read. Since four reviewers (including the AC) struggled to understand the paper, we believe the presentation of the paper should be improved. In particular, the claims of the paper should be better put into context."
"I have not worked in the optimization filed and I am only gently followed the NAS field. I might under-valued the theoretical contribution. <sep> This work provides  theoretical analysis for the NAS using weight sharing in two aspects: <sep> 1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015). The author also provided an alternative to SBMD that uses alternating successive convex approximation (ASCA) which has similar convergence rate. <sep> 2) The author provide generalization guarantees for this objective over structured hypothesis spaces associated with a finite set of architectures. <sep> My biggest concern is the validity of the proposed exponentiated gradient update, at least empirically. We indeed observed slightly improvement in test error over DARTS on the CIFAR10 benchmark but how reproducible the results are? Can you compare at least on the other benchmark (PENN TREEBANK) used in Liu et al 2019? Also, comparing to first order DARTS, search cost is the same and this is hard to justify the better convergence rate for EDARTS. In addition, the results on feature map selection is not very encouraging as the gap to the successive halving is significant. <sep> The author proposed ASCA, as an alternative method to SBMD. Why we need such alternative? What is the advantage of ASCA comparing to SBMD? When should I use ASCA and when SBMD? How do they empirically different? <sep> Then I feel some wording can be improved. For example, ""while requiring computation training …"",  ""…which may be of independent interest"".","Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint. I agree with reviewer 2 on the following points, which support rejection of the paper: <sep> 1) Only CIFAR is evaluated without Penn Treebank; <sep> 2) The ""faster convergence"" is not empirically justified by better final accuracy with same amount of search cost; and <sep> 3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper. <sep> The scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view."
"The authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [Dai et al. (2017)]. The main contribution is to consider each of the steps taken by Dai et al. to solve combinatorial problems on graphs, and adapt them to the considered scheduling problem. <sep> Not being an expert in RL, my assessment should be discounted. However, I am not sure I follow properly the main idea of the paper. The point of Dai et al. was to use RL to solve a wide family of combinatorial problems. Now, the authors claim to build upon these ideas to solve... what looks essentially like a far more standard RL problem, and not necessarily a combinatorial optimization problem. The main insight by Dai et al. was to highlight the fact that combinatorial problems are usually solved (or approximated) without ""warm starts"", i.e. they do not consider distributions on problem instances to learn from. The problem considered by the authors is, quite on the contrary, a typical RL problem where information is extracted from the problem's structure (here a maze). Therefore, I feel there is something of a fundamental contradiction going on at a fairly high-level, in the sense that the paper ""uses RL to solve a subset of combinatorial problems that were studied by RL before"". The absence of other baselines in experiments make this even more suspicious. Therefore I believe the paper's presentation could be greatly improved if it were better ""located"" within the RL literature (which is almost non-existent in the very brief bibliographic section) and that the authors were able to show that  their proposals are original, within an RL context. <sep> minor points: <sep> * the comment ""While learning-based methods are generally believed to suffer exponentially increasing training requirements as problem size (number of robots and tasks) increases, our method's training requirement is empirically shown not to scale while maintaining near-optimal performance"" --> this is too loose a statement. Provide more evidence or references.","Unfortunately, the reviewers of the paper are all not certain about their review, none of them being RL experts. Assessing the paper myself—not being an RL expert but having experience—the authors have addressed all points of the reviewers thoroughly."
"This paper presents a dissection analysis of graph neural networks by decomposing GNNs into two parts: a graph filtering function and a set function. Although this decomposition may not be unique in general, as pointed out in the paper, these two parts can help analyze the impact of each part in the GNN model. Two simplified versions of GNN is then proposed by linearizing the graph filtering function and the set function, denoted as GFN and GLN, respectively. Experimental results on benchmarks datasets for graph classification show that GFN can achieve comparable or even better performance compared to recently proposed GNNs with higher computational efficiency. This demonstrates that the current GNN models may be unnecessarily complicated and overkill on graph classification. These empirical results are pretty interesting to the research community, and can encourage other researchers to reflect on existing fancy GNN models whether it's worth having more complex and more computationally expensive models to achieve similar or even inferior performance. Overall, this paper is well-written and the contribution is clear. I would like to recommend a weak accept for this paper. If the suggestions below can be addressed in author response, I would be willing to increase the score. <sep> Suggestions for improvement: <sep> 1) Considering the experimental results in this paper, it is possible that the existing graph classification tasks are not that difficult so that the simplified GNN variant can also achieve comparable or even better performance (easier to learn). This can be conjectured from the consistently better training performance but comparable testing performance of original GNN. Another possibility is that even the original GNN has larger model capacity, it is not able to capture more useful information from the graph structure, even on tasks that are more challenging than graph classification. However, this paper lacks such in-depth discussions; <sep> 2) Besides the graph classification task, it would be better to explore the performance of the simplified GNN on other graph learning tasks, such as node classification, and various downstream tasks using graph neural networks. This can help demystify the question raised in the previous point; 3) The matrix \\tilde{A} in Equation 5 is not well explained (described as ""similar to that in Kipf and Welling (2016)""). It would be more clear to directly point out that it is the adjacency matrix, as described later in the paper.","This paper proposes to split the GNN operations into two parts and study the effects of each part. While two reviewers are positive about this paper, the other reviewer R1 has raised some concerns. During discussion, R1 responded and indicated that his/her concerns were not addressed in author rebuttal. Overall, I feel the paper is borderline and lean towards reject."
"Summary: The paper uses a Gaussian Processes framework previously introduced in [1] to identify the most important samples from the past for functional regularization. For evaluation authors report their average accuracy on Permuted MNIST, Split-MNIST, and CIFAR10-100 and achieve superior performance over EWC, DLP, SI, VCL-Coreset, and FRCL. <sep> Pros: <sep> (+): The paper is well-written, addressed the prior work quite well despite missing a few important work from the past (more on this later) <sep> (+): The paper is well motivated <sep> Cons that significantly affected my score and resulted in rejecting the paper are as follows: <sep> 1- lack of support for ""scalability"": <sep> Authors claim their method is scalable in several parts of the paper (abstract in line 7, Section 3 in the 1st paragraph, and Section 5 in Discussion). However, this claim is not supported in the experimental setting as the benchmark used are only toy datasets (Permuted MNIST, Split MNIST, and CIFAR10 followed by CIFAR100) where the maximum # of task considered is 10 and the maximum size of the datasets is 60K which is not convincing for ability to scale. There is also no time complexity provided. <sep> 2- Incremental novelty over the prior work (FRCL by Titsias et al 2019): <sep> This baseline is the closest prior work to this work which according to the experiments shown in Table 2 are slightly outperformed by the proposed method. (for example for P-MNIST the gain is 0.6%+-0.1) where there is a lack of complete discussion on how the two methods are different. Particularly I suggest that the authors elaborate more on their claimed differences stated on page 4, paragraph 5 such as ""tractability of the objective function only when we assume independence across tasks"". Do authors mean assuming clear task boundary between tasks? If so, have they considered a ""no-task"" or an ""overlapping"" task boundary in their experiment? Isn't it necessary to back up this if it is stated as a shortcoming of FRCL? Also, how are these methods differ in their computational expenses? <sep> 3- Lack of measuring forgetting: <sep> This is the most important drawback in the experimental setting. Authors indicate on page 3 ""Our goal in this paper is to design methods that can avoid such catastrophic forgetting."" and reiterate on this on other parts of the paper yet there is no forgetting evaluation to support this claim. Authors can simply report the initial performance of the model on each task so that readers can compare it with the reported accuracy after being done with all tasks. Having a method with high average accuracy does not necessarily mean it has minimum forgetting. You can use forgetting measurements such as Backward Transfer (BWT) introduced in [1] or forgetting ratio defined in [4] for this assessment. <sep> 4- Ambiguous claims about prior work: <sep> (a) On page 1, paragraph 3, when authors mention that methods such as GEM or iCaRL use random selection to pick previous samples, I think the line of follow-up work on these methods should be mentioned as well that have explored different techniques for sample selection and have provided benchmark comparisons (ex. [2,3]). In fact it would be beneficial if authors could compare the samples selected by their method versus other sampling techniques. <sep> (b) On page 1, paragraph 3, they mention some prior work such as GEM and iCaRL ""do not take uncertainty of the output into account"". While it is true, there have been methods proposed that use uncertainty of the output for parameter regularization [5]. It appears to be a parallel work to this but it's worth mentioning to prevent false claims. <sep> 5- Claim on the state of the art should be double-checked: <sep> Although the results shown for the experiments are superior to the provided baselines, there is an important baseline missing which has achieved higher performance than the reported ones. Also missed to be cited in the prior work list. Serra et al [4] proposed a method at ICML 2018 called HAT, which is a regularization technique with no memory usage that learns an attention mask over parameters and was shown to be very effective on small and long sequence of significantly different tasks. They do not use samples from previous task but yet achieved good average ACC as well as minimum forgetting ratio. Note that 5-Split MNIST is not reported in [4], but a recent work has reported HAT's performance on this dataset (https://openreview.net/forum?id=HklUCCVKDB) that achieves 99.59%. I recommend authors provide comparison of their own on the given benchmarks with the original HAT's implementation (https://github.com/joansj/hat) before claiming to be SoTA. In my opinion, it is not an issue if a novel method achieves a slightly lower performance to the sota because I think it still adds value and proposes a new direction. However, a false claim should not be stated. <sep> Less major (only to help, and not necessarily part of my decision assessment): <sep> 1- Providing upper bound? <sep> It is common to show an upper bound for any continual learning algorithm by showing joint training performance which is considered to be the maximum achievable performance. I also recommend showing the naive baseline of fine-tuning for the proposed method  which often can give insight to maximum forgetting ratio. <sep> 2- Forward transfer? <sep> Regularization techniques combined with memory might have an ability to perform zero-shot transfer or so called FWT. I recommend authors provide such metric to further support their method. <sep> 3- Hyper parameter tuning? <sep> It is also worth mentioning how the tuning process was performed. In continual learning we cannot assume that we have access to all tasks' data, hence authors might want to shed some light on this. <sep> References: <sep> [1] Khan, Mohammad Emtiyaz, et al. ""Approximate Inference Turns Deep Networks into Gaussian Processes."" arXiv preprint arXiv:1906.01930 (2019). <sep> [2] Chaudhry, Arslan, et al. ""Continual Learning with Tiny Episodic Memories."" arXiv preprint arXiv:1902.10486 (2019). (https://arxiv.org/abs/1902.10486) <sep> [3] Aljundi, Rahaf, et al. ""Gradient based sample selection for online continual learning."" arXiv preprint arXiv:1903.08671 (2019). (https://arxiv.org/abs/1903.08671) <sep> [4] Serrà, J., Surís, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557 <sep> [5] Ebrahimi, Sayna, et al. ""Uncertainty-guided Continual Learning with Bayesian Neural Networks."" arXiv preprint arXiv:1906.02425 (2019). <sep> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- <sep> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- <sep> POST-REBUTTAL Response from R1: <sep> Thank you for taking the time and replying to comments. Here are my responses to authors' replies: <sep> [Authors' response:] 1. Scalability: Our algorithm only adds a small computational overhead on top of Adam on a standard neural network. This is what we mean by scalable. The additional complexity scales cubically in M, the coreset size. This is due to the inversion of the kernel in fr_grad. Another overhead is the computation of Jacobian which is order PKM, where K is the dimensionality of the output and P is the number of network parameters. Both of these additional costs are small for small coreset sizes M. We will add these details to make these points clear in the paper. <sep> [Reviewer's response:] I still insist on the fact that simply explaining the overhead of a method is not a support for scalability claim versus showing the performance on a large scale dataset and comparing it with other CL methods that also have high scalability given the fact that authors only use MNIST and CIFAR datasets. <sep> [Authors' response:] 4. Prior work: We discuss other works in Section 1 (""two separate methods are usually used for regularisation and memory-building""), and we will expand upon this sentence, going into more detail, and also referencing iCaRL and other works (including [3]). Note that our method of choosing a memorable past follows directly from the theory in Section 3.1, and is achieved with a single forward-pass through the trained network (as mentioned in the paper). Other techniques for sample selection do not integrate so naturally with the framework, and are not as straightforward to understand or implement either. <sep> [Reviewer's response:] I disagree with authors on this because GEM, its faster version (A-GEM (Chaudhry et al. 2018)), and all other methods explored in the recent study which I mentioned in my review (Ref#2) use the single epoch protocol and are perfect match to be compared with this method but there is no memory-based baseline except for VCL with coreset and FRCL (only for MNIST variations) which makes it difficult to measure this method's capabilities (performance, memory size, and computational time) against methods which only require one epoch to be trained. <sep> Authors have provided FWT for their method as 6% which is unbelievably large for this metric (see GEM paper) and hence does not make sense to me. Please double check whether you computed this value right. <sep> While I accept the response for the remaining questions from authors but I am still concerned about the weak experiments and an issue brought up by R3 regarding lack of enough comparisons with FRCL on any other datasets besides split MNIST and P-MNIST. Also in  CIFAR experiment, what is the architecture used across the baselines? More importantly in results reported for VCL on CIFAR, it is not clear to me how authors obtained this results. Did they use a conv net? VCL was originally shown on MLPs only and it is one of the downside of this method that was never shown to be working in convolutional networks. Therefor, it is important to mention how they are obtained. This might explain the reason for the huge forgetting reported for VCL with coreset (−9.2 ± 1.8) as opposed to −2.3 ± 1.4 for EWC which is really strange as VCL even without coreset (on permuted mnist for example) is reported superior to EWC by a large margin (6%) in the original VCL paper. Overall I am concerned about the experimental setup and some of the reported results and hence intend to keep my score.","This work tackles the problem of catastrophic forgetting by using Gaussian processes to identify ""memory samples"" to regularize learning. <sep> Although the approach seems promising and well-motivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications. These justifications could come, for example, from further experiments, including ablation studies to gain insights. Making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature (as argued by reviewers)."
"I Summary <sep> The paper directly answers two sanity checks for saliency maps proposed by Adebayo et al (2018): 1. randomizing the weights of a model to prove that the input's resulting saliency map is different from a trained model' saliency map.  2. randomizing the inputs' labels to make the same proof. The authors propose a ""competitive version of saliency method"" which uses the saliency scores of every pixel for each labels and zero out: positive scored pixels that would not be maximal for the predicted class and negative scored pixels that are not minimal for the worst predicted label. <sep> Overall the method solves the aforementioned sanity checks, the authors claim it also generates more refined saliency maps. <sep> II Comments <sep> 1. Content <sep> The paper can be hard to read, due to multiple writing mistakes, abrupt phrasing, not well-articulated sentences. However, the idea is easy to understand and interesting but the contribution does not seem strong enough in its actual state. <sep> My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). Quantitative measures (ROAR & KAR, Hooker et al. 2018) or surveys to show that the newly obtained saliency maps are more refined or help to best localize regions of interest would be a big bonus. <sep> 2. Writing <sep> The paper comports numerous typos, those do not impact the score of the review except if the sentence is not understandable. Please see the following points as support to improve the clarity of the paper. <sep> - Abstract last sentence: ""Some theoretical justification is provided"" -> ""Some"" is vague and makes your claim less credible -> ""theoretical justifications are given in the last paragraph to support our method..."" <sep> - Intro paragraph 2 first sentence lack some words, l 2 product -> a product <sep> ""See paper XX et al"" -> ""As in XX et al, we can see that"" or ""As stated in XX et al"", ""See"" is too familiar, formalizing the phrasing gives more credibility to your work <sep> - Related work <sep> ""To give an example, does the map change a lot if we blank out or modify a portion of the image that humans find insignificant Kim et al. (2019)? "" This is not very well articulated, ""a lot"" is vague and a little familiar, ""significantly"" could be used here. Moreover, the citation is a little abrupt ""as we can see in XX"" would work better <sep> Little typo on etc.. <sep> ""fare best"" -> far better? The wording is still vague, it would help to add a quantitative measure that's -> that is <sep> - Section 3 <sep> ""This figure highlights"" -> which figure? (I think you just missed citing the fig here) <sep> - Section 4 <sep> First sentence: Why is it a good idea? The claim is a little abrupt and could be detailed a little more <sep> ""destroy the saliency map"" -> destroy is a very strong word <sep> ""These random variables are complicated."" -> This statement seems a little out of place and abrupt <sep> ""some constants"" -> ""constants"" (too vague otherwise as before) <sep> - Subsection 4.1 <sep> ""randomly sampling a few such methods"" I believe there is a typo? <sep> ""See figure 3"" is abrupt as a sentence itself ""as you can see in figure 3 bla bla"" <sep> Figure 4 The image is small and hard to see on printed paper (same for the images in the appendix, they could be stacked over multiple lines instead of just one horizontal row) <sep> Definition 2 punctuation at the end <sep> - Section 5 <sep> ""The available code for these maps is slow, and computing even gradient for all 1000 ImageNet labels can be rather slow."" What is the aim of this sentence? <sep> - subsection 5.3 <sep> lables -> labels <sep> III Conclusion <sep> The method itself is interesting, it would be interesting to see more qualitative results on the obtained saliency map itself: Does it produce more information? Is it more meaningful etc. Because as of now, it only seems to answer the two aforementioned sanity checks.  As for the writing, it is not always clear and can impede the understanding of the paper. I would be glad to change my review if those points are addressed.","This submission proposes a method to pass sanity checks on saliency methods for model explainability that were proposed in a prior work. <sep> Pros: <sep> -The method is simple, intuitive and does indeed pass the proposed checks. <sep> Cons: <sep> -The proposed method aims to pass the sanity checks, but is not well-evaluated on whether it provides good explanations. Passing these checks can be considered as necessary but not sufficient. <sep> -All reviewers agreed that the evaluation could be improved and most reviewers found the evaluation insufficient. <sep> Given the shortcomings, AC agrees with the majority recommendation to reject."
"This submission proposes a self-supervised segmentation method, that learns from single-object videos by finding the region where it can segment an object, remove the entire bounding box around it, inpaint it, then finally put the object back. The loss is a balance between a reconstruction error and a negative inpainting error (high error means object is probably present, due to the weak correlation with the background). <sep> My decision is Weak Accept. I like the method very much and think it's a clever and well-executed algorithm. The reason for being Weak is because the experimental evidence could be stronger, especially comparing with Croitoru et al. and Rhodin et al. The paper leaves some open problems, inviting future work to be built on top of it (i.e. leveraging time more and handling multiple objects better). I think it should be accepted to promote such future work. <sep> Method: <sep> The method is interesting and clever. Similar efforts have been made, such as Bielski & Favaro and Crawford & Pineau. However, key contributions relax some of the contrived requirements of these past methods (e.g. simple foreground translation over background; requirement of plain background). Thanks to the inpainter, the importance sampler, and avoidance of collapsing into trivial solutions, this paper is able to put together a method that works on moving cameras and fairly complex scene semantics (still limited to one object though). Actually getting this to converge with only two loss terms balanced against each other is impressive. Having certain heads of the network trained only on one of the two terms seems to be a key contribution.  <sep> Experimental results: <sep> The comparison with Rhodin et al. on H36M is characterized as ""slightly"" lower, but I would call 71% vs. 58% a significant difference. Of course, I understand that Rhodin et al. relies on the static background, so this is not a fair comparison. <sep> Ski-PTZ-Dataset should then offer a better comparison, but here the method struggles to compete with Croitoru et al. 2019, which has a 11-point higher F-measure. On Handheld190k, the dataset proposed in this paper, is where the method finally shines, but still only offers a 1-point F-measure improvement over Croitoru et al. That being said, considering how different the methods are, and how Croitoru et al. requires two-stage training, there are many benefits to this method. Croitoru et al. also relies on video to extract the object features, and this requirement is not as explicit in this work. Actually, that brings me to one question I had. The paper states that ""as long as videos or picture collections of a single object in front of the same scene are available."" I didn't quite understand why this must be trained on multiple images of the same scene. If the inpainter is general to any background scenery, couldn't it work on single-images as well? The conclusion even says you do not use temporal cues. <sep> Other: <sep> I don't think it's that meaningful to include precision/recall in the tables. It is also not that meaningful to point out that your method's precision is higher than that of Croitoru et al., when the F-measure is shy 11 points. The reason is because many points on the precision/recall could be constructed simply by applying for instance a gamma curve on the segmentation predictions. The high precision is clearly at the cost of a low recall, and another point on this tradeoff curve could be presented. This is why F-measure and average precision are much better.","This work proposes a self-supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte-Carlo based training strategy to explore object proposals. <sep> Reviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response. <sep> For these reasons, we recommend rejection."
"This paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). <sep> By mode connectivity, the paper refers to a specific instance where the final trained SGD solutions are connected by a linear interpolation path without loss in test accuracy. When networks trained with SGD reliably find solutions which can be linearly interpolated without loss in test accuracy despite different data ordering,  the paper refers to these networks as 'stable.' <sep> Matching sparse subnetworks refer to subnetworks within a full dense network that matches the test accuracy of the full network when trained in isolation. <sep> The paper introduces a novel improvement on the existing iterative magnitude pruning (IMP) technique that is able to find matching subnetworks even after initialization by rewinding the weights. This allowed the authors to find matching subnetworks for deeper networks and in cases where it could not be done without some intervention in learning schedule. <sep> The paper then finds a relationship that only when the subnetworks become stable, the subnetworks become matching subnetworks. <sep> ——— <sep> Although finding a connection between two seemingly distinct phenomena is novel and interesting, I would recommend a weak reject for the following two reasons: <sep> 1) The scope of the experiment is limited to a quite specific setting, <sep> 2) there are unsupported strong claims which need to be clarified. <sep> ——— <sep> 1) <sep> In the abstract the paper claims that sparse subnetworks are matching subnetworks only when they are stable, but the results are shown in a limited setting only at a very high sparsity. <sep> They tested stability on the highest sparsity level at which there was evidence that matching subnetworks existed, but how would the result generalize to other sparsity levels? <sep> With lower sparsity level (if weights are pruned less), is stability easier to achieve? <sep> The paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. <sep> As acknowledged in the limitations section, other relationships may exist between stability and matching subnetworks found by other pruning methods, or in different sparsity levels, <sep> which could be quite different from this paper's claim. <sep> In order to address this concern, I think the paper needs to show how the same relationship might generalize to different sparsity levels, <sep> or alternatively modify the claim (to what it actually shows) and highlight the significance of the connection between matching subnetworks and stability in this highly sparse subnetwork regime. <sep> 2) <sep> As addressed above, in the Abstract and Introduction, the paper's claims are very general about mode connectivity and sparsity, claiming in the sparse regime, ""a subnetwork is matching if and only if it is stable."" However, the experiments only show it is true in a limited setting, focusing on specific pruning method and at a specific sparsity level. <sep> Furthermore, the statement is contradicted in Footnote 7: ""for the sparsity levels we studied on VGG (low), the IMP subnetwork is stable but does not quite qualify as matching"" <sep> There are also a few other areas where there are unsupported claims. <sep> ""Namely, whenever IMP finds a matching subnetwork, test error does not increase when linearly interpolating between duplicates, meaning the subnetwork is stable."" <sep> -> Stability was tested only at one specific sparsity level, and it is not obvious it would be stable at all lower sparsity levels where IMP found matching subnetworks. <sep> ""This result extends Nagarajan & Kolter's observation about linear interpolation beyond MNIST to matching subnetworks found by IMP at initialization on our CIFAR10 networks"" <sep> -> Nagarajan & Kolter's observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order. <sep> Related to the first issue, I think some of these stronger claims can be modified to describe what the experiments actually show. <sep> The relationship found between stability and matching subnetworks in the high sparsity regime is a valuable insight that I believe should be conveyed correctly in this paper. <sep> ——— <sep> I also have some minor clarification question and suggestions for improvement. <sep> How was the sparsity level (30%) of Resnet-50 and Inception-v3 chosen in Table 1? (which was later used in Figure 5) <sep> — In Figure 3 and 5, the y-axis ""Stability(%)"" is unclear and not explained how this is computed. I first thought higher amount of stability(%) was good but it doesn't seem to be true. <sep> — The ordering of methods for plots could be more consistent. In some figures VGG-19 come first and then Resnet-20 while for others it was the other way around, which was confusing to read. (Also same for Resnet-50 and Inception-v3) <sep> — There are same lines in multiple graphs, but the labeling is inconsistent, potentially confusing readers: <sep> Figure 1: (Original Init, Standard) is the same as Figure 4: (Reset), <sep> and Figure 1: (Random Reinit, Standard) is the same as Figure 4: (Reset, Random Reinit)","This paper investigates theories related to networks sparsification, related to mode connectivity and the so-called lottery ticket hypothesis. The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance. The authors made substantial changes to the paper which are admirable and which bring it to borderline status."
"This paper proposes a new method for geometric matrix completion based on functional maps. The proposed algorithm is a simple shallow and fully linear network. Experimental results demonstrate the effectiveness of the proposed method. <sep> The proposed method is new and has been shown good empirical results. The paper also points out a new way to interpret matrix completion. On the other hand, the proposed method seems ad hoc and there is no clear evidence why it is better than other baselines except the empirical results. The paper also has some clearance issues, making it hard to understand. I vote for a weak reject of the paper at the current pace and would like to increase my score if the following questions can be clearly answered. <sep> 1. Why do we need to propose the algorithm? Is it because we have the functional maps technique motivated from shape correspondence, and we can see some connection of such technique with matric completion? If it is true, we surely can have a new algorithm based on such a new technique. But I can still not understand why the method work, at least, in an intuitive way. <sep> 2. What is the sample complexity of the proposed matrix completion algorithm? <sep> The introduction of the paper is poorly written. The first paragraph and the third one both contain some introduction to matric completion, which results in a lot of redundant information. The second paragraph and the fourth one are redundant in the same way since they both focus on geometric matrix completion. I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. However, I can only see from the end of the second paragraph (some simple models need to be proposed) and the fifth paragraph (""The inspiration of our paper"") some motivation information. The introduction part needs to be re-organized to provide more useful information about the paper rather than a literature review. <sep> There is some unclear/inaccurate/subjective statement in the introduction part. For example, ""Self-supervised learning"" needs a reference. Why geometric matrix completion generalizes the standard deep learning approaches is not clear. What does it mean by ""their design is … cumbersome and non-intuitive""? The shape correspondence is never explained until very later in the paper.  Also, there are some unclear issues besides the Introduction part. For example, what does it mean by ""the product graph""? All these issues need to be clarified before the paper can be accepted. <sep> --------------------------------------------------- <sep> Thank you for the detailed rebuttal. For Q1, it clearly explains how does the method work. However, it is still not clear why does the method work. I also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper? For Q2, it is interesting to see some theoretical results on the sample complexity, rather than an experimental one. The paper would also be much better if the clearance issues can be addressed. Even if I would not vote for an accept this time, I am looking forward to a revised version in the future.","This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state-of-the-art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself. <sep> Reviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works. Authors have clarified their explanations in the revisions and provided requested experiments (e.g., on the importance of the initialization size), however important reservations re. why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper. <sep> Therefore, we recommend rejection."
"This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference). <sep> A field-theory formalism was developed for Gaussian Processes (2001). Here it is thus extended to the NTK case. There are 3 important theoretical results which are both proven and backed by numerical confirmation.  These results, in particular the first, provide a very accurate prediction for the learning curve of some models.  The paper is well situated within this literature.  I am not very knowledgeable about NTK or even Gps, however I understand the challenges of understanding DNNs and I am familiar with field theory and renormalization group. <sep> Given the importance and quality of the results, and the overall quality and clarity of this (dense) paper, I recommend acceptation without hestiation. <sep> There are a couple of points however that could be improved, that would make the paper more useful for the community. <sep> Given the density of results in the paper, I would relax the length constraint, allowing up to 9 or 10 pages if possible, to add more explanations (not computations). <sep> I would like the paper to present more explicitly how the regression target labels g(x) are generated. Maybe it is said but I couldn't easily understand, for sure, how they are generated. <sep> Also, please explain early enough what is meant by uniform dataset (I understood it simply means the data x is drawn uniformly at random over a manifold, here this manifold is often the d-dimensional hypersphere). <sep> Claim II states that '...lead to clear relations between deep fully-connected networks and polynomial regression''. This is, I believe, supported by theoretical proof and numerical double-check, however it is not discussed enough for the abstract's promise to be fulflled 'a coherent picture emerges wherein fully-connected DNNs ...'. <sep> I think this point deserves a more ample discussion in section 7. <sep> More generally, the claims in the introduction or at the end of section 3 are stated rather explicitly, but very densely, and the careful reader can get the hypothesis of each result from the text. <sep> However for the sake of ease of read of less patient readers, I think it would be appropriate to have, somewhere, a more self-contained description of the results' list. This paper is technical and some readers will be interested of simply knowing the hypothesis made and type of results obtained. <sep> For instance, the sentence ''They [results] hold without any limitations on the dataset or the kernel and yield a variant of the EK result along with its sub-leading correction.'' is misleading: as stated in the previous sentence in the text, this is for the fixed-teacher learning curve, etc. <sep> Please try to explain a bit more the intuition behind renormalization / trimming terms q>r (r integer fixed, the higher the less approximated).  More specifically, it is not very clear to me how it can be interpreted in terms of how we look at the data. You mention (x.x')^r being negligible or not depending on r,d etc, but I wounder if there is some kind of simple 'geometrical' interpretation (is it a coarse graining of the data in angular space, the 'high energy' eigenvalues corresponding to the high frequency, high resolution distinction between very close angles ?).  On that point I am a bit lost and it's a pity because your results are strong and rely on few, rather simple/elegant assumptions (which call for some intuitive understanding). <sep> Could you explain intuitively, to the inexperienced reader, why the noiseless case is harder to deal with than the finite-noise one ? <sep> In appendix D, it is mentioned that you need averaging over about 10 samples to have a decent average. For a single realization of a N-sized training set, there is additional variation (Adding or subtracting to the error epsilon).  Given actual experiments are typically performed for a single realization of the data, I think this point should be mentioned in the main text more explicitly. Ideally, you could add error bars to the data, accounting for the dispersion inherent to a single-realization case. <sep> In early section 3, a short definition of a GP should be provided (there, or before). <sep> Around Eq. 2, you should specify the interpretation of P_0[f] <sep> After Eq. 3, 'where F is some functional of f.' I would add: '[where F is some functional of f,] for instance Eq. 2.' <sep> The derivation of eq 6 is not obvious. You do detail it in the appendix, but forgot to cite the appendix ! <sep> 'Notably none of these cited results apply in any straightforward manner in the NTK-regime.' : could you quickly explain why (no matched priors ? Noise ?) <sep> ''The d^-l scaling of eigenvalues'' : at this point, the variable 'l' had not been defined. <sep> ''notably cross-talk between features has been eliminated'' : has it been eliminated or does it simply become constant ? <sep> '3% accuracy' [relating to figure 1] <sep> I understand the idea but accuracy seems misleading. I would replace everywhere with something like 'relative mismatch'.  OR explain better why you call this accuracy: usually a high accuracy is preferred, and here you are proud with this very low imprecision of 3% <sep> ''Taking the leading order term one obtains the aforementioned EK result with N'' : maybe (just a suggestion here) you could recall it here, given it was in page 1 (and in-line). <sep> Appendix B:  could you explain why this difference increases with N ? I would have expected this kind of quantity to decrease with N. <sep> Appendix F: there are typos in the r.h.s. in the first line. <sep> \\sum_j f_j \\phi_j  (I think). <sep> Appendix G: 'noisy targets' : you mean fully random or Kernel + some degree of noise with variance sigma^2 ?  I think it's the first time you use this phrasing. <sep> Appendix G: you denote \\partial / \\partial \\alpha for the functional derivative. I would replace with \\delta to stress out it is a functional and not regular derivative. <sep> Beyond  appendix G.1 : I confess I didn't have time to read it. <sep> Despite the overall quality of the text, there are a number of wrong singular/plural matchings, which can easily be corrected. Here are some of them, with other typos as well: <sep> 'Furthermore since our aim was to predict what the DNNs would predict rather [THAN?] reach SOTA predictions' <sep> 'a factor of a factor of about 3.' <sep> as do for – > as we do for uniformally - > uniformly","This paper studies deep neural network (DNN) learning curves by leveraging recent connections of (wide) DNNs to kernel methods such as Gaussian processes. <sep> The bulk of the arguments contained in this paper are, thus, for the ""kernel regime"" rather than ""the problem of non-linearity in DNNs"", as one reviewer puts it. <sep> When it comes to scoring this paper, it has been controversial. However a lot of discussion has taken place. On the positive side, it seems that there is a lot of novel perspectives included in this paper. On the other hand, even after the revision, it seems that this paper is still very difficult to follow for non-physicists. <sep> Overall, it would be beneficial to perform a more careful revision of the paper such that it can be better appreciated by the targeted scientific community."
"This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. Specifically, the paper uses a combination of recently proposed techniques in graph representation learning (Graph Neural Network) and Graph Generation (GraphRNN [You et. al. 2018]). Given a sequence of graphs as input, a GNN (to obtain low-dimensional representations of the graphs in this sequence) and LSTM (to model the sequence of these representations)  based encoder is used to compute a vector representation of the topology of next graph in the sequence. The learned vector is then used as input to a GraphRNN decoder to generate a graph that would serve as a predicted next graph in the sequence. The proposed approach is validated with experiments on three synthetic datasets and one real-world dataset (Bitcoin is same dataset from two different resources with little difference in characteristics) and compared against random graph models. <sep> This paper should be rejected due to following reasons: <sep> (1) The authors do not justify/discuss the motivation and importance of the task and corresponding applications that would require to predict topology of complete graph in the next step. <sep> (2) The proposed techniques are an adhoc combination of existing techniques with major concerns (details below) but also with little novelty (if any) for achieving this combination. <sep> (3) The empirical efforts are very limited and does not provide enough evidence about the efficacy of the method, miss several details and does not serve as motivation for designing such a method in first place. Please note that negative results on cycle graphs has no role to play in this assessment. In fact, <sep> I appreciate the authors for reporting negative results as it provides a transparent insights into the effectiveness of model in different settings. <sep> Overall, the paper needs lot of work on all aspects - motivation, technique and experiments to make it fit for a conference publication. <sep> Major Concerns: <sep> (a) Motivation: The authors do not discuss or motivate the problem and why it is important to the community. The authors mention that many existing work on dynamic graphs focus on learning representations. This is the case because learned representations can then be used for various downstream applications and even future event predictions. When such methods can be used to do future predictions required for most applications, why does one need to predict the topology of complete next graph? The authors need to provide concrete justification for the problem they address, instances where such a task would be useful and discussion on other techniques that can do similar tasks but lack in aspects that such a method can capture. For instance, as a preliminary step, can the authors explain how solving this problem would be helpful to bitcoin? <sep> (b) Technical: The technical contributions of this paper lack novelty and has several flows: <sep> - Figure 1 seems to show that graph only grows in size. While the authors do provide an experiment with removal process, that experiment does not seem to perform well. So, does the method is only good to support growing graphs? <sep> - Authors mention that the edge and node attributes are considered to be fixed. However, if the number of nodes and edges change, X and L should also change in terms of dimensions and adding values for new nodes/edges. so why should it not be considered time-varying? <sep> - What was the motivation for using GRU for update function in Eq 3? Was simple MLP tried and not useful? Was GRU used to capture some long term dependencies in structure? If so, the authors must explain how it is useful for this task. <sep> - Why Set2Set was used for ReadOut function? This seems to be a particularly adhoc and odd choice. when sum did not work well, jump to Set2Set is not justified. Can the authors provide an explanation for the same? <sep> - The authors claim that the embedding h_G_T incorporates topological information -- I find this claim highly unsubstantiated and needs justification. For instance, can you provide some rigorous analysis to demonstrate that this is the case? At the least, can the authors use this vector and pass it through a graph decoder to recover the original graph? <sep> - What is novel in 3.2.3 as compared to You et. al.? Infact, it is hard to see any novelty in the entire combination. Was it challenging to achieve this combination? If so, what was the challenging part? It is not clear what the authors contributed to address such a challenge. Was the training challenging? If so, please explain. If not, why is this a novel approach? <sep> (c) Empirical: The empirical efforts are inadequate and raises more questions than answers. <sep> - Synthetic datasets are simple and more datasets should be used e.g. You et. al. 2018 to validate the performance. Only one real-world dataset from two different sources is used. It is hard to understand author's motivation in doing so. Why not use various graph datasets available in papers that learn representations (e.g. cited by authors themselves) What is special about bitcoin dataset that makes it suitable for this task? <sep> - Node/edge attributes are chosen in adhoc manner and it is unclear what role they perform. Do they help with prediction? If not, would it be useful to first show experiments without them? Or does this method absolutely need attributes? It is not clear why it is useful to set all attributes for edge as 1. <sep> - How was window size of 10 chosen? Why is the same window size good for all graphs? What impact does window size has on performance? <sep> - What is the motivation for using Graph kernel for similarity? The authors borrow the decoder from You et. al. 2018 which also provides a principled method to compare graphs using MMD based on statistics. Why not employ the same? <sep> - GraphRNN (You et. al.) and other generative models can learn over multiple graphs? Did the authors try to feed the sequence of graphs to such models and then try to generate a new graph to see if they can produce similar results? It is true that those generative models do not specifically model temporal sequence, but such an experiment would help to distinguish the efficacy of the proposed method. <sep> -The technique of using MLP for generating predictions using random graph models seem to be highly unfair for the baselines. Can you elaborate more as it is difficult to understand why one should handicap those models by using learned information instead of data information? <sep> - A rigorous discussion on insights explaining the results is required. The authors show high performance on Bitcoin dataset. However,  it is not clear what part is contributing to the performance. Similarly, authors should dig deep into the failure cases and provide justification of why such a method would fail in particular cases and propose alternatives. <sep> - Why was Graph size used as a statistic to report? Two graphs of same size can be entirely different and I do not see any merit in using such a metric. Again, something like MMD based metric may be useful. <sep> Improvements that would make future revision strong but has not impacted current assessment: <sep> Overall, the presentation of the paper is very unpolished. The authors are missing many important details as described above while spending a lot of time in describing (repeating) known techniques verbatim as original works. This can be removed and condensed into very short preliminary section. <sep> - Notations: The authors must use clear notations. For instance, on Page 2, L is used to  describe edge attributes but then it is replaced by E in Page 3. Also, both X and L are shown to have dimension d. Are edge and node attributes of same dimension? w is used for window-size of sequence used as input and also as neighbor node. When modeling evolution of graphs where a sequence is available over time points 0...T, it is not useful to use T to also represent time step of GNN propagation. Infact, authors should avoid using time steps to signify GNN iterations. <sep> - Empirical details: The details provided for datasets and experimental setup is inadequate. Why are the two Bitcoin datasets different from each other? What does Pos. Edges in Table 1 mean? What does  Mean and 90th percentile in Table 2 signify? Authors only talk about train-test split but then mention validation set for hyper-param tuning. How was this validation set obtained? Also, what hyper-params were tuned and what was sensitivity of those hyper-params? Authors use GNN and multiple RNN's, what was the model capacity used and how it impacted the performance? Figure 5 (c) what is a circle graph?","The paper proposes a combination graph neural networks and graph generation model (GraphRNN) to model the evolution of dynamic graphs for predicting the topology of next graph given a sequence of graphs. <sep> The problem to be addressed seems interesting, but lacks strong motivation. Therefore it would be better if some important applications can be specified. <sep> The proposed approach lacks novelty. It would be better to point out why the specific combination of two existing models is the most appropriate approach to address the task. <sep> The experiments are not fully convincing. Bigger and comprehensive datasets (with the right motivating applications) should be used to test the effectiveness of the proposed model. <sep> In short, the current version failed to raise excitement from readers due to the reasons above. A major revision addressing these issues could lead to a strong publication in the future."
"Summary: <sep> The paper proposes an autoencoder-based initialization for RNNs with linear memory. The proposed initialization is aimed at helping to maintain longer-term memory and instability during training such as  exploding gradients (due to linearity). <sep> Pros: <sep> 1. The paper is well written, the motivation and methods are clearly described. <sep> Cons. <sep> 1. The authors claimed the proposed method could help with exploding gradient  in training the linear memories. It would be helpful to include some experiments indicating that this was the case (for the baseline) and that this method does indeed help with this problem. <sep> 2. The experiments on the copy task only showed results for length upto 500, which almost all baseline models are able to solve. I am not too sure how the proposed initialization helps in this case. <sep> 3. TIMNIT is a relatively small speech recognition dataset. The task/ dataset does not require long-term memorization. It is nice to see that the initialization helps in this case. However, it is still a little how this experiment corresponds to the messsage that the authors are attempting to deliver at the end of the introduction. <sep> 4. In general, it seems that the experiments could be more carefully designed to reflect the contributions of the proposed method. Some suggestions for future edits are, more analysis on gradients, maybe more experiments on the stability of training such as gradients could help. <sep> Minor: <sep> 1. There are some confusions, on P2 ""we can construct a simple linear recurrent model which uses the autoencoder to encode the input sequences within a single vector"", I think the authors meant encode the input sequences into a sequence of vectors? Equation 1 and 2 suggest that there is a vector m^t per timestep (as oppose to having 1 for the entire sequence). <sep> 2. Although the copy task was used in ((Arjovsky et al., 2015), I believe the original task was proposed in the following paper and hence this citation should properly be the correct one to cite here, <sep> Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9(8): <sep> 1735–1780, 1997.","The paper explores an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) that is better than random initialization and the approach is tested on various MNIST and TIMIT data sets with positive results. <sep> Reviewer 3 raised concerns about the breadth of experiments and novelty. Reviewer 2 recognized that the model performs well on its MNIST baselines and had concerns about applicability to larger settings. Reviewer 1 acknowledges a very well written paper, but again raises concerns about the thoroughness of the experiments. The authors responded to all three reviewers, responding that the tasks were chosen to match existing work and that the approach is complementary to LSTMs to solve different tasks. Overall the reviewers did not re-adjust their ratings. <sep> There remains questions on scalability and generality, which makes the paper not yet ready for acceptance. We hope that the reviews support the authors further research."
"I thank the authors for their detailed response and appreciate their hard work in bringing us this paper. <sep> I think that my main point is that this work relies too much on the extra information/constraints in the synthetic env. E.g., 1. since the vocab size is small, thus the feature map could be designed 'equal to the vocabulary size' 2. The bag-of-words representation is effective but it is not the case for natural language. Although the authors kindly point me to some recent works on sim2real, I am still not convinced whether this proposed method could be transferred to real setups based on the referenced papers. <sep> However, it is a personal research taste that I always take real setup into considerations, because I have worked on both synthetic and real setup (on both lang and visn sides) for years and observed a large gap. My opinion is that methods of synthetic setups are not naturally convertible to the real ones. If AC/meta-reviewer considers the ability of vision-and-language interactions could be effectively studied through this setup with synthetic language and simulated-unrealistic images, I am OK with acceptance. I have downgraded my confidence scores (but kept my overall score) for this purpose. <sep> ----------------------------------------------------------------------------------- <sep> Pros: <sep> (1) The proposed model makes sense to me, which tries to have two attention layers to extract the information related to the questions. It seems to have the ability to deal with ""and""/""or"" logical relationships as well. <sep> (2) Fig. 4 is impressive. It is clear and well-designed. <sep> (3) The results in Table 2 are convincing. They show that both the proposed dual-attention method and multi-task learning would contribute to the performance. <sep> Cons: <sep> (1) It seems that the two main contributions are related to the language. Thus the synthetic language might not be proper to study. For example, in Eqn. 2, the first GA multiplies the BOW vector with the vision feature map, which could filter out unrelated instruction. This method could not be directly transferred to a real setup where natural language and natural images are involved. <sep> (2) The designed attention modules is lack of generalizability. It implements a two-step attention module, while the first step selects the related visual regions w.r.t the words and the second step gathers the information regarding these attended regions. However, it might not be aware of the spatial relationships and thus be limited to simple questions. For example, if the question is ""What is the object on top of the apple?"". To my understanding, the current module would not explicitly handle this one-hop spatial relationship. <sep> Comments: <sep> (1) According to Sec. 3, 70 instructions and 29 questions are involved in this task. Using GRU to encoder these questions seems to be redundant. A simple one-hot embedding for these instructions might already be enough to encode the information. <sep> (2) I am not sure why the visual attention map x_S could be used as the state of the module. <sep> (3) After Eqn. 3, the paper says that ""ReLU activations ... make all elements positive, ensuring ..."". I am confused about the intuition behind this argument because of the softmax activation. Softmax will projects 0 to 1. So the sum of the all-zero vector would still be non-zero after softmax. <sep> Typo: <sep> - In Sec. 4, X_{BoW} \\in \\{0, 1\\}^V. <sep> - In Sec. 4.1, ""this matrix is multiplied ..."" --> this tensor.",This paper offers a new approach to cross-modal embodied learning that aims to overcome limited vocabulary and other issues. Reviews are mixed. I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.
"This paper proposes an unsupervised hierarchical approach for learning graph representations. The proposed architecture is constructed by unrolling k-steps of a parametrized algebraic multigrid approach for minimizing the Wasserstein metric between the graph and its representation. The node distance (transport cost) used in the Wasserstein metric is also learned as an L2 distance between the embeddings of some graph embedding function. The approach is compared against 6 other state of the art approaches on 5 graph classification tasks, showing significant improvements 4 of them. <sep> The paper is reasonably well written, however, I think some of the explanations can be tightened further. Especially a lot on the background of AMG is not really that relevant, since the authors are not transferring technical results from AMG. Also, it seems like a better flow for presenting this argument might be to switch the order of sections 3.2.1 and 3.1.2. It looks like the main point is  that this architecture is trying to emulate iterative coarsened residual optimization of the Wasserstein metric between a graph and its representation. How the coarsening matrix is derived is more of a technical point (it looks like the results would be much more sensitive to a switch of metric than to a switch of parametrization for S). <sep> The empirical results are quite intriguing. There are, however, natural and important questions left unanswered. First and foremost, how does the amount of downsampling (compression) compare between methods. How many parameters do different methods require? It would also be good to see what the baseline performance would have been without any input compression as to understand how close these approaches are to the upper bound. <sep> Finally, I think the main issue of this paper, is left unresolved, namely, what is the point of not having supervision from the downstream task. As a user of graph representations trying to solve some problem, the only thing I would want from my representation is to capture some notion of sufficient statistics that are small enough to be efficient and allow me to solve my problem. I would not necessarily care about how well the learned representation resembles the original graph unless I believed that my downstream task was hard to evaluate  and that it was very smooth in the Wasserstein metric. I read the paper multiple times, trying to find any discussion on this, but it seems that the fact that an unsupervised representation is a good thing is taken for granted. A point could at least be made using the same representation for different tasks experimentally. Or, perhaps, literally doing an AMG-type unpacking of the downstream task itself as a comparison. This would shed light on the question of whether the iterated residuals or the choice of distance is what's driving the observed results.","This paper presents a differentiable coarsening approach for graph neural network. It provides the empirical demonstration that the proposed approach is competitive to existing pooling approaches. However, although the paper shows an interesting observation, there are remaining novelty as well as clarity concerns. In particular, the contribution of the proposed work over the graph kernels based on other forms of coarsening such as the early work of Shervashidze et al. as well as higher-order WL (pointed out by Reviewer1) remains unclear. We believe the paper currently lacks comparisons and discussions, and will benefit from additional rounds of future revisions."
"This paper researches the pooling operation, which is an important component in convolutional neural networks (CNN) for image classification. Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling). The key motivation is to make the pooling operation shift-equivalent and anti-aliasing. This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal. The F-pooling is then implemented with matrix multiplications and tested with recent convolutional neural networks for image classifiation on CIFAR-100 and a subset of ImageNet dataset. <sep> It is interesting to take the perspective from signal processing to give pooling operation in CNN a formal treatment. As indicated in the recent literature, enforcing shift-invariance does help to improve the performance of a CNN on classification accuracy and the robustness with respect to image shift. At the same time, this work can be further enhanced at the following aspects: <sep> 1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness. This will help to make this paper more self-contained. <sep> 2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x. Considering that the ultimate goal is classification, the information to be maximally preserved through each operation through the layers shall be the information that relates to the class label y. In light of this, some justification and explanation shall be provided for using this criterion for optimality. <sep> 3. The experimental study is weak. Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling. Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent. For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both. In Table 3, the F-pooling consistently shows inferior classification performance, although obtaining slightly higher consistency. This makes the advantage of F-pooling over the existing AA-pooling unclear.","This submission has been assessed by three reviewers and scored 3/6/1. The reviewers also have not increased their scores after the rebuttal. Two reviewers pointed to poor experimental results that do not fully support what is claimed in contributions and conclusions. Theoretical support for the reconstruction criterion was considered weak. Finally, the paer is pointed to be a special case of (Zhang 2019). While the paper has some merits, all reviewers had a large number of unresolved criticism. Thus, this paper cannot be accepted by *CONF*2020."
"The paper introduces a problem ""few-shot few-shot learning"" that aims to firstly transfer prior knowledge from one domain to the domain where the base training tasks reside, and then train a few-shot learning model on training tasks and apply it to novel test tasks. The two ""few-shot"" in the name refers to base training tasks and novel test tasks. In their algorithm, they use a model pre-trained on another dataset as the prior knowledge and fine-tune it on training tasks. During the test, they use the weighted average of samples' representations per class as the prototype of each class, where the weight is large for samples with more discriminative prediction over pre-trained domain's classes. Afterward, classification is reduced to finding the nearest neighbor among the class prototypes. Some experiments show that the pre-trained model can improve few-shot classification accuracy. <sep> My major concerns: <sep> 1) They try to propose a new problem, but their description shows that the problem is exactly the same as what most ""few-shot learning"" works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks. <sep> 2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification. The so-called ""prototypical classifier"" is actually the nearest neighbor classifier since no prototypical network structure is learned in the proposed method. <sep> 3) I would not call the weighted average as ""attention"" because it is not: the weight in attention is computed by a module with learnable parameters, while the weight in this paper is computed by the entropy of a pre-defined model's output prediction. <sep> 4) The ""spatial attention"" only makes sense when the pre-trained domain's classes can describe the main concepts appearing in the images of novel classes. This assumption is too strong since it requires class-level (rather than lower-level) relationships. <sep> 5) The base training is not necessary in the algorithm: it is used to only fine-tuning theta and W. As the author said in the beginning of Section 4.1, they can directly solve novel tasks based on the pre-trained model. <sep> 6) The experiments show that the pre-trained model is helpful in few-shot learning, which is a known fact. <sep> 7) The writing of this paper is very poor: a lot of typos and grammar errors, inconsistency between narratives, abuse of notations, wrong equation reference, even missing punctuations. They make the paper hard to understand. <sep> ------------- <sep> Update: <sep> Thanks for the authors' rebuttal! After reading their rebuttal, I still have main concerns about the novelty of the problem and the writing quality. The proposed method tends to be incremental.","This paper tackles the interesting problem of meta-learning in problem spaces where training ""tasks"" are scarce. Two criticisms that seems to shared across reviewers are that (i) it is debatable how ""novel"" the space of meta learning with ""few"" tasks is, especially since there aren't established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions. As an AC, I down-weight criticism (i) because I don't feel the paper has to be creating a new problem definition; it's acceptable to make advances within an existing space. However, criticism (ii) seems to remain. After conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer's opinions on this issue, and so the paper does not have enough support to justify acceptance. The paper certainly addresses interesting issues, and I look forward to seeing a revised/improved version at another venue."
"Summary & Pros <sep> - This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble. <sep> - The proposed scheme provides the same advantages of the ensemble in terms of uncertainty estimation and predictive performance, but it is computationally efficient compared to the ensemble. <sep> - This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 in terms of accuracy and uncertainty. <sep> Concerns #1: Novelty of the proposed method <sep> - The multi-head architectures have been widely used in various settings, especially multi-task learning. As the authors mentioned, it also used for online distillation [1]. Although its goal is different from this paper, just applying such multi-head architectures seems to be incremental. <sep> Concerns #2: Insufficient experiments <sep> - To evaluate OOD detection quality, ID/OOD datasets should be stated and various metrics (e.g., AUROC) should be measured like other literature, e.g., Table 2 in [2]. Such OOD detection quality is important to evaluate the quality of uncertainty estimation. <sep> - This paper provides experiments on only small-sized 10-class datasets, MNIST and CIFAR10. To verify the effectiveness of the proposed distillation method, other large-sized datasets should be tested, e.g., CIFAR-100, ImageNet. <sep> - There is no ablation study on the effect of the number of size of heads in Hydra. To achieve similar performance to the ensemble, how many heads are required? <sep> Concerns #3: Week efficiency <sep> - As reported in Table 5, in the case of CIFAR10, Hydra has 14x more parameters and 6x more FLOPs. Despite such a large number of parameters, the performance gain seems to be incremental. <sep> - A comparison with an ensemble with M=14 models should be tested because this ensemble has the same number of parameters compared to Hydra with M=50 heads. I think it might achieve good performance on the evaluation metrics. <sep> Concerns #4: Incremental improvements <sep> - Accuracy gain is too marginal even Hydra uses 14x more parameters. <sep> - ONE [1] might be a stronger baseline because ONE achieves 94% accuracy on CIFAR-10 using ResNet32 with only 2~3 heads while Hydra achieves only 90% even it uses 50 heads. Moreover, since ONE has multiple heads, uncertainty estimation is also available. So it should be compared with the proposed method. <sep> - In OOD detection tasks, Hydra underperforms Prior Networks on 5 of 8 datasets (note that PN (2.60) is better than Hydra (3.11) in the case of MNIST (test)). To overcome this gap, the proposed method requires more parameters. <sep> [1] Zhu, Xiatian, and Shaogang Gong. ""Knowledge Distillation by On-the-Fly Native Ensemble."" Advances in Neural Information Processing Systems. 2018. <sep> [2] Andrey Malinin and Mark Gales. ""Predictive Uncertainty Estimation via Prior Networks."" Advances in Neural Information Processing Systems. 2018.","This work introduces a simple and effective method for ensemble distillation. The method is a simple extension of earlier ""prior networks"": it differs in which, instead of fitting a single network to mimic a distribution produced by the ensemble, this work suggests to use multi-head (one head per individual ensemble member) in order to better capture the ensemble diversity. This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 (they added CIFAR-100 in the revised version) in terms of accuracy and uncertainty. <sep> While the method is effective and the experiments on CIFAR-100 (a harder task) improved the paper, the reviewers (myself included) pointed out in the discussion phase that the limited novelty remains a major weakness. The proposed method seems like a trivial extension of the prior work, and does not provide much additional insight. To remedy this shortcoming, I suggest the authors provide extensive experimental supports including various datasets and ablation studies. <sep> Another concern mentioned in the discussion is the fact that these small improvements are in spite of the fact that the proposed method ends up using many more parameters than the baselines. Including and comparing different model sizes in a full fledged experimental evaluation would better convey the trade-offs of the proposed approach."
"The method proposes a method for continual learning. The method is an extension of recent work, called orthogonal weights modification (OWM) [Zheng,2019]. This method aims to find gradient updates which are perpendicular to the input vectors of previous tasks (resulting in less forgetting). However, the authors argue, that the learning of new tasks is happening in the solution space of the previous tasks, which might severely limit the ability to adapt to new tasks. The authors propose a 'principal component'-based solution to this problem. The method is considering the 'task continual learning' scenario (also known as task-aware) which means that the task label is given at inference time. <sep> Conclusion: <sep> 1. The paper is not well-positioned in related works. I think the work is more related to works with 'parameter isolation methods' such as Piggyback, Packnet, HAT. These methods reserve part of the capacity of the network for tasks. I think the authors should relate their work with these methods, and provide an argument of the problem with these previous methods, which is addressed by their approach. I can see that rather than freezing weights (PackNet) or features (HAT) , the method freezes linear combinations of features. But it is for me not directly clear that that is desirable. In HAT the backpropagated vector is projected on the mask vector which coincides with the neurons (activations). <sep> 2. The experimental verification of the paper is too weak, and only comparison to EWC and OWM (not well known) are provided. At least a comparison with the more related works PackNet and HAT should be included. For more recent method for task-aware CL see also 'Continual learning: A comparative study on how to defy forgetting in classification tasks'. Also results seem bad. For example on CIFAR10, 5 tasks in TCL setting is two-class problem per task; I would expect better results. <sep> 3. The authors claim that OWM is effective if tasks are similar, but not when dissimilar. And the proposed PCP solves this problem. However, all experiments are on similar tasks, and no cross domain tasks are considered, e.g. going from MNIST (task1) to EMNIST-26 (task2) etc. This would empirically support the claim. Also, the authors expect the difference between PCP and OWM to be even larger then. <sep> 4. Some more analysis of the success of PCA in representing the distribution would be appreciated, e.g. the percentage of total energy which is captured (sum of selected eigenvalues divided by sum of all eigenvalues). Such an analysis of P_l^k as a function of the tasks (and for several layers) would be interesting to see, for example for EMNIST-47(10 tasks). <sep> 5. Novelty with respect to OWM is rather small. <sep> 6. The authors should mention that the method is pretrained on ImageNet in section 4.3. Given these datasets, I think it makes more sense to train from scratch and I would like to see those results. <sep> Minor remarks: <sep> - I wonder if you use OWM or PCP you discard the possibility of positive backward transfer. Maybe the authors could comment on that. <sep> - The authors write that 'TCL setting the classification results are usually better than those of the CCL' is that not per definition true ? Anything correctly classified under CCL is correctly classified under TCL but not the other way around.","There is no author response for this paper. The paper addresses the issue of catastrophic forgetting in continual learning. The authors build upon the idea from [Zheng,2019], namely finding gradient updates in the space perpendicular to the input vectors of the previous tasks resulting in less forgetting, and propose an improvement, namely to use principal component analysis to enable learning new tasks without restricting their solution space as in [Zheng,2019]. <sep> While the reviewers acknowledge the importance to study continual learning, they raised several concerns that were viewed by the AC as critical issues: (1) convincing experimental evaluation -- an analysis that clearly shows how and when the proposed method can solve the issue that [Zheng,2019] faces with (task similarity/dissimilarity scenario) would substantially strengthen the evaluation and would allow to assess the scope and contributions of this work; also see R3's detailed concerns and questions on empirical evaluation, R2's suggestion to follow the standard protocols, and R1's suggestion to use PackNet and HAT as baselines for comparison; (2) lack of presentation clarity -- see R2's concerns how to improve, and R1's suggestions on how to better position the paper. <sep> A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarifications, more empirical studies and polish to achieve the desired goal."
"This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN). An immediate related work is LayerNet, where a deep neural net is created by replicating the same layer. Here, this paper extends replication down to the neuron level. <sep> I am leaning towards rejecting this paper because the experimental setup is not well justified and a few important details are missing before conclusions can be drawn. I would like to ask a few clarification questions. Depending on the authors' answers, I might be willing to adjust my rating. <sep> (1) Is there missing a delta in the first half of line 6 in Algorithm 1? <sep> (2) Throughout the experiments, for the same hyperparameter (e.g. Table 4 in A.2) do you run Algorithm 1 more than once and select the best sample architecture? If the answer is yes, summarizing all masks as one parameter will not be reasonable. Given a yes answer, I would also like to ask if the same number of samples have been considered for FC (for the same hyperparameter). <sep> (3) Is there any intuition behind why FC does a much worse job of fitting curves than ACN with much less parameters? This refers to Fig. 2, if we compare FC with 41 parameters to ACN with 18 parameters. I am confused because MSE on sampled points often goes down when we increase the number of parameters for the application of curve fitting. <sep> (4) Convolution can be thought of as a special case of ACN. ConvNet is the default architecture for working on image datasets. Since MNIST and CIFAR are considered, why not also compare to ConvNet? <sep> (5) The claims that ""ACNs achieve compression rates of up to three orders of magnitudes compared to fine-tuned fully-connected neural networks with only a fractional deterioration of classification accuracy"" is quite misleading. Given fully-connected neural networks achieve up to 528 times with also a fractional deterioration (Sec. 4.3), by presumably having a shallower architecture.","This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks. The idea looks simple and effective. However, the reason why it works is not well explained. The experiments are not sufficient enough to convince the reviewers."
"This work introduces GQ-Net, a novel technique that trains quantization friendly networks that facilitate for 4 bit weights and activations. This is achieved by introducing a loss function that consists of a linear combination of two components: one that aims to minimize the error of the network on the training labels of the dataset and one that aims to minimize the discrepancy of the model output with respect to the output of the model when the weights and activations are quantized. The authors argue that this has the effect of ""guiding"" the optimization procedure in finding networks that can be quantized without loss of performance. For the discrepancy metric the authors use the KL divergence from the predictive distribution of the floating point model to the one of the quantized model. The authors then propose several extra techniques that boost the performance of their method: 1. scheduling the weighting coefficients of the two loss terms (something which reminisces iterative pruning methods), 2. stopping the gradient of the floating point model w.r.t. the second loss term, 3. learning the parameters of the uniform quantizer, 4. alternating optimization between the weights and the parameters of the quantizers and 5. using separate batch normalization statistics for the floating point and quantized models. The authors then evaluate their method on Imagenet classification using ResNet-18 and Mobilenet v1 / v2, while also performing an ablation study about the extra tricks that they propose. <sep> This work is well written and in general conveys the main idea in an effective manner. Quantization friendly neural networks in an important subject in order to make deep learning tractable for real world applications. The idea seems on a high level to be interesting and simple; train floating point models that can fit the data well while also encouraging them to be robust to quantization by enforcing the predictive distributions of the fixed and floating point models to be similar in the KL-divergence sense. Nevertheless, I do have some comments that would hopefully help in improving this work: <sep> - It does seem that GQ-Nets need extra tricks in order to perform well, and those tricks come with their own set of hyperparameters that need to be tuned. For example, at section 4.3 you mention that the top-1 accuracy of vanilla GQ-Nets is 60.95, which is lower than the RelaxedQuant baseline (that has 61.52). This raises the question whether the boost in performance is due to the several additional steps employed (which in general can be applied to other quantization techniques as well), and not due to the main idea itself. <sep> - Do you employ the straight-through estimator (STE) for the weights in the L_q objective? In the second paragraph of the second page you argue that due to the biased gradients of STE the performance is in general reduced, so I was wondering whether STE posed an issue there or whether you used an alternative estimator. <sep> - How is batch normalization handled? Do you absorb the scale and shifts in the weights / biases before you perform quantization or do you quantize the weights and then apply the BN scale and shift in full precision? <sep> - How do you ensure and ub > lb when you learn the quantizer? In general learning the quantizer can be also done with alternative techniques (e.g. simply learning the scale and offset) so I was wondering whether you noticed benefits from using the ub, lb parametrization compared to others. <sep> - Do you show the pre-quantization distributions at Figure 2b? In the caption you mention quantized but the resolution seems to be higher than the 16 values you should get with 4 bits. Furthermore, it should be noted that the discrepancy in BN in quantized models was, as far as I am aware, firstly noticed at [1] (and subsequently at RelaxedQuant) and both of these methods simply re-estimated the moving averages during the inference time. <sep> Overall, I am on the fence about this work and tend to reject. Having said that, I am of course willing to revise my score after the discussions with the authors / other reviewers. <sep> [1] Probabilistic Binary Neural Networks, Jorn W.T. Peters, Max Welling","The paper propose a new quantization-friendly network training algorithm called GQ (or DQ) net. The paper is well-written, and the proposed idea is interesting. Empirical results are also good. However, the major performance improvement comes from the combination of different incremental improvements. Some of these additional steps do seem orthogonal to the proposed idea. Also, it is not clear how robust the method is to the various hyperparameters / schedules. For example, it seems that some of the suggested training options are conflicting each other. More in-depth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful."
"The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors show that using CPC for representation learning allows to achieve better results than other self-supervised methods. Moreover, CPC is shown to be useful for semi-supervised learning (on par with SOTA method), and transfer learning. All results are very impressive and is in-line with current trends of using a linear classifier on top of a deep feature extractor (e.g., Nalisnick et al., ""Hybrid Models with Deep and Invertible Features""). The paper is rather well written and the results are convincing. However, The whole idea of the paper is based on the original paper: <sep> * Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" arXiv preprint arXiv:1807.03748 (2018). <sep> Technically speaking, the paper is outstanding, but it lacks novelty in terms of new ideas. I highly appreciate new results and new architectures, but it is not enough for a full conference paper. <sep> Remarks <sep> - In Section 2.1, the problem statement for Contrastive Predictive Coding (CPC) is unclear. For instance, the authors explain CPC by mentioning about masked convolutional layers that is unnecessary at this point. I understand that from engineering perspective it is crucial information, but it does not help to understand CPC. As a result, without knowing the original paper on CPC, Section 2.1 is hard to follow. <sep> - The paper can be treated as an uptaded version of the original CPC paper. I really appreciate all new results and implementation of the idea. The paper is well written and it is technically correct. However, I do not find much novelty compared to the original paper. This would be a perfect workshop contribution, but I am afraid that it is not enough for a full paper. <sep> ==== AFTER REBUTTAL ==== <sep> I would like to thank the reviewers for their rebuttal. It was not my intention to dismiss your effort in providing new technical results. Please forgive me if you read it in this way. My point is that the paper presents exactly the same idea as the original paper of CPC, but with new, very interesting results. However, I doubt if this is enough for a full conference paper. This point is debatable and I would be happy to further discuss it with other reviewers and the AC. At this point, I keep my original score, but of I am open for a discussion.","The paper tackles the key question of achieving high prediction performances with few labels. The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al. 2018). The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch-based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few-label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network. <sep> This paper generated a hot discussion. Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at *CONF*. Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth. <sep> The area chair thinks that the paper raises profound questions (e.g., what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors'. <sep> The authors might also want to leave the claim about the generality of the CPC++ principles (e.g., regarding audio) for further work - or to bring additional evidence backing up this claim. <sep> In conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components."
"This paper presents a technique for encoding the high level ""style"" of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global ""style embedding"".  Additionally, the Music Transformer model is also conditioned on a combination of both ""style"" and ""melody"" embeddings to try and generate music ""similar"" to the conditioning melody but in the style of the performance embedding. <sep> Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments. <sep> In terms of technical presentation, I think the authors should clarify how the model is trained. It took me a couple of passes and reading the Music Transformer paper to realise that in the melody and performance conditioning case, the aim is to generate the full score (melody and accompaniment) while conditioning on the performance style and melody (which is represented using a different vocabulary). This point can be easily clarified in Figure 1, by adding the input to the encoder as input to the decoder for computing the loss. Although I understand the need for anonymity and constraints while referring to unreleased datasets, it would still be useful for the reader/reviewer to have some details of how the melody was extracted and represented. ""An internal procedure"" is quite mysterious. <sep> Measuring music similarity is a difficult problem and the topic has been the subject of at least 2 decades of research. I find the description of the ""performance feature"" to be lacking in necessary background and detail. Firstly, I am not sure what the final dimensionality of the feature vector is. Is it real valued? The authors mention (Yang and Lerch, 2018) but use a totally different set of attributes compared to that paper. I also don't see the connection between this proposed feature vector and using the IMQ kernel for measuring similarity. This connection is not motivated adequately and after reading (Jitkrittum et. al. 2019) its not obvious to me why this is the most appropriate metric. Finally, it would be useful if the authors comment on existing methods for measuring music similarity in symbolic music and how their proposed feature fits into existing work. A lot of work has been published on this topic, most recently in the context of Query-by-Humming [1]. <sep> Minor Comments <sep> 1. ""...which typically incorporate global conditioning as part pf the training procedure"" Could you elaborate on this point? Is the global conditioning the samples from the noise distribution? <sep> 2. Figure 1 should be clarified or another figure should be added to show how the melody conditioning works. Maybe a comment on the melody vocabulary or a reference would also be useful. <sep> 3. The MAESTRO dataset is described in terms of the number of performances while the internal dataset is described in terms of the number of hours of audio. Its not possible for the reader to get a sense of the relative sizes of the 2 datasets and how the results should be interpreted. <sep> 4. There should be more background and description in Section 4. Where does the performance feature come from? Why use this feature compared to existing techniques for measuring similarity between symbolic music pieces? Is it computational efficiency? Why not compare the conditioning melody with the generated performance similar to query-by-humming? Where does the IMQ kernel come from? What is the size of the feature vector? <sep> 5. In section 5.2, a conditioning sample, a generated sequence and an unconditional sample are used to compute the similarity measure. Which terms do these correspond to in the MMD-like term (x,y,y')? <sep> 6. I like the experiments performed in Section 5.3 with the linear combination of 2 performance embeddings. <sep> [1] A Survey of Query-By-Humming Similarity Methods: athitsos/publications/kotsifakos petra.pdf"" target=""_blank"" rel=""nofollow"">http://vlm1.uta.edu/ athitsos/publications/kotsifakos petra.pdf","Main content: <sep> Blind review #3 summarizes it well: <sep> This paper presents a technique for encoding the high level ""style"" of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global ""style embedding"". Additionally, the Music Transformer model is also conditioned on a combination of both ""style"" and ""melody"" embeddings to try and generate music ""similar"" to the conditioning melody but in the style of the performance embedding. <sep> -- <sep> Discussion: <sep> The reviewers questioned the novelty. Blind review #2 wrote: ""Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments."" <sep> However, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote ""We emphasize that our goal is to provide users with more fine-grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies)."" <sep> -- <sep> Recommendation and justification: <sep> This paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time. As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches."
"In this paper, the authors propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. They use enhanced loss scale, quantization and stochastic rounding techniques to balance the numerical accuracy and computational efficiency. Finally, they get a slightly better validation accuracy compared to full precision baseline. Overall, this paper focuses on engineering techniques about mixed precision training with 8-bit floating point, and state-of-the-art accuracy across multiple data sets shows the effectiveness of their work. <sep> However, there are some problems to be clarified. <sep> 1. The authors apply several techniques to improve the precision for training with 8-bit floating point, but they do not show the gain for each individual. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique? This should be clearly presented and more experimental comparison is expected. <sep> 2. The paper should present a bit more background knowledge and discussion on the adopted techniques. For instance, why the stochastic rounding method proposed in this article by adding a random value in probability can regulate quantization noise in the gradients? And why Resnet-50 demands a large scaling factor? <sep> 3. On Table 3, in comparison with Wang et al. (2018), the authors use layers with FP32 (not FP16 in Wang). Thus, it is hard to say the improvement comes from the proposed 8-bit training. This should be clarified. <sep> 4. How to set the hyper-parameters, such as scale, thresholds and so on, is not clear in the paper. There are no guidelines for readers to use these techniques. <sep> 5. The authors did not give a clear description of the implement for the enhanced loss scaling. They apply different loss scaling methods for different networks. This should be explained in detail. <sep> 6. In the experiment, for a single model, some layers are 8-bit, some layers are 32-bit and some layers are 16-bit.  Is the 8-bit training only applicable for a part of the model?  How do we know which layer is suitable for 8-bit training?","This paper propose a method to train DNNs using 8-bit floating point numbers, by using an enhanced loss scaling method and stochastic rounding method. However, the proposed method lacks novel and both the paper presentation and experiments need to be improved throughout."
"The work is based on the recent paper 'Proximal Backpropagation', Frerix et al., *CONF* 2018, which views error back propagation as block coordinate gradient descent steps on a penalty functional comprised of activations and parameters. <sep> Instead of taking proximal steps on the linear layers as in (Frerix et al., 2018), the authors also pull the non-linearity \\sigma into the proximal steps. Another interesting deviation is the idea to consider the newly updated weights {W_i}^{k+1} when updating the activations F_i^{k+1} in the backward pass. <sep> While potentially offering a faster convergence with respect to epochs, the nonlinear updates have two major drawbacks: <sep> 1) While there are preliminary theoretical results (fixed points of the method are critical points), it remains unclear whether the computed update is still a descent direction on the original energy.  While not crucial, such a result would be reassuring and might give further insights into the method. <sep> 2) Each update requires the solution of a nonconvex, nonlinear least squares problem which is prohibitively expensive to solve. Note that such nonlinear least squares updates are already proposed in (Carreira-Perpinan & Wang, 2014). When using ReLU activation, the non smoothness might be an issue for standard nonlinear least squares solvers such as Levenberg-Marquadt. <sep> Furthermore, the numerical results are unfortunately a bit discouraging. The experiments evaluate toy models on toy datasets and even there only a minor improvement with respect to epochs over SGD and Prox-BP is shown. Furthermore, the plots only consider epochs and not the running time. Due to the non-linear least squares problem, I assume that each epoch for the proposed method is way more costly. Therefore I consider the experimental evaluation too preliminary. A proper evaluation would require an implementation as an optimizer in state-of-the-art deep learning frameworks and a comparison with respect to running time to standard optimizers such as SGD with momentum or Adam on the GPU. <sep> The reported performances for MNIST are surprisingly poor. Note that vanilla SGD with momentum reaches ~98.6% test set performance on such an architecture, while the overall highest reported accuracy in this paper is 98.0%. This might be due to momentum, and it would be interesting whether the proposed method could be combined with momentum or other optimizers such as Adam as in (Frerix et al. 2018). <sep> Overall, I don't see this a practical algorithm for training deep networks and there are few theoretical results. Therefore, I cannot recommend acceptance at this stage. <sep> To improve the paper, I would like to see an implementation of the method on the GPU in a recent deep learning framework and an evaluation on larger models / datasets. But I am doubtful this will reach competitive performance to standard optimizers. Also, it would be interesting to see how the precision in the inner nonlinear conjugate gradient solver effects outer convergence. It might be that the subproblem does not have to be solved with very high accuracy. <sep> Minor comments: <sep> * Missing citation: 'Difference Target Propagation', https://arxiv.org/abs/1412.7525, studies a similar type of algorithm.","The reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited. The authors do not react to the reviewers' comments."
"The paper proposes a differentiable Bayesian neural network. Traditional BNN can model the model uncertainty and data uncertainty via adding a prior to the weight and assuming a Gaussian likelihood for the output y. However, it's slow in practice since evaluating the loss function of BNN involves multiple runs over the entire network. Also when the input data is non-stationary, the output function can not be differentiated with respect to the input data. The paper proposes to use an online code vector histogram (OCH) method attached to the input and output of a classical DNN. Input OCH captures the distribution of both input data and network weights, and the output OCH captures the distribution of the predictions. Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. <sep> I think the idea is interesting and novel. It explores a different way of modeling distributions with DNN. Instead of adding priors, DBNN relies on histograms, which is usually used to describe distributions for discrete observed input data. So the paper is well-motivated. <sep> 1. The paper needs more literature review in the area of data streaming. Papers, such as [1], have proposed to use a vector quantization process that can be applied online to a stream of inputs. This paper introduces the vector quantization but doesn't mention the use of it in streaming data in related work, which kind of blurs the contribution a bit. Moreover, it would be helpful for readers to learn about useful techniques for streaming data from this paper. <sep> [1] Hervé Frezza-Buet. Online computing of non-stationary distributions velocity fields by an accuracy controlled growing neural gas <sep> 2. I think the paper might need a bit more explanation about codevector, since it's not a very well-acknowledged concept in this field. The main issue for me to understand it is how to get these codevectors. When DBNN deals with streaming data and starts from no input, is the set of codevector empty at the beginning? The input data points are accumulated as codevectors? I hope the authors could clarify this process a bit more. <sep> 3. Given the insufficient understanding of codevector, figure 2 is a bit hard to read. 1) (a)-(d) are figures for x0 at t=0, which is not time-variant. 2) what are these codevectors picked. 3) It seems that the codevectors are out of the regime of the distribution of y. But according to algorithm 2, y_*<-T(c_*), would that be a problem? I think (a)-(d) are informative but not straightforward to read. The authors need to put more text to explain these figures, since this simulated example can help readers to understand what is codevector and how it helps for uncertainty estimation. <sep> Overall I think the paper is well-written. The idea is novel and practical in the scenario of DNN. I would vote for accept.","The main contribution is a Bayesian neural net algorithm which saves computation at test time using a vector quantization approximation. The reviewers are on the fence about the paper. I find the exposition somewhat hard to follow. In terms of evaluation, they demonstrate similar performance to various BNN architectures which require Monte Carlo sampling. But there have been lots of BNN algorithms that don't require sampling (e.g. PBP, Bayesian dark knowledge, MacKay's delta approximation), so it seems important to compare to these. I think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as *CONF*."
"The paper suggests a new method for knowledge transfer from teacher neural network to student: student-teacher collaboration (STC). The idea is that teacher not only provides student with the correct answer or correct intermediate representation, but also instructs student network on how to get the right answer. The paper suggests to merge the front part of the student network and back part of the teacher network into the collaboration network. In this way, the weights in the student subnetwork are learned from the loss backpropagated from the teacher subnetwork. The target labels for collaboration network are outputs of teacher network. The method is adapted for different tasks, classification and bounding box regression are presented in the experiments. It outperforms previous methods on various datasets. Furthermore, the method shows good results when integrated with traditional knowledge distillation. <sep> Overall, the paper is a significant algorithmic contribution. The related work section provides thorough review of different methods to decrease computational costs, including not only knowledge distillation, but also pruning, compressing and decomposition approaches. The idea is elegant and, to the best of my knowledge, has never been suggested in other works. Considering the theoretical part, it is clearly shown how the gradient signal from the teacher sub-network guides the student network on which part of the weights should be paid attention on. All derivations are easy to follow. The paper also considers how the suggested idea is aimed to solve the problems of previous knowledge transfer methods. The experimental section is consistent and clearly shows the advantage of the suggested method. Teacher and student networks used are different sizes of ResNet, Wide ResNet and VGG. The paper presents classification experiments on CIFAR-10, CIFAR-100, ImageNet and object detection experiment on PASCAL VOC 2007 dataset. STC outperforms previous methods, both with KD integration and without. The performance is always better than pure student training (which was not always the case for previous methods) and sometimes the results are even better than teacher performance. Finally, the choice of teacher output as target over soft target and ground truth, which was previously motivated in the theoretical section, is shown to be superior in the experiment. <sep> Possible improvement of the paper is the instruction on how to choose the intermediate layer from where to teach the representation, i.e. where the student sub-network ends and teacher sub-network begins. For object detection experiment the choice of the border is naturally defined by the architecture of the network in Faster-RCNN approach. Could the choice be different? May be somewhere inside the BackBone part of the networks? For classification, it could be interesting to study how this choice influences the results. However, this question didn't affect my score of the paper, and, as far as I know, it is also not considered in the previous works on knowledge distillation. <sep> Minor comments <sep> 1. In the context of accelerating the models using decomposition, Lebedev et al., *CONF* 2015 could be cited. <sep> 2. Page 2: difference tasks -> different tasks <sep> 3. Page 2 the first bullet point: additionally utilizes -> additionally utilizing/which additionally utilizes <sep> 4. Page 2 the third bullet point: brings good generalizability -> which brings good generalizability <sep> 5. Page 5: ""training strategy is more accelerate than"" – odd phrase <sep> 6. Page 6: while KT has conflicts with KD in some cases -> while FT has conflicts with KD in some cases.","This paper has been assessed by three reviewers scoring it as follows: 6, 3, 8. The submission however attracted some criticism post-rebuttal from the reviewers e.g., why concatenating teacher to student is better than the use l2 loss or how the choice of transf. layers has been made (ad-hoc). Similarly, other major criticism includes lack of proper referencing to parts of work that have been in fact developed earlier in preceding papers. On balance, this paper falls short of the expectations of *CONF* 2020, thus it cannot be accepted at this time. The authors are encouraged to work through major comments and resolve them for a future submission."
"The authors introduce a method (B2B) for disentangling effects of correlated predictors in the context of high dimensional outcomes. Their model assumes the outcomes are constructed by a linear transformation of a set of true causes plus measurement noise. Specifically, they fit the model Y=(XE+N)F, where X are the predictors, E is a binary matrix indicating the true causes, N is a noise term, and F is a mixing term. They provide a closed form solution the model fit based on a pair of l2-regularized regressions. They simulate from the given model and provide comparisons against least-squares regression, canonical correlation analysis, and partial least squares. They also apply the method to brain imaging data containing 102 individuals reading 120 sentences plus scrambled sentences, with the goal of inferring which features of the words have an effect on imaging results. <sep> This paper appears to be technically sound, but it should be rejected based on 1) the relatively limited applicability of the model and 2) a lack of thorough experimentation indicating that this is an appropriate method under more general circumstances. It is odd that the model assumes the outcomes are measured without error. Instead, it is assumed that the causes are measured with error, and mixed via F. A more appropriate model may be: Y=(XE+N)F+M, where an additional noise term M allows for Y to be measured imprecisely. Viewed in this light, the model starts to look a lot like canonical correlation analysis. Consider a model Y=ZF+M, X=ZG+N, if dim(Z) = dim(X) and G is invertible, this can be re-written as Y=(X inv(G)-N)A+M, and we arrive at a similar model with specific restrictions about the structure of inv(G) (E will in general not be invertible, so they are not the same). It is particularly odd, then, that the authors do no comparisons against any regularized form of CCA, which would seem to be the most natural method to use in this circumstance. Moreover, in the simulations where they show B2B outperforms CCA, they use 1000 training samples with 10-100 possible causes. In their experiments on real data, where it seems the CCA results are much closer to the results that B2B gives, they have 2700 samples and 4 possible causes. This seems to imply the real data case might be better conditioned than the simulated case, so that regularization would have less of an impact. <sep> In conclusion, the authors present a sound method for disentangling correlated possible causes when the outcome is high-dimensional. However, the authors do not provide enough evidence that this method is generally useful and better than established methods to merit acceptance to *CONF*. A comparison to regularized CCA, application to more datasets and simulations under violations of the model would greatly improve the paper. I also have two minor points. 1) the word ""causal"" can mean many things, and here it refers specifically to disentangling correlated predictors, rather than confounding in observations or direction of effect. It would improve the paper to add some discussion of this point. 2) The comparisons in the experiments are done between E estimated from B2B and E=sum_j H_j^2 for other methods that do not directly estimate E. However, a more natural comparison might be against EF as this also includes estimates of the strength of influence of each observation, which is implicit in the sum above.","The authors introduce a method for disentangling effects of correlated predictors in the context of high dimensional outcomes. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of *CONF* due to its limitations in terms of limited applicability and experiments. The paper will benefit from a revision and resubmission to another venue."
"** post rebuttal start ** <sep> After reading reviews and authors' response, I decided not to change my score. <sep> I remark that this paper still requires a lot of revision; comparison in the main paper is somewhat unfair and all new results are in the appendix. <sep> Also, the performance of their replication of the prior method is far lower than reported. In worst case, the performance gain from the compared method would be from their incorrect implementation on the prior works. In this kind of case, I suggest the authors to put {the numbers in the original paper} as well as {their replication} and claim that they fail to replicate the number. Ideally, if their method is evaluated in the same condition, it should outperform prior works in any case. <sep> Detailed comments: <sep> 1-(a). Adversarial attack and OOD (which is hard to detect) are closely related: they are both in off-manifold. Their main difference would be, while adversarial attack is very close to the clean data in the data space, OOD is relatively far from the in-distribution in the data space. <sep> Though it does not talk about OOD, you may refer to [R1] for analysis in perspective of manifold. The difficulty of OOD detection can be considered to be coming from overlapped manifolds in the latent space. <sep> [R1] Stutz et al. Disentangling Adversarial Robustness and Generalization. In CVPR, 2019. <sep> 1-(b). Though the numbers are much lower than those reported in the original paper, I am happy to see the fair comparison. However, according to the original paper, simple FGSM is used for validation, so I am not sure such a huge difference can actually happen. In this kind of case, I suggest the authors to put {the numbers in the original paper} as well as {their replication} and claim that they fail to replicate the number. <sep> 2. I am happy to see that their revised method has better performance. <sep> ** post rebuttal end ** <sep> - Summary: <sep> This paper proposes an out-of-distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. They specifically address a problem of the state-of-the-art method satisfying the constraints, and propose a new distance metric inspired by data compression. Experimental results on several benchmarks with different deep neural network architectures support their claim. <sep> - Decision and supporting arguments: <sep> Weak reject. <sep> 1. The problem setting is clear and their approach is interesting and makes sense. However, the method for comparison is not properly set. As the authors addressed, Mahalanobis detector proposed by Lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without OOD data by generating adversarial samples as proposed in the same paper. Although Table 2 in Lee et al. (2018b) shows that the performance is not better than the case when we have an explicit OOD data for validation, it reasonably works well. Therefore, rather than comparing with the vanilla version (only using last latent space) or the alternative ""assemble"" method (concatenating all average-pooled features), they had to compare their method with the model validated by adversarial samples, which essentially satisfies the constraints. <sep> 2. Also, I wonder the main body of the proposed method itself is really effective or some minor tweak they made is essential. According to Table 2 in the submission, their method is better than ""Mahalanobis vanilla"" only when all components are applied. Though the idea is interesting, I am skeptical about the effectiveness of the proposed method. <sep> - Comments: <sep> 1. As addressed by the authors, feature concatenation (""assemble"") is not effective for the Mahalanobis method but the proposed method. How about to do an ablation study about weighted averaging vs. concatenation on the proposed method as well? Again, weights can be validated by adversarial samples to satisfy the constraints.","This paper proposes an out-of-distribution detection (OOD) method without assuming OOD in validation. <sep> As reviewers mentioned, I think the idea is interesting and the proposed method has potential. However, I think the paper can be much improved and is not ready to publish due to the followings given reviewers' comments: <sep> (a) The prior work also has some experiments without OOD in validation, i.e., use adversarial examples (AE) instead in validation. Hence, the main motivation of this paper becomes weak unless the authors justify enough why AE is dangerous to use in validation. <sep> (b) The performance of their replication of the prior method is far lower than reported. I understand that sometimes it is not easy to reproduce the prior results. In this case, one can put the numbers in the original paper. Or, one can provide detailed analysis why the prior method should fail in some cases. <sep> (c) The authors follow exactly same experimental settings in the prior works. But, the reported score of the prior method is already very high in the settings, and the gain can be marginal. Namely, the considered settings are more or less ""easy problems"". Hence, additional harder interesting OOD settings, e.g., motivated by autonomous driving, would strength the paper. <sep> Hence, I recommend rejection."
"This paper tackles the issue of scalability in distributed SGD over a high-latency network. <sep> Along with experiments, this paper contributes two ideas: delayed updates and temporally sparse updates. <sep> - strengths: <sep> · Clear-looking overall presentation. <sep> Good efforts to explain the network problems (latency, congestion) and how they are tackled by delayed updates and temporally sparse updates. <sep> - weaknesses: <sep> · While the overall presentation looks clean, the paper does not talk about the setting (one parameter server and many workers, only workers, etc). <sep> This is arguably basic information, and the readers is left to understand by themselves that the setting consists in many workers without a parameter server. <sep> · There is no theoretical analysis of the soundness of the proposed algorithm. <sep> For instance in ASGD (that the authors cite), stale gradients are dampened before being used, which in turn is used to guarantee convergence. <sep> In the proposed algorithm, there is no dampening nor apparent limit of the maximum value of t; such a difference with prior art should entail a serious (theoretical) analysis. <sep> · Finally, using SSGD for comparison is not very ""fair"", as communication-efficient algorithms have already been published for quite some time [1, 2, and follow-ups (e.g. searching for ""local sgd"")]. <sep> At the very least a comparison with ASGD (cited) is necessary, as in a realistic setting latency is indeed a problem but arguably not bandwidth <sep> (plus, orthogonal gradient compression techniques do exist, e.g. as in ""Federated Learning: Strategies for Improving Communication Efficiency""). <sep> questions to the authors: <sep> - Can you clarify the setting? <sep> - Can you give at least an intuition why accepting stale gradient is correct (i.e. does not impede convergence)? <sep> There is no theoretical limit on the value of t; can workers take any arbitrary old gradient? <sep> So when the training is close to convergence (if it reaches it), i.e., when the norm of the gradients are close to 0,  the algorithm could then use very old gradients (which norms could be orders of magnitude larger) to ""correct the mismatch"" caused by the local training; is this correct? <sep> To solve that issue, ASGD introduces a simple dampening mechanism (which is necessary in the convergence proof), which your algorithm does not have. <sep> The related work section focuses on gradient compression techniques (which tackle low bandwidth, not latency) and asynchronous SGD (which is more prone to congestion, with a single parameter server), <sep> but seems to overlook that sparse communication techniques already exist (this fact should at least be mentioned). <sep> The idea of sparse communication for SGD exists since at least 2012 [1, Algorithm 3]. <sep> A first mention of the use of such techniques for ""communication efficiency"" dates from (at least) 2015 [2]. <sep> [1] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. <sep> Optimal distributed online prediction using mini-batches. <sep> J. Mach. Learn. Res., 13(1):165–202, January 2012. <sep> [2] Sixin Zhang, Anna E. Choromanska, Yann LeCun. <sep> Deep learning with Elastic Averaging SGD. <sep> NeurIPS, 2015. <sep> I do not see a clear novelty, nor a proof (or even intuitions) that the proposed algorithm is theoretically sound. <sep> The comparison with SSGD is arguably unfair, since SSGD is arguably not at all the state-of-the-art in the proposed setting (hence the claimed speedup of x90 can be very misleading).","The paper introduces a distributed algorithm for training deep nets in clusters with high-latency (i.e. very remote) nodes. While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis."
"The paper ""aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. "" The author's empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. Specifically, the authors empirically study <sep> - the ability of networks to recover latent variables <sep> - the effects of extreme overparameterization <sep> - the effects the training method (e.g. batch size) <sep> - latent variable stability over the course of training <sep> In line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. This is an interesting and useful observation, particularly since it at first sight appears to be in disagreement with some earlier work (the authors suggest explanations for the differing observations). <sep> As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. <sep> The authors perform studies on a range of different real-world and synthetic datasets. <sep> The paper is well-written, well-structured, and easy to follow. Relevant literature has been cited. The appendices contain a wealth of details that will make this work reproducible. <sep> Decision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger *CONF* papers. <sep> A small gripe: the authors promise "" a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings"". I would argue that ""making precise"" is too strong for what the paper actually delivers. I suggest rewording this.","This paper studies over-parameterization for unsupervised learning. The paper does a series of empirical studies on this topic. Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models. The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification. One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over-parameterized model for latent generative models. I agree with the reviewers that this paper is ""compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods"". I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important. I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures. In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures. I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time. I strongly recommend the authors to revise and resubmit. I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision."
"Summary: <sep> This paper is clearly written and well structured. After categorizing difference causes of posterior collapse, the authors present a theoretical analysis of one such cause extending beyond the linear case covered in existing work. The authors then extended further to the deep VAE setting and showed that issues with the VAE may be accounted for by issues in the network architecture itself which would present when training an autoencoder. <sep> Overall: <sep> 1) I felt that Section 3 which introduces categorizations of posterior collapse is a valuable contribution and I expect that these difference forms of posterior collapse are currently under appreciated by the ML community. I am not certain that the categorization is entirely complete but is nonetheless an excellent step in the right direction. One source of confusion for me was the difference between sections (ii) and (v) --- in particular I believe that (ii) and (v) are not mutually exclusive. <sep> Additionally, the authors wrote ""while category (ii) is undesirable, it can be avoided by learning γ"". While this is certainly true in the affine decoder case it is not obvious that this is true in the non-linear case. <sep> 2) Section 4 provides a brief overview of existing results in the affine case and introduces a non-linear counter-example showing that local minima may exist which encourage complete posterior collapse. <sep> 3) On the proof of Proposition 4.1. In A.2.1 you prove that there exists a VAE whose ELBO grows infinitely (exceeding the local maxima of (7)). While I have been unable to spot errors in the proof, something feels odd here. In particular, the negative ELBO should not be able to exceed the entropy of the data which in this case should be finite. I've been unable to resolve this discrepancy myself and would appreciate comments from the authors (or others). The rest of the proof looks correct to me. <sep> 4) I felt that section 5 was significantly weaker than the rest of the paper. This stemmed mostly from the fact that many of the arguments were far less precise and less rigorous than those preceding. I think the presentation of this section could be significantly improved by focusing around Proposition 5.1. <sep> a) Section 5 depends on the decoder architecture being weak, though this is not clearly defined formally. I believe this is a sensible restriction which enables analysis beyond the setting of primary concern in Alemi et al. (and other related work). <sep> b) In the third paragraph, you write ""deep AE models can have bad local solutions with high reconstruction [...]"". I feel that this doesn't align well with the discussion in this section. In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. <sep> c) Equation (8) feels a little too imprecise. Perhaps this could be formalized through a bias-variance decomposition of the right hand side similar to Rolinek et al.? <sep> d) The discussion of optimization trajectories was particularly difficult to follow. It is inherently difficult to reason about the optimization trajectories of deep auto-encoding models and is potentially dangerous to do so. For example, perhaps the KL divergence term encourages a smoother loss landscape and encourages the VAE to avoid the local stationary points that the auto-encoder falls victim to. <sep> e) It is written, ""it becomes clear that the potential for category (v) posterior collapse arises when ϵ is large"". This is not clear to me and in fact the analysis seems more indicative of collapse presented in category (ii) (though as mentioned above, I am not convinced these are entirely separate). Similarly, later in this section it is written, ""this is more-or-less tantamount to category (v) posterior collapse"". I was also unable to follow this reasoning. <sep> f) ""it is actually the AE base architecture that is effectively the guilty party when it comes to posterior collapse"". If the conclusions are to be believed, this only applies to category (v) collapse. <sep> g) Unfortunately, I did not buy the arguments surrounding KL annealing at the end of section 5. In particular, KL warm start will change the optimization trajectory of the VAE. It is possible that the VAE has a significantly worse loss landscape than the autoencoder initially and so warm-start may enable the VAE to escape this difficult initial region. <sep> Minor: <sep> - The term ""VAE energy"" used throughout is not typical within the literature and seems less explicit than the ELBO (e.g. it overlaps with energy based models). <sep> - Equation (4) is missing a factor of (1/2). <sep> - Section 3, in (ii), typo: ""assumI adding γ is fixed"", and ""like-likelihood"". In (v), typo: ""The previous fifth categories"" <sep> - Section 4, end of para 3, citep used instead of citet for Lucas et al. <sep> - Section 4, eqn 6 is missing a factor of 1/2 and a log(2pi) term. <sep> - Section 5, ""AE model formed by concatenating"" I believe this should be ""by composing"". <sep> - Section 5, eqn 10, the without γ notation is confusing and looks as though the argmin does not depend on gamma. Presumably, it would make more sense to consider γ∗ as a function of θ and ϕ. <sep> - Section 5 ""this is exactly analogous"". I do not think this is _exactly_ analogous and would recommend removing this word.","This manuscript investigates the posterior collapse in variational autoencoders and seeks to provide some explanations from the phenomenon. The primary contribution is to propose some previously understudied explanations for the posterior collapse that results from the optimization landscape of the log-likelihood portion of the ELBO. <sep> The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work investigating the landscape properties of variational autoencoders and other generative models. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the technical difficulty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the results. There were also concerns about novelty. In the opinion of the AC, the manuscript in its current state is borderline, and should ideally be improved in terms of clarity of the discussion, and some more investigation of the insights that result from the analysis."
"This paper has two main contributions. First, the authors perform an extensive study to understand the source of what they refer to as 'environment bias', which manifests itself as a gap in performance between environments used for training and unseen environments used for validation. The authors conclude that of the three sources of information provided to the agent (the natural language instruction, the graph structure of the environment, and the RGB image), the RGB image is the primary source of the overfitting. The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data. <sep> This paper has some pretty exhaustive treatment diagnosing the source of the agent's 'environment bias' (which, as I discuss below, I believe is more accurately referred to as 'overfitting') in Sec. 4. To me, this is this highlight of the paper, and some interesting work; the investigation of the behavior of the system is interesting and informative. It provides a framework for thinking about how to diagnose this behavior and identify its source. The authors use this rather extensive study to motivate the need for new features (semantic features) to replace the RGB image that their investigation finds is where much of this 'environment bias' is located. Unfortunately, it is here that the paper falls flat. The authors proposal methods perform nominally better on the tasks being investigated, but much of the latter portion of the paper continues to focus on the 'improvement' in the metric they use to diagnose the 'bias'. As I mention below, the metric for success on these tasks is performance on the unseen data, and, though an improvement on their 'bias' metric is good anecdotal evidence their proposed methods are doing what they think, the improvements in this metric are largely due to a nontrivial decrease in performance on the training data. Ultimately, this is not a compelling reason to prefer their method. I go into more details below about where I think some of the other portions of the paper could be improved and include suggestions for improvement. <sep> High-level comments: <sep> - I am uncertain that 'bias' is the right word to describe the effect under study. In my experience, environment bias (or, more generally, dataset bias) usually implies that the training and test sets (or some subset of the data) are distinct in some way, that they are drawn from different distributions. The learning system cannot identify these differences without access to the test set, resulting in poor performance on the 'unseen' data. In the scenario presented here, the environments are selected to be in the train/test/validation sets at random. As such, the behavior described here is probably more appropriately described as 'overfitting'. The shift in terminology is not an insignificant change, because using 'bias' to describe the problem incorrectly suggests that the data collection procedure is to blame, rather than a lack of data or an overparamatrized learning strategy; I imagine that more data in the training set (if it existed) could help to reduce the gap in performance the paper is concerned with. That being said, I imagine some language changes could be done to remedy this. <sep> - Perhaps the biggest problem with the paper as written is that I am not convinced that the 'performance gap' between the seen and unseen data is a metric I should want to optimize. This metric is instructive for diagnosing which component of the model the overfitting is coming from, and Sec. 4 (devoted to a study of this effect) is an interesting study as a result. However, beyond this investigation, reducing the gap between these two is not a compelling objective; ultimately, it is the raw performance on the unseen data that matters most. The paper is written in a way that very heavily emphasizes the 'performance gap' metric, which gets in the way of its otherwise interesting discussion diagnosing the source of overfitting and some 'strong' results on the tasks of interest. The criteria should be used to motivate newer approaches, rather than the metric we should value for its adoption. This narrative challenge is the most important reason I cannot recommend this paper in its current state. <sep> - Using semantic segmentation, rather than the RBG image, as input seems like a good idea, and the authors do a good job of motivating the use of semantics (which should show better generalization performance) than a raw image. However, the implementation in Sec. 6.3 raises a few questions. First (and perhaps least important) is that 6.3 is missing some implementation details. In this section, the authors mention that 'a multilayer perceptron is used' but do not provide any training or structure details; these details should be included in an appendix. More important is the rather significant decrease in performance on the seen data (11% absolute) when switching to the learned method. Though the performance on the unseen data does not change much, it raises some concerns about the generalizability of the learning approach they have used: in an ideal world with infinite training data, the network would perfectly accurately reproduce the ground truth results, and there should be no difference between the two. Consequently, the authors should comment on the discrepancy between the two and the limits of the learned approach, which I worry may limit its efficacy if more training data were added. <sep> Smaller comments: <sep> - I do not fully understand why the 'Touchdown' environment was included in Table 1, since the learned-semantic agent proposed in the paper was not evaluated. The remainder of the experiments are sufficient to convince the reader that this gap exists, and I would recommend either evaluating against the proposed technique or removing this task from the paper. <sep> - Figure captions should be more 'self-contained'. Right now, they describe only what is shown in the figure. They should also describe what I, as a reader, should take away or learn from the figure. This is not always necessary, but in my experience improves readability, so that the reader does not need to return to the body of the text to understand. <sep> - The use of a multilayer perceptron for the Semantic Segmentation learned features, trained from scratch, stands out as a strange choice, when there are many open source implementations for semantic segmentation exist and could be fine-tuned for this task; a complete investigation (which may be out of scope for the rebuttal period) may require evaluating performance of one of these systems.","The submission is a detailed and extensive examination of overfitting in vision-and-language navigation domains. The authors evaluate several methods across multiple environments, using different splits of the environment data into training, validation-seen, and validation-unseen. The authors also present an approach using semantic features which is shown to have little or no gap between training and validation performance. <sep> The reviewers had mixed reviews and there was substantial discussion about the merits of the paper. However, a significant issue was observed and confirmed with the authors, relating to tuning the semantic features and agent model on the unseen validation data. This is an important flaw, since the other methods were not tuned in this way, and there was no 'test' performance given in the paper. For this reason, the recommendation is to reject the paper. The authors are encouraged to fairly compare all models and resubmit their paper at another venue."
"First of all, I must confess that my knowledge is quite limited to read this paper. Perhap the authors present something that I can not catch up at the present. <sep> I conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. <sep> The paper is somewhat cumbersome in the introduction that makes the reader (here is myself) can not understand the main idea. At first the authors introduce about evolution in genetics and genomics that is a bit different from what I known. Then the authors claim that they can show that the brain circuits can evolved in their model. <sep> There are so many mistake and/or typos in sentences and in mathematical formulation. These make me can not finish reading the paper. I have to stop reading at the end of Section 2. <sep> Here are my concerns and questions: <sep> 1). What is ""STDP"" in the 2nd papragraph in Introduction? <sep> 2). in the 3rd papragraph in Introduction: ""Suppose that the brain circuitry for a particular classiﬁcation task, such as ""food/not food"",is encoded in the animal's genes, assuming each gene to have two alleles 0 and 1"". This is realistics, the allels in animal is 0 1 or 2 if encoded. <sep> 3). The authors use the words ""gene, genotype, phenotype"" in a special way that is different to what I known in genomics (in GWAS). <sep> 4).  in the 3rd papragraph in Introduction: ""At each generation, a gene is an independent binary variable with ﬁxed probability of 1"". What do you mean by fixed probability of 1? I can not understand in anysense that I know. <sep> 5). In Section 2, What is n? the authors start the mathematical formulation but I can not find out what is n? Is it the sample size? <sep> 6). In Section 2, paragraph 2, you define y~ \\mathcal{D}, BUT then in all formulas later you denote y ~ D. What is D??? I can not understand. <sep> 7).  In Section 2, paragraph 2, you define a label of y as \\ell(y), BUT then in the 1st sentence of the 3rd paragraph in Section 2 you wrote    L(NNE_x(t), l(y)). What is l(.) here ??? <sep> 8). In the equations (1) and (2), what is p^t  ???  You have NOT defined it. <sep> 9). Right after equations (1) and (2), What is \\epsilon ???? Can NOt understand. <sep> 10). The sentence right after the equation (3): ""This is the standard update rule in population genetics under the weak selection assumption."" This is NOT trivial to me, and even the machine learning comunity, we do not know this rule, it is not obvious. PLEASE provide exact reference. <sep> 11). the first equation in the PROOF of LEMMA 1 wrong  \\mathcal{L} (p^t)  should be equal to E_{x~p^t}  NOT p. <sep> 12). in the PROOF of Theorem 1, I can NOT find out where \\gamma has been defined? <sep> 13). What is d  in Theorem 2? is it the dimension? I make too many guesses !!!","Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper. Indeed, if all reviewers and the area chair do not interpret the paper well, the authors' best response would be to rewrite the papers rather than disagree with all reviewers. <sep> In the area chair's opinion, the current form the paper does not merit publication. The authors are advised to address the reviewers' concerns, rework the paper, and submit to a conference again."
"This paper assesses the effects of training an image classifier with different label types: 1-hot coarse-grained labels (10 classes), 1-hot fine grained labels (30 labels which are all subcategories of the 10 coarse-grained categories), word vector representations of the 30 fine-grained labels. They also compare the representations learned from an unsupervised auto-encoder. They assess the different representations through cosine similarity within/between categories and through comparison with human judgments in an odd-one-out task. They find that (i) the auto-encoder representation does not capture the semantic information learned by the supervised representations and (ii) representations learned by the model depend on the label taxonomy,  how the targets are represented (1-hot vs. wordvec), and how the model is trained (e.g. fine-grained then coarse grained stages), (iiii) the different representations predict human judgements to differing degrees. the first finding is obvious and I'm not even sure why it needs to be stated -- of course semantics of images are not inherently encoded in the pixels of an image! The second point again, is not surprising . This paper starts to get at some interesting questions but does not follow through.  It is also quite confusing to read despite thee simple subject matter. This paper is also missing a related work section! There has been so much word on adding structure to the label space of image classifiers (e.g. models that learn image/text embedding space jointly, models that predict word vectors, graphical model approaches to building in semantic information, etc.) and none of this is discussed. There has also been work on comparing convnet representations to human percepts e.g. https://cocosci.princeton.edu/papers/Peterson_et_al-2018-Cognitive_Science.pdf)and none of this work is discussed! This work needs to be better situated within the context of previous work in this field. Please write a related work section. <sep> Detailed comments/questions: <sep> - It would be good to add a super-basic model to table 1 for comparison (i.e. first train of coarse level categories and then fine-tine on the more fine-grained taxonomy). <sep> - It would be good to compare the use of word vector representations at both the basic and superordinate levels; the 1-hot vs word vector targets and the basic vs superordinate taxonomy seem like orthogonal axess to explore and I'm not sure why the authors didn't test all combinations. <sep> - the authors found the imagenet categorical representations were most predictive of human judgements in the odd-one-out task. This seems highly unsurprising since (i) the humans saw images from the Imagenet dataset (not THINGS) and (ii) humans leverage semantic information when making similarity judgements. <sep> - What categories had the least inter-rater agreement.. was there any relationship between these categories and the similarity of representations learned by the convnet? <sep> - It seems the odd-one-out comparison always involves averaging image representations at the basic category level. In the case where the items come from three different superordinate classes it would bee interesting to see the results when averaging over superordinate classes as well. <sep> - In table 3, what does the FastText column just list ""true""/""false"" rather than accuracies? I would expect this column to show the accuracy when the FastText embeddings for the three words are used to compute similarity. I don't understand what the ""true""/""false"" is meant to indicate. Also it's not clear to be what the two rows in table three are meant to correspond to? <sep> - The authors claim ""Surprisingly, the kind of supervised input that proved most effective in matching human performance on the triplet odd-one-out task was training with superordinate labels"". This should be qualified to say that, when there are two or more superordinate classes represented in the triplet, the superordinate labels are highly effective when the three items come from three different superordinate classes. I'm also not clear why this would be surprising? Could the authors elaborate? <sep> - I'm surprised more space isn't given to discussing the wordvec representations since these should capture some of the semantic information that the 1-hot encodings might miss. In fact, the word vectors targets seem to perform as good as or close to the other representations on the odd-one-out <sep> In short, I really like the overall idea of comparing convnet representations with human perceptions of images. However, this work barely scratches the surface of what could be done here and mostly reveals incredibly obvious results. There are so many interesting questions to ask regarding the relationship between how humans perceive similarity and what is encoded in a convnet representation. For example, it would have been very interesting to test the effects of asking the human rates to cue in on different aspects of the image. Focusing on semantic similarity, visual similarity, etc. would all likely give different ratings. <sep> ---------------------------------------------------------- <sep> Update (in light of rebuttal) <sep> I appreciate the authors lengthly and considered response. In particular, the updated related work and expansion of the empirical experiments. While I am more comfortable with this paper being accepted that previously (and have updated by score to ""weak accept"" to reflect this), I still think the paper has a lot of room for improvement. In particular, I suggest a more expansive analysis of human perceptions and a discussion f the implications of the findings.","This paper explores training CNNs with labels of differing granularity, and finds that the types of information learned by the method depends intimately on the structure of the labels provided. <sep> Thought the reviewers found value in the paper, they felt there were some issues with clarity, and didn't think the analyses were as thorough as they could be. I thank the authors for making changes to their paper in light of the reviews, and hope that they feel their paper is stronger because of the review process."
"SUMMARY OF REVIEW <sep> This paper discusses an interesting problem of BO in the cases of robustness and antifragility to aleatoric noise/uncertainty. To tackle this problem, the authors replace the conventional homoscedastic GP model with a heteroscedastic GP model. In the case of robustness to aleatoric noise/uncertainty, the authors have modified EI by simply either scaling down [32] or subtracting from its value more when the aleatoric noise increases. In the case of antifragility to aleatoric noise/uncertainty, they do the opposite. <sep> The modifications of EI to handle robustness and antifragility to aleatoric noise/uncertainty are simple and straightforward, one of which is similar to the augmented EI of [32]. <sep> There are some technical ambiguities, as detailed below. In particular, the choice of objective function (equation 9) for this problem needs to be justified and motivated by the practical applications described in Section 1. <sep> Since no convergence guarantee is given, a more extensive empirical analysis with real datasets needs to be provided to better understand the performance and behavior of the proposed BO algorithms. In particular, though the authors have motivated their problem using the compelling applications of materials and drug discovery, no experimental result for these applications has been provided to support the motivation of this work. <sep> DETAILED COMMENTS <sep> The authors seem to motivate the significance of their problem of interest through the key applications of materials and drug discovery which I can appreciate. Unfortunately, experimental results in such applications were not available in this paper to ""close the loop"" in supporting the motivation of this work, begging the question whether their proposed BO algorithm indeed works for these key applications. For example, why is the FreeSolv hydration energy dataset not used for your experiments? <sep> For Fig. 1, how exactly do you extract the error magnitudes from the FreeSolv hydration energy dataset? How do you exactly define the notion of calculated vs. experimental uncertainties? Fig. 1 shows that the noise peaks with a relatively high frequency at a single error magnitude value. Would the assumption of homoscedastic noise at this peaked value be detrimental to BO? A sensitivity analysis would be useful here. <sep> For the soil phosphorus fraction dataset (Fig. 2), the skewed distribution of the measurements (few extremely large measurements and many small-valued measurements) may not be due to heteroscedastic aleatoric uncertainty. In fact, in the literature of earth/environmental science, such a dataset is often modeled using a log-Gaussian process (or log-normal kriging), that is, the log-measurements follow a GP: <sep> Webster, R., and Oliver, M. 2007. Geostatistics for Environmental Scientists. John Wiley & Sons, 2nd edition. <sep> Can the authors provide supporting evidence (in the form of references) that such a dataset is due to heteroscedastic aleatoric uncertainty? <sep> On page 4, step 2 of the most likely heteroscedastic GP algorithm [28] cannot be understood: What is E[x]? Isn't G_1 a GP? Why is it able to accept x_i and D as inputs? How is z_i defined? <sep> The authors say that ""A note on the form of this variance estimator is give in Appendix B."" There is no Appendix B. <sep> Can the authors give a detailed discussion why is the expression of f(x) = g(x) + s(x) in equation 9 the right one to be minimized in practice (e.g., in the context of materials and drug discovery)? For example, do material scientists use such an objective function? Provide references. Furthermore, why is the same equation 9 being minimized for both the cases of robustness and antifragility to aleatoric uncertainty? <sep> Caption of Fig. 3: I can't understand the sentence: ""The combined objective, which when optimised maximises the sin wave subject to the minimisation of aleatoric noise"". This was repeated in Section 5.5: ""finds the first maximum as that which minimises aleatoric noise"". Isn't the minimum of the aleatoric noise at the origin (Fig. 3b)? <sep> Can the authors provide information on how much initial data was provided prior to running BO? How much data is used for learning the GP hyperparameters? <sep> The performance advantage of heteroscedastic ANPEI over homoscedastic does not appear to be significant for Branin-Hoo function (Figs. 5b and 6b). Can the authors explain this? <sep> Minor issues <sep> There are two different font types of x in bold. <sep> Page 5: fixed aleaotric <sep> Which phrase is correct? ""is obtained by subtracting the noise function from the 1D sinusoid"" in the caption of Fig. 3 or ""The objective function in all cases is the principal objective g(x) minus one standard deviation of the ground truth noise function s(x)""? <sep> Fig. 5b: Shouldn't the vertical axis be labeled as ...+ Noise? <sep> Fig. 6a: Shouldn't the vertical axis be labeled as ...- Noise?","The reviewers initially gave scores of 1,1,3 citing primarily weak empirical results and a lack of theoretical justification. The experiments are presented on synthetic examples, which is a great start but the reviewers found that this doesn't give strong enough evidence that the methods developed in the paper would work well in practice. The authors did not submit an author response to the reviewers and as such the scores did not change during discussion. This paper would be significantly strengthened with the addition of experiments on actual problems e.g. related to drug discovery which is the motivation in the paper."
"This paper proposes a fractional graph convolutional networks for semi-supervised learning. The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. In addition, the authors adopt a parallel system and weighted combination of max and average pool. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph-based neural networks for all datasets except one. <sep> The key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks. However, this idea is too incremental and applying the classification function to graph filter is very trivial. This works also combines the fractional GSSL with a parallel system and weighted pool. But, it is not clear which contribution actually improves the results. Moreover, the intuition of the fractional approach is not clear too, e.g., how the optimization (equation (2)) is derived?, and some explanations are unnatural to demonstrate the methodology, e.g., equation (4). For these reasons, this paper is under the bar of acceptance. <sep> Main concerns: <sep> 1. What is the intuition of the optimization of GSSL? How is it obtained? And among all fractional methods (e.g., SL, NL, and PR), which one doe achieve the best performance? <sep> 2. The FGS filter in equation (7) is the sum of infinite terms. However, in practical, it is impossible to compute the infinite terms. Does this approximate the sum of finite terms? If does, what is the number of truncation? <sep> 3. The authors mention that they establish a theoretical guarantee of the parallel system. But, I could not find any theoretical results. It would be better to include the analysis in the paper. <sep> Minor concerns: <sep> 1. In page 3, please edit ""forulation"" -> ""formulation"" <sep> 2. In equation (8), I think ""X+\\alpha \\tilde{L} X"" should change to ""\\tilde{L} X""","This paper proposes a fractional graph convolutional networks for semi-supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function. This leads to good results on several tasks. <sep> Reviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details. Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases. <sep> However, concerns that the proposed innovations are too incremental remain. <sep> Therefore, we cannot recommend acceptance."
"This paper presents a technique for model based RL/planning with latent dynamics models, which learns the latent model only using reward prediction. This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. The paper suggests that by removing the state reconstruction loss, the agent can learn to ignore irrelevant parts of the state, which should enable better performance in settings where state reconstruction is challenging. <sep> Overall the motivation for this work is good, and the idea is promising. Difficulty in reconstructing high dimensional states is a challenge for learning latent dynamics models. The paper is also very well written and easy to follow. <sep> My concerns are centered around the experimental evaluation. Specifically, I see the following issues: (1) the experimental environments seem artificial, and hand tailored for this method, (2) given that the proposed method is a minor modification to the PlaNet paper, it seems that PlaNet should be included as a comparison (especially because it has been shown to work on high dimensional states), and (3) the proposed method seems very prone to overfitting to the given task, and there should be an analysis of how the proposed change affects generalization and robustness. <sep> (1): The testing environments contain many distractor pendulums/cheetahs, which makes state reconstruction especially challenging. While this does seem to be the point the authors are trying to show, the environments are an extreme, almost artificial, case of difficult state reconstruction. Would the same results hold in more realistic settings, for example, visual robot manipulation in a cluttered scene? Model based RL with video prediction models has been shown to work in such real cluttered robot manipulation environments. Showing that the proposed method can outperform such approaches in robot manipulation settings would be a powerful result. The results on images in the appendix seem to show a delta between the true and predicted reward, suggesting that the proposed method does not yet work on images. Why might this be the case? <sep> (2): From what I can see, the proposed method is very similar to the PlaNet algorithm with state reconstruction loss removed. Given the similarity, PlaNet should be included as a comparison in both the pendulum and cheetah environments. Similarly why was DeepMDP performance not shown in the Cheetah environment? <sep> (3): One of the strengths of model based reinforcement learning is the ability to plan to reach unseen goals with a model trained via self-supervision or different goals. Does the proposed approach lose some of this, by overfitting to only the task reward? I suspect that in generalizing to unseen tasks, a model trained with state prediction would potentially perform much better. If trained on many tasks, could this method achieve similar generalization? <sep> Due to some of these questions which remain unanswered by the experimental evaluation my current rating is Weak Reject. If the authors are able to clarify some of the questions above I may adjust my score.","The authors propose a model-based RL algorithm, consisting of learning a <sep> deterministic multi-step reward prediction model and a vanilla CEM-based MPC <sep> actor. <sep> In contrast to prior work, the model does not attempt to learn from observations <sep> nor is a value function learned. <sep> The approach is tested on task from the mujoco control suit. <sep> The paper is below acceptance threshold. <sep> It is a variation on previous work form Hafner et al. <sep> Furthermore, I think the approach is fundamentally limited: All the learning <sep> derives from the immediate, dense reward signal, whereas the main challenges in RL <sep> are found in sparse reward settings that require planning over long horizons, where value <sep> functions or similar methods to assign credit over long time windows are <sep> absolutely essential."
"This paper proposes a new kind of episodic finite MDPs called ""deep hierarchical MDP"" (hMDP). An L-layer hMDP can be *roughly* thought of as L episodic finite MDPs stacked together. A variant of UCRL2 [JOA10] is proposed to solve these hMDPs and some results from its regret analysis are provided. <sep> Pros: <sep> 1. The essential result (Theorem 4.1) on the regret bound of the proposed algorithm seems correct. I have not checked the proofs in detail but in part because it does not seem surprising and that a precise assessment is hindered by many typos (see Min2 and Con2). <sep> Cons (in descending order of their weights in my decisions): <sep> 1. The proposed hMDPs do _not_ seem to capture important features or challenges in hierarchical RL. My understanding is that the transitions in hMDPs work _like_ a clockwork (more on this in Mis6), the algorithm interacts with the sub-MDPs at each layer in turns according to their fixed horizons H_l's. This structure is very rigid temporally and seems to exclude the mentioned example of autonomous driving: the number of decision steps between intersections would be fixed. <sep> 2. There are many (typographical) errors in both the text and mathematical expressions. Some of them are more severe than others hindering understanding. <sep> 3. Possible as a consequence of Con2, some quantities defined seem unclear or incorrect at worst. For example, the ""standard regret"" defined in (2) is an expectation, not a random variable as in convention. <sep> 4. There are some notable deviations from similar settings in prior works. They might be worthwhile innovations but their significance or motivations is omitted. For example, the rewards in hMDPs are defined as a function of the full state, i.e. in general not decomposable to rewards on the states of each layer, yet the analogy for hMDP is ""L levels of episodic MDPs."" <sep> A non-exhaustive list of obvious mistakes/typos: <sep> 1. In the title, ""Provably"" -> Provable. <sep> 2. In the abstract, ""often both"" -> often requires both. <sep> 3. In Organization, ""theoremm"" -> theorems. <sep> 4. In Section 2, ""between exploration"" -> between exploration and exploitation. <sep> 5. Above Section 3, ""carried"" -> carried out. <sep> 6. Below (1), ""amount reward"" -> amount of reward. <sep> 7. The definition of horizon H is incorrect. Consider H_1 = 2 and H_2 = 3, the algorithm will interact with the sub-MDPs in the following order within one episode: 1, 1, 2, 1, 1, 2, 1, 1, 2. There are 9 steps not 6 = 2 * 3 as defined. <sep> 8. Section 3.3, ""able accumulate"" -> able to accumulate. <sep> 9. Section 3.3, the definition of V_h^\\pi, there should be not \\max. <sep> 10. (5), ""H"" -> H - h. <sep> 11. Section 6, ""tabular R"" -> tabular RL. <sep> 12. In References, ""Posterior sampling for reinforcement learning: worst-case regret bounds"" -> Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. <sep> 13. In References, ""Temporal abstraction in reinforcement learning"" should be cited as a PhD thesis. <sep> Some other possible errors/inconsistencies: <sep> 1. Related work listed regret bounds from prior works (the presentation closely mirrors that of [JABJ18]) assume an episodic MDP with non-stationary transitions, i.e. P_t ≠ P_{t'} in general. However, in 3.1 the transitions are stationary. Relatedly, regardless of the stationarity of the transitions, there may not be an optimal _stationary_ policy in an episodic MDP contrary to the claim in the paper. <sep> 2. Indexing seems inconsistent near the top of page 3. The initial state is s_0 but the trajectory starts with s_1. <sep> 3. Near the top of page 3, V_h^\\pi and Q_h^\\pi should sum from h'=h, not h'=1. I assume that the authors intend to define h-step values (to appear in the Bellman equations). <sep> 4. Section 3.3, what are the k's in the equations? <sep> 5. (6), what is n(k-1, e)? <sep> Minor (factored little to none in my decision): <sep> 1. The claim in Introduction that some games ""do not require high-level planning"" while others do is highly speculative and vague. Note that any policy can be written a function with codomain in the primitive actions. In fact, many people thought to solve a game like chess or Go requires some temporal hierarchy (opening, mid-game, and end-game). <sep> 2. The comparison to running UCRL2 on hMDP ignoring the given structure seems weak. Given the knowledge of the particular clockwork-like structure of hMDP at each layer (horizons, states, actions), the natural attempt would be run O(L) copies of UCRL2, one for each sub-MDP (under different terminating states of the immediately lower sub-MDP). Frankly, in my understanding, that seems to be roughly what the authors propose as the solution (thus the results unsurprising). Moreover, it is not immediately clear that UCRL2 can apply to the proposed setting of hMDP without checking regular conditions like communicating (diameter being finite). <sep> 3. The claim that RL with options ""can be viewed as a two-layer HRL"" needs much elaboration if not correction. Note that in the former, primitive actions are always taken in the original MDP at consecutive steps. <sep> 4. There is a limited relevance to deep learning or deep RL central to the themes at *CONF*, i.e. the general issue of representation. This work may be more suitable for other general ML venues. <sep> Some suggestions <sep> I agree with the authors' sentiment that our theoretical understanding of hierarchical RL is relatively limited. I applaud the authors' effort to address this limitation. But judging from this aim of advancing our theoretical understanding, I think the paper may be improved by <sep> 1. better articulating the motivations for hMDPs (concrete examples would help) <sep> 2. contextualizing hMDPs with respect to other well-known models such as semi-MDPs (technical and precise comparison would help). <sep> To put it in a different way, it is unclear to the readers why we want to solve this special class of hMDPs and what does hMDPs have to do with the general issues in hierarchical RL. Technically, I feel that assuming episodicity seems against the spirit of hierarchical RL where subtasks are often delimited by their subgoals instead of durations. <sep> In conclusion, I cannot recommend accepting the current article. <sep> (To authors and other reviewers) Please do not hesitate to directly point out my misunderstandings if there is any. I am open to acknowledging mistakes and revising my assessment accordingly. <sep> Post-rebuttal update: <sep> Thank you for replying to my review and incorporating some of my suggestions into your revision. However, I found many concerns (and mistakes) unaddressed, such as Mis7. The use of driving in Manhattan as an example troubles me because even stopping for a traffic light seems to disrupt the fixed temporal hierarchy of decisions. In conclusion, I will maintain my recommendation.","This paper pursues an ambitious goal to provide a theoretical analysis HRL in terms of regret bounds. However, the exposition of the ideas has severe clarity issues and the assumptions about HMDPs used are overly simplistic to have an impact in RL research. <sep> Finally, there is agreement between the reviewers and AC that the novelty of the proposed ideas is a weak factor and that the paper needs substantial revision."
"This paper proposed deep evidential regression, a method for training neural networks to not only estimate the output but also the associated evidence in support of that output. The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type-II maximum likelihood estimation similar to the empirical Bayes method [1,2]. The authors demonstrated that the both the epistemic and aleatoric uncertainties could be estimated in one forward pass under the proposed framework without resorting to multiple passes and showed favorable uncertainty comparing to existing methods. Robustness against out of distribution and adversarially perturbed data is illustrated as well. <sep> On the technical side, the novelty is incremental. The extension from the classification regime to the regression regime, from the conjugate Dirichet prior to the conjugate Normal-Inverse-Gamma prior, is quite straightforward. Besides, the presentation of the paper could be largely improved. It is not easy to follow the derivation in Section 3. The discussion of concepts and problem definitions look fragmented and incoherent. Even though the presentation largely follows (Sensoy et al., 2018) and uses terms from theory of evidence, the derivation actually is more aligned with the prior network [3] under the Bayesian framework which is missing from the references. It is really confusing that the authors talked about the variational inference when conjugate prior is used, and it is unclear how the variational distributions are used in Section 3.2 or how the ""I don't know"" loss term relates to the KL-divergence between the variational distribution and the prior in Section 3.3. This term was manually added as additional regularization to ""prefer the evidence to shrink to zero for a sample if it cannot be correctly classified"" in (Sensoy et al., 2018), and a different regularization was used to encourage distributional uncertainty in [3]. I hope that the authors could spend more efforts clarifying their ideas, especially the derivations in Section 3.2 and 3.3. <sep> On the other hand, there is no referring to the input x in the entire derivation and problem formulation in Section 3. It took me a while to realize that the formulation in (4) actually defines the generation for a particular input, not for all the inputs. That is, the model is trying to model heteroscedastic uncertainty, not the homoscedastic counterpart. It could be better to call out the dependence on the input explicitly. <sep> On the quantitative side, the baseline models considered in Section 4 are mainly concerned with epistemic uncertainty estimation. So it would be good to explicitly discuss which uncertainty estimation was compared with. This work estimates both aleatoric and epistemic uncertainties, so a better comparison is to models that estimate both quantities (Kendall & Gal, 2017)[4] which has been shown to give better output estimation comparing to epistemic uncertainty estimation only (Kendall & Gal, 2017). <sep> Other comments: <sep> - What is the \\pi in equation (8)? <sep> - The ""I don't know"" loss introduced in Section 3.3 used L-p norm. What is the originality of the L-p norm here? In practice, which p value should be used? In the experiments, which p value was used? <sep> - The RMSE results of the depth estimation presented in Table 2 are orders of magnitude smaller than those from existing work, for example Table 2(b) in (Kendall & Gal, 2017). Was a different RMSE computation used in this work? <sep> - From the caption in Table 2, it seems that only 5 samples were used in MC-dropout, which is considerably smaller than those used in existing work (Kendall & Gal, 2017). <sep> [1] D.J.C. MacKay. Hyperparameters: optimize, or integrate out? Maximum Entropy and Bayesian Methods, Springer 1996. <sep> [2] B. Efron. Two modeling strategies for empirical Bayes estimation. Statistical Science, 2014. <sep> [3] A. Malinin and M. Gales. Predictive uncertainty estimation via prior networks. NeurIPS 2018. <sep> [4] Y. Kwon, J.-H. Won, B.J. Kim, and M.C. Paik. Uncertainty quantification using Bayesian neural networks in classification: application to ischemic stroke lesion segmentation. MIDL 2018.","This paper presents a method for providing uncertainty for deep learning regressors through assigning a notion of evidence to the predictions. This is done by putting priors on the parameters of the Gaussian outputs of the model and estimating these via an empirical Bayes-like optimization. The reviewers in general found the methodology sensible although incremental in light of Sensoy et al. and Malinin & Gales but found the experiments thorough. A comment on the paper pointed out that the approach was very similar to something presented in the thesis of Malinin (it seems unfair to expect the authors to have been aware of this, but the thesis should be cited and not just the paper which is a different contribution). In discussion, one reviewer raised their score from weak reject to weak accept but the highest scoring reviewer explicitly was not willing to champion the paper and raise their score to accept. Thus the recommendation here is to reject. Taking the reviewer feedback into account, incorporating the proposed changes and adding more careful treatment of related work would make this a much stronger submission to a future conference."
"The author(s) posit a Mixture of Gaussian's prior for a compressed latent space representation of high-dimensional data (e.g. images and documents). They propose fitting this model using the Variational Information Bottleneck paradigm and explicate its derivation and tie it to the variational objective used by similar models. They empirically showcase their model and optimization methodology on the MNIST, STL-10, and Reuters10k benchmarks. <sep> The idea of using a latent mixture of Gaussian's to variationally encode high-dimensional data is not new. The author(s) appropriately cite VaDE (Jiang, 2017) and DEC (Xie, 2016), ""Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders"" (Dilokthanakul, 2017). However, the author(s) unfortunately did not mention ""Approximate Inference for Deep Latent Gaussian Mixtures"" (Nalisnick, 2016). Nalisnick et al. consider the exact generative process as the proposed by the author(s), but with the addition of a Dirichlet prior on the Categorical distribution. Nalisnick et al. fit their model with a VAE and circumvent Dirichlet intractability by positing a Kumarswamy stick-breaking variational posterior (the resulting distribution is on the simplex). They achieve 91.58% accuracy, but do so after fitting a KNN to the latent space. New techniques such as ""Implicit Reparameterization Gradients"" (Figurnov, 2018) and ""Pathwise Derivatives Beyond the Reparameterization Trick"" (Jankowiak, 2018) allow direct use of a Dirichlet posterior in VAEs. These methods are respectively implemented in the TensorFlow and PyTorch APIs. My point here is that there are many ways to fit this pre-existing model. From my review of these other works we have, for ""best run"" on MNIST, that author(s) > VaDE > Nalisnick > Dilokthanakul > DEC. Thus, the author(s) are SoTA for this particular generative process for their best run. It would be nice to see their standard deviation to gauge how statistically significant their results are. However, I am dubious of the SoTA claim as I detail later. <sep> The author(s) derivation was sound, but a bit confusing for me. In particular, I found keeping track of P's and Q's very burdensome after reading sections 2.1 and 2.2. In my experience, Q is typically reserved for a variational distribution that approximates some intractable P distribution, while P is used to describe the generative process (i.e. likelihood) and other exact distributions. Furthermore, one typically introduces the generative process first using P distributions. Once, I got past this confusion everything else made sense. I might suggest introducing the generative process first and with P distributions instead of Q's. The variational model can follow with the Q distributions as the author(s) have it. Equations 4, 5, and 6 all exactly match the unsupervised information bottleneck objective (Alemi, 2017)--see appendix B. I am therefore confident in those equations. I carefully checked their derivation of the VADe comparison (equation 10 and appendix B). Their derivation is straightforward. The principle trick is using the MC assumption C->X->U for P distributions to claim p(u|x,c) = p(u|x). Equations 12-16 all follow naturally from equation 11. If equation 17 is indeed an approximation or bound approximation, I would suggest not using the equals sign. Instead, consider another appropriate operator or rename Dkl to indicate it is the approximated version (just as in equation 24). <sep> If the author(s) like my suggestion regarding generative vs variational nomenclature, I would also change the second sentence of page 6 to something like ""We use our variational Qc|u distribution to compute assignments."" Thereafter, I would drop the star indicator for optimal parameters. These parameters are not necessarily optimal given the non-convexity of the DNN. Replace with, ""after convergence, we ..."" Similarly, drop optimal from line 2 of algorithm 1. <sep> Circling back to the experiments, the author(s) use reported values from DEC and VaDE. Those works compute cluster assignments using a KNN classifier on the latent space. This paper however uses the arg max of equation 19. I much prefer this article's method, but for comparison purposes, the author(s) should similarly use a KNN classifier on their latent space to compute accuracy in the same manner. The use of KNN in these other works allows them to consider a number of latent clusters larger than the number of true classes (e.g. 20 clusters for MNIST). I like that the author(s) stick to 10 clusters for MNIST, but for comparison I would have liked to see a KNN generated accuracy alongside their equation 19 based accuracy. It looks like the author(s) implemented VADe for STL-10 based on table 1. If so, it seems they could easily implement equation 19 for their STL-10 value for VADe. If not, please correct this. Not to belabor further, but I would really like to see a table 2 that reports both equation 19 and KNN accuracies when available. Namely, the author(s) should report both values for their model and can leave equation 19 accuracies blank for reported values. <sep> Lastly, I always raise an eyebrow when I see tSNE latent space representations. STL-10 was used to generate figure 6, where one needs dim(u) >> 2. However, MNIST can be well reconstructed using just 2 latent dimensions. In this case, tSNE is unnecessary. The author(s) state ""Colors are used to distinguish between clusters."" This statement is unclear as to whether the author(s) are using the class label or learned latent cluster. If it is the former, figure 6 makes sense in that it shows misclassifications (i.e. red dots in the green cluster). However, if it is the latter then I am concerned tSNE is doing something weird. <sep> To summarize, I enjoyed the paper. My only concern is novelty. Being the first to pair an existing model with an existing method, in my eyes, does not necessarily meet the *CONF* bar. The author(s) seemingly achieve SoTA, but without KNN-based accuracies for their model it is hard to compare to cited works. Having these KNN results and error bars would strengthen their case substantially to: achieving SoTA for an existing model by being the first to pair it with an existing method.","This paper proposes to use a mixture of Gaussians to variationally encode high-dimensional data through a latent space. The latent codes are constrained using the variational information bottleneck machinery. <sep> While the paper is well-motivated and relatively well-written, it contains minimal novel ideas. The consensus in reviews and lack of rebuttal make it clear that this paper should be significantly augmented with novel material before being published to *CONF*."
"*Synopsis*: <sep> This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. Specifically, they propose to cluster the embedding using the clusters to provide feedback to the agent by applying a positive reward when the agent enters the goal cluster. In more complex domains they add another negative distance term of the embedding of the current state and goal state. Finally, they provide empirical evidence of their algorithm working in toy domains (such as four rooms and U-maze) as well as a set of control environments including AntMaze and Pendulum. <sep> Main Contributions: <sep> - Using the embedding learned through Contrastive Predictive Coding for reward shaping. <sep> - A reward shaping scheme that seems generally applicable to any embedding. <sep> *Review*: <sep> I think the paper provides a compelling motivation, and is well written. I think using the embedding learned through CPC could provide meaningful semantics for representation learning and for reward shaping (as done in the current paper), and encourage the authors to continue down this line of inquiry. Unfortunately, I have several concerns over the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which I detail below. Given these concerns I am unwilling to recommend accepting this paper, unless several of these concerns are addressed. <sep> 1. This algorithm, by nature, is purely offline as the CPC and clustering all are currently done offline. Furthermore, the clustering portion of this approach requires states to be randomly sampled from the environment to create a nice set of clusters which are representative of the environment's full state space. These two requirements significantly limit this approach, especially when looking at domains where simulation is not possible, or the underlying state distribution is unknown. By and all, I don't think this means we should completely discount this method entirely and the authors do mention this as a detriment to their algorithm in section 6.3. I'm wondering if this paper should look at implementing an online version before publication, but think this is less prescient to the other concerns. <sep> 2. I am concerned about the current policy learning scheme (i.e. tiered policy (1) go to the correct cluster (2) go to the goal) which seems to only be used by your approach. This invalidates the comparisons made with the other reward shaping schemes as this goes beyond reward shaping (you are learning two separate policies). <sep> 3. The current competitors are unsatisfactory as they don't include other reward shaping techniques from the literature. Also the related works section seems to completely disregard this part of the literature. I would recommend comparing your approach to (methods you even cite!): <sep> - ""Reward Shaping via Meta Learning"" by Zou et. al https://arxiv.org/pdf/1901.09330.pdf <sep> - ""Potential based reward shaping"" by Gao et. al. https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/10930 <sep> (I'm sure there are others beyond these) <sep> 4. I'm also concerned with the current significance of the results, particularly AntMaze, Half Cheetah, and Reacher. I'm especially concerned because I don't feel your competitors are not relevant/representative of the current state of reward shaping. <sep> 5. It would be worthwhile to try this method on several RL algorithms (i.e. Q-learning, TRPO, SAC, DDPG, etc...). This will help readers understand if this approach is a general method, or only applicable to PPO. <sep> 6. I quite like the idea of predictive coding (albeit the original scheme presented by Rao and Ballard 1999) as an unsupervised representation learning scheme, but am unsure this is critical for your method and the current approach is not really predictive coding in a sense (or at least the ideas I'm familiar with from cognitive computational neuroscience). I am concerned with the predictive coding idea being highlighted here as a key ingredient, but none of the papers containing the originating idea of predictive coding are mentioned. Rao and Ballard is one, but there is a rich literature following from this work into the free energy formulation (Friston) and active learning. These ideas should appear in your introduction, as you heavily rely on them. Also there are other predictive coding schemes for unsupervised representation learning (such as PredNets from David Cox https://coxlab.github.io/prednet/), which I believe should be mentioned. In light of these other methods, I'm not sure your discussion on only using predictive features for reward shaping is accurate, and instead these claims should be softened for only features learned through CPC. <sep> Missing experimental settings: <sep> - Number of runs tested <sep> - What are the error bars in your plots? <sep> I would also like to recommend ""Deep Reinforcement Learning that Matters"" by Henderson for a reference on how to conduct meaningful Deep RL experiments using policy gradient methods (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16669). I think this paper could benefit from the recommendations made there. <sep> More questions/clarifications: <sep> Q1: What constitutes success in the grid world domains for table 1? <sep> Q2: If you were to train a single policy using your reward shaping scheme (i.e. not the tiered approach currently employed) how would the new policy perform? Are there problems with the current scheme where you have to have the tiered policy? <sep> Q3: Why CPC and not say an autoencoder or VAE? A comparison over many types of unsupervised embedding learning algorithms could be interesting and make your method more general than currently presented. It could also strengthen your argument for using CPC. <sep> Q4: How long were the trajectories for training CPC? <sep> *Other minor comments not taken into account in the review* <sep> - I think your labels are backwards in table 1.","The paper proposes to use the representation learned via CPC to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal. <sep> The reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by. One reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods. <sep> I recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere."
"This work examines the fundamental properties of two popular architectures -- PointNet and DeepSets -- for processing point clouds (and other unordered sets). The authors provide a new universal approximation theorem on real-valued functions that doesn't require the assumption of a fixed cardinality of the input set. They further provide examples of functions that can't be mutually approximated by PointNets and DeepSets. <sep> - It is important in order to know the representational power and fundamental limitations of basic algorithmic building blocks. However, for the two base architectures that are examined in this work, UATs where already provided in the original manuscripts. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance). The paper doesn't give any hints here. <sep> - The paper shows two specific functions, where one can be approximated by PointNet but not by DeepSets and vice-versa. The authors again note that this might vanish when a fixed cardinality of the input point cloud is assumed. Again, it seems that this is a mild restriction and I'd like the authors to elaborate on the importance of removing this restriction. <sep> - The paper is clearly targeted at a specialist audience and invokes dense and advanced mathematical concepts. It might find a better audience at a venue that is more specialized in this type of work (e.g. applied mathematics or more theory-focused machine learning venues). <sep> Summary: UATs are important and interesting, however, they do exist for the architectures that are targeted in this paper. It is not clear what is gained by the main difference, i.e. removing the cardinality. I'd be grateful if the authors could comment on this. <sep> Disclaimer: While I believe to have a reasonable mathematical background, I'm not an expert in this field and my assessment is primarily based on the bottom line of these proofs. <sep> === Post rebuttal update === <sep> I'd like to thank the authors for their efforts and additional insights. The additional illustration improves the accessibility of the paper. However, the rebuttal does not alleviate my concerns about additional impact beyond the UATs in the original paper. I thus maintain my recommendation of weak reject","The present paper establishes uniform approximation theorems (UATs) for PointNet and DeepSets that do not fix the cardinality of the input set. <sep> Two nonexperts read the paper and came away not understanding what this exercise has taught us and why the weakening of the hypotheses was important. The authors made no attempt to argue these points in their rebuttals and so I went looking at the paper to find the answer in their revisions, but did not find it after scanning through the paper. I think a paper like this needs to explain what is gained and what obstructions earlier approaches met, and why the current techniques side step those. One of the reviewers felt that the fixed cardinality assumption was mild. I'm really not sure why the authors didn't attack this idea. Maybe it is mild in some technical sense? <sep> What I read of the paper seemed excellent in term of style and clarity. I think the paper simply needs to make a better case that it is not merely an exercise in topology. I think the result here is publishable on its own grounds, but for the paper to effectively communicate those findings, the authors should have revised it to address these issues. They chose not to and so I recommend *CONF* take a pass. Once the reviewers revised the framing and scope/impact, provided it doesn't sound trivial, I think it'll be ready for publication."
"This paper attempts to study if learned word embeddings for common objects contain information about ""numerical common sense"". The hypothesis is that certain numerical information may co-occur with the words for certain objects/measurement units within their context windows. To verify this hypotheses, the authors have created a dataset through a crowd-sourcing service which represents ""numerical common sense"". Using this dataset, the authors examine the predict abilities of regressors trained on learned word embeddings and the aforementioned crowd-sourced dataset. The hypothesis is that if the regressors demonstrate good accuracy, then the word embeddings contained information relevant to ""numerical common sense"". To the best of my knowledge, this is the first paper that attempts to analyze learned word embeddings in the context of numerical common sense. <sep> This paper should be rejected because (1) the NCS datasets are too small to represent ""numerical common sense"" (2) the NCS datasets contain faulty data points and (3) the results from the experiments conducted are not sufficient to accept or refute the hypotheses. <sep> Main argument <sep> The first question that we must ask is - are the NCS-50x1 and NCS-60x3 datasets reliable for experiments on ""numerical common sense"". No, because of two flaws: <sep> (1) The number of samples in the dataset is too small to represent ""numerical common sense"". Consider the histogram for object ""dog"" in Figure 2. If the largest data point in this plot was absent, the average of the distribution would be smaller by several orders of magnitude. Perhaps there are other objects in the dataset which are missing samples from the tail end of the distribution that could have large effects on the mean of the collected dataset. <sep> (2) Some data points in the dataset don't make sense to me. For example, Fig 2 represents the ""small"" dataset, yet I see samples like 400m long dogs, 40m long cats, 150m long monitors and 20m long mice? <sep> Also, it is not clear how the confidence scores of the participants were taken into account when training the regressors or if they were used at all. <sep> If the NCS dataset does not represent ""numerical common sense"", it invalidates all experimental results from the paper. <sep> My second issue with the paper is that it is not possible to conclude if the experimental results support or refute the hypothesis (ignoring the issue with the dataset): <sep> 1. In tables 2 and 3, the correlation coefficients were quite low and and the MAEs were pretty large. In Table 3, rows 1 and 2, even though the correlation is 0.57 and 0.48, the MAE is 100 million yen and 7.5 million yen respectively which is quite large. To me this suggests that just because the correlation is larger we cannot conclude mean that the model is performing well. <sep> 2. It is unclear why the correlation coefficient was chosen to decide that ARD is the superior model in experiment 1. The MAE for random forests with concatenated feature vectors was an order of magnitude smaller than that of the ARD model. <sep> 3. Why are the correlation coefficients missing for the unit-only experiment in Table 2? The LS model shows very good MAE relative to the other models and perhaps the correlation should have been measured for that as well? In fact, if the correlation coefficients for this case is comparable to the case with concatenated features, it would mean that the word embedding for the object is not helping at all! Moreover, I find it surprising that the LS model with concatenated features performs worse than the unit-only features. We cannot conclude if paper's interpretation about the results is correct unless this missing information is provided. <sep> 4. It is hard to judge what a correlation coefficient of 0.57 means. Why didn't you provide a scatter plot of the predictions vs targets as well? It often happens that even noisy plots demonstrate good correlations. <sep> 5. The paper should have additional ablation studies - for example, what would happen in the concatenated feature vector experiment if you trained the regressors using randomly initialized word embeddings instead of the trained word embeddings? Do you get the same performance as learned word embeddings?","The authors tackle an interesting and important problem, developing numerical common-sense. They use a crowdsourcing service to collect a dataset and use regression from word embeddings to numerical common sense. <sep> Reviewers were concerned with the size and quality of the dataset, the quality of the prediction methods used, and the analysis of the experimental results. <sep> Given the many concerns, I recommend rejecting the paper, but I encourage the authors to revise the paper to address the concerns and resubmit to another venue."
"This paper proposes to apply MAML-style meta-learning to few-shot semantic segmentation in images. It argues that this type of algorithm may be more computationally-efficient than existing methods and may offer better performance with a higher number of examples. They further propose to perform hyper-parameter search to choose a new learning rate for the inner learning process after optimizing for the network parameters. <sep> As far as I know, this is the first paper to apply gradient-based (i.e. MAML-style) meta-learning to this specific problem. Existing approaches to few-shot semantic segmentation have mostly used multi-branch conv-networks to condition the output on the training examples. This paper shows that (FO)MAML achieves similar accuracy to the FSS-1000 baseline. This is an empirical contribution in itself. The paper also demonstrates that the EfficientNet architecture can be applied to segmentation. <sep> Major concerns: <sep> (1.1) The improvement obtained by the hyper-parameter optimization seems quite marginal (79.0 - 81.4 and 73.3 - 73.9) and there is no study of the variance of the results. The fact that better performance is obtained by tuning the learning rate on the *training* set suggests to me that the improvement might not be significant. You could repeat the experiment by sampling multiple different training and testing sets (with different classes) to estimate (some of) the variance. <sep> (1.2) The formalization in Section 4 is mathematically appealing but seems unnecessary. In the end, the paper is essentially arguing that it's better to use different learning rates (for the inner loop) during meta-training and meta-testing. This seems obvious, since this includes equal learning rates as a special case. The paper proposes to optimize the latter learning rate using a gradient-free method. This argument can be made without considering generalization bounds. <sep> (1.3) One of the central claims of the paper is that ""meta-learned representations smoothly transition as more data becomes available."" It's not entirely clear what this means. I suppose it means that, with few shots, it should perform as well as existing few-shot methods, but with many shots, it should perform as well as a standard learning algorithm. The paper failed to present any evidence that existing algorithms for few-shot segmentation do not satisfy this property. It would strengthen the argument to include an existing few-shot segmentation algorithm in Figure 2. <sep> (1.4) The details of the experiment in Figure 2 are not clear. Was a different number of iterations used when there are hundreds of examples? Were different hyper-parameters used when optimizing from a pre-trained checkpoint? It would be unfair to use the same hyper-parameters which had been optimized specifically for the meta-learned initialization. <sep> Other issues: <sep> (2.1) It's not clear what it means to achieve human-level performance in the few-shot task and the many-shot task. What is human-level performance at few-shot segmentation? It seems to me that humans are capable of segmenting novel objects (i.e. zero-shot) with almost perfect accuracy. Does this mean that your method should achieve the same accuracy with few- and many-shots? <sep> (2.2) The use of early stopping was unclear. Do you use a fixed number of SGD iterations during training and a variable number of iterations (determined by a stopping criterion) during testing? However, this seems to be contradicted by the statement that the UHO algorithm determined an optimal number of iterations (8) for testing? On the other hand, this seems like too few iterations with hundreds of shots. Maybe the automatic stopping criterion was only used with many shots? Or maybe training proceeds until either the stopping criterion is satisfied or the maximum number of iterations is reached? Furthermore, the early stopping criterion was not specified. <sep> (2.3) Missing reference: Meta-SGD (arxiv 2017) considers a different learning rate for every parameter and updates the learning rates during meta-training . <sep> (2.4) There was no discussion of the running time of different methods. This would be particularly interesting in the many-shot regime. How slow are the RelationNet approaches? <sep> (2.5) It is claimed that Figure 1 demonstrates that  ""the estimated optimal hyperparameters for the update routine ... are not the same as those specified a priori during meta-training"". However, it seems that the optimal learning rate is awfully close to the dotted blue line (within the variance of the results). <sep> (2.6) For the IOU loss (equation 10), what are the predicted y values? Are they arbitrary real numbers? Do you use a sigmoid to constrain them to real numbers in [0, 1]? <sep> Minor: <sep> (3.1) It is not worth stating the optimal learning rate to more than 3 or 4 significant figures. <sep> (3.2) Use 1 \\times 1 instead of 1 x 1. <sep> (3.3) Is there a reference for the Dice score? Where does the name come from?","The reviewers reached a consensus that the paper was not ready to be accepted in its current form. The main concerns were in regard to clarity, relatively limited novelty, and a relatively unsatisfying experimental evaluation. Although some of the clarity concerns were addressed during the response period, the other issues still remained, and the reviewers generally agreed that the paper should be rejected."
"The authors survey a wide variety of implementation-level and hyperparameter decisions in reinforcement learning for continuous control tasks. They train over 250.000 agents with different settings and suggest empirical guidelines. <sep> As the authors indicate, there's not too much related work so one could call this work pioneering: The sort of work conducted by the authors is crucially important for a field afloat with tricks and tweaks, many of which are typically not discussed in the scientific literature due to a misplaced conceit around this being ""not research, just engineering"" entirely absent from established fields of science such as experimental Physics. It is also typically not done as it is just plain hard to do. Combined with the often lackluster response from the community, the cost-benefits trade-off has just not been worth it, especially for junior researchers. It's all the more commendable that the authors have engaged with this formidable task of bringing some of the ""secret sauce"" out of the heads of senior engineers in the various labs and into published and peer-reviewed science. <sep> The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec. 3.1), Networks architecture (Sec. 3.2), Normalization and clipping (Sec. 3.3), Advantage Estimation (Sec. 3.4), Training setup (Sec. 3.5), Timesteps handling (Sec. 3.6), Optimizers (Sec. 3.7), and Regularization (Sec. 3.8). The high-variance nature of training RL agents makes it such that the individual factors in these configurations often have surprising non-linear cross-relations such that the problem space cannot be evaluated incrementally (i.e., it's often not possible to establish ""the best"" architecture first and select the right learning rate afterwards). The authors propose a novel approach of considering for each choice the distribution of values among the top 5% configurations trained in that experiment. Their experimental design is such that the values for each choice are distributed uniformly at random and thus if certain values are over-represented in the top models this indicates that the specific choice is important in guaranteeing good performance. <sep> As for improvements on the paper, I have one major and only a few minor comments. My major comment is that the paper does not indicate anywhere that the research code is released, only that it's based on SEED RL. I believe an authoritative public implementation of the configurations considered would be extremely worthwhile, both for the community and the authors. If they haven't already done so (there's no supplements to this submission and I refrained from doing any web searches to preserve anonymity), I'd urge the authors to invest the time to release a (possibly cleaned-up) version of their code. <sep> As for minor comments, I'm not clear about the philosophical distinction of something being ""due to the algorithms or due to their implementations"" (in the Introduction). I very much see the point the authors are making, which is an important one -- what makes RL results work is often ""nitty-gritty"" details not mentioned in the main part of the relevant publications (and often just barely mentioned in appendices). However, in the strictest sense, the algorithm very much is the implementation -- that's what produces a given result. It's worthwhile to keep the distinction between an idea (say, PPO) and a given implementation of that idea (e.g., presumably the authors had to re-implement PPO in TF2 when using SEED RL and couldn't use OpenAI's original implementation). It's also fine to call the idea ""the algorithm"", but I'd have preferred to see this distinction more clearly defined. <sep> Somewhat related: The authors are very much correct about what they call ""standard modus operandi of algorithms [...] such as PPO"", namely iterating between generating experience using the current policy and using the experience to improve the policy. I'd add that strictly speaking no iteration is necessary, as for instance IMPALA, coming from the A3C line of development, does both asynchronously in parallel, and I suspect so do the authors given their use of SEED RL. My suggestion would be to slightly rephrase this sentence and mention IMPALA along with PPO. Perhaps there could also be a comment somewhere about what constitutes ""PPO"" (or ""IMPALA"") -- e.g., IMPALA consists of (1) an asynchronous actor/learner split [with further choices of when/how the weights are copied from learner to actor, see e.g. this comment], (2) a specific type of policy-gradient loss, v-trace, (3) a specific neural network architecture, optionally including recurrence via an LSTM and potentially even (4) a specific type of preprocessing for environments such as Atari or DMLab [according to some papers, swapping out the implementation of the frame-downscaling algorithm in Atari has measurable impact on final performance -- this will matter a lot when evaluating re-implementations of an ""algorithm""]. I'd like for the authors to take on this opportunity and propose a common language to discuss these distinctions, which in practise are often confusing to junior researchers (and some senior researchers, too). <sep> Further related to IMPALA and v-trace, I was surprised about the word ""unsurprisingly"" and the explanation in ""Perhaps unsurprisingly, PG and V-trace perform worse on all tasks. This is likely caused by their inability to handle data that become off-policy in one iteration, either due to multiple passes over experience [...] or a large experience buffer in relation to the batch size."" While the results speak for themselves, my understanding of v-trace was that it was specifically designed for the very goal of dealing with the ""slight"" off-policiness produced by asynchronous actor/learner splits in a PG setting. Perhaps the authors have an intuition I'm lacking at this point, but if so I'd appreciate further elaboration. <sep> As a final and perhaps trivial comment, I was slightly irritated by the notation/typography for the inverse cumulative density function of a binomial distribution. In LATEX, the symbols icdf read as the in-context nonsensical i⋅c⋅d⋅f while the authors would presumably want to use icdf (compare exp(x) vs exp⁡(x) or sin(x) vs sin⁡(x)). I'd propose \\mathrm{icdf}  as the correct syntax for this in LATEX. <sep> In follow-up work, I'd like to see a similar paper for various ""discrete RL"" tasks (a subset of Atari, VizDoom, DMLab, MiniGrid, BabyAI, ProcGen, and perhaps even Obstacle Tower, Minecraft, StarCraft (I or II) or the recent NetHack environment) with similar factors of configurations. I assume this is a task yet more daunting, but no less useful to the overall community of researchers. <sep> Overall, this is a strong paper and I recommend it for publication.","There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code). <sep> This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings."
"Summary <sep> This paper shows a relationship between the project rule weights of a Hopfield network (HN) and the interaction weights in a corresponding restricted Boltzmann machine (RBM). The mapping from HN to RBM is facilitated by realising that the partition function of BN can be seen as the partition function of a binary-continuous (Bernoulli-Gaussian) RBM. The authors comments on the mapping from RBM to BN. The experiments show the advantages of training RBM with weights initialised from BN projection weights in generation and classification. <sep> Strong points: <sep> I am not familiar with the literature, but the results seem new to me. <sep> The experiments show advantages of BN initialisation, pointing to new directions of improving RBM training. <sep> The paper is fairly clearly written. <sep> Weak points <sep> The HN -> RBM mapping is quite clear, but the reverse RBM -> HN mapping is not very well established, and there are no experiments showing how effective the approximate reverse mapping works on associative memory tasks typical for HNs. I also believe this lowers the impact of this paper, given that the forward mapping is based on a simple revelation. <sep> The authors' description of the experimental results are not accurate enough. and the results raise several questions to be addressed. <sep> Recommendation <sep> I'm in favour of rejection, but some concerns can be addressed fairly easily (with experiments) so I'm open to raising my score if questions are well-addressed. <sep> Issues and questions to address <sep> The authors should provide experiments on the reverse mapping as suggested above. <sep> I do not agree that figure 3 shows that RBM training ""simply 'fine tunes'"" the weights -- the difference is quite stark. How about increasing the batch size so that there is little SGD noise? <sep> Figure 4a: traces are cut-off just when random initialization is catching up with HN initialization. This also applies to Figure 5. <sep> There are a few descriptions suggesting ""HN init. appears to train much faster than random init"". However, the rate of increase in of likelihood in Figure 4 is shallower for HN than for rand init. Is the advantage only at the 0'th RBM epoch? <sep> The author only compared with purely random initialisation, which is perhaps the most naive baseline. I would suggest comparing to a (slightly) more clever initialisation, perhaps PCA or something better (those mappings in previous work the authors cited and in Appendix B). Or, the authors could also initialise the RBM by first training it on the within-class cluster centres (using a very large number of sleep samples for the sleep-phase) which may also be a more fair comparison? <sep> In the classification objective, if I understand correctly, the feature function is essentially quadratic in the input patterns. Should there be an ideal test error that is computed by a quadratic neural network trained with supervision by backpropagation? If the HN classifier (blue in Figure 5) is approaching this idealization, then this will strengthen the claim. <sep> The discussion on the extension to more generic, deep architectures is not well supported, and I do not see the extension to be so straightforward given the content of the current paper. In generation, supervised labels and clustering are used to simplify learning. Is the network able to learn just on the MNIST digits, even for the real images within a single class (e.g. ""7"")? <sep> Can the authors try to characterise whether the HN initialisation is related to log-likelihood training? I wonder if there is any interesting theory; otherwise, measuring model performance by log-likelihood seems a bit arbitrary (though it makes the comparison to contrastive divergence easier). <sep> Detailed suggestions (not to affect decision) <sep> Reference to RBMs should include more historic ones from Hinton (e.g. 2006) <sep> I do not see the purpose of (7) and (8), and they are only referred to in the Appendix (the review content in the Appendix is informative by itself though). <sep> Eqn (11), should it be wμwμT in the sum? <sep> Third line above (16), should H and Z be indexed by μ? <sep> Line above (D.4), WWT=…BpT? <sep> ==== update ==== <sep> I thank the authors for providing such detailed response. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. I hope to see this paper accepted.","Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks. They considered the work very well developed, theoretically interesting and also of potential practical relevance. A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments. Following the authors' responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted. With the unanimously positive feedback, I am recommending the paper to be accepted."
"Pros: <sep> This paper nicely unifies two different classes of approaches (NAS + sparsity) for determining the topology of neural networks. They are combined into a single optimization problem, with binary indicators on network components and connections. <sep> Experiments illustrate the behavior of the method. It is good to see that the experiments dig a big deeper than end-result accuracy. For instance, the ""budget-aware growing"" is shown well to work as described by Fig 3. <sep> Cons: <sep> No attention to random seeding. <sep> The sparsification dynamics seem likely to change somewhat from one run to the next. The submission does not describe how random seeding was done for training. Multiple runs with different seeds are not shown, and the distribution of accuracies across runs is unknown. Attention to randomness for this kind of training process seems especially important given the variances in results in the Lottery Ticket hypothesis paper. <sep> No comparison to simple random baseline. <sep> A large portion of the method consists of a search method over the space of possible sparse networks, combining it with growing the network to get a NAS-like method. It has been observed, though, that in a sufficiently general space of this kind one can randomly sample connections and see high accuracies. So to identify the sources of empirical gains, it is good to consider experimental baselines, such as random sampling, that separate the contribution of the search space and the search method: <sep> Xie et. al. ""Exploring randomly wired neural networks for image recognition"" <sep> Li & Talkwaker ""Random search and reproducibility for neural architecture search"" <sep> Yu et. al. ""Evaluating the Search Phase of Neural Architecture Search"" <sep> Radosavovic et. al. ""On Network Design Spaces for Visual Recognition."" <sep> Note that the submission's method also is randomly choosing connections, through a somewhat involved process that also accounts for the observed sparsity of G during training. The simplest baseline seems to be the ""uniform pruning"" described in Section 4.4. This only ablates part of the method that doesn't seem to meet the same criterion here. <sep> Incomplete illustration of the cost/accuracy tradeoff. <sep> The gold standard for comparison in both sparse-neural-network papers and NAS is to consider the accuracy at a range of different model costs. See for example: <sep> Blalock et. al. ""What is the state of neural network pruning?"" Figs 1, 3 <sep> (Yang et. al., 2018) Figs 5-9 <sep> This clearly illustrates whether a method is overall better (i.e. produces better models across the entire pareto frontier), or is only better for some ranges or on one metric. For a result like the first one in Table 5 in the submission, it is unclear which model is better: they may simply be considering different points on the same cost/accuracy curve. <sep> As a less important aside, ""budget-aware growing"" seems to be an ad-hoc reinvention of something similar to an Augmented Lagrangian method. Explicitly describing the differences from standard optimization techniques might be good. <sep> Reasoning for rating: <sep> While the experiments are extensive, I think they miss the key comparisons that show how useful the method and each of its components is. Given that many different innovations are included in the submission, it may be a muddle for follow-up research to sort out how good each individual one is. <sep> Misc comments <sep> Check spacing around (6) in Algorithm 1 <sep> Colon instead of comma after ""trainable variables"" in §4.1 <sep> ""For better analyze the growing patterns"" -> ""To better analyze the growing patterns"" on page 14 <sep> Wortsman et. al. ""Discovering Neural Wirings"" is another closely related work at the intersection of NAS and pruning. (with major differences from the submission) <sep> After rebuttal <sep> The authors have gone above and beyond in providing additional experimental results. All of the points raised above that deal with methodological issues are completely addressed. <sep> The sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions. I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method.",The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation.
"SUMMARY <sep> The submission proposes a score-based generative model, which uses an SDE to map the data distribution to a simple noise distribution and the corresponding reverse-time SDE to generate samples by mapping the noise to the data space. The proposed model builds upon and generalises two existing models (SMLD and DDPM) by transforming the data using a continuous SDE dynamics as opposed to perturbing the data with a finite number of noise distributions utilized by these models. <sep> ################################################################## <sep> REASON FOR SCORE <sep> The paper provides a clear motivation for the proposed modifications to SMLD and DDPM, <sep> as well as a clear technical description of these modifications, their analysis and discussion. <sep> I think the proposed model and sampling algorithms offer substantial conceptual improvements to the existing models and should be of interest to the community. The paper is well written and structured. <sep> ################################################################## <sep> PROS <sep> Clear motivation for the work. <sep> Detailed technical description of the proposed models. <sep> Interesting discussion of similarities and differences between SMLD, DDPM, and the SDE <sep> based model, as well as corresponding sampling algorithms. <sep> Extensive experiments. <sep> ################################################################## <sep> CONS <sep> I found the discussion of equivalent neural ODE and its differences to reverse SDE <sep> a bit short, especially given that it is used in multiple experiments. <sep> The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly, leaving it unclear if using a general SDE would require relatively simple changes, or if the proposed model is limited to SDEs derived from <sep> SMLD and DDPM. <sep> ################################################################## <sep> QUESTIONS and COMMENTS <sep> Is it correct that the function \\sigma(t) in Eq. (6) is assumed to be monotonic and bounded by \\sigma_max, while \\beta(t) in Eq. (8) is bounded by 1, but doesn't have to be monotonic? <sep> In the case of general SDE for noise perturbations in Eq. (9), are there any assumptions on drift and diffusion function (such as monotonicity or boundedness)? <sep> In the case of SDEs (6) and (8), \\nabla_x p_{0t} (x(t)) is available in closed-form. Is it always necessary to have such a closed form expression in order to compute the objective (11), or can it be estimated somehow without it? (I guess for a general SDE, there is typically no closed-form \\nabla_x p_{0t} (x(t)) available) <sep> Why do you think the FID values for the PC sampler with corrector are higher than without it for VP SDE? (Table 1b) <sep> In section 4.2: ""[...] PC samplers significantly outperform the corrector-only method, and can improve over predictor-only approaches for most cases without extra computation."" Why does full PC sampler (with predictor and corrector) not incur extra computation in comparison to prediction-only approaches. Don't we need to evaluate the approximate score function s_\\theta(x, i) for each of the M steps in the corrector sampler? <sep> In section 4.3: ""[...] deterministic process whose trajectories induce the same evolution of densities"". Does it mean that ODE (12) and reverse SDE (10) map the same noise distribution p(x(T)) to the same distribution in the data space? If so, are there any advantages of using a reverse SDE instead of equivalent ODE if the latter is easier to solve numerically and admits an exact computation of likelihoods?",All reviewers agree that this is a well-written and interesting paper that will be of interest to the *CONF* and broader ML community.
"The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs. The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x, z) VAE space using reparameterization. The method is shown to achieve high quality samples on several modern image datasets, good FID scores and mode coverage. Ablation studies show the contribution of the different elements. <sep> This is, in my opinion, a very good work, which combines a novel and well-motivated idea with clear writing and extensive experimental evidence. <sep> Some comments and questions: <sep> Does the separate twos-stage training enable the model to reach the optimal point that can be reached in joint training, or is it an approximation? If its an approximation, I think it should be discussed or perhaps bounded. <sep> Does the combined model allow computing the likelihood? Can it be evaluated and compared to other models in terms of bits/dimension (e.g. as in VAE or NVAE)? <sep> It might be interesting (not something that I think is mandatory) to measure the NVAE log-likelihood of samples generated by the combined model compared to samples generated just by the NVAE. <sep> To summarize: <sep> pros: <sep> novelty significance experimental evidence quality of writing cons: <sep> combining two separately trained models - perhaps sub-optimal <sep> Update: I thanks the authors for their answers and revised version and keep my positive rating.","This work presents a method to combine EBMs and VAEs in two stages. First, the VAE model is learned; second, an EBM-based correction term is learned via MLE. The methodology is novel and of interest to the *CONF* community."
"Originality, Significance: This paper establishes reference points for modern LTR research. The fact that RankLib is a very popular but also a weak baseline has been exploited by too many researchers for too long. When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM. This fact has been fairly well known, but apparently not widespread enough. Having an *CONF* paper published on this issue will help spreading the fact, which is significant on its own. <sep> Quality: Considering the popularity of RankLib, deeper analysis of why LightGBM outperforms RankLib would've been very nice, however. Authors do mention that LightGBM has more features, but it is unclear which exact feature of LightGBM contributed to such a significant difference between the two. Understanding the reason for LightGBM's superiority could potentially help us to develop better LTR models, neural or not neural. Comparison against Catboost https://github.com/catboost/catboost would've also been useful, as it is often claimed to outperform LightGBM. <sep> Proposed DASALC framework is quite simple and uses mostly standard techniques, and this is an advantage as a reference point. Still, DASALC significantly outperforms previous neural LTR approaches. Also, although the idea of applying these standard techniques on LTR seems straightforward, but I argue that's only due to the benefit of hindsight; neural LTR has been a fairly active area of research, yet these techniques haven't yet been widely used in LTR literature. It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well. <sep> In summary, I believe this paper will foster more productive research by establishing the strong baseline on both decision-tree based method (although it has been known) and neural method (on which authors make good technical contributions). <sep> Clarity: The paper is quite easy to follow.","Three reviewers are positive, while one is negative. The negative reviewer is well-qualified, but the review is not persuasive. Overall, this paper should be published as a wake-up call to the research community. Unfortunately, the lesson of this paper is similar to that of several previous papers, in particular <sep> Armstrong, T. G., Moffat, A., Webber, W., & Zobel, J. (2009, November). Improvements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 601-610). <sep> This submission should be a spotlight, to maximize the chance that future researchers learn its lesson."
"Summary of the paper: This paper studies the problem of robustness against word substitutions. The authors propose a novel Adversarial Sparse Convex Combination (ASCC) method in which they model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution. Based on the ASCC, they also propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experimental results show that their method outperforms the existing SOTA on two tasks -- sentiment analysis and natural language inference. <sep> Strength of the paper: <sep> The idea proposed in the paper is quite straightforward yet effective. Using a convex hull could satisfy the three aspects as stated by the authors -- Inclusiveness, Exclusiveness, and Optimization. The experimental results do show the advantage of using the proposed convex hull. <sep> Besides the robustness of the model, it also achieves robustness over word vectors. <sep> The experiments are well designed including both qualitative analysis, quantitative results, and reasonable ablation to show the effectiveness of their method. In general, the paper is well-structured and easy to follow. <sep> Question for the authors: <sep> I am curious about how much headroom there still exists for this task? Let's say if we have a perfect way to defend against such kind of attacks, how good are the current approaches? <sep> What is the computational cost of the proposed method compared to others? In equation (3), the authors compute wij through a softmax parameterized by wij^. If the substitution set of a word is infinite or too large, how are you going to deal with such kind of situations? <sep> Seen from Table 1, the proposed method is much better than other models at PWWS attacks than the genetic attacks. Can you give an intuitive explanation of why? <sep> Reason for score: Overall, I vote for accepting this paper. I like the idea of using a convex hull and the way they regularize the model to achieve sparsity. It would be helpful if the authors could address the questions raised above.","All three reviewers are positive, and the authors have addressed essentially all the questions raised by the reviewers. The main insight of the paper is clear, and the empirical results are good, so a spotlight is deserved."
"Summary: This work focuses on the study of (global) convergence and gradient dynamics of a recently proposed family of models, the deep equilibrium  (linear) models (DELM) under common classes of loss functions. Exploiting the Neumann series convergence and the PL inequality analysis, the authors proved convergence to global optima of DELM without prior assumption on the width m of the model (relative to the number of data n). <sep> Here is my general opinion: <sep> While deep linear models in general (as the authors acknowledged) has been widely studied, and despite the extreme simplicity of the DELM when compared to the original DEQ model that Bai et al. [1] studied, I found this work interesting and important in establishing a solid foundation for the theoretical study of this class of implicit-depth models. The authors managed to demonstrate that the gradient dynamics and convergence assumptions of DELM is indeed different from typical ""stacked"" deep linear models that prior approaches study, such as deep linear ResNet. And throughout the arguments of the paper and the proof in the appendix, I can tell how the ""equilibrium"" property of DELM is making the story different, and think this paper sets a good starting point for future similar in this direction for general implicit-depth models. But still, the paper has a limited scope in terms of the structure it studies. <sep> Pros: <sep> One of the first theoretical works on the gradient dynamics and convergence properties of the deep equilibrium models [1] (and implicit models in general [2,3]), which are quite different from conventional deep networks. <sep> Clear notation and theoretical insights, with proof relatively easy to follow. The proof seems overall correct (there are some that I didn't check closely though). <sep> Clear discussion of the relation, including (and especially) the differences of the prior analysis on deep linear neural networks. <sep> Cons: <sep> The very definition of DELM, which the author provided a particular formulation of, is of limited scope (see my comment below that expands on this point). <sep> The empirical studies to validate the conclusion of the theoretical results could be strengthened. <sep> I have some comments/questions for the authors, detailed below: <sep> The major limitation that I found while reading the discussion and the proof of this paper is that while the authors claim to study deep equilibrium linear models, the insights mostly only apply to the models converging with Neumann series guarantee, and can be written in the form BU−1ϕ(x). I understand the motivation for fixating a provably convergent equilibrium model formulation. But: <sep> a) A provably convergent deep equilibrium linear model doesn't need to be Neumann (for its Jacobian) for the implicit function theorem to work. In the simplest case, the fixed point of a function h(x) on 2D can have a local derivative with absolute value > 1. This certainly implies that repeatedly unrolling the function h(x) may not converge, and yet there still is a unique fixed point and one can reliably solve for it (see [4]). However, without the nice Neumann series form, which allows one to write (I−γσ(A))−1 as a closed-form representation for the ""infinite-depth"" network forward pass, I don't think the theorems will hold directly. Typically, the (I−J)−1 term should only appear in the implicit function theorem, which is used for the backward pass. I expect the authors to clarify this further. <sep> b) The very design of σ(A) in the model the authors study is a bit bizarre to me. Why applying a ""softmax"" on the weight? Is it just to ensure that proposition 1 holds (i.e., that you have a handy, provably-convergent linear model)? The authors stressed a few times that the h(⋅) function is thus ""non-linear"" w.r.t. σ(A), but I fail to directly see why it matters so much as the model is still linear w.r.t. the input (it's really a one-linear layer; though the inverse from Neumann does make a difference on A), and in terms of the gradient dynamics, the major difference this makes will merely be ∂σ(A)∂A. I might have missed something here and would appreciate if the authors can clarify. <sep> I didn't quite get the specific point the authors were trying to make in Section 3.2 in terms of the implicit bias. Could you expand on that? <sep> Overall, I feel that the empirical support of the theoretical findings can be stronger, for instance, by inspecting different initialization (A0,B0), or validating the radius discussion at the end of Section 3.1 for the logistic loss. Like in Zou et al. [5], some synthetic data could probably work just fine. What is the reason for only using 200 images from the MNIST/CIFAR datasets? Is it to keep the size of Φ small (but I didn't see the authors report anything about it in Section 2.2). And since the primary purpose of Sec. 2.2. is to ""discuss whether the model would also make sense in practice"" (which I take to mean that you only want to compare the test accuracies coming out of these models), wouldn't a 200-sample version of MNIST/CIFAR too small to draw a robust conclusion on this? <sep> One that I think could be useful for further thought is the convergence property not just for GD, but also SGD, like Zou et al. provided in [5] (up to a probability). <sep> Minor things: <sep> i) Page 3: ""the outputs of the deep equilibrium linear models fθ(x)=⋯ are nonlinear and non-multilinear in the optimization variables (A,B)."" Non-linear in even B? <sep> ii) Page 14: Vq∗ --> Bq∗ <sep> iii) Page 16: ∇FL(A,B) --> ∇AL(A,B) <sep> [1] https://arxiv.org/abs/1909.01377 <sep> [2] https://arxiv.org/abs/1908.06315 <sep> [3] https://arxiv.org/abs/2009.06211 <sep> [4] https://arxiv.org/abs/2006.08591 <sep> [5] https://arxiv.org/abs/2003.01094","The paper analyzes the gradient flow dynamics of deep equilibrium models with linear activations and establishes linear convergence for quadratic loss and logistic loss; several exciting results and connections, solid contribution, accept!"
"Summary: <sep> This paper considers communication games when agents use experience replay. The agents' communication protocol may change over time, leaving outdated symbols in the replay buffer which are then trained on. This paper proposes replacing the old communication actions with up-to-date actions as the transitions are sampled, and shows that this leads to greatly improved convergence speed and higher performance plateaus. <sep> Positives: <sep> The problem of multiagent communication and how to learn it is important and relevant to the *CONF* community. The solution presented here seems like a natural fit with the problem and popular agent architectures and is well presented. <sep> The paper is well motivated and well written.  Overall it was an enjoyable and easy read! <sep> The experiments in Figures 4, 5, and 6 seem like great choices to show the strengths of the approach.  They're simple, well described, and well targeted. <sep> Negatives: <sep> I feel like there's a pretty obvious question about ""What happens in richer domains?"" that (unless I missed it) isn't addressed in the paper - I'll expand on that in my 'Questions to clarify recommendation' section below. While the technique seems to work very well in the experiments chosen for the paper, I wish the paper touched a bit more on upcoming challenges, possible foreseen problems, and next steps. <sep> Recommendation and Justification: <sep> Overall, I feel like this was a strong paper and should be accepted. My only real negative was that I am excited to know more about what comes next. <sep> Questions to clarify recommendation: <sep> The three environments presented in the paper, if I've understood them correctly, are pretty straightforward in that 1) the speaker only has communication actions (and no environment actions), and 2) seems to only have one consistent message to communicate during the entire episode (after perhaps waiting to receive a message from others, in Hierarchical Communication). But right from the abstract onwards, I was wondering about possible problems in richer domains, where it seems like this technique could be harmful. Specifically, what if by updating the old communication action to one chosen by the current policy, we present a communication action that no longer aligns with the old environment action, which we do not update?  I felt like this was a pretty natural question, but unless I missed it, the paper doesn't mention possible problems like this. <sep> I'll ground this in an example, similar to Cooperative Communication.  Imagine a two-player gridworld where the players cannot see each other, but are rewarded for arriving at the same map location.  Similar to Bach and Stravinsky / Battle of the Sexes, each player has a different preference over locations, but being at the same location is most important. Let's call the locations Left and Right.  To enable coordination, let one player be a Speaker that can take communication actions to signal the other player as to where they should meet in that episode. While that permits greedy Speaker policies (always announce their preferred location and then go there) and greedy Listener policies (always go to their preferred location, regardless of Speaker's announcement), it would also allow the speaker to arrange a correlated equilibrium: announce a randomly chosen location in each episode and then go there, to maximize joint reward beyond any greedy Nash equilibrium policy. <sep> Here's where I see a possible failure with the technique in this paper.  Assume that the replay buffer contains an episode where the speaker emitted symbol L (for left) and then took environment actions to move to the Left location.  Later in training, using the technique presented here, we might sample this experience, update the symbol to L', and still move Left.  As described in this paper, I would expect that should work, and converge faster than by using the out-of-date symbol L.  However, it seems possible that the newer Speaker policy might prefer to move Right on that episode instead.  Updating the symbol would change it from old L to new R', but since the technique does not (and cannot, without a world model) update the environment actions, it seems like the listener and speaker would then train on this misaligned tuple of communication action R' and environment actions to move Left.  I would expect this confusing example to be much worse than training on the original example with the outdated but still aligned communication actions. <sep> More generally: how can we make sure that the updated communication actions still align in intent with the agent's environment actions that we cannot change? It seems like conditioning the communication action on the agent's environment action for that timestep might help, but would only be a partial solution: if an agent must speak now but take their first significant environment action in the future, we would have the same problem. <sep> My questions regarding this point are: <sep> Do you agree that this could be a problem in richer environments than those presented in the paper? <sep> If so, do you foresee an easy solution, or will this be a challenge for future work? <sep> I think the paper is strong enough as-is, and does not need an experiment in this paper to investigate richer games like this.  However, if the authors agree that the technique could fail to help or could harm convergence in richer settings than those presented in the paper to support the technique, then I think a couple of sentences about future challenges and future work are warranted. <sep> Issues and Suggestions: <sep> Nit: Pg2, Experience Replay. The first sentence describes the agent as receiving (s_t, a_t, ..., s_t+1) at each time step. Should this be (o_t, ..., o_t+1), since the agent receives observations and not environment states? <sep> Typo: Pg2, MADDPG.  'uses deterministic polices' --> policies <sep> Suggestion: Pg3, Methods section and Equation 4. Equation 4 describes the tuple as containing r^e_t+1, r^m_t+1, but the text in the paragraph above only mentions r_t'+1, and the text below only indirectly clarifies what r^e and r^m are when it describes the cheap talk setting where r^m=0.  This threw me for a while when I read the equation, and scanned back up the page to try to see where r^e and r^m were defined, and they aren't. I suggest changing the sentence in the previous paragraph from ""receives rewards r_t'+1"" to something like ""receives rewards r_t'+1 (split into an environmental reward r^e and a messaging cost r^m)..."" to clarify this before the symbols are used. <sep> Typo: Pg4, Ordered Relabelling. ""may themselves by conditioned"" --> ""may themselves be conditioned"" <sep> Clarify: Pg4, under equation 6. The sentence ""...we sample an extra o^m_t-1 in order to determine (using the other agents' policies) the new ^{o}^m_t, which allows us to relabel..."". I don't understand what this sentence is trying to say. Which player is this for? The symbol ^{o}^m_t doesn't appear in equations 5 or 6, so I don't understand what sampling an extra o^m_t-1 would do, since it's to compute a symbol that doesn't connect with the equations being discussed. Maybe I'm just missing something obvious, but I spent a couple of minutes trying to figure this out, before giving up and moving on. <sep> Nit: Pg5, Implementation. Extremely minor, but the phrasing ""we can therefore only relabel..."" suggests a limitation of the approach (e.g., we are only able to do this...) whereas I think you're suggesting a performance win (we can do this using only...). I feel like flipping the words to ""we can therefore relabel only a single..."" better communicates that. <sep> Typo: Pg9 and 10, References. In both of the references including Pieter Abbeel, his affiliation is prefixed (OpenAI Pieter Abbeel). No other authors' affiliations are listed, so this just seems like a .bib typo.","This work proposes a simple and intuitive way to improve how to learn a communication protocol off-policy in the non-stationary situation in which messages received in the past do not reflect an agent's current policy. The authors introduce a communication correction that relabels the received message adjusting it to the current policy. The authors show that this method, besides being simple, is effective in a number of experiments. As observed by some reviewers, an issue with the method is that it is not clear how it would scale up to more complex environments than those considered. However, the authors addressed the concerns during the response phase, both adding new experiments, and with a clear statement of what are the outstanding issues. The paper is certainly a clever and solid contribution to the area of multi-agent communication learning, and I am strongly in favour of accepting it."
"The authors adopt a data-driven approach to neural system identification. They train a neural network consisting of a ""core"" and a ""readout"" in an end-to-end fashion to learn stimulus (visual inputs) -- response (single neuron activity) pairs. Since the core is shared across neurons, these stimulus-response pairs can be learnt in a massively parallel manner. In particular, they propose a novel readout mechanism that is parameter efficient and drives the core to learn better and generalizable features of the visual inputs. They find that their representations are more suited to predict neural responses in the mouse visual cortex when compared to representations derived from task-driven learning, especially in the context of transfer to previously unseen animals. Lastly, they also observe that the combination of their core+readout is more sample efficient than other naive alternatives. <sep> Pros: <sep> One of the major positives about this paper is the presented dataset. It seems to be relatively large and well-curated. This can certaily support several follow-up studies. <sep> The authors identify that ""global"" use of features (i.e. the full hXwXc representational tensor) in the readout is a wasteful strategy (in terms of learned parameters per neuron) and instead adopt a local approach where they only select specific feature columns per neuron to drive the readout. Though this is of minor technical novelty, this constraint forces the core to learn appropriate representations while allowing the entire module to be more data efficient, given the big reduction in the number of free parameters. <sep> The sample efficiency studies are neat and informative. The dissociations gleaned from diff-core/best-readout vs best-core/diff-readout scenarios are useful. Though it needs more work to fully justify this claim, their demonstration that transferred representations seem to be more effective than direct training is surprising and interesting. <sep> Cons: <sep> Though this study is certainly valuable, the manuscript needs several clarifications before it can be publication-ready. <sep> (i) The authors seek to develop better core representations indirectly by controlling the readout mechanism. This is fine, but there is little justification as to why they chose the current ""core"" architecture. This choice contains arbitary decisions (such as including depth-separable convolutions) that are not justified. Was there a systematic procedure behind a search that led to this architecture? Were other non-standard architecures tested? <sep> (ii) Figure 2 currently seems to be adding very little value and needs to be improved. Given that the proposed readout mechanism was a major contribution of this paper, the authors could have used the Fig. 2 space to visually depict this readout procedure, on top of the readout position network. The arrow to a neuron is also a bit misleading. <sep> (iii) One of my main concerns is with respect to the liberal use of the term ""generalization"". The authors repeatedly state that train-val-test splits were based on neurons and not images. This, coupled with the fact that their readouts leverage retinotopy, it is surprising that the authors never discuss the spatial segregation of the ""held-out"" neurons (say H) from the neurons in the training set (say T). If most H neurons were spatially proximate to T neurons, then this amounts to an ""interpolation"" regime for the network as opposed to ""extrapolation"". If my understanding here is wrong, could the authors please clarify why? <sep> (iv) The authors report that transfered ""core"" representations work better than direct-training in their generalization experiments. This result is surprising and needs to be more strongly justified computationally. Is it possible that a sub-optimal training regime was used for direct-training? Is this anyhow related to issue (iii) raised above? <sep> (v) The authors report that task-driven cores (such as VGG-16 pretrained on imagenet) perform badly in generalizaing across animals. Is this due to impoverished data regimes? Or are there more systemic issues? Also, VGG-16 isn't the best ventral stream model that best fits neural data. Do the authors think that this claim would hold for more recent task-driven systems, like CORnet-S for example. <sep> (vi) Though not necessary for this manuscript per se, it would be helpful to test the usefulness of the generalizable core representations presented here for visual tasks supported by early visual areas. Perhaps some commentary on this would be nice. <sep> Minor: <sep> ""Code for the analyses and the weights of the best generalizing representation will be shared in the final version of the paper"". The authors do not commit to making the dataset public. Is this oversight or intentional? <sep> how sensitive to number neurons in a scan? <sep> Clarity: (Fig. 4 caption) ""a fully trained core"": I think the authors are referring to a core trained on all available data, which is different from ""fully training"" a network as this alludes to loss saturation. Also language like ""a sub-optimal"" core is vague and misleading.","This paper has received four positive reviews. The main intellectual contribution of the paper is the introduction of a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning across neurons and even across animals. The reviewers commented on the technical strength of the paper. At the same time, the main contribution remains relatively incremental from a technical standpoint, and while the approach may be of value to future work, the impact of the current study on neuroscience (which is the target here) is quite limited. Nonetheless, there seems to be sufficient enthusiasm from the reviewers to recommend this paper be accepted."
"Note: I updated my review score after the original review was written. See comments below for details. <sep> Summary <sep> One important application of Neural Architecture Search is to find neural network architectures with good accuracy/inference time or accuracy/energy tradeoffs on a specific hardware device. However, the submission convincingly argues that many existing NAS benchmarks focus only on accuracy, or only provide very limited data about inference times. <sep> In my view, the submission's main contribution is a promise to publicly release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet. The submission also provides some analyses on this data. For example: the authors measure the correlation between inference times for the same network architectures on different hardware devices. <sep> The authors convincingly argue that properly performing on-device inference time/energy benchmarks properly is challenging for practitioners because it ""requires various hardware domain knowledge including machine learning development frameworks, device compilation, embedded systems, and device measurements."" This is a major motivation for the paper and associated datasets, which present the use of pre-computed latency measurements as an easier alternative for ML researchers. <sep> Pros <sep> The proposed dataset seems useful for research on hardware-aware NAS algorithms. The authors' datasets, which contain inference time measurements for several different hardware devices, should make it easier for researchers to experiment with NAS algorithms for finding better accuracy/inference time tradeoffs. <sep> The paper contains some interesting analyses. I particularly liked Figure 5; the authors identify Pareto-optimal architectures on Edge GPU and show the accuracy/inference time tradeoffs for these same devices on other hardware platforms. <sep> Cons <sep> The submission calls itself a ""unified benchmark for HW-NAS,"" which may be a bit misleading. Other NAS benchmark papers like NASBench-101 and NASBench-201 try to ensure that benchmark results from different researchers are comparable to each other; the submission does not. For example: two papers could both use the data from HW-NAS-Bench but produce incomparable results if searched for network architectures with different inference times. <sep> I'm not sure what FBNet-related code is publicly available / will be released, and would like a clarification.  While I was able to find official reproductions of specific FBNet models on github, I'm not sure whether there's an official open-source implementation of the search space itself. I'm hoping the authors can clarify, since releasing raw benchmark numbers for FBNet may not be very useful unless they're accompanied by code that can train/evaluate any architecture in the FBNet search space. If there's an official implementation, I hope the authors can provide a pointer. If the authors are using their own reproduction of the search space, I'd like to understand what they've done to verify the correctness of their implementation. (Ditto for NASBench-201 if the authors are not using the official implementation.) <sep> For the FBNet search space, the information about correlations between predicted and true latency measurements is quite limited. The author provide Pearson correlations on a random sample of architectures in Appendix A, but additional information (e.g., plotting predicted vs. true latency for a random sample of architectures) would strengthen the results. In addition, Appendix A only includes one ""Pearson correlation"" measurement per hardware device, and it's not clear to me whether this number is for the authors' CIFAR-10 benchmark, their ImageNet benchmark, or the union of the two. Breaking down the measurements and providing separate CIFAR-10 and ImageNet numbers would make this analysis stronger. <sep> In addition: the usefulness of the authors' code/dataset in practice will largely depend on how easy-to-use/well-designed the code library for querying inference times is, and the paper doesn't contain enough information for me to evaluate this. This is a limitation of the review process. <sep> Notes on Rating: I've given the paper a borderline score (5) in my initial review, due to the open questions mentioned in the ""cons"" section above. However, I believe proposed dataset could be a valuable contributions to the ML research community, and would lean toward accepting the paper if the concerns are suitably addressed. <sep> Experiments presented in paper <sep> The paper promises to release on-device inference time measurements for NASBench-201, as well as a lookup-table based inference time prediction model for FBNet. For NASBench-201, measurements are provided on Edge GPU (NVIDIA Jetson TX2), Raspberry Pi 4, Edge TPU, Pixel 3, and ASIC-Eyeriss). For FBNet, latency it appears that the same devices are used, except that Edge TPU is omitted. (Although I could not find a direct explanation, Appendix A suggests that Edge TPU was excluded because a latency table-based model was not very predictive of on-device measurements.) <sep> In addition to the raw benchmark numbers, the submission presents some experiments / sanity checks on these benchmarks (Section 4): <sep> Table 2: Rank correlations between model FLOPS/Parameter counts and on-device latency/energy measurements. <sep> Figure 3: Rank correlations for the inference latencies of the same model on different hardware devices. <sep> Figure 5: Taking network architectures which have pareto-optimal accuracy/latency tradeoffs on Edge GPU, and evaluating how close to optimal the network architectures are on Edge GPU in the NASBench-201 search space. <sep> In addition, the authors present results from running three architecture searches using ProxylessNAS with different target hardware devices. (Section 5.1). <sep> While Figures 2 and 3 and Section 5.1 mirror similar results from earlier papers like ProxylessNAS and FBNet, I still think they're valuable because they successfully validate earlier experimental claims on new search spaces and target hardware devices. <sep> Clarity <sep> In general, the paper seems clear and well-organized. While the authors generally did a good job of proof-reading, I did notice a few minor typos. For example: the Section 2.1 title says ""HareWare"" instead of ""Hardware""; in Section 3.2 under ""Edge TPU"", ""runitime"" should be changed to ""runtime""; and in Appendix D, ""TensorFLow"" should be ""TensorFlow"". <sep> Additional Comments <sep> The authors provide detailed information about their experimental setups in in Appendix D. I did my best to spot-check these descriptions, and the descriptions looked reasonable to me. However, I don't have enough experience with on-device benchmarks to independently certify that the benchmarks were performed correctly. <sep> The submission includes a promise that ""all the codes and data will be released publicly upon acceptance."" I consider this to be a major contribution of the paper, and the paper would need to be reviewed again if this promise cannot be fulfilled for any reason.","This paper presents a new NAS benchmarks for hardware-aware NAS. For each of the architectures in the search space of NAS-Bench-201, it measures hardware performance (energy cost and latency) for six different hardware devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware-aware measurements on as many as six (very different) devices. <sep> The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime. All reviewers appreciated the paper and gave (clear) acceptance scores. <sep> Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware-aware setting, and I expect this work to change this, and to open up the very important field of hardware-aware NAS to many more researchers. For this reason I recommend to accept this paper as a spotlight."
"The logic in the introduction is a bit contradictive to me: <sep> Some are able to achieve better asymptotic complexity (citations). while it is more challenging to improve on shorter sequences: the additional computation steps required by some approaches can overshadow the time and memory they save. <sep> Doesn't this simply mean that for short sequences there is no such computational burden? <sep> I think the story starts with pointing out the importance for long-sequence but turns to the topic on short sequence which is confusing. The need for short sequence acceleration needs to be justified IMO. <sep> Following 1, the baseline should be added. <sep> For a fair comparison, I think the baseline should add those methods as claimed in the introduction <sep> (Lee et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020, inter alia), (Kitaev et al., 2020; Wang et al., <sep> 2020; Roy et al., 2020, inter alia) and let us know how badly they performed under the short sequence. In particular we don't know if the sacrifice of short sequence time would benefit a lot in long sequences for existing methods. The current experimental baseline can't reflect this. <sep> Is the speedup over total computational time or just the attention part? <sep> To best of my knowledge, under many circumstances in particular for short sequence, attention alone might not be the most time-consuming part of the model. I think it will be helpful for authors to have a complete graph of the computational model used instead of only figure 1 concept graph. Specifically, is there any feed-forward computation involved and how many layers of the models used in comparison. <sep> Introduction of Eq 6,7 is confusing. <sep> Up to eq 5 it's clear whatr's going, but it comes from nowhere to intorduce these 2 modules in eq 6,7. So my understanding is that <sep> RFA simply refers to the approximation of computing the softmax. So the statement: <sep> for softmax-attention. The latter is typically used in two different ways in the transformer architecture, each resulting in a different computation for RF <sep> is confusing as the RFA is now redefined. I believe RFA should only refer one thing and I don't think eq(6) and eq(5) leads to the same result. On the other hand, eq 7 should be the same as eq 5. Is this correct? <sep> In addition, the notation in (6) looks wrong to me. \\phi(x) as introduced in eq 2 is in R^{2D} but S_{t-1} is in R{D}, not sure what does + mean in this context. <sep> I couldn't find out where you properly define the meaning of D either. <sep> Clarification of contribution <sep> Eq 6,7 reads like RNN style update but the intuition is lacking. Do you want to claim that this structure design is inspired by RNN and it leads to a better result? <sep> Put in another way, using RFA in transformer is from Rawat et al., 19 so do you think your major contribution is to design such a gated usage of RFA? <sep> Discussion of D <sep> Since RF is not the major contribution, you summarize existing results of FA in sec2.2. I think I'd like to see a discussion of sufficient number D analytically or empirically. Could you also cite the convergence bound on this approximation? To me, D looks to be an important efficiency tradeoff. Say sequence length is M <sep> and feature in d dimension. Original Attention is O(M^2 d). The computation of RFA <sep> requires outer product, which is O(D^2d) so overall it's O(M D^2 d), if M is around 64 or 128 (common usage) and D is 64, I actually don't see why RFA could improve 2x. Do you pre-compute and pre-store anything? <sep> Time analysis on language modeling is not presented. Since it's a efficiency paper, I think it should be complete. <sep> Overall, I think the paper provides an interesting view of discussion, but there are many flaws in the current version which needs to be corrected before a more serious consideration.  Especially, in terms novelty, the paper is relatively limited as the RF is explored in Rawat et al., 19. So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger.","This paper proposes an efficient attention mechanism linear in time and space using random features. <sep> The approach has some similarities with the simultaneous *CONF* 2021 <sep> submission ""Rethinking Attention with Performers"", with a key difference of a gating <sep> mechanism present in this work, motivated by recency bias. This paper is a <sep> valuable contributions to the efficient attention research topic. The reviewers <sep> appreciate the experiments and the in-depth analysis. I recommend acceptance. <sep> A noteworthy concern brought up in the discussion period has to do with whether the attention mechanism dominates the feed-forward computations in the neural network, and how much this is architecture-specific. The authors provide TPU timings, but I encourage the authors to add a discussion and timings of relative performance of feed-forward vs. attention layers that covers GPU and CPU optimizers as well."
"The paper provides generalization bounds for seemingly complex neural networks on the basis of much simpler ones. That is a good idea and something that is currently very relevant I think and the approach seems to be the natural one to take. <sep> Essentially the bounds proved, bound the out of sample error with a form of in sample error, average difference in predictions between complex and simple network, and complexity term for the simple network in terms of Rademacher complexity. <sep> The authors provide a general framework which can be applied and show a particular way of using it and use recent results (Bartlett et al) to provide interesting application of the framework. <sep> In terms of the actual bound achieved there are a few things I feel should be discussed more. <sep> In Lemma 1.1. <sep> The in-sample error. First it is not the in-sample classification error but the sort of a margin error that essentially is never zero for any prediction. Usually margin errors have a linear penalty on the wrong side of the margin and zero on the correct side of the margin. <sep> Second, there is a factor 2 in front of it. Normally, and in uniform convergence bounds, there are no constants in front of the in-sample error. (The other constants are of no concern). <sep> The authors state that their work can be applied to  generalization bound of Arora et a. 18 that only worked for a compressed network but not the original one. Given the above comments about the actual bound achievable it is not clear to me what exactly one would get out of ""distilling"" the construction in Arora et al, but it seems it does not become the same bound as for the compressed network as shown by Arora et al. Comments on that would be appreciated. <sep> Another small question: What does the early distillation phase on page 4 means (below lemma 1.3)? <sep> I like the experimental setup, particularly using gradient descent to try and find a network to distill to. <sep> Overall, I think the paper is well written including the proof sketches that make me believe the statements are actually provable (I did not rigorously check) <sep> Overall, I think this is an interesting paper and should be accepted.","This paper provides a novel generalization bound for neural networks using knowledge distillation. In particular, they argue that <sep> ""test error <= training error + distillation error + distillation complexity"" where the distillation complexity is typically much smaller than the original complexity of the neural network. This is motivated by the empirical findings that neural networks can typically be significantly compressed in practice using KD without losing too much accuracy. <sep> I found this result novel and the direction is very promising. This is a clear accept for *CONF*."
"Although a mesh embedded in 3D space may be treated as a graph, a graph convolution network uses the same weights for each neighbor and is thus permutation invariant, which is the incorrect inductive bias for a mesh: the neighbors of a node are spatially related and may not be arbitrarily permuted.  CNNs, GCNs, and G-CNNs demonstrate the value of a weight sharing scheme which correctly reflects the symmetry of the underlying space of the data.  The authors argue convincingly that for a signal on a mesh, the appropriate bias is symmetry to local change-of-gauge.  In short, the weights should depend on the relative orientation of a node's neighbors.  They design a network GEM-CNN which is equivariant to change of gauge.  The design is similar to a GCN but incorporates parallel transport to account for underlying geometry and uses kernels similar to those of SO(2)-equivariant E(2)-CNN (Weiler & Cesa 2019).  The experiments show the network is able to adapt to different mesh geometries and obtain very high accuracy in the shape correspondence task. <sep> I suggest accepting this paper.  The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets.  Though I would prefer more interesting experiments, they are sufficient to validate the design. <sep> Specific Strength, Weaknesses, Points, and Questions: <sep> The symmetry of a graph convolutional network can be broken by including spatial coordinates as features.  If xp and xq are inputs, then a function F(fq,xq,xp) can process data in an orientation aware way even though F is isotropic.  How would this compare to the proposed method in the paper? <sep> Page 2: One strength of the paper is the argument for why gauge symmetry is necessary in the first place.  Features defined on a mesh may be vector-valued and defined with respect to a frame of reference at each point on the mesh.  However, since the geometry is curved, there is no consistent way to assign a frame of reference.  Thus an arbitrary choice must be made when recording data.  Since this choice is arbitrary, the output of the network should clearly be independent of it.  Gauge equivariance encodes this symmetry. <sep> The argument for encoding the geometry of the mesh is reasonable.  But then why only parameterize K by θq; why not also include the distance rq? <sep> Page 6: RegularNonlinearity is an important contribution of the paper.  The authors are correct that non-linearities have been a bottleneck for using equivariant neural networks with representations other than the regular representation.  Transforming to sample space (i.e. embedding in the regular representation) to apply a pointwise non-linearity and then transforming back is a nice idea for addressing this.  Furthermore, Theorem E.1 provides a nice theoretical analysis of the asymptotic error.  It would be nice to include practical non-asymptotic error bounds as well. <sep> The experiments are okay, but are a weaker part of the paper.  The argument for a geometry-aware NN is that it processes signals on the geometry better.  It is not clear that embedding MNIST on a mesh illustrates data which is best understood in terms of the underlying mesh.  Far better would be to develop a signal natively on the mesh, for example by solving a PDE directly on the mesh.  Arguably the FAUST shape correspondence data addresses this issue better since the signal is inherently linked to the geometry. <sep> Both experiments also treat only scalar data of type ρ0.  While it is plausible and reasonable to model such data using vector features of type ρi in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. <sep> Changing roughness to the embedded MNIST distorts the signal in the geometry of the manifold (changing distances and angles), so why should we expect generalization across different roughnesses? <sep> Page 7: I don't understand the argument for the value of symmetry breaking.  The gauge equivariant network can be orientation aware by encoding ρ1 features.  Why is it desirable to be dependent on arbitrary gauge choices?  What breaks down about the original argument for incorporating equivariance in this case?  Is it possible the improvement is due to a different trade off between bias and expressivity at lower layers? <sep> Page 7, Para 4: The paper argues other high performing methods in shape correspondence use complicated pipelines.  It is not clear to me (probably from lack of familiarity) which is most complicated.  It seems both this method and the other method contain different complexities and subtleties. <sep> Minor Points: <sep> All of the citations in the paper use \\citet, but it would be more readable to use \\citep. <sep> Page 3: Should not ρ(gq→p) be invertible in Rcin×cin? <sep> Page 5: The notation kρl is non-standard, compared to ρlk, but it is more readable. <sep> Page 13, Kneighθpq−g) is missing a parenthesis <sep> Page 13, ""which is true for any features, if"".  if could be if and only if, correct? <sep> Update From Author Reply <sep> I am grateful for the author's replies, edits, and additional evaluation, all performed within limited time. This helps me feel confident in my accept (7) recommendation. My reason for not giving a higher score remains the limited experiments (which is likely not something to be addressed in two weeks), but even so I think the work is quite worthy of being accepted. The methods are a significant contribution and the experiments are sufficient to demonstrate they work.","This paper addresses a crucial problem with graph convolutions on meshes. <sep> The authors identify the issues related to existing networks and devise a sensible approach. <sep> The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. <sep> The reviewers unanimously agree on the both the importance of the problem and the impact the proposed work could have. <sep> Suggestions for next version: <sep> The paper is unreadable without the appendix and somehow it would be better to make it self-contained <sep> Additional references as suggested in the reviews. <sep> Expanded experiments as suggested by R4, will also improve reader's confidence in the method. <sep> I would recommend acceptance. I would request the authors to release a sufficiently documented and easy to use implementation. This not only allows readers to build on this work but also increase the overall impact of this method."
"The authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G.  Steerable CNNs are similar to CNNs but replace channels with G-reps and enforce an equivariance constraint on the kernels.  Though Cohen et al 2019 state the constraint, and Cohen et al 2019, Cohen & Welling 2016, and Weiler & Cesa 2019 and several other papers solve this constraint for different groups and representations, there has not been a general formulation which applies to all compact groups.  Here, solving the constraint means to construct a basis of the space of steerable kernels.  Any steerable kernel is then a linear combination of this basis and the network can then be trained by learning the coefficients. <sep> The problem of finding a basis for the space of steerable kernels is non-trivial and critical for constructing G-steerable CNNs.  Up until now this has done group by group.  The theorem proved in this work unifies such previous efforts and provides a useful method for approaching further G.  This paper is a significant contribution to the field. <sep> The appendix provides a complete and approachable background in the area as well as detailed and precise proofs.  Moreover, the effort by Cohen & Welling to frame equivariant deep learning in the proper context of representation theory is continued and extended here to good profit.  The appendix is quite verbose and the language is more casual than I am accustomed to in a mathematical text or research paper, but it serves the goal of being didactic and approachable. <sep> A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G.  That said, Appendix E does a good job providing evidence that this can be done for many individual groups.  However, in that case, we are still back to solving the problem on a group by group basis. <sep> Specific Additional Points: <sep> 1.My opinion is that the language of physics does not add to the paper.  While Clebsch-Gordan and harmonic functions first arose in physics, they can be described in terms of representation theory.  In this way, both steerable CNN and quantum mechanics are applications of rep. theory and so it is not necessary to use physics here to describe steerable CNN. <sep> 2.Page 5, the notation [j]=dim(Vj) seems unusual to me. It would be better to use something more standard.  In particular, in Defn 3.5, brackets are used as parenthesis, making this more confusing. <sep> 3.Page 5, the fact that input and output representations Vin and Vout decompose into irreps does not immediately explain how to construct a steerable kernel basis for Vin→Vout given ones between irreps Vi→Vj.  Though it is not complicated, I would include this. <sep> 4.Page 5, I was confused by the inclusion of EndG,K(Vj) at first since it does not appear when working over C due to Shurr's lemma.  It is explained in the paper and more so in the appendix that it is necessary over R, but it could be a bit clearer and earlier in the paper.  Namely, you could note that over C EndG,K(Vj)=C and give the possibilities over R. <sep> 5.Page 5, Thm 3.4 and Page 43, Thm C.7.  Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that δx is informally considered as in L2(X) is very imprecise.  Not only does this make the proof informal, it means the maps in the theorem are not even defined.  Can you replace L2(X) with an appropriate space of distributions in order to make the statement precise?6.Page 6, ""which is zero for almost all J"" should be ""which is zero for all but finitely many J"" <sep> 7.Appendix E, U(1) is isomorphic to SO(2), so it is strange to use both notations.  The difference between these subsections is whether the representations are real or complex. <sep> Updates from Author Feedback <sep> While I would have liked to see a draft with the changes, I feel reasonably sure the authors will improve Appendix C to make the statements mathematically precise. I am confident in the statements and proofs. While the presentation can be verbose and casual, I think it is justified to increase accessibility, so long as the proofs and statements are formal and precise. Based on Author responses I have increased my confidence.","Reviewers generally agree that the main result of the paper, which generalizes the classical Wigner-Eckart Theorem and provides a basis for the space of G-steerable kernels for any compact group G, is a significant result. There are also several concerns <sep> that need to be addressed. R4 notes that the use of the Dirac delta function (e.g. Theorem C.7) is informal and mathematically imprecise and needs to be fixed. R1 notes that it would be helpful to at least describe how this general formulation can be applied in machine learning. <sep> Presentation and accessibility: the current version of the paper will be accessible to only a small part of the machine learning audience, i.e. those already with advanced knowledge in mathematics and/or theoretical physics, in particular in representation theory. If the authors aim to make it more accessible, the writing would need to be substantially improved."
"SUMMARY: <sep> The paper considers the problem of using CNNs on an irregularly sampled grid. It proposes to incorporate the numerical uncertainty related to the disretisation of the domain using an approach inspired by probabilistic numerics. This involves defining continuous convolutional layers as affine transformations of the input GP and use rectified GPs to include the nonlinearity in the mapping. The resulting non-Gaussian distribution is then approximated with another GP in each layer. These are then approximated with a GP with an RBF kernel using Monte Carlo to produce observations with input dependent noise. The model is trained using MAP estimation. <sep> ################################################################## <sep> REASON FOR SCORE: <sep> I like the motivation for this work and the paper was generally a pleasure to read. Naturally, many technical challenges arise when trying to approach this problem from a probabilistic perspective; they are discussed in a fair bit of detail and generally seem reasonable. However, I still have a number of questions that are listed below that would help me better understand the main ideas and contributions. <sep> ################################################################## <sep> PROS: <sep> The motivation for the work seems sound. <sep> The main technical contribution seems to be the definition of a continuous convolution on GPs with an RBF kernel. This is presented clearly and in a fair bit of detail. <sep> The writing quality is very good. The appendix was very informative as well. <sep> ################################################################## <sep> CONS: <sep> The paper makes quite a few approximations (which mostly make sense) but doesn't give a thorough discussion of the effect of these choices. <sep> The paper doesn't seem to make full use of the setup (see questions below). <sep> I found Fig.2, especially 1 and 3, hard to interpret and the figure caption wasn't very informative. It appears from the left figure that the mean is very smooth in all layers. Is that desirable? In the right figure, what is gt on the y-axis? <sep> ################################################################## <sep> QUESTIONS and COMMENTS: <sep> My understanding is that PN approaches are typically used for tasks like, (a) to impose specific priors, (b) to improve the design of the problem or (c) to learn something about the implicit priors in established deterministic methods. <sep> It would be interesting to think about these in the context of your model. For example, (a) is it reasonable to use a GP with a smooth kernel as an interpolant of the inputs? If the kernel choice was not limited by the analytic tractability, would you still choose an RBF kernel? For (b), could you use the model to decide which points in the input domain are the most informative for the end task? I suppose that you would need the full posterior distribution rather than just a point estimate, which would be very tricky given the multi-layer setup. Also, given that your approach seems to outperform existing (deterministic?) models or some tasks, how could you change the existing models to gain a similar advantage? <sep> It would be interesting to know what is the effect of the various approximations, for example, the Gaussian approximation in 4.6 and the MC in 4.7, on the final task. <sep> The CLT argument in Appendix I is not very clear to me. The Lyapunov CLT holds in cases of RVs being independent, but not necessarily identically distributed. Is it correct that in your case the RVs are weakly-dependent and identically distributed? What is c in the inequality |c - c'| >> 1? More generally, is the argument in this section some well-known result? It shares some ideas with the argument of Neal [1996] of an infinite limit of NNs being equivalent to GPs but it would be good to either give a more thorough discussion of this argument or provide links to related results in the literature. <sep> It would be informative to discuss the computational and memory complexity of the proposed approach and other related questions, e.g. the number of samples in the MC steps. <sep> I'm a little surprised that removing the noise gives worse results in your MNIST experiment. I would guess that this may be more related to some optimisation phenomenon than to actual model. For example, if instead of using the data-dependent noise parameter as described in Sec. 4.7 you used a spherical Gaussian noise with fixed sigma for all inputs, would that give better results than discarding the noise entirely? And how would it compare with your data-dependent noise? Using spherical Gaussian noise relates with some of the practices in DGP models where the input to each layer is assumed to be a noisy version of the outputs of the previous layer. <sep> Related literature: It might be interesting to compare to some DGP models that also use convolutions, e.g. Deep Gaussian Processes with Convolutional Kernels [2018] by Kumar et al and Deep convolutional Gaussian processes [2019] by Blomqvist et al. <sep> ################################################################## <sep> MINOR: <sep> p. 1 - Probalistic Numeric p. 6 - the number the number p. 15 - infinitesmal p. 18 - many many observations p. 19 - a variant a variant","This is a fairly technical paper bridging deep learning with uncertainty propagation in computations (i.e. probabilistic numerics). It is well structured, but it could benefit from further improvements in readability given that there are only very few researchers that are experts in all sub-domains associated with this work. Given the above, as well as low overall confidence by the reviewers, I attempted a more thorough reading of the paper (even if not an expert myself), and I was also happy to see that the discussion clarified important points. Overall, the idea is novel, convincing and seems well executed, with good results. The technical advancements needed to make the idea work are fairly complicated and are appreciated as contributions, because they are expected to be useful in other applications too (beyond irregular sampled data) where uncertainty propagation matters."
"Summary <sep> This paper analyses an existing algorithm (LSVI-UCB) with generalized linear function approximation instead of conventional linear function approximation.  Under this generalized linear setting, they propose a so-called ""optimistic closure"" assumption which is shown to be strictly weaker than the expressivity assumption in the conventional linear setting. The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. The paper also derives a general error propagation through steps that do not require a closed-form expression of the empirical dynamic and reward functions as in the linear case; this could be applicable to general function approximations. <sep> Strong points <sep> Novelty: The generalized linear setting appears novel and generalizes the linear settings. <sep> Significance: The optimistic closure appears novel and is strictly weaker than the linear MDP assumption in the prior works. <sep> Correctness: A complete analysis that successfully retains a sublinear regret and honest comments on the limitations of the present work. <sep> Weak points <sep> The work is almost merely about analysis of an existing algorithm with modest algorithmic contribution (which however is not a big problem). There are some parts of the proofs pointed out in the Minor comment section that potentially require some attention (but I believe these are minor points which could be fixed if there is any issue) <sep> Minor comments <sep> Period '.' after the first sentence of the second paragraph of section 2. <sep> First sentence of section 3: 'MPD' -> 'MDP' <sep> Lemma 1: Should it be πh,t instead of πt there? <sep> In Appendix A: ""We believe these technical results will be useful in designing RL algorithms for general function classes"". It seems that an analysis of LSVI-UCB with general function classes has recent done in [1] (?) <sep> In Corollary 4, shouldn't it be 2 γ|ϕ(s,a)| instead of γ|ϕ(s,a)|…? <sep> At the end of Page 12: ""The first term forms a martingale"" -> shouldn't it be a ""difference martingale"" instead? <sep> The equation between eq. (5) and eq. (6) on page 14 does not look very right. I think the correct one should be the one with the RHS replaced by ⟨xτ,θ^−θ¯⟩f′(⟨xτ,sθ^+(1−s)θ¯⟩) for some s∈[0,1] (according to the mean value theorem). If this is true, I am afraid the bounds of the difference between Dτ (after Eq. (10)) might not be precise. <sep> The second paragraph on page 12: ""Hence yh,τ is not measurable with respect to the filtration Fτ ,  which prevents us from directly applying a self-normalized martingale concentration inequality"". Should it be Fτ−1 instead of Fτ? <sep> On page 15, the paper says that E[   xi_tau^# | x_{1:tau}, xi_{1 : tau-1}^#   ] = 0. Do we really need that martingale structure when we already consider a fixed g_{epsilon}? Given a fixed g_{\\epsilon}, we already have E[   xi_tau^# | x_tau   ] = 0. <sep> Questions for the authors <sep> In Chi Jin et al. 2019, the regret is the difference between the optimal value function and the value function estimate while in the present paper, the regret is the difference between the optimal value function and the expected value of the cumulative rewards by the algorithm. What is the difference between these two notions of regret? Can it make the two results comparable? <sep> In the proof of 'Fact 1', why Q^*_H \\in \\mathcal{G}? For that to hold, it seems to require that the expected reward \\mathbb{E}[r_H] has a generalized linear form of \\mathcal{G}? If so, one way to fix it is maybe letting 1 <= h <= H (instead of 1 <= h < H) in Assumption 2? <sep> It seems that [1] already analyses LSVI-UCB with general function approximations which means that [1] is more general than the present work (?) If so, could the authors comment on the benefit of this work for a generalized linear function class given that an analysis for a general function class has been done? For example, does the present work give a tighter bound when considering generalized linear function as compared to the bound for a general function class in [1]? <sep> My initial recommendation <sep> Overall, I vote for accepting. An extension from linear settings to generalized linear settings is novel and natural, and it must be done at some point. I think this work is nice for filling in that gap. <sep> My final recommendation <sep> I remain my initial score after the discussion. <sep> References <sep> [1] Ruosong Wang et al. ""Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension"" <sep> Additional comments about the correctness of the proof of Lemma 8 <sep> I have recently checked their proof of Lemma 8 and noticed one thing that looks a bit strange to me. Since the discussion is over, I hope the authors will clarify/fix it in their final paper. That is, in the proof of Lemma 8 in the step where they applied Lemma 7 (Azuma's),  they used cτ′=|q(uτ′,ϕ′)|, but the Azuma's inequality requires that cτ′ is a constant while here |q(uτ′,ϕ′)| is a random variable (depending on the random variable uτ′). How is this possible to apply Azuma's inequality here when cτ′=|q(uτ′,ϕ′)| is random?","This paper analyzes a version of optimistic value iteration with generalized linear function approximation. Under an optimistic closure assumption, the algorithm is shown to enjoy sublinear regret. The paper also studies error propagation through backups that do not require closed-form characterization of dynamics and reward functions. <sep> Overall, this is a solid contribution and the consensus is to accept."
"The paper considers the regularization of latent space toward achieving adversarial robustness against latent space attack. The paper demonstrates the applicability of disentanglement promoting VAEs for achieving adversarial robustness and further enhancing such VAEs by considering their hierarchical counterparts. The paper demonstrates their results in the benchmark datasets considered in the disentanglement and computer vision literature. The overall research direction pursued by this paper is exciting. However, I have some concerns, which include: <sep> The paper attempts to establish the connection between disentanglement and robustness. The linkage, however, is not clear. In section 3, the paper argues for the smoothness of the encoder mapping and the decoder mapping. Toward this, the paper postulates additional regularization to enforce ""simplicity"" or ""noiseness"". First of all, it is unclear how disentangled latent space helps achieve ""simplicity"" in the ""encode-decode process"". Secondly, regarding ""noiseness"", it is not explained what extra would disentangled version of VAEs (e.g., TCVAE) provide compared to the standard setup. <sep> In section 3.2, the paper empirically demonstrates the connection between disentanglement and adversarial robustness. However, the evaluation carried out are not explicit. Firstly, to demonstrate the connection, the paper uses the attacker's achieved loss \\delta (from Eqn 1) as the metric. Although the \\delta is shown across different \\beta values, it is still unclear if disentanglement is directly related to robustness. Can the authors point out some disentanglement metrics (e.g., MIG) for each beta and compare MIG vs. \\delta? Also, the curves are combined for all the d_z. What is the motivation behind doing that? Because it has been known that disentanglement behavior is related to the dimension of latent space. Also, authors could consider decomposing the first term of \\delta for all the latent space dimensions and analyze if the disentangled dimensions are robust compared to the entangled dimension. This could be more helpful to establish the linkage. Secondly, authors have picked TCVAE considering ""reconstruction quality"" compared to \\beta-VAE, but in Fig 2 (right), ELBO is compared. Can the authors compare the reconstruction error? Also, for fig 3, I think it is natural to see the comparison with \\beta-VAE. Why is such a comparison not included? <sep> In section 4, for the motivation for the hierarchical TC-penalised VAEs, the paper states that ""TC-penalisation in single layer VAEs comes at the expense of model reconstruction quality"". However, this directly contradicts the use of TC-VAE in the previous section. Although the results presented afterward support the authors' statement, the motivation must be clear and well written. The same comments for section 3.2 apply here too. <sep> The experimental results demonstrating protection against downstream tasks is performed using a simple 2-lear MLPs. This is different from the regular CNN network commonly considered for these datasets. Although this was meant to demonstrate the proposed model's efficacy, it would be more clear if the experimental setup is consistent with the current literature setting. Also, can the authors point out the initial results for the models before the attack? <sep> Minor comments: <sep> There are a lot of grammatical errors and hard-to-follow sentences. Some examples: <sep> "".. are not only even more .."" <sep> "".. attack the models using methods outlined .."". But Eq (1) refers to only one method, right? <sep> ""… then \\delta too is small .."" <sep> (Update): The score has been updated after a rebuttal from the authors.","This paper presents a hierarchical version of β-TCVAE that promotes disentanglement in the latent space and improves the robustness of VAEs over adversarial attacks, without (much) degeneration on the quality of reconstructions. The analysis on the relationship between disentanglement and adversarial robustness is valuable and the method is new. The results look promising. The comments were properly addressed."
"In this submission the authors are trying to tackle the very important problem of safe RL with safety guarantees. The problem formulation is rather clear, and the paper is overall well written. The main idea is to formulate the safe RL problem as  a CMDP problem, but with worst-case bounds to ensure that the safety constrained is guaranteed throughout the learning. <sep> The general idea is fine, albeit not new (please check/compare against recent works on robust-CMDPs), however, I do have several issues with the lack of rigor in the mathematical proofs, as well as the rather amplified statements about safety guarantees 'throughout learning'. <sep> Indeed, the CMDP problem (or its worst-case bound) is solved using a Lagrangian formulation, which is well-know as a soft constraints formulation, i.e., you do not have guaranteed safety during learning, as claimed in the Introduction (paragraph 3), and throughout the paper. In that case, the dramatic drone motivational example used in the Introduction (paragraph 1) is and exaggeration, i.e., this method will lead to a crash too. <sep> Besides, mathematically, many of your variables are not defined, not even in the appendix, e.g., in equation (2) B^ is not defined; α which seems a key tuning parameter in Theorem 1 is not defined, etc. <sep> Most importantly, the authors keep referring to the paper about CPO (Achiam et al 17) to support their proofs and technical derivations, whereas that paper is about continuous constraints costs. This paper on the other hand, is clearly using discontinuous constraints, not even C0, and thus one cannot just use parts of the results in (Achiam et al 17) without further carful examination of the technical challenges introduced with a discontinuous cost function, e.g., you are using some Taylor developments throughout, while these only make sense for analytic functions, etc. <sep> Furthermore, the probability bounds proposed in Theorem 1 and 2 seem to be rater weak bounds, since ξ is bounded by a term inversely proportional to the confidence parameter ω, which means the probability of being safe is high when the safety bound is loose, i.e., χ+ξ, for ξ→+∞ . Similarly, for tight safety bound, i.e., χ+ξ, for ξ→0, the safety probability drops to zero. The authors seem to minimize this point by the 'proper tuning' of α which remains a mysterious parameter (even after checking the proofs in Appendix). <sep> This is all to say that the authors have to tone down their statements about guaranteed provable safety bound, quite a bit. <sep> Finally, the numerical simulations are interesting, but only confirm my point about the fact that the obtained safety is asymptotic only, i.e., in steady state, and absolutely not during learning. This is clear from the plots in Figure 3 (bottom) where we see that in average the proposed algorithm necessitates more than 1000 iteration before reaching a truly safe behavior. One can also note that the CPO algorithm behaves almost similarly to the proposed algorithm, when tested on the car navigation example, and the laikago robot (Fig. 3, bottom number 3 and 4). One also wonders why in the first set of tests, in Fig. 3- top number 2, one can reach a performance cost of 20 with the proposed method, while it seems to plato at 10 in the second set of experiments, in Fig. 4-top number 2. Another point that is worth clarifying is that the tests in Fig. 4-top number 1, we see that the performance cost reaches 10 for the proposed method with large safety constraint bound (0.2), which is intuitively an almost unconstrained case. However, the unconstrained algorithm 'Base' in the first tests, Fig. 3-top number 1, the Base algorithm does not achieve a similar performance, could it be better tuned in that case ? <sep> In summary, I found the paper well written, but it needs to be carefully revised for technical rigor, and toned down in terms of what is really achieved here (maybe somehow safer CMDP algorithm but definitely not safe during exploration).","Summary: <sep> This paper introduces a different, interesting definition of safety in RL. The paper does a nice job of showing success with empirical results and providing bounds. I think it provides a nice contribution to the field. <sep> Discussion: <sep> The reviewers agree this paper should be accepted. The initial points brought up against the paper have been successfully addressed or mitigated."
"Summary: <sep> This paper proposes a new algorithm called EGRL to improve computation graph running time by optimizing placement of the graph's components on memory. Specifically, the authors demonstrate the algorithm on the Intel Neural Networks Processor for Inference (NNP-I), which allows them to map neural network components on one of three memory hierarchies, each with different tradeoffs. The authors demonstrate that this technique provides speedups on BERT, Resnet-50 and Resnet-101. <sep> Pros: <sep> Some past papers (for eg. [1]) in this domain evaluate their work in simulators instead of real hardware, and often, the simulators make assumptions that are not realisitc. The paper tests its technique on actual hardware, and this is definitely a plus. <sep> The authors promise that they will open-source their code. This is important since many of the efforts in this domain remain fragemented and difficult to reproduce. This is primarily due to the lack of open source code or a standard benchmark. <sep> The paper is well written. <sep> The visualisations of the learned policy vs the baseline in Figure 6 are quite good. <sep> EGRL directly builds upon CERL so it is not very novel, but it has not been applied before to this domain. <sep> Cons: <sep> The paper evaluates the technique on just 3 workloads. This is in contrast to [1] who evaluate on 372 different workloads and [2] who evaluate on 96 synthetic graphs. [3] and [4] also evaluate on a very small number of workloads, but I believe they probably got a freepass since they were the earliest works in their domain. <sep> The baseline that the experiments are being compared against might be weak - in figure 3, it looks the policy from both EGRL and EA in iteration 0 itself beat the baseline for Resnet-101 and BERT! <sep> How long does it take to perform one iteration? And how long does it take to train the policy? This would be useful to get an idea of how EGRL fairs against [3] and [4] which also trained on real hardware and took many hours to finish training the policy. <sep> The demonstration of generalizability is insufficient - it is difficult to conclude that EGRL can generalize to other workloads. For eg. in Figure 4 (left), the policy performs worse than the baseline for Resnet-50. Moreover, two of the workloads are from the Resnet family. <sep> If it takes a long time to train each policy and if the model also shows poor zero-shot generalizability, it makes me question if this approach is practical for a compiler setting where a user would typically want the compilation to be completed quickly. <sep> Overall: <sep> I felt that the paper has some interesting ideas but needs more experiments. <sep> Questions and Clarifications: <sep> I believe that the related work section should add a clarification - [1], [2], [3] and [4] primarily deal with device placement, i.e., placing components of computation graph on different CPUs/GPUs to optimize run time via better parallelization. While this work is concerned with mapping components to different memory hierarchies on the same device. <sep> While EGRL's action space is larger than [5], the action space in [1] is much larger - for a graph with 2000 nodes to be placed on 2 devices, there are 2^2000 possible choices ~ 10^603 <sep> In Figure 6 (Bottom), is there any reason why you didn't show the result for BERT? <sep> References: <sep> [1] Aditya Paliwal,  Felix Gimeno,  Vinod Nair,  Yujia Li,  Miles Lubin,  Pushmeet Kohli,  and Oriol Vinyals. Reinforced genetic algorithm learning for optimizing computation graphs. arXiv preprint arXiv:1905.02494, 2020 <sep> [2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, and Moham-mad Alizadeh. Placeto: Efficient progressive device placement optimization. In NIPS MachineLearning for Systems Workshop, 2018 <sep> [3] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V. Le, and Jeff Dean. A hierarchical model for device placement. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hkc-TeZ0W. <sep> [4] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, NaveenKumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement optimization with reinforcement learning. arXiv preprint arXiv:1706.04972, 2017. <sep> [5] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, Azade Nazi, Jiwoo Pak, Andy Tong, KavyaSrinivasa, William Hang, Emre Tuncer, Anand Babu, Quoc V. Le, James Laudon, Richard Ho,Roger Carpenter, and Jeff Dean. Chip placement with deep reinforcement learning. arXiv preprint arXiv:2004.10746, 2020.","Most of the reviewers agree that this paper presents interesting ideas for an important problem. The paper could be further improved by having a thorough discussion of related works (e.g. Placeto) and construct proxy baselines that reflect these approaches. <sep> The meta-reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments. <sep> Thank you for submitting the paper to *CONF*."
"Edit on second review <sep> I apologize again for the tone of my first review, I sincerely tried to understand the paper but I could not when I first read it. A re-read the paper and finally understood it during the review. I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations. With the new equation (1) the paper is hopefully more understandable now. <sep> I increase my grade from 3 to 5. The findings are quite interesting but I still believe that the paper is not well written: the equations are interesting but the explanations between the equations are often unclear. One has to understand each equation and be quite imaginative to finally identify the contributions of the paper (even for somebody only ""very slightly"" off from the research topic). <sep> Summary <sep> The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. <sep> This relationship suggests a mapping between the two models which is imperfect, a loss seems to be derived to reduce this mismatch along the network training. The method is tested on CIFAR-10 and CIFAR-100, and compared with some other methods for converting ANNs to SNNs. <sep> Critical review <sep> This topic is potentially important since spiking neural network are gaining popularity. But this paper is clearly badly written and it is extremely hard to understand, both in the math and in the text. I don't think it would help the progress of the field to publish the article in the current form. <sep> I tried to read that carefully and got lost after equation (4), the transition to equation (5) and (6) are not clear at all. I do not understand what is an approximation, what is a definition and what is a derivation. <sep> Also (5) seems wrong in itself, the authors are trying to approximate a rectified linear network but it suggest that the activity will be equivalent to a linear network (at least when v(T) is small) ? And magically this changes in (6), and a clip non-linearity is introduced ? <sep> The Figure 1 seems very encouraging at first, because it suggests that there is a clear and easy mapping between accumulating the spikes and computing a relu. I did not understand where this is appearing in the math and I cannot check whether the intuition conveyed by the figure is correct or not. <sep> I was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? This is not shown.  I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa. <sep> Since I had not understood the basics of the paper, it was impossible for me to understand the later section about the conversion error. My only take is that it seems wrong at first sight: how minimizing the error in the loss would minimize the mismatch between the network activity?","The work tackles the task to convert an artificial neural networks (ANN) to a spiking neural network (SNN). The topic is potentially important for energy-efficient hardware implementations of neural networks. There is already quite some literature available on this topic. <sep> Compared to these, the manuscript exhibits a number of strong contributions: It presents a theoretical analysis of the conversion error and consequently arrives at a principled way to reduce the conversion error. The authors test the performance of the conversion on a number of challenging data sets. Their method achieves excellent performances with reduced simulation time / latency (usually, in order to achieve comparable performance to ANNs, one needs to run the SNN for many simulated time steps- this simulation time is reduced by their model). <sep> One reviewer criticized that the article was hard to read, but this opinion was not shared by other reviewers and the authors have improved the readability in a revision. <sep> In summary, I believe that this manuscript presents a very good contribution to the field."
"This paper proposes Wasserstein-2 Generative Networks (W2GNs) which is an optimal transport framework for learning generative models. Unlike minimax problems of Wasserstein GANs, the proposed approach which is based on minimizing the 2-Wasserstein distance reduces to a single-level optimization problem. The paper numerically shows that the new approach enjoys faster convergence and improves upon the performance scores of Wasserstein GANs and other optimal transport baselines. While the paper's idea on applying optimal transport tools for training generative models seems interesting, the discussed theoretical and numerical results are not supportive enough to show that the proposed approach indeed improves upon WGANs. Also, the theoretical sections have been written in a convoluted way with several weakly supported claims and the final algorithm has not been stated clearly. I, therefore, do not recommend the paper for acceptance. <sep> To further explain my concerns, let me start with section 3 which reviews the dual formulation to 2-Wasserstein distance in Eq. (8) and also the connection to the convex conjugate optimization in Eq. (9). Here, the paper vaguely mentions that the optimization problem for computing the convex conjugate is ""convex and very complex"" followed by a brief explanation which I do not find satisfactory. Specifically, the paper's way of reasoning does not convince me why it is necessary to switch from Eq. (8)  to the paper's formulation in (12). Both the issues mentioned for problems (8) and (9) also apply to the formulation in (12) as (12) still requires taking the gradient of the convex conjugate \\psi_w. Also, I highly recommend replacing the terms ""very complex"" and ""impossible"" with more solid theoretically or numerically supported statements. <sep> Next, let me refer to the final sentences of the first paragraph of section 4.1: "" Yet such a problem is still minimax. Thus, it suffers from typical problems such as convergence to local saddle points, instabilities during training and usually requires non-trivial hyperparameters choice."" These sentences argue that every minimax problem suffers from instability and convergence to local solutions. However, the paper's own formulation in (12) also leads to a non-convex optimization problem for which the authors show no convergence guarantees to a global solution. The paper should either remove these sentences or precisely explain why the proposed non-convex problem enjoys better convergence properties than the minimax problems. Let me also add that while Eq. (12) states an optimization problem, it still does not completely characterize a learning algorithm, because it is unclear how one wants to take the gradient of \\psi_w's convex conjugate. I think the paper should include an algorithm clearly stating the steps of learning the generative model. <sep> Finally, the theoretical guarantees in Theorem 4.1 and 4.2 do not analyze the algorithm's performance for the class of input convex neural nets. Theorem 4.1 connects the optimization error to the closeness of the generative and underlying distributions. Yet, it does not provide any guarantee on how large the optimization error could be for convex neural nets. The result of Theorem 4.2 also immediately follows from the assumptions and offers little understanding of the algorithm's performance with convex networks, since it considers the set of all differentiable functions instead. The theoretical guarantees should somehow analyze the algorithms' convergence and approximation properties for convex neural nets rather than all differentiable convex functions. Overall, the paper seems to carry several nice ideas, but the theoretical discussion needs to be significantly improved. <sep> Review update: I thank the authors for their response and for revising the paper based on the comments. The revision addresses several of my concerns. I still think the theoretical guarantees should be stronger and therefore change my score to borderline 5.","The reviewers have different views on the papers but agreed that the paper can be accepted. However, they suggested <sep> some points of improvements including the writing (clarity and style) and experiments showing strong improvements <sep> compared to WGAN."
"This work investigates a classic unsupervised outlier detection problem, in which we do not have any label information and need to learn a detection model from those unlabeled data to identify any inconsistent data points as outliers. The key approach here is to apply existing self-supervised contrastive feature learning methods to extract feature representations and then apply a cluster-based method to calculate outlier scores. It also presents two ways to leverage labeled outlier data if available, including an improved mahalanobis distance method and the application of supervised contrastive learning methods proposed recently. The methods, including unsupervised and semi-supervised methods that use a few labeled outlier data, are evaluated using four datasets. As I read through the paper, I find the following major issues. <sep> the question this work intends to answer, ""Can we design an effective outlier detector with access to only unlabeled data from training distribution?"", is a classic and well-studied problem in the anomaly/outlier detection community. There have been many studies over this problem. We cannot simply ignore those previous work  and states it as a new problem. see resources like Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3), 1-58. or Aggarwal, C. C. (2015). Outlier analysis. Springer, Cham. for numerous previous work on using shallow methods to address this problem. <sep> There are a number of studies on self-supervised outlier detection approaches as well as what is called few-shot outlier detection approaches, but I cannot find any discussion of those work and the empirical comparison to these methods. The authors may refer to some recent survey papers, such as ""Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2020). Deep Learning for Anomaly Detection: A Review. arXiv preprint arXiv:2007.02500."", to find some of these studies. Some closely related methods are: self-supervised methods such as GT and E3Outlier that learns feature representations using a pre-text task in a self-supervised way; unsupervised outlier detection methods such as RDA, REPEN, ALOCC, OCGAN, etc.; methods that use a few labeled outlier data such as Deep SAD, DevNet, REPEN, etc. Please see table 1 in that survey paper for more details. The authors are suggested to discuss and differentiate their method from these existing studies, and to compare empirical comparisons to these closely related methods. <sep> The claiming of ""a parameter-free detector"" is misleading and incorrect. Similar to other methods, the presented methods still have a huge number of hyper-parameters. They may be able to work without parameter tuning on each dataset, but this is not parameter-free. Also, many existing methods can also work in this way. <sep> The way that the presented outlier detection methods utilizes the labeled outlier data may less effective than previous work, because the cluster-based anomaly scoring here is separated from the representation learning. More advanced approaches (see some of the methods mentioned above) can unify the anomaly scoring and the representation learning together and the labeled outlier data is used to optimized the entire anomaly detection pipeline, rather than the representation learning stage only. <sep> The experiment settings are not properly designed to justify the paper's arguments. First, closely related deep unsupervised and semi-supervised methods are missing in the comparison in tables 1 and 3. Second, I think it is important to include some popular baselines here. For example, how is the performance of using traditional outlier detectors such as iforest, lof, and knn distance on feature representations extracted with a pre-trained resnet-50? How many benefits does the computationally extensive contrastive representation learning gain compared to those simple solutions?  Third, why is the cluster-based outlier scoring method used? can we use other traditional outlier detectors such as iforest, lof, and knn distance?","There is some positive consensus on this paper, which improved somewhat after <sep> the very detailed rebuttal comments by the authors. The use of limited amounts of OOD data is interesting and novel. There were some experimental design problems, but these were well-addressed in rebuttal. <sep> A reviewer points out that <sep> anomaly/outlier detection does not explicitly assume that there is only one <sep> class within the normal class (or in-distribution data). The one-class <sep> assumption is mainly made in some popular anomaly detection methods, such as <sep> one-class classification-based approaches for anomaly detection. The authors <sep> should take this into careful consideration when preparing a final version of <sep> this work."
"The goal of this paper is to complete the theoretical subset inclusion relationships given in (Geifman et al., 2020). <sep> The authors proved that the RKHS of the the neural tangent kernel (NTK) with any number of layers is a subset of the RKHS of the Laplace kernel. <sep> Combined with the results in (Geifman et al., 2020), <sep> if the input domain is restricted to the sphere of (d−1)-dimensional real space, <sep> the RKHS of NTK with any number of layers is equal to the RKHS of the Laplace kernel. <sep> The writing of this paper is very clear. <sep> I can easily get what the authors intend to express. <sep> However, I still have some doubts about the details. <sep> In Section 2.1, <sep> the original Laplace kernels and exponential power kernels are all classic shift-invariant kernels. <sep> Here the authors provided the dot-product versions of these two kernels. <sep> Why do the authors need to do these changes? <sep> For the sphere Sd−1, the shift-invariant versions are equivalent to the dot-product kernels? <sep> Under Lemma 3, <sep> if Σk(x,x)=1, <sep> we can only get Σk(u)=Σk−1(u). <sep> What does the notation κ1(k) means here? <sep> Theorem 1 in (Aronszajn, 1950, p. 354) just stated that if K1≼K2, then K1⊆K2. <sep> I am very curious and doubting about Lemma 4. <sep> For any positive constant γ>0, <sep> does Lemma 4 always exist? <sep> Under Lemma 4, the authors stated that: <sep> ""Then the Maclaurin series of KLap(u) and Nk(u) have all non-negative coefficients by the classical approximation theory."" <sep> ""If the Maclaurin series of K(u) have all non-negative coefficients, <sep> K is a positive definite kernel on the unit sphere. <sep> I cannot find the original documents of (Schoenberg, 1942) and (Cheney and Light, 2009). <sep> Could you please provide these results in the Appendix such that we can check the correctness of these results? <sep> This is because, in the following part, all the proofs will be focused on γ2[zn]KLap (z)≥[zn]Nk(z). <sep> In Section 3, <sep> I have two doubting points. <sep> The first one is under the definition of Δ-domain. <sep> The authors used z to replace u and u=x⊤y as shown Section 2.1. <sep> If we put u=x⊤y=1, <sep> the Laplace kernels and exponential power kernels given in Section 2.1 will all become constant. <sep> Could the authors explain more about this assumption? <sep> The second one is under Lemma 5. <sep> The authors stated that ""Careful singularity analysis then gives ..."". <sep> It is very difficult for me to easily get these two results about Maclaurin coefficients. <sep> Could you give the detailed steps for the derivation of these results? <sep> In Theorem 2, <sep> I am very curious about the seemingly contrary results: <sep> $\\mathcal{H}{K{\\text {exp }}^{\\gamma_1, \\sigma_1}} \\left(\\mathbb{S}^{d-1}\\right) \\subseteq \\mathcal{H}{K{\\text {exp }}^{\\gamma_2, \\sigma_2}}\\left(\\mathbb{S}^{d-1}\\right),and\\mathcal{H}{K{\\mathrm{exp}}^{\\gamma_2, \\sigma_2}}\\left(\\mathbb{R}^{d}\\right) \\subseteq \\mathcal{H}{K{\\mathrm{exp}}^{\\gamma_1, \\sigma_1}}\\left(\\mathbb{R}^{d}\\right).$ <sep> Could you provide more explanations about this point in Section 4. <sep> The superficial explanations cannot help us understand this point. <sep> We need to go deeper into the proof sketch to figure out what makes this phenomenon happen. <sep> I didn't go much deeper into the Appendix, <sep> so it is hard for me to check the correctness of the whole paper. <sep> Since *CONF* is a highly selective conference, <sep> I will give a score 5 at present. <sep> After the authors solved my concerns and other reviewers verifies the correctness of all proofs, <sep> I may raise my score to 6 or 7.","The paper closes an important gap in our understanding of neural tangent kernels. <sep> In addition, the used techniques are novel. <sep> My low confidence is mainly based on the fact, that the review process at conference is not perfectly suited to deal with such papers, since their review would actually require both expert reviewers and substantially longer reviewing periods."
"Summary: <sep> The paper builds upon prior work that shows that overparameterized networks learned by ERM can have poor worst-case performance over pre-defined groups. Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis-calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re-training the classification layer. They show improvements over ERM in worst-case subgroup error. <sep> Strengths: <sep> The paper is very well-written and easy to follow. <sep> The paper provides a better understanding of worst-case generalization in overparamaterized models by isolating the issue to the classification layer, which can help machine learning practitioners better understanding how they can address the issue of poor worst-group performance. <sep> Weaknesses: <sep> The scope of the work seems largely limited to the set-up from Sagawa a, b. Given that all three datasets are simplified/synthetic (only one attribute, max. 4 subgroups), it would have been great to see how this paper's analysis applied to more complex settings. <sep> It is not immediately clear why we would not expect classifier retraining/threshold correction (which as the author notes, are standard techniques) to work for the overparameterized setting? The richness of learned representations is well known, so in some sense the findings are not too surprising, especially given the simple (e.g. binary label) settings that make post-hoc corrections less complex. Could the authors better explain why overparameterization reduces our expectations on the effectiveness of these post-hoc procedures? <sep> The requirement of knowing the groups a priori seems rather significant. While one of their main cited works (Sagawa a) seems to have touched on group attribute mis-specification, this was not explored here -- how does having imprecise knowledge of the groups effect performance when using threshold correction or classifier retraining? <sep> I have trouble fully understanding Figure 2 without a sense of what ""insufficient"" or poor representations would look like in a tSNE visualization. <sep> Recommendation:I recommend acceptance. While I remain concerned about the limited scope of the experiments, I believe the paper adds valuable insights to the overall important topic of robustness / worst-case generalization. <sep> Questions: <sep> Does the nature of the attribute (land vs water, or hair color) have any effect on the observed poor worst-group performance, or are the results are mainly due to the fact that some groups are rarer than others? For example, would the authors expect similar results if y=male,female and A=blond,dark for celebA? <sep> Did the authors visualize the embeddings of models trained with DRO, to see whether there is any improvement in the learned representations ability to distinguish subgroups?","This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques. All reviewers were positive about the paper, though R5 questioned the novelty of the paper which built heavily on a few previous papers (in particular, it builds heavily on Sagawa et al. 2020a,b). The AC is satisfied with the authors`' response clarifying the novelty. Given that this topic is quite timely and of interest to the *CONF* community, and that this paper presented a clean investigation on it, the AC recommends acceptance."
"Summary <sep> The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model. <sep> The approach introduces discontinuities in the input-prediction space by (i) training an ensemble of models such that for a given OOD input, the predictions are randomized (ii) at inference time hashing an input to a particular input in the diverse ensemble. <sep> Evaluation on simple datasets (MNIST, Fashion, CIFAR) show that the approach is reasonably effective at defending and moreover without a significant degradation of the utility. <sep> Strengths <sep> 1. Well-motivated problem <sep> The defenses for model stealing are not as well-investigated as attacks. I appreciate the authors take a step towards addressing defenses; the initial results look promising. <sep> 2. Insight <sep> I like the insight used by the authors in the defense i.e., to generate discontinuities in the input-prediction space. This is well illustrated in Fig. 1. <sep> 3. Writing <sep> The paper is written well and is easy to follow. <sep> Concerns <sep> Major Concerns <sep> 1. Din vs. Dout <sep> It is a nice insight that adversarial users indeed rely on querying data from a different distribution. However, my first concern is that is also naturally true for benign users. After all, no one except the victim/defender has access to the training data distribution and all queried data (whether by adversary or benign users) is out of distribution. <sep> As a result, I wonder how applicable the defense is -- would the defense intentionally mispredict in non-IID query setups? For instance, when queried with a particular dog breed unseen during training? Or querying slightly translated digit images to standard MNIST classifier? <sep> More generally, the proposed approach requires drawing a strict line between in and out distribution which I find is not clearly analyzed (follow-up remarks in point 4). <sep> 2. Diversity Objective <sep> If I understand the training objective correctly (Fig. 2b, Eq. 3), each model fi in the ensemble is encouraged to generate a random prediction when queried with out-of-distribution (OOD) inputs. Moreover, in expectation, results in a uniform distribution over predictions on OOD data. For in-distribution data, the predictions ideally remain unchanged. <sep> As a result, my concern is that this appears to be simple OOD detection followed by misprediction/calibration in disguise. Consequently, I wonder if one can simply leverage existing advances in OOD detection to achieve the objective. <sep> Moreover the OOD implicitly performed here requires access to a corresponding dataset Dout to model unknown OOD samples. This contrasts some existing OOD detection works (e.g., ODIN, Liang et al., *CONF* '18) which shows success without explicitly modelling OOD data. <sep> 3. Hashing <sep> I am also concerned that the hashing strategy (Sec. 4.2) employed by the authors appears to be a weak link in the defense. <sep> Specifically, it appears that a robust defense requires the hashing function to consistently select the same model in the ensemble in spite of small changes in the input. However, going by Fig. 6 it does not necessarily seem to be the case. <sep> As a result, I wonder if an attacker can exploit the hashing function to recover whether the output is clean/poisoned -- such as by aggregating predictions over a set of transformed inputs. <sep> 4. Evaluation <sep> While the results look promising, I have two concerns here related to the evaluation. <sep> First, I would have preferred if the results in Table 3 were presented as curves (with defense vs. attacker accuracy). This would have provided some insights on how the defense performs w.r.t the important λ hyperparameter in their objective. Additionally, it would also make the comparison with AM baseline fair, especially given that AM was evaluated on a curve and it is unclear which specific instance was used for comparison. <sep> Second, which also connecting to point 1, is that the paper's experimental setting assumes a significant discrepancy between the distributions e.g., Victim's Din = MNIST, Dout = KMNIST, Attacker's data = FashionMNIST. Here, the data and semantic classes are completely disjoint. I wonder how the defense performs with other choices of attacker's data e.g., EMNIST which is a bit more similar to MNIST. <sep> Minor Concerns <sep> 5. Experimental settings <sep> I find missing some important experimental parameters that is not mentioned in the paper: how many images were queried by the attacker for results in Table 1? What was the value of λ used? <sep> 6. Subverting schemes <sep> I was also disappointed that the authors do not demonstrate that the defense is robust to some simple schemes used by an attacker to bypass the proposed defense. For instance, connecting to point (3), by aggregating predictions over a set of transformed inputs. <sep> Nitpicks <sep> 7. Some nitpicks <sep> λ is overloaded: in Eq. 3 and also for the JBDA attacks <sep> ""AM trades off benign accuracy for improved security"" -- would the proposed approach also trade this off by increasing the value of λ? <sep> Post-rebuttal updates <sep> Thanks for the detailed response and additional evaluation. <sep> Q1. Fair point -- it appears that defenses do assume attackers query from a reasonably different distribution. This defense does make this assumption explicit by including it in the training objective (Dout in Eq. 3). But then again, it seems typical for OOD-based defenses (e.g., AM). <sep> Q3. This concern also generally connects to Q6. It would be nonetheless interesting to analyze scenarios where the attacker attempts to use some auxiliary knowledge to break the defense. This is also shared by some other reviewers. <sep> Q4. Thanks for presenting the curves. Assuming strictly non-overlapping OOD data does seem like a strong assumption though. <sep> Nitpick editorial comment: Please serif-based fonts for text in equations e.g. argmax(⋅)→argmax(⋅),index→index <sep> I am slightly increasing my rating.","This paper proposed an ensemble of diverse models as a mechanism to protect models from theft. <sep> The idea is quite novel. There are some concerns regarding the robustness of the hashing function (that I share), however not every paper has to be perfect, especially when it introduces a novel setup. <sep> AC"
"---- Summary ---- <sep> The paper extends video-to-video translation model of (Wang'18) to video prediction by first generating a sequence of segmentation masks and then translating them into videos. Variational video prediction is used to generate a sequence of segmentation masks. The model produces impressive high-resolution and long-horizon results, and is extensively evaluated on Kitti, Cityscapes, and dancing data, outperforming some previously proposed methods. <sep> ---- Decision ---- <sep> The paper proposes a relevant method for hierarchical video prediction with impressive high-resolution and long-horizon results. As the paper notes, this sets a new standard for video prediction methods, and will likely spur more research into achieving similar results without the labeled data requirement. I am willing to accept the paper provided the author's response clarifies my questions. <sep> ---- Strengths ---- <sep> The paper presents a modern version of Villegas'17a, powered by the advances in probabilistic video prediction and generative adversarial networks. The proposed model significantly outperforms prior work, scaling to high-resolution images (256x256) and long horizons (2500 frames). <sep> ---- Weaknesses ---- <sep> A weakness in the experimental setup is that the compared baselines are not controlled for the number of parameters. In particular, SVG-extend, which also works well on Kitti data, seems to contain the same number of parameters as the segmentation prediction network of the proposed model. It is also unclear how the baselines were tuned and what was the range of considered hypeparameters for all methods. More generally, it would be good for the paper to present some simple setting in which SVG-extend works well to analyze where hierarchical prediction is most important. <sep> ---- Additional comments ---- <sep> There is a number of minor inaccuracies in the paper: <sep> The authors of Wichers'18 are cited as ""Ruben Wichers, Nevan Villegas"". The real authors of this paper are Nevan Wichers and Ruben Villegas. <sep> Page 2, ""We employ the sequence model based on VAE (Denton & Fergus, 2018)"". Denton and Fergus did not invent VAEs, neither they invented sequential VAEs. You likely want to cite the original VAE papers (Kingma'14, Rezende'14), or the sequential VAE papers (Chung'15, Fraccaro'15). Somewhat strangely, none of these papers are cited elsewhere in the paper either. If you do want to cite Denton & Fergus, the sentence would need to be ""We employ the sequence model based on VAE proposed by Denton & Fergus (2018)"""". <sep> Page 3, ""we skip hidden representations of the encoder to the decoder at every time step during testing to handle longterm dynamics in structure"". Would be great to cite some papers that do also that, e.g. Villegas'17a or Finn'16. <sep> ""Extension to object boundary prediction"". This paragraph seems to be hastily written and full of incorrect statements. The G is not an autoencoder (and not a denoising autoencoder either). It predicts e from s, maybe just call it a boundary prediction network? The conditional GAN objective does not maximize p(e,s). In fact, p(s) is a constant. Instead, it tries to match p(e|s), but not by maximizing likelihood, but by minimizing an approximation to Jensen-Shannon divergence (see Goodfellow'14). <sep> Eq (1) is not a variational lower bound for beta <> 1.","This paper proposes a new implementation of a previously proposed two-stage process for video prediction: first predict future segmentation maps, then map them to video frames. Combined with other advances in video prediction and image generation, this simple idea is shown empirically to work very well, producing video predictions up to many hundreds of frames into the future in real stochastic settings with unprecedented quality. Strong ablation studies over the course of the review process further serve to confirm the value of various design choices involved in the implementation."
"Summary <sep> Current methods on adversarial robustness certificates consider data points independently which are highly pessimistic for structured data. This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. <sep> This work focuses on Graph Neural Networks comparing between a Naive collective certificate (baseline) and a proposed collective certificate that combines single-node certificates effectively. <sep> The experiments compares these two methods against certified ratio vs. attribute and edge perturbations on the datasets Cora-ML, Citeseer, and PubMed. <sep> Pros <sep> The paper is well-written and easy to follow. <sep> The paper tries to address a very common problem of adversarial attacks where data points are structured. Although it is a common problem, it was not explored with respect to collective robustness certificates before this work. <sep> The paper shows a novel, effective way of combining individual certificates by incorporating locality. <sep> The paper presents an LP-relaxation method that allows us to solve the certificate fast for large graphs where mixed-integer problems are prohibitively costly. <sep> The paper shows strong theory and experiments to illustrate the efficacy of the proposed collective certificate. <sep> The experiments show run-time and uncertainty measures with multiple runs for statistical significance. <sep> Questions <sep> Is there a reason why [1] was not discussed in the paper? it is highly relevant as it also studies adversarial robustness for structural attacks. <sep> How does the proposed method apply to robust Associative Markov Networks (AMN) in [1]? <sep> Did you try running the proposed method on WebKB and Reuters which are present in this work [1]? <sep> How does the linear relaxation in this work differ from the one provided in [1]? <sep> How would the proposed method compare against robust AMN [1]? <sep> Can the findings be used to come up with robust methods outside classification, like segmentation and scene understanding? <sep> In summary, I like the novelty of this method and the through experiments that were conducted that illustrate the efficacy of the proposed collective certificate, thus I recommend an accept. <sep> [1] Kai Zhou, Yevgeniy Vorobeychik. Robust Collective Classification against Structural Attacks. UAI 2020. <sep> ------- Post rebuttal <sep> I am satisfied with most of the rebuttal the authors have provided, and I have raised my score to an 8.","This paper considers a new setting of robustness, where multiple predictions are simultaneously made based on a single input. Different from existing robustness certificates which independently consider perturbation of each prediction, the authors propose collective robustness certificate that computes the number of predictions which are simultaneously guaranteed to remain stable under perturbation. This yields more optimistic results. Most reviewers think this is a very interesting work and the authors present an effective method to combine individual certificate. The experimental results are convincing. I recommend accept."
"Update after response: I appreciate the authors making their contributions clearer, and adding details about the training loss and error. I have increased my score accordingly. <sep> Original Review: <sep> This paper presents an empirical evaluation of whether flatness correlates with generalization using a few different definitions of flatness - local energy and local entropy. The authors study two training procedures - entropy sgd and replicated sgd, and show that deep networks trained using these procedures are able to locate flatter solutions that also generalize better. <sep> While the paper presents some interesting empirical confirmation of the correlation between local entropy/energy and generalization, the algorithms presented in this paper have also been defined in previous work, and this phenomenon has been repeatedly observed with different definitions of flatness [1,2]. The authors also do not present any reasons to expect that generalization is related to flatness that are grounded in theory. The contributions of this paper thus seem to be confirmation of previously observed phenomena [3,4]. <sep> Moreover, in the experiments it is not clear whether the solutions that are reached by the training procedures actually correspond to local or global minima of the cross-entropy loss function (There is also the issue that minimizers of the cross-entropy loss function occur at infinity). The authors do not report the training loss at the solutions that they choose to plot, and the training error also does not seem to be zero (plots corresponding to Figure 3). I am not sure whether it makes sense to call these solutions minima, and compare their flatness. <sep> [1] Neyshabur, B., Bhojanapalli, S., McAllester, D., & Srebro, N. (2017). Exploring generalization in deep learning. In Advances in neural information processing systems (pp. 5947-5956). <sep> [2] Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., & Tang, P. T. P. (2016). On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836. <sep> [3] Baldassi, C., Pittorino, F., & Zecchina, R. (2020). Shaping the learning landscape in neural networks around wide flat minima. Proceedings of the National Academy of Sciences, 117(1), 161-170. <sep> [4] Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., ... & Zecchina, R. (2019). Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12), 124018.","This paper studies the link between generalization behavior and ""flatness"" of the loss landscape in deep networks. Specifically, the authors study two measures of flatness (local entropy and local energy), and show that these two measurements are strongly correlated with one another. Moreover they show via a careful set of numerical experiments that two previously proposed algorithms (entropy SGD and replica SGD) that optimize for local entropy tend to both find flatter minima as well as provide better generalization. <sep> Despite the fact that the paper proposes no new models or algorithms, the experiments are compelling and provide non-trivial insights into predicting generalization behavior of deep networks, as well as solid evidence on the benefits of entropy regularization in SGD. The authors also seem to have satisfactorily answered the (numerous) initial concerns raised by the authors. Overall, I recommend an accept."
"Summary <sep> The paper introduces a noise-robust loss function CORES2, motivated by peer loss. The novel loss adds a regularization term that promotes confident prediction and pushes the model prediction away from the prior of the label. Using this loss function, the authors propose a dynamic sample sieve to separate the clean data and corrupted data on-the-fly, by the magnitude of CORES2 loss. The author's approach is to rule out samples whose losses are larger than an adaptive threshold. Importantly, the process of sieving successfully sieves out corrupted samples, both in a theory of 'better than random guess classifier' and in practice. The authors, then show that the proposed CORES2 can be decoupled under the instance-dependent noise setting. Then CORES2 is proved to be noise-robust, which means CORES2 is equivalent to minimizing the original cross-entropy loss. They also show a principle approach for finding the hyperparameters β. Further, a consistency loss is adopted after sample sieve on the corrupted samples. The author conducts extensive experiments, including CIFAR10, CIFAR100, and Clothing1M under different settings of noise. CORES2 achieves the SOTA results in all the experiments. <sep> Contributions i) Proposal of a novel confidence regularized loss for image classification and a novel algorithm that dynamically sieves out corrupted samples basing on their loss. <sep> ii) The proposed loss is proved to be noise-robust. This theoretical result is valuable since less the instance-based noise is not well studied. A principle way to choose the hyperparameter β. <sep> iii) Application to CIFAR10, CIFAR100, and Clothing1M (real-world dataset), with the extensive comparison. <sep> Issues: <sep> i) The motivation behind the dynamic sample sieve is unclear to me. One hypothesis is that if we set CORES2 as the objective, the model will fit clean samples much faster than corrupted samples at the beginning of training.  Hence the model can sieve the corrupted samples out. However, Fig2 shows that the cross-entropy can also separate clean/corrupted samples at the early training stage. I am curiously about the performance of cross-entropy loss + dynamic sample sieve. The comparison between CORES2 and cross-entropy in Fig2 is unfair. <sep> Theoretically, the paper only shows that once the model is better than random guess, the dynamic sample sieve will not sieve clean samples out. And the assumption is fxn(yn)>1/K. But fxn(y~n)>1/K would also frequently happen, for yn≠y~n, which means the dynamic sample sieve will also keep these corrupted samples. <sep> Overall, I think the high-level insight into the sieving process is unclear in the paper. <sep> ii) The training stability of the confidence regularizer: The authors show that confidence regularizer can promote confidence prediction. So some entries in f(x) is near 0, and will make the optimization process unstable since the confidence regularizer -> inf in this case. <sep> There is another concern related to theorem 4. Since the Bayes optimal classifier is fx∗[y]=1, its CORES2 loss would be -inf, which seems problematic. <sep> iii) Selection for β, according to theorem 4: theorem 4 provides a principal way for estimation of \\beta in different datasets. However, to estimate β, we have to estimate the noise transition matrix beforehand. And there may exist some adversary samples that make the rough estimation impossible. In another perspective, we can also give some meaningful upper/lower bound for β in the instance-independent noise setting. So it's unclear to me what makes the difference in the instance-dependent setting if we do not go through all the samples but do a rough estimation. <sep> iv) One motivation of Confidence Regularizer is that confident prediction counters the overfitting of noise labels. But if the model capacity is sufficiently large, I think it can both overfit CORES2, and overfit those noise samples. <sep> Minors: <sep> a) The assumption shows that CORES2 does not favor the non-diagonal dominant noise rate, which means the diagonal entries in the noise transition matrix T can be smaller than some entries in the same row. Does CORES2 fail in this setting in practice? <sep> Overall, the paper's approach is novel and easy to implement. I am willing to raise my score if the authors can address my issues. <sep> EDIT------------ <sep> I think the authors replied to some of my concerns in a convincing way, hence I raise the score to 6. <sep> Unfortunately, I think the theoretical analysis for the noise-robust loss is orthogonal to their sampling sieving approach. And the analysis for choosing β does not dependent on their instance-dependent noise settings. We can get the same β by very rough estimation(their approach) in instance-independent noise settings. In addition, I guess fx∗[y]=1 is still problematic in the theoretical analysis since CORES2=\\inf given the ideal classifier. <sep> Overall, following author's response, I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision.","Dear Authors, <sep> Thank you very much for your very detailed feedback to the reviewers. They have highly contributed to clarifying some of the concerns raised by the reviewers and improved their understanding of this paper. <sep> Overall, all the reviewers acknowledge the merit of this paper and thus I suggest acceptance of this paper. <sep> However, as Reviewer #4 pointed out, there are conceptual and theoretical issues that need to be more carefully addressed. <sep> Please clarify these issues in the final version of the paper."
"This submission proposes a way to prune neural networks using a continuous penalty function. <sep> == Pros == <sep> The experiments include some interesting illustrations beyond the basic accuracy comparisons. This includes illustrations of how weights are distributed between the layers (Fig 5), ablations on different components of the penalty function, and experiments on the transferability of the selected sparsity pattern. <sep> There is a very complete description of the training procedure. Writing overall is clear. Code is included with submission. The code is readable and easy to get running. <sep> == Cons == <sep> Many previous works propose a new sparsity penalty function, and associated optimization, for neural network training. As well as (Lemaire et al) and (Louizos et al) cited in the submission, there's is works such as: <sep> Srinivas et al. ""Training Sparse Neural Networks."" CVPR 2017. <sep> Manessi et al. ""Automated Pruning for Deep Neural Network Compression."" ICPR 2018. <sep> Zhu et. al. ""Improving Deep Neural Network Sparsity through Decorrelation Regularization."" IJCAI 2018. <sep> Yang et. al. ""DeepHoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures."" *CONF* 2020 <sep> Within this general class of approaches, though, the specifics of the penalty function and optimization in this paper are novel. Whether that is a valuable novelty depends entirely on the empirical results: since the higher-level idea is well-explored this adds to the literature iff the submission has made the best design choices among these other methods. <sep> Despite their overall heft, the experiments have some missing pieces that make it hard to evaluate this. To start, the evaluation is on a less common (within the sparse neural network literature) choice of backbones for each dataset. See: <sep> Blalock et. al. ""What is the State of Neural Network Pruning?"" Sections 3.3 & 4.2 <sep> Wider-ResNet would be expected to have greater redundancy than ordinary ResNet, perhaps making an easier problem for sparsifying. However, this choice is inherited from BAR. <sep> Ordinary ResNet-50 is a more common choice, but less so on CIFAR-10. This choice differs from the corresponding experiments in the (Liu et al) work that is compared against, that uses VGGNet, DenseNet-40, and ResNet-164. <sep> The code appears to have partial support for a much wider variety of models (see line 19 of pruning.py), though maybe not all are actually supported: for instance r32 and r152 don't seem to be included in the branches in models/resnet.py lines 421-428. <sep> And generally there are few comparisons made. This is possibly unavoidable due to the overall fragmentation of metrics. Choices of comparisons are the more closely similar papers in the literature (this is good), but lacks more some of the simple baselines (that can often do surprisingly well): <sep> Gale et. al. ""The State of Sparsity in Deep Neural Networks"" <sep> I don't see experiments that show proposed optimization has consistent convergence across runs. Experiments dealing with ""stability"" are either highlighted single comparisons, or look at stability w.r.t. hyperparameters. Some of the reported accuracy differences in comparison are quite small, especially at higher levels of sparsity, so seeing the standard deviation of multiple runs of each method compared might be especially important. <sep> == After Rebuttal == <sep> I had previously missed that Lemaire et. al. had some of the key comparisons (simple baselines done for the same architecture/dataset pairs) that are necessary to judge this method. I thank the authors for pointing this out. It is also helpful that these baselines are now included for completeness. <sep> C.6 still seems like an indirect measure of optimization stability, but it is reasonable to conclude from it that the differences from BAR are real, given the very low standard deviation in accuracy. <sep> I remain on the side that this is still a relatively incremental addition to the pruning literature, but I am now satisfied that it is well-validated.","This paper proposed a new method to prune neural networks using a continuous penalty function. All reviewers suggest acceptance (some are on borderline though) as the authors did a good job in the rebuttal phase. AC also could not find any particular reason to reject the paper (in particular, the overall writing is clear) and thinks that this paper is a meaningful addition to *CONF* 2021."
"The contributions of the paper center on i) the introduction of diffusion-related tools for studying classifier decision boundaries; and ii) using those tools to connect decision boundary geometry, adversarial robustness, and generalization. The paper provides an analysis that provides insight into how curvature and local decision boundary geometry is influenced by adversarial defences, suggests a method for checking adversarial robustness, and then makes statements about model generalization based on geometric properties revealed by monte-carlo simulation of diffusions. <sep> Strengths: <sep> The paper brings together beautiful areas of mathematics, probability, and random processes in an effort to characterize decision boundaries of DNNs and the behavior of DNNs on unseen examples, adversarial or otherwise. Curvature, heat diffusions, diffusion geometry, compression play a role. Feynman-Kac duality is leveraged to pass from the intractable analytical methods in the literature to stochastic simulations that can be undertaken by practitioners with data. The paper appears to make some novel links between generalization and decision boundary diffusion geometry, offers an apparently novel analysis of the impact of common adversarial defenses to Brownian adversaries, and at a minimum offers some new insights into how we might think about, and interpret, complex decision boundaries learned by neural nets or other nonlinear classifiers. The paper also follows up with experiments in the context of real applications and complex models. A series of Appendices provide technical details and experimental protocols. <sep> Weaknesses: <sep> The breadth of topics, the range of tools, steps, and quantities seems to have left the authors without enough space to get it all across. While the Appendices provide valuable details, additional definitions and discussion, the main body of the paper lacks clarity, context, and sufficient organization to allow an average reader to follow, or even appreciate, some of the arguments and key contributions. This is potentially the main weakness of the paper. If this were a longer journal submission, it might even make sense to separate the work into two papers: one exploring adversarial learning, and another exploring generalization. <sep> The introduction and motivation sections of the paper could be improved by explicitly stating at a high level what the paper is contributing, and what insights will come out of the analyses. For example, the abstract states: ""This leads to new insights concerning the ""flattening-of-boundary"" phenomenon."" What insights are in store, specifically? Imagine the paper were to be selected for a popular pod-cast, or a 5-minute lightning oral presentation at a conference: how would you distill it down to the essential contributions, components, and logical steps required to arrive at the key results? <sep> Another concern is that while adversarial defense training by studying hitting times is valuable, it feels slightly misplaced/incomplete because an adversary might (often?) follow geodesics/geometry to very quickly obtain an error sample in cases where diffusion distances are high (e.g. a dumbbell). So while it's helpful to be able to say that a random walk has a low probability of becoming an error sample from a point x, it doesn't necessarily give us a guarantee about a ""determined"" adversary, who isn't constrained at all to follow random walks (""Brownian attacks""). <sep> Recommendation: <sep> Overall I recommend a borderline reject. The paper has some interesting ideas, and probably novel contributions, but needs revising for clarity and conciseness to be digestible or impactful. The paper needs to also be more specific about how the results relate to, leverage, and complement those in the literature/references. For example, the paper says in a few places that conclusions ""agree with"" reference [X]. Does this mean they have just rederived the same result, offering no additional insight? What is the significance of the agreement, and what does it provide to the community as a takeaway? It would be helpful to call out very concretely what is being contributed, and how it relates/differs relative to the literature. <sep> Other suggestions: <sep> Perhaps clarify early on in the paper where the Brownian motion is happening. A reader might wonder: Is it in the ambient space? Is it on a graph? Is it on a manifold parameterized by some kind of intrinsic (or local) coordinates extracted from the model? <sep> Discuss the significance and interpretation of Lemmas 2.2 and 2.3. Why are they introduced? What are they saying intuitively, and how are they going to support your arguments later? There's very little text around them (maybe due to a space problem and some ruthless trimming to make it all fit!) <sep> For the sake of completeness, define explicitly how you intend to handle ties in taking the argmax, and by extension, when defining E(y) (i.e. so that N is a subset of E(y)). <sep> Section 4 seems to have lost sight of the overall desired result, by omitting how L(g) controls L(f). Apologies if I've missed something obvious, but it seems like the strategy to control generalization of f in terms of hitting times is to pass to the compressed version, and control that. So where's the link between generalization of f and generalization of g? Does Def 1 say it? <sep> Eq (7) -- \\phi_{E} should be introduced to make the definition self contained. Discuss why Def 1 is reasonable and what it means intuitively. The paper says it's an ""initial suggestion"". Why did you suggest this particular defn, over others? <sep> pg. 4 ""isocapacitory"" results: the analogy abruptly jumps from heat diffusion to ""charge"" accumulation. Try to link the two and provide a transition (without the reader having to refer to the lengthy appendices). <sep> It could be interesting to make a connection to heat diffusion classifiers (e.g. Szlam, <sep> Regularization on Graphs with Function-adapted Diffusion Processes, JMLR 2008), in which heat is instead diffused outward from the labels to classify new points, giving a potentially interesting characterization of the decision boundary (by construction). Maybe such methods can locally approximate more complex learning algorithms thereby providing global insight into their decision boundaries? <sep> UPDATE TO REVIEW FOLLOWING AUTHOR REVISIONS AND COMMENTS <sep> I thank (and commend) the authors for their detailed, point-by-point responses. <sep> The authors have made a good effort in their revisions to improve and clarify the exposition, and rectify the other comments made by the reviewers. The paper could still benefit from a deeper rewrite -- there's just so much that can be packed into a conference paper with limited real estate, and the authors are seeking to make several contributions under the umbrella of one submission (as the title suggests). So clarity suffers, and impact will suffer as a result. But, in my mind that shouldn't necessarily be a show stopper at this stage, in light of the revisions. I am therefore upgrading my recommendation.","Four reviewers have reviewed this paper and after rebuttal, they were overall positive about the proposed idea. We congratulate authors on the paper."
"Summary <sep> This paper proposes a method for domain adaptation in RL where the source and target domains differ only in the transition distriubtions. A theoretical derivation based on RL as probabilistic inference is presented that starts with the objective of matching the desired distribution of trajectories in the target domain with the distribution achieved by the policy in the source domain. The final objective appears as a modification to the reward function while training in the source domain and is implemented easily with just two binary classifiers that predict the domain given either state-action or state-action-next-state tuples. Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. Experiments are presented that show improved performance in terms of rewards vs experience on target domain on environments such as broken reacher, broken ant, etc (where the target domain has some ""broken"" component). Further, it is also shown that the reward modification on source visually matches the reward expected in target (Fig 4), that without the reward modification the policy usually exploits the source domain's transitions which cannot be exploited in the target domain, and finally, that safety emerges from the proposed objective. <sep> Strengths <sep> The theoretical bound presented in Theorem 4.1 is strong given that Assumption 1 is quite mild. In fact, Assumption 1 is trivially met in most of the environments used in Fig 6, e.g.: for half cheetah obstacle, the target policy does not run in to the obstacle and hence will get the same reward in the source and target environment. <sep> The final form of the objective as a modification to the reward is a simple and easy to implement objective with the two binary classifiers. Further, the max-entropy derivation makes it applicable to any maximum-entropy algorithm. <sep> The empirical evidence shows that the proposed method has very similar performance to the RL on target baselines while improving over other domain adaptation baselines in four continuous control environments. <sep> Weaknesses <sep> The variety in the source-target domain shift seems limited in the experiments shown in Figure 6. Three tasks have the crippled-limb setting and one task has an obstacle in the target domains. The introduction started off by giving some great examples such as aggressive driving failing to work on target domains with icy roads. It would have been great to see application of this method on more subtle but dangerous changes to the target domain, similar to the car driving example. For example, reducing friction on the HalfCheetah or Ant environments -- it would be quite interesting to see what strategies develop in the source domain; does the agent take shorter steps to staty in contact with the floor? <sep> The assumption that the rewards for the target and source domains match is suitable for the choice of environments in Section 6 but seems quite limiting to a more general setting where certain rewards may never be observed in the source domain. Further, there is an implicit assumption that the source domain is ""free"" and the target domain is ""restrictive"" (e.g. broken ant, obstacle halfcheetah). It would be good to discuss the rare but still possible case where the opposite is true, the target domain allows for more ""free"" movement. <sep> Other issues/comments <sep> Theorem 4.1 has a crucial typo: psource is mentioned on the right hand side but it should actually be ptarget. The appendix (Theorem B.3) has the correct version of this theorem statement. <sep> In Theorem 4.1, the reward-maximizing (entropy-regularized) policy in the target domain is said to satisfy Assumption 1. To confim, does this mean that, in the target domain, the policy that maximizes the entropy-regularized reward objective is the same as the policy that maximizes rewards without the maximum-entropy constraint? <sep> Minor typo in Section 5, Algorithm Summary: qθSAS should be qθSA in the second line. <sep> I don't understand the transparent colored lines in Figure 6, are they supposed to represent error bars or are they the non-smooth version of the darker colored lines? <sep> Typo/stray sentence in the last para, last line of Section 1. <sep> Feedback to authors <sep> To further study the usefulness of this approach (as future work perhaps), it would be good to apply it to an offline RL setting where a finite set of ""good"" trajectories from the target domain are provided and no further trajectories can be collected during training. Only during test time is the agent then deployed in the target domain to measure performance (and possibly safety). <sep> Post-rebuttal Update <sep> The authors have shown new experiments on icy environments that show good results for the proposed method (DARC). This directly addresses my point (and recommendation) about trying out experiments similar to the aggressive driving on icy road example that was mentioned in the introduction. Having read through the other reviews and responses by the authors, I feel that most major concerns have been addressed. As such I am inclined to increase my score from 7 -> 8, recommending acceptance of the paper and entrusting the authors to include the new experiments in the main paper. <sep> A minor note: My second point under ""Other issues/comments"" section was not answered in the rebuttal. I hope the authors can clarify this in the future either in the main paper or appendix.","This paper presents an approach to domain adaptation in reinforcement learning. The main idea behind this approach, DARC, is to modify the reward function in the source domain so that the learned policy is optimal in the target domain. This is achieved by learning a classifier that learns to discriminate between the data from the source domain and those from the target domain. <sep> Overall, reviewers appreciated the intuitiveness of the approach as well as its formal analysis. They had some concerns with respect to experiments, which was sorted out in the author response period. Given the overall positive reviews, I recommend accepting the paper."
"Summary: <sep> This work aims to explore why unsupervised contrastive pretraining works as well (if not better) than the tried-and-true Supervised ImageNet classification pretraining.  They explore a number of different transfer tasks to give some intuition: <sep> Interesting findings: <sep> Augmentation doesn't make much difference for supervised transfer (from imagenet) and is essential for unsupervised transfer, with the effect monotonically increasing as more augmentations are added. <sep> Dataset semantics are less important for unsupervised pretraining:  They transfer from a variety of tasks, faces, objects, etc using supervised and unsupervised pretraining. <sep> Imagenet pretraining leads to greater localization errors (using analysis of Hoiem 2012), and  more generally a loss of spatial information (tested via image reconstruction) <sep> Propose a new supervised pretraining method to combine the idea of unsupervised contrastive training with supervised exemplar training. <sep> Extend MoCo to use supervised labels so the loss doesn't contrast examples from the same class. <sep> This improves supervised transfer performance from ImageNet to the other tasks. <sep> Finally, they look at the impact of this new pretraining on two other tasks <sep> Few shot learning: The Supervised Exemplar model outperforms the unsupervised methods, and the original cross-entropy Imagenet supervised model. <sep> Facial landmark prediction: Both the unsupervised and exemplar-supervised pretraining perform similarly and outperform either training from scratch or imagenet-supervised pretraining.  This again supports the observation that imagenet classification pretraining dilutes the spatial acuity of the model. <sep> Positives: <sep> They introduce a supervised pretraining method that can transfer better than the unsupervised method and the original supervised imagenet method. <sep> Overall, this work is clearly written. <sep> Ultimately, I do believe I have a better understanding of the differences between the supervised and unsupervised pretrained models. <sep> Negatives: <sep> The major insight into the differences is limited:  mainly that we pay a price when the low-level information is lost by the supervised pretrained model. <sep> Recommendation: <sep> These analysis papers are always tricky to rate -- often quite a bit of work goes into what seems like a small insight (maybe even obvious in retrospect).  However, I do think that this work is worthwhile for the community because 1) it shines light on a somewhat mysterious exciting new technique and 2) already shows how the findings are useful by using it to improve supervised pretraining, and a new vocabulary for evaluating pretraining techniques. <sep> Minor comments: <sep> Eq (2), should vi be qi?","The paper aims at understanding why self-supervised/contrastive learning methods transfer well when used as pretraining for fine-tuning downstream tasks (compared to e.g., supervised pretraining based on the cross-entropy loss). Three reviewers recommend acceptance, whereas one reviewer recommends borderline rejection, arguing the take home message of the paper is not very clear. While this is a legitimate concern, the AC agrees with the majority that the paper does shed light on the differences between supervised and self-supervised pretraining (based on interesting empirical findings) and recommends acceptance."
"The authors propose two meta-learning algorithms in the reproducing kernel Hilbert space (RKHS) induced by the recently proposed Neural Tangent Kernels (NTK). The authors show how their algorithms obviate an explicit inner loop or task-adaptation step in the meta-learning training phase. In first algorithm, no explicit adaptation function is used, whereas in the second, a close form adaptation function which invokes the NTK is proposed - which is a simpler adaptation than that of MAML and hence, offers computational efficiency. The work is interesting and  supported by theory inspired from the NTK theory, and adds to the newly expanding literature in the use of kernels in meta-learning (unlike the authors' claim in the introduction, theirs is not the first meta-learning paradigm in the RKHS cf (Wang et al 2020, Cerviño et al 2019)). The authors perform extensive experiments on regression and classification datasets and compare their results with other MAML-type algorithms. The experimental results do not show significant gains in terms of performance over the existing MAML approaches, except in the case of out-of-distribution datasets and adversarial attacks, where it is shown to outperform the others. The performance similarity to other methods is not surprising since the proposed approaches can be seen as an efficient approximations of the MAML. <sep> I give my detailed comments next: <sep> -Firstly, I believe the title of the paper could be changed to 'Meta-Learning in the RKHS induced by the NTK' or something more specific.  Currently, it comes off as rather broad and disproportionate to the work and existing work. <sep> -I had some issues with the claim that both the Meta-RKHS approaches do not need an explicit inner-loop adaptation: this does not seem true. In the case of MetaRKHS-I , an inner adaptation is not needed in the meta-training, but necessary just like the MAML during for a test task. In the case of MetaRKHS-II, the NTK gradient flow based adaptation in Eq(6) forms the inner-loop— just that it is a more efficient inner update than the MAML. The authors must consider rephrasing the claim to reflect these. <sep> Meta-RKHS-I: <sep> From what I understand, the treatment is based on an approximation to the MAML with k-inner gradient descent steps through a Taylor expansion. To this extent, one can expect the performance to be similar to that of k-step MAML on using the step size kα in equation (4) - indeed we see this in the experimental results. <sep> It is unclear as to how once the meta-parameter θ is learnt, the parameters for a new task are obtained from it— from what I see, it is obtained by k-steps of actual gradient descent just like the MAML. This was not mentioned anywhere clearly in the manuscript, I estimated this from the experiment plots in Figure 5 of the Appendix which mentions different inner steps. <sep> The Taylor expansion in of equation (2): Under what case is this expansion valid, does this again assume a high degree of similarity between the tasks? <sep> Before equation (3): '…equation 2 is an unbiased estimator of …' This claim is not immediately evident to me, would be better if the authors could expand a bit here <sep> The connection to the NTK seems a bit weak and superficial — Based on eq (3), the authors propose the energy functional in eq(4) for learning the meta-parameter and this is where the RKHS comes in. However, in the very next paragraph in Theorem 2, the equivalence of the gradient in parameter space and the functional gradient is claimed. Here I do not see what properties of the NTK are being invoked. Eq (4) can be with the the parameter gradient instead of the gradient in the RKHS and the approach will continue to hold valid. The authors should bring out the connection to RKHS and NTK more clearly, at present, it seems to me that the approach does not explicitly have connections to an RKHS. Indeed, the proofs of Theorem 3 and 4 also do not seem to use the properties of RKHS or the NTK. <sep> -Meta-RKHS-II <sep> Here, the authors propose an adaptation function based on the NTK and the gradient flow. This is an interesting adaptation function that evokes the NTK and in the process can help approximate a k-order inner gradient and yet be free of the computational difficulties that come due to this in the standard MAML systems. Unlike Meta-RKHS-I, the connection to NTK is strong and clear here. The authors should consider expanding and emphasising this portion better. For example, what is the implication of Theorem 5? <sep> -Last paragraph on page 5: 'Intuitively,…thus can be deem more robust.' I do not follow this, could the authors expand here? I also found the robustness discussion to be needing clarity on the whole. <sep> Figure I:  The axes and the legends are not readable. <sep> -Section 4.3 and 4.4: The Meta-RKHS methods significantly outperform other approaches in the case of adversarial attacks and out-of-the-distribution tasks. This is of merit since it is known that MAML type approaches are far too sensitive to the outlier tasks. Can this robustness be better explained or mathematically analysed in terms of the RKHS or the NTK? This would greatly strengthen the contribution. <sep> Overall, I feel that this is an interesting  and novel contribution, particularly in terms of mathematical concepts, though the approach does not necessarily outperform similar methods by a significant margin except in the case of out-of-the-distribution tasks or adversarial attacks. The connection to RKHS seems a bit weak and currently appears in the form of NTK and that too evidently only in MetaRKHS-II. A suggestion is also consider dataset cases where MAML necessarily requires multiple inner gradient steps to just one step inner gradient update ( the first order MAML) . (Currently all the examples in the paper show similar performance for both MAML and first order MAML). This could help verify if the Meta-RKHS approaches can indeed achieve similar performance as multi-step inner gradient MAML, while having much lower complexity. <sep> References: <sep> (Wang et al 2020)  Haoxiang Wang, Ruoyu Sun, and Bo Li. Global convergence and induced kernels of gradient-based meta-learning with neural nets, 2020. <sep> (Cerviño et al 2019) J. Cerviño, J. A. Bazerque, M. Calvo-Fullana and A. Ribeiro, ""Meta-Learning through Coupled Optimization in Reproducing Kernel Hilbert Spaces,"" 2019 American Control Conference (ACC), Philadelphia, PA, USA, 2019, pp. 4840-4846, doi: 10.23919/ACC.2019.8814419.","This paper considers meta-learning based on MAML. The authors use Neural Tangent Kernels (NTKs) to develop two meta-learning algorithms that avoid the inner-loop adaptation, which makes MAML computationally intensive. Experimental results demonstrate favorable empirical performance over existing methods. <sep> The paper is generally well written and readable. The proposed methods are well motivated and based on solid theoretical ground. The emprirical performance shows advantages in efficiency and quality. This work is worth acceptence in *CONF* 2021."
"After rebuttal: I am uneasy about the overstated claims made in section 2. That the architectures are small should really be mentioned more prominently. However reviewers #1 and #2 make a good case that what matters are the improvements presented in Section 5. Thus, I reluctantly recommend acceptance. <sep> Paper summary <sep> The authors empirically investigate the calibration performance of NN-GPs in CIFAR10 and several UCI data sets, in three forms: <sep> Bayesian inference for the NN-GP function-space prior, through a softmax link function <sep> Heuristics to convert the classification-as-regression posterior into class probabilities <sep> As a ""head"" to a pre-trained network <sep> Generally, they find the NN-GPs are competitive with the examined alternatives. <sep> High-level comments <sep> I think empirical work like this paper is important: we Bayesians like to justify ourselves using calibration, but unless the beautiful Bayesian methods are actually calibrated, they are not useful. This paper mainly explores some properties of existing algorithms that are not understood, which is great. <sep> Table 1's architectures are too small (2 layer CNN?) <sep> However, I will criticise the methodology. The architecture of the NNs used in Table 1 is specified in the supplement, and it is not pretty: the CNN is restricted to have only 1 or 2 layers (Appendix B)q. The convolution filter shape is unspecified, so I'll assume it is 3x3 like most modern CNNs. Because these networks are so shallow, the receptive field of a given convolutional location is not even close to the full input image. <sep> This architecture is simpler than even the 5-layer 1998 LeNet , and is very far from the 2015 ResNet (He et al.) that is a reasonable CIFAR10 baseline; let alone the current state of the art. It is also much simpler than the networks considered for kernel architectures by Shankar et al. (2020). <sep> Given how far these architectures are from things used in practice that have decent CIFAR10 performance, I'm not sure we can extrapolate much from the data in Table 1. Perhaps the good calibration and superior performance of the kernel will stop being true once we go to 18 or 20 layers, or once we add mean-pooling to the CNN.  We know that the superior performance of the kernel stops being true at a certain point for classification-as-regression, and I see no reason to think it would be different for ""proper"" Bayesian inference. (In fact, in my experience, variational inference for GP classification with these kernels has worse accuracy than classification-as-regression). <sep> It is true that calculating the kernel matrix for more complicated CNNs is much more expensive. However, you could do that experiment without tuning the hyperparameters with Vizier. Since the complexity of MCMC once you have the kernel matrix is the same, you should be able to do it. I suspect the accuracy will be much better for the modern CNNs without tuning, than for the 2-layer CNNs you found (the calibration may be better or worse). <sep> At the very least, the caveat that the CNNs are tiny should be clearly stated in Section 3 (and not hidden in the appendix). <sep> Ignores similar work doing Bayesian linear regression on top of a NN base <sep> The paper ""Deep Bayesian bandits showdown"" (https://arxiv.org/abs/1802.09127) compares various Bayesian deep learning methods on a bandit task, where calibrated uncertainty is crucial. As a result, most of them fail. The one that seems to come out on top is to change the last layer of a network to a Bayesian linear regression layer. This is equivalent to the method shown in Section 5 of this paper (except this paper sometimes uses a kernel, that is not just the linear regression kernel). It should be at least acknowledged, and ideally it would be included in Table 3. <sep> Table 3 should also acknowledge Bradshaw et al. (2017, https://arxiv.org/abs/1707.02476), which adds an RBF kernel on top of a neural network. All of these should perform similarly to NNGP-LL, especially given how close the RBF kernel is to the NNGP in Table 2. <sep> Unclear that NNGPs have better uncertainty in Table 2 <sep> And that's okay! But it should be a bit more emphasized than ""competitive in NLL"" just before Section 4.2. Also, the non-standard training for RBF kernels (selecting hyperparameters on a validation set, instead of maximising the train likelihood) may hurt their performance in Table 2. <sep> That NN-GPs are good in accuracy at UCI data sets was noted by Arora et al. (https://openreview.net/forum?id=rkl8sJBYvH), though only for classification <sep> In conclusion: overstated claims <sep> I think this paper is good, but it overstates how good the methods evaluated turn out to be. It paints a rosy picture of everything, like calibration in NNs is now solved, but that is not true. However, I do think this paper is a meaningful contribution, that puts together a few ideas that were lying around and shows that you can use them to make progress in calibration in NNs. <sep> The authors should clearly caveat in section 3 that the results may not extrapolate to bigger CNNs or, ideally, add a data point with bigger CNNs. The authors should compare to similar algorithms that will be competitive in Table 3. <sep> Minor points <sep> Top of pg. 2: SNR -> show acronym. <sep> before section 1.1: ""It is possible to disambiguate between the uncertainty properties of the NN prior and those due to the specific optimization decisions by performing Bayesian inference"". What do you mean? Transforming the prior into a posterior using Bayesian updating is one such decision. Also, because you're approximating the prior, deciding which Bayesian inference algorithm to use matters. <sep> Section 3: ""by avoiding heuristic approaches to inference, we are able to directly evaluate the prior"". I realise now that by this you mean the same as what I asked above, whatever it is. But the phrasing would be clearer with ""we are able to directly evaluate the [properties conferred by the] prior"". The fact is, you can evaluate the prior density for any weight setting fairly trivially! <sep> Page 3, top: ""under gradient flow to minimize a loss, the output distribution remains a GP"". This is only true for squared loss, as far as I know.","This paper presents an empirical study focusing on Bayesian inference on NNGP - a Gaussian process where the kernel is defined by taking the width of a Bayesian neural network (BNN) to the infinity limit. The baselines include a finite width BNN with the same architecture, and a proposed GP-BNN hybrid (NNGP-LL) which is similar to GPDNN and deep kernel learning except that the last-layer GP has its kernel defined by the width-limit kernel. Experiments are performed on both regression and classification tasks, with a focus on OOD data. Results show that NNGP can obtain competitive results comparing to their BNN counterpart, and results on the proposed NNGP-LL approach provides promising supports on the hybrid design as to combine the best from both GP and deep learning fields. <sep> Although the proposed approach is a natural extension of the recent line of work on GP-BNN correspondence, reviewers agreed that the paper presented a good set of empirical studies, and the NNGP-LL approach, evaluated in section 5 with SOTA deep learning architectures, provides a promising direction of future for scalable uncertainty estimation. This is the main reason that leads to my decision on acceptance. <sep> Concerns on section 3's results on under-performing CNN & NNGP results on CIFAR-10 has been raised, which hinders the significance of the results there (since they are way too far from expected CNN accuracy). The compromise for model architecture in order to enable NNGP posterior sampling is understandable, although this does raise questions about the robustness of posterior inference for NNGP in large architectures."
"Summary <sep> This paper addresses the problem of dense RGB-D SLAM. The key idea is to formulate the problem as a deep state-space model and infer the state of the latent variables (i.e. pose and geometry) using variational inference. The experiments demonstrate that the method performs well in a challenging quadcopter dataset. However, the advantages of the approach are not demonstrated. <sep> Strengths <sep> The proposed approach is a very elegant and principled formulation of the dense SLAM problem. The probabilistic graphical model considers the observations, dynamics and latent states of the agent (i.e. the pose and dense map). As far as I am aware, not many deep slam approaches consider all these factors in the estimation process. The proposed model is therefore a nice unification of neural networks, dynamics and multi-view geometry. <sep> The method has a number of interesting capabilities such as the ability to predict the future trajectory of the camera. The demonstration of the usefulness of the inferred map in downstream tasks such as navigation is also very interesting. <sep> Weaknesses <sep> The related work is severely lacking. Specifically, the paper seems to marginalise most of the recent methods that try to combine deep learning with dense SLAM. The authors summarize the whole collection of such approaches with a single sentence: ""depth or semantic representations with existing SLAM methods has also become a prevalent direction of research"". This isn't a fair representation of existing works and doesn't really do much to position the authors' novel contribution with respect to the existing literature. The authors need to discuss and contrast their approach to works such as BA-Net, DeepFactors , Neural RGB->D, DeepSFM etc. Currently, the related work is too focussed on traditional SLAM approaches (like VinsMono) and 2D/2.5D methods like VAST/DVBF-LM. <sep> The proposed method requires RGB-D as input which is obtained from SGM in the paper. How sensitive is the approach to noise in the depth input, and can the approach work with just RGB input? The reason I consider this a weakness is that many existing RGB-D SLAM approaches (even traditional ones like ORB-SLAM2) have very high accuracy and in some sense can be considered a ""solved problem"". This is not really the case for dense monocular SLAM which is still a very challenging problem. <sep> Furthermore, the advantages of the proposed approach are not described in the paper. In the evaluation the improvement over ORB-SLAM2 in terms of accuracy doesn't appear to be very significant. Is it simply an improvement in accuracy or are there other advantages as well? <sep> The experiments and comparisons are also rather limited. The authors have only evaluated on a single quadcopter dataset which doesn't really show the generalizability of the trained model. ORB-SLAM2 and VINS-MONO don't have any trained components and are therefore completely general (apart from the need for tuning some settings). How does the proposed approach perform when trained on Blackbird and evaluated on EuRoC, for example? <sep> The runtime of SLAM approaches is quite important, especially for application on platforms such as quadcopters. It would be good if the authors could include some indication of the runtime and whether the inference can be done in realtime. <sep> Minor points <sep> It would be good to bold the best performing methods in the table as this makes it easier to see the relative performance of the methods.","The paper proposes a method for SLAM like dense 3D mapping (colored occupancy grid) based on differentiable rendering with a possibility to provide a probabilistic generative predictive distribution, evaluated on UAVs. <sep> Initially this paper has a wide spread of reviews, with ratings between 4 and 9. Reviewers appreciated the elegant and principled formulation and the interest of the predictive distribution. On the downside, several issues were raised on the incremental nature wrt to DVBF-LM; presentation and writing being very dense and difficult to follow; positioning wrt to prior art; performance with respect to known visual SLAM SOTA baselines; limited evaluations. <sep> The authors provided responses to many of this questions and also updates to the paper, which convinced several reviewers, who unanimously recommended acceptance after discussion. <sep> The AC concurs."
"Summary of paper <sep> This paper introduces a continual learning method called Contextual Transformation Networks (CTNs). CTNs consist of a base network and a controller, which outputs task-specific feature modulators. Both these have independent memories used to reduce forgetting. Additionally, the base network has an additional regularisation term. These two networks are trained together (formulated as a bi-level optimisation problem). Experiments on many different benchmarks are provided, with CTNs outperforming competing baselines on different metrics, often by large amounts. Some good additional experiments are also provided (different memory sizes, smaller datasets, ablations of the three major parts of CTNs). <sep> Review summary <sep> For me, the great experimental results/section means I am suggesting this paper be accepted. I do not see the intuition behind many of the design considerations in CTN: there is essentially a shared base network, with task-specific scaling/shifting (given by a controller network) and task-specific heads; a key difference to previous works is the use of two different memories for the two networks. But CTN has very strong performance in continual learning benchmarks. <sep> Pros of paper <sep> The strongest part of this paper for me is the experiments section. The results are extremely good and consistent over many different benchmarks. The authors provide different metrics for continual learning (not just average accuracy), and compare against some strong baselines. The additional experiments in Tables 3 and 4 are also very nice to see. <sep> The method is presented well in Section 2, and Table 1 (comparing number of additional parameters) is nice. <sep> Cons of paper <sep> There are related works that I think the authors can mention, which have some similar ideas as in CTN (although all the ideas are never all put together as in CTN): <sep> (a) FiLM layers for meta-learning / continual learning / multi-task learning: <sep> [1] Requeima et al., 2019, ""Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes"" <sep> [2] Loo et al. 2020, ""Combining Variational Continual Learning with FiLM Layers"" <sep> [3] Rebuffi et al., 2018, ""Efficient parametrization of multi-domain deep neural networks"" <sep> (b) Soft targets for continual learning <sep> [4] van de Ven and Tolias, 2019, ""Generative replay with feedback connections as a general strategy for continual learning"" <sep> How long does it take to perform the bilevel optimisation in CTN (can the authors provide training times ideally, or else complexity analysis)? <sep> Section 3.1: ""Dynamic architecture approaches... suffer from the unbounded growth of the network size and extensive resource usage"". Is this true for all methods? Don't some methods try and achieve sublinear growth in parameters? Doesn't CTN also grow in a similar way? <sep> Additional comments/suggestions <sep> I found Figure 1 more difficult to understand than I think it needs to be. I think it would be useful for the authors to think how they can simplify it further for a future version of the paper. <sep> Did the authors try some other continual learning algorithms to avoid forgetting in the base network other than ""behavioral cloning""? (Eg EWC or GEM or any other method.) <sep> Update to review <sep> I shall keep my rating at 7. The authors updated the paper and I think it is good enough to be accepted. I still note how Figure 1 is difficult to parse (what are all the variables? Why are there so many arrows and boxes? It is not possible to understand what is going on without reading the text in detail, by which time it does not add much to the reader's understanding); in my opinion it requires starting from scratch again.","This work develops a novel framework for online continual learning, which they authors name Contextual Transformation Networks (CTN). This framework comprises a base network, which learns to map inputs to a shared feature representation, and a controller that efficiently transforms this shared feature vector to task specific features given a task identifier. Both of these components have access to their own memory. The optimization of the both the controller and base network parameters is framed as a bi-level optimization framework. <sep> Pros: <sep> important and challenging problem <sep> strong results <sep> Cons: <sep> Currently the writing creates the impression of limited novelty from a technical perspective. I would encourage the authors to more crisply highlight the technical novelty of their method."
"Summary <sep> The paper proposes a novel approach for third-person visual imitation learning from observations, ie for imitating a different agent in a potentially different environment purely from visual demonstrations. The main difference over prior work (eg Stadie et al, 2017) that used domain confusion objectives is the introduction of new regularization objectives on the learned representation of the visual scene. Empirically, the paper shows that the proposed regularizations can improve performance across a wide range of third-person visual imitation tasks, transferring between simulated agents with different appearances and/or morphologies. <sep> Strengths the paper is very clearly written and easy to follow, necessary background is adequately covered, all components of the model are explained in detail all novel design choices are ablated in the experimental section experiments are conducted across multiple environments and source/target variations, both appearance and morphology -- empirical performance improvements are demonstrated experiments include higher-dimensional, robotic manipulation environments, more complex than those tested in prior work <sep> Weaknesses not fully fair comparison to baselines: since the main novelty lies in the introduction of novel regularization objectives, the ""DisentanGAIL w/ domain confusion loss"" is the main comparison method since the only difference to the proposed method is the representation regularization function. However, this baseline does not have access to the additional collected prior datasets used for the introduced ""prior regularization"" loss. Looking at Figure 2, it seems that the performance of DisentanGAIL w/o Prior Data and DisentanGAIL w/ domain confusion loss are near-equivalent which suggests that the access to the additional prior data might be the main factor that contributes to DisentanGAIL's superior performance --> an additional baseline ""DisentanGAIL w/ domain confusion loss & prior data"" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data missing baseline results on harder tasks: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches <sep> Questions from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for any two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. How does that fit to the explanation made in Section A + the quant results in Fig 5? <sep> what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? <sep> why does the paper report the ""max cumulative reward so far"" not the expected cumulative reward of the current episode? the latter could give a better idea of the training stability of the different algorithms out of curiosity: the shown scenarios are visually still pretty similar between source and target (eg background etc) --> how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another? <sep> Suggestions to improve the paper add an additional baseline ""DisentanGAIL w/ domain confusion loss & prior data"", as discussed in the ""weaknesses"" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> such an experiment could help to further motivate the need for the new regularizations the additional assumption of a ""prior dataset"" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2 --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated ""Problem Statement"" section for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable <sep> I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks. Right now, while there are differences in the link dimensions, the main difference still seems to be in the visual appearance (at least from looking at the provided pictures) --> maybe one could try to eg try to transfer between robots with different gripper morphologies (eg one agent has U-shaped gripper vs another agent with T-shaped ""gripper"" for the pusher task), or from a 4DOF arm to a 7DOF arm etc. <sep> as mentioned in one question above: while the agent appearance and morphology changes between the experiments, the largest part of the observation, the background, is usually constant across source and target environment --> it would be interesting to see experiments with different background appearances to see the robustness of the method <sep> Overall Recommendation <sep> The paper is well written and the experiments are covering a wide array of third-person visual imitation problems on which the proposed method shows strong results. The experimental evaluation seems thorough, all design choices are ablated. My main doubts are about the fairness of comparison to the baselines (particularly with regard to the additional data available to the proposed method), and some lacking baseline results on the harder tasks. Therefore I cannot fully support acceptance yet, but if the authors are able to provide the additional evaluations and answer the questions posed above adequately, I am willing to increase my score and vote for acceptance. <sep> Post-Rebuttal Comments <sep> I thank the authors for their detailed feedback. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting! <sep> I am not sure whether it is standard to show the learning curves with the ""max achieved return so far"" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm. Potentially, the stability could also be increased by adding a learning rate decay schedule? <sep> Overall, the rebuttal addressed my questions and incorporated many of the suggestions, therefore I am increasing my score and vote for acceptance.","The reviewers raised a number of concerns about the novelty of the paper and comparisons. The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published. I do think however that this paper is quite borderline. I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons. However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques. Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots. In comparison, the experiments in this paper are quite simplistic, using toy domains and ""demonstrations"" obtained from a computational oracle (i.e., another policy). Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline. That said, I would defer to the reviewers in this case -- I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental. I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples-to-apples comparison. <sep> One thing I would request of the authors for the camera ready though is: Please tone down the claims. ""Human-like 7 DOF Striker"" is not human-like, it's a crudely simulated robotic arm that was recolored. It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples-to-apples manner the particular algorithmic innovations in the method."
"The paper is a nice read. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. This typical trend is due to the memory requirements for training a multi-modal transformer end-to-end. <sep> To allow for end-to-end learning, the paper argues for shared parameters (across the network), primarily: <sep> a) sharing weights in CNNs of the same model [understandable] <sep> b) sharing weights between layers of transformer for the same modality c) sharing weights between modality transformers d) performing mid-fusion by having modality-specific transformer followed by cross-modality transformer, <sep> e) sharing position encoding parameters between modalities and transformer layers f) decomposing transformer weights, so some are distinct and others are shared <sep> -- among other suggestions [I didn't list fully]. <sep> In addition to the above, the paper showcases the need for context-aware negatives, rather than random sampling, in line with a number of concurrent works that address this issue, some submitted to *CONF* [which I reviewed as well coincidentally]. <sep> The paper then runs a number of experiments to prove their approach, all showcasing the combined advantage of end-to-end learning with shared parameters. This is tested on the ""usual suspect"" set of datasets: Kinetics, Audio Set, with downstream tasks on short-range datasets (e.g. UCF) as well as long-term (e.g. Charades). Performance improvement over the baseline is consistent. <sep> Two aspects of the paper are disappointing, <sep> First, the motivation that the approach will enable end-to-end learning with transformer, should be re-written to say: ""This would enable end-to-end learning with multi-modal transformer for a handful of labs who have 64 V-100 GPUs which can be trained for 220K batches [I'm presuming that's many days/weeks]."" It is quite impossible for almost all researchers to utilise the findings of this paper. It's true that the number of parameters has dropped significantly, but in any forward/backward pass, the memory requirements of multiple slow-fast (ResNet-50) with all transformers in memory and a necessarily large batch size keeps the same limitation of an ""end-to-end-to-end"" approach more likely to be used by the community. Apart from knowing of this finding, I am not sure how this will transform the community's go-to solutions. <sep> Second, the experimental results (tables and commentary on tables) are not designed for easy consumption. This makes the readability of the experimental section below acceptable bar IMO. This should be fixable, but disappointing that it is submitted in the current form. Let me give you a few examples of how difficult it was to read the tables of results: <sep> Ex1: Table 1, the caption talks about a, b and c but these are not referenced in the actual tables. It took me several minutes to realise you are referring to the two tables on the first row as the gap between the two tables is not easy to observe. <sep> Ex2: Table 1, names of sampling methods ""similar/dissimilar"" do not align with how the paper describes hard negatives. Using varying terminology you need to rely on the brief description to know what's happening. <sep> Ex3: Table 2, the decision to list the dataset references like this makes the table and checking the references an impossible task, many abbreviations (e.g. KS for Kinetics-Sounds) are not common. On the first two table within Table 2 you use Ours, which I presume is M-BERT in the right table? It is not clear why V- and A- were not tested independently for the tables on the left, but are ablated on Charades and Kinetics Sound. <sep> These tables were very hard to follow/read and check for correctness. <sep> A few minors: <sep> *) I am not sure the results on Charade represent the SOA on this dataset. They seem to only reference the first 2017 paper as a baseline? The same for Kinetics-Sounds, this seems to be a very old baseline? <sep> *) The manuscript talks about the audio being a ""real valued audio signal"" and it's only in the appendix that the log-mel-scaled spectrogram is explained. This can be deceiving to the reader. <sep> *) Given very few can replicate these results, the fact that an input of 1 second was only tried in all experiments limits our knowledge of the impact of this critical parameter. <sep> *) On the issue of Task 2 (correct pair prediction) other works have discussed the need for asynchronous understanding from the audio-visual signal that are worth referencing, e.g. <sep> Kazakos and Zisserman (2019). Audio-Visual Temporal Binding for Egocentric Action Recognition. ICCV <sep> Morgado et al (2020). Audio-Visual Instance Discrimination with Cross-Modal Agreement. ArXiv April 2020 [recent work understandably] https://arxiv.org/abs/2004.12943v1","Three out of four reviewers are positive about the paper after the author response and during the discussion. <sep> Strengths include <sep> The proposed method for parameter reduction in transformers allows end-2-end learning cross-modal representations especially on long videos, which has not been possible before <sep> Good performance on audio and video understanding <sep> Extensive set of ablations <sep> Concerns include a somewhat incremental nature of the paper and the still large computational resources to run the experiments. <sep> I think, both, the ideas and results are interesting to the community and recommend accept."
"The paper proposes a pre-trained language model variant which extends XLM-R (multilingual masked model) with two new objectives. The main difference to most other models is that the new losses are contrastive losses (however, as pointed out by the authors, other contrastive losses had been used before in e.g. ELECTRA). The first additional loss is a sentence-level one - where a [CLS] token is trained to be close to the positive sample, the paired sentence, with other sentences as negative samples. The same is done at word level, where the bag of words constructed from two sentences becomes the set of positive samples and other vocabulary words are negative samples. <sep> Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM-R model. The review of previous work is thorough and very helpful to place the work proposed in the existing literature. However I had difficulties understanding the impact of the changes proposed and disentangling the different factors that may have led to the results. At the end of reading this paper, I am not sure if implementing what the authors proposed, versus other variations of existing models, would have given the same improvements: While these improvements can be seen across many data sets, they are often modest. The proposal does not offer any other advantages, such as computational efficiency. <sep> For the NMT experiments, additional experiments on En-Es and En-Ro (to follow experiments in Zhu et al 2020), and/or back-translation experiments would have made the impact of the method clearer. Given that the main contribution of the paper is empirical (none of the ideas are new), better and more comprehensive experimental results would have strengthened this work. <sep> The following are clarification questions/comments: <sep> The query and key terminology used in section 2 is confusing: why not use negative/positive sample notation from the Saunshi et al, 2019 and  Oord et al, 2018 papers? Section 3.1 introduces r_x, which is yet another notation for the query q. <sep> Figure 1: please clarify the notation used in the caption (e.g. the set B is defined only later, similarly n, m, V). <sep> The losses in equations (2) and (3) are symmetric: if the data pairs are symmetric, which seems to be the case, why distinguish between queries and keys at all and define two identical, but symmetric losses? <sep> First paragraph in Word-Level CTL in Section 3.1: This should be rephrased in order to clarify the motivation for the word level loss. <sep> I couldn't find details regarding the negative samples for the sentence loss: no of negative samples, how are they obtained, etc.","This paper presents two new representation learning tasks (losses) based on contrastive learning that---when combined with a language modeling loss---result in a better multilingual model. Experiments on machine translation and XTREME demonstrate the benefit of the proposed method compared to strong baselines. <sep> I think this is an interesting paper that advances multilingual representation learning. The authors have incorporated many suggestions from the reviewers to improve the paper during the rebuttal period. I recommend to accept the paper, but also strongly suggest the authors to make an official submission to XTREME to validate their results."
"This paper proposes that recent methods that used graphical neural networks to help solve the multitask reinforcement learning problem and assume that there's an advantage from being able to encode the agent's morphology using a graphical neural network do not provide additional generalization and benefits for learning. Instead, they claim that the benefits from being able to encode this morphology are counteracted by the difficulty in having to train the graphical neural network using the message passing system. This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. <sep> The motivation that graphical neural networks are bogged down by their message passing framework is not necessarily a motivation for using transformers. There needs to be a separate motivation for why you want to use transformers and why they should perform better than GNNs or normal networks. <sep> The author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing. This could be correct the explanation but the paper so far hasn't gone into enough detail for the reader to understand the importance of this message passing and how it works and how it is not improving training for GNNs. <sep> Figure one does provide some information related to how the transformer is used with respect to some morphology it would be far more helpful if this figure method was described well enough so that anyone can understand how to apply this to a different morphology. One of the challenges with reading and understanding this paper is the lack of information on how graphical neural networks are used and designed to understand later comments in the paper. <sep> There needs to be an ablation with respect to the residual connections added to the Transformer based network to make sure the improvement for amorphous is not working well just because of these residual connections. <sep> While I do agree that training a graphical neural network to be able to produce a quality policy for a number of control tasks from the opening item environment is difficult the author of the paper might be missing at least one of the key points from the previous work in that you can learn a stronger modularization of policy. And that a goal of the SMP work was to understand how more modular policies or policies with modular components could be learned. <sep> It is stated in the paper that amorphous does better for state of the art incompatible continuous control? What is meant by incompatible continuous control? This term has not been defined anywhere in the paper and without this definition, it's difficult to understand the contribution this paper is making. <sep> ----- Post Discussion ---- <sep> I have updated my rating for the paper after the authors have provided additional discussion and experiments to address my concerns.",The paper shows that using graph neural networks to address multi-task control problems with incompatible environments does not provide benefits to the learning process. The authors instead propose to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. <sep> The paper is well written and the analysis of the literature has been appreciated. The contribution is original and relevant to the community. <sep> All the reviewers agree that this paper deserves acceptance. We invite the authors to modify the paper by following the suggestions provided by the reviewers. In particular: <sep> improve the analysis of the empirical results <sep> update the plots <sep> add the suggested references
"This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. An example of this described in the paper is an environment with a vase: if demonstration data shows an intact vase in the presence of an embodied agent then breaking the vase is unlikely to be the intended behavior. Otherwise the vase would already be broken in the demo. <sep> To achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. The paper then derives a gradient of the log probability of a demonstration state. The gradient estimator involves simulating a possible past from a demonstration state (using a learned inverse policy and inverse transition function) and then simulating forward from the possible past (using the policy and a simulator) The gradient is then the difference between features counts from the backward and forward simulations. <sep> The paper is generally clearly written and works on a crucial problem in reinforcement learning, namely how to specify a human preference without resorting to tedious reward engineering. Novel, scalable approaches to this problem would certainly be of interest to the *CONF* community. The primary technical contribution of the paper is the derivation of the gradient estimator which is correct. <sep> I find the idea of the paper very interesting and the results showing meaningful behavior emerge from a single demonstration are quite nice. However I think the paper is limited in a number of ways: <sep> It requires access to a pretrained state representation <sep> It requires access to a simulator of the environment which requires being able to reset the environment to arbitrary states. This seems quite limiting for real world applications. Worryingly, appendix D states that learning a dynamics model was attempted by the authors but failed to yield good results. <sep> I think the choice of evaluation environments is a little odd and simplistic. I think environments more aligned with the eventual application areas for a method such as Deep RLSP would make the paper much more compelling. Given the motivation of the paper, I think perhaps manipulation environments where a robot arm interacts with multiple objects could be an interesting choice. <sep> From the empirical results, it is not clear that Deep RLSP works substantially better than the simple average features baseline. <sep> Overall I think the paper has the potential to be a good paper but could still be substantially improved and I'm leaning towards rejection. <sep> Minor comments and questions for the authors: <sep> I'm curious how you choose the gradient magnitude threshold? Does Deep RLSP fail without the curriculum? Could you provide an ablation that shows the effect of using a curriculum? <sep> I would also be interested in an ablation of the cosine-similarity weighting heuristic. <sep> I think the phrase recent work in the abstract could use a reference. <sep> I'm a bit confused by the description of the environment suite by Shah et al. in appendix A, in particular the different rewards. Could you clarify and expand the description a bit?","First as a procedural point, the paper got 7, 7, 5, 5. AnonReviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score. They did not do so, but I must interpret their last messages as indicating they now support the paper. AnonReviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal. They did not update their score, but were happy to leave their certainty low and defer to other reviewers' recommendation. As such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews. <sep> The paper adapts a method from tabular RL to Deep RL, allowing (as the title aptly says), agents to learn What to do by simulating the past. Reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method. It is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, I am happy to go with the consensus and recommend acceptance."
"Summary: <sep> The authors develop a hierarchical Bayesian memory allocation scheme to bridge the gap between episodic and semantic memory via a hierarchical latent variable model. They take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, the authors simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to distribute information within the memory. <sep> Pros: <sep> The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. Also, its speed is about 2x faster than the Dynamic Kanerva Machine. <sep> The authors show the efficiency of Temporal Shift Module (TSM) (Lin et al., 2019) in the encoder of memory models. Replacing a standard convolutional stack by TSM improves the ELBO in Dynamic Kanerva Machine by 6.32 nats/image for the Omniglot dataset. <sep> The experiments are well reported for different tasks, such as reconstruction and generation, on various datasets. <sep> Cons: <sep> The whole article is just like a mechanical mixture of old ideas. To be specific, the K++ model is Kanerva Machine + Spatial Transformer + a powerful encoder (namely, Temporal Shift Module). The authors do not introduce any significant improvement or novel insight for old models and techniques. <sep> Is there any theory support for the idea that we should use an allocated deterministic memory instead of a full variational memory? The authors mention the theory of complementary learning system in the Abstract and heap allocation at the beginning of Section 1, but there is no further analysis for these two theoretical intuitions. <sep> Comments and questions: <sep> The basic idea is clear and reasonable, but the authors should provide deeper analysis as well as deeper insights for their new model (and for old models that they use, if possible). <sep> The authors use q_phi(Z|X) in the ELBO (eq. 4) instead of q_phi(Z|X, Y, M) as in the Kanerva Machine. How is the memory used in the read model? <sep> Where do the results in Table 1 come from? For example, in Kanerva Machine and Dynamic Kanerva Machine paper (Wu et al, 2018a;b), the authors did not report the negative likelihood for CIFAR10 dataset. <sep> Is there any explanation for the significantly low negative likelihood (-2344.5 bits/dim) of K++ for CIFAR10 dataset? <sep> The authors should include a brief introduction section about Kanerva Machine and Dynamic Kanerva Machine. Moreover, the function δ in eq. 5 is not defined beforehand. <sep> REFERENCES <sep> James L McClelland, Bruce L McNaughton, and Randall C O'Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. <sep> Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pp. 2017–2025, 2015. <sep> Yan Wu, Greg Wayne, Alex Graves, and Timothy Lillicrap. The Kanerva machine: A generative distributed memory. *CONF*, 2018a. <sep> Yan Wu, Gregory Wayne, Karol Gregor, and Timothy Lillicrap. Learning attractor dynamics for generative memory. In Advances in Neural Information Processing Systems, pp. 9379–9388, 2018b. <sep> Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7083–7093, 2019.","The paper proposes a variant of Kanerva Machine Wu et al. (2018) by introducing a spatial transformer to index the memory storage and Temporal Shift Module Lin et al., (2019). The KM++ model learns to encode an exchangeable sequence locally via the spatial transformer. The proposed method is evaluated on conditional image generation tasks. The empirical results demonstrated the nearby keys in the memory encoded related and similar images. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in a way that satisfied the reviewers. The basic ideas in the paper are interesting to both the machine learning and the wider cognitive science communities. However, additional experiments should be included in Table 1 to complete the ""DKM w/TSM (our impl)"" row on Fashion MNIST, CIFAR-10, and DMLab in the final revision for completeness."
"Summary <sep> The paper presents a framework to reduce internal redundancy in the video recognition model. To do so, given the input frames, the framework predicts two scaling factors to conduct temporal and channel dimension reduction. The remaining part is reconstructed by cheap operations. The authors show that the framework achieves favorable results on several benchmarks. <sep> Strengths <sep> The paper is well written. <sep> Strong quantitative and qualitative results (Consistent improvement over the state-of-the-art baselines). <sep> Solid ablation studies and analysis. <sep> Weakness <sep> The core idea of using a light-weight neural module to maximize the video model efficiency is not novel. <sep> For example, AR-Net already has shown that a policy network can decide the video input resolution, i.e., spatial dimension, adaptively, leading to improve both efficiency and accuracy. In this work, the key concept is basically the same and only the primitive operation unit, i.e., temporal and channel dimensions, is different. <sep> Please provide experiment-level comparisons with AR-Net <sep> The following SOTA video models are missing in the reference. <sep> DynamoNet: Dynamic action and motion network, ICCV 2019 <sep> Video Modeling with Correlation Networks, CVPR 2020 <sep> RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition, ECCV2020 <sep> Directional Temporal Modeling for Action Recognition, ECCV2020 <sep> Question <sep> Can the idea be applied to dense video understanding tasks, such as video semantic segmentation? <sep> Rating <sep> Aiming at acquiring an efficient model in a data-driven manner is indeed important for video models. <sep> While the key idea is not novel, the paper has obvious empirical contributions that may help communities to invigorate future researches in this direction.",This paper presents work on efficient video analysis. The reviewers appreciated the clear formulation and effective methodology. Concerns were raised over empirical validation. The authors' responses added additional material that assisted in clarifying these points. After the discussion the reviewers converged on a unanimous accept rating. The paper contains solid advances in efficient inference for video analysis and is ready for publication.
"Summary <sep> This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup. The approach achieves better performance than using backprop on small datasets (CIFAR10 and TinyImageNet), and comparable or slightly improved performance on ImageNet with 2x claimed training speedup. Learned network architecture choices seem to transfer between datasets. <sep> Strong points <sep> S1: The method outperforms comparable baselines, and significantly improves performance over standard backprop for small datasets (CIFAR10 and TinyImageNet) <sep> S2: The learned network architectures transfer between tasks. <sep> S3: On ImageNet, the method achieves slightly improved performance with claimed 2x training speedup. <sep> Weak points <sep> W1: This seems like a pretty complicated approach to only get 2x speedup in training, and besides slight improvement in performance, that seems like the only benefit this method achieves for realistically large datasets like ImageNet. <sep> W2: ""Speedup"" is not really defined. i assume this is the wall-clock time to achieve convergence, but I couldn't find a definition of convergence or stopping criterion, or how speedup is measured. <sep> W3: Complicated optimization tricks seem necessary to get the bilevel optimization to work (Section 3.3: ""In bilevel optimization, meta variables (α, β) depend on the learning trajectory of layer and auxiliary weights (θ, φ) (i.e. a sequence of values of (θ, φ) during inner optimization). As a consequence, there exists a risk of overfitting the meta variables to a specific episode""). If I have the choice between 2x training time with straight-forward optimization, versus 1x training time with complicated bilevel optimization that needs to be tuned, I would probably choose the former option, since I need to spend a lot less time debugging and tuning parameters. <sep> Recommendation <sep> I think this paper is marginally below the acceptance threshold. The results seem decent, though a little underwhelming (see W1), but the biggest concern for me is that key concepts are unclear or ambiguous (e.g. definition of speedup is unclear, see W2, ""confidence of \\alpha at lower layers"", see C2, and how the ""backprop"" gradients are computed and used in equation (2), see Q2). <sep> If these issues were clarified I would be inclined to increase my score. But as it stands, this feels like a complicated method without much payoff (only 2x training speedup with slightly improved performance). It's possible that I'm missing something or not understanding the potential impact of the results, and explicit discussion of this and/or improved clarity of the description and evaluation of the method may help. <sep> Questions <sep> Q1: What is the absolute wall-clock time for training that the speedup is reported on? <sep> Q2: Given that the gradients used to train each block are a combination of local and backpropped gradients, how does this method avoid the locking problem, and thus provide speedup? <sep> Other comments <sep> C1: The ""2.02x"" speedup number looks a little funny; are two decimal places justified? Perhaps better to just say ""2x speedup""? <sep> C2: In Section 4.4, the paper says, ""Surprisingly, SEDONA yields much more confident values of α1(l) (i.e. close to either 1 or 0) at lower layers than upper layers,"", but when I look at Figure 4, at first glance I thought that \\alpha is less confident at lower iterations, according to the provided definition, compared to later iterations (which are all at 1). But then I realized the plot uses a log scale. Consider using a linear scale here instead? Or at least describe the confidence in terms of the log scale. <sep> C3: I find the usage of all-caps CONV to be a little odd, at least I've never seem that before. Maybe just say ""convolution""? Doesn't seem like CONV saves much space. <sep> C4: I find this key statement to be unclear: ""denotes whether layer l should utilize local gradients (i.e. the last layer of a block) or backpropagated gradients (i.e. the inside layer)."" Is this saying that if a local gradient is used for a layer, then it is a candidate for being the last layer of a block, and if backpropped gradient is used, then it is a candidate for being the inside layer of a block? Some rewording of this sentence should help. <sep> C5: Typos, in Appendix B: ""to flat the learning landscape"" -> ""to flatten the learning landscape"", Section 4.1: ""to let auxiliary networks in the pool computationally lightweight"" -> ""to let auxiliary networks in the pool be computationally lightweight"", Section 3.3: ""Such warm start"" -> ""Such a warm start"", Section 1: ""If local signals are lousy representative of the global goal"" -> ""If local signals are not representative of the global goal""","The paper proposes a novel method for greed layer-wise training by considering the learning signal from either backprop or from the additional auxiliary losses. SEarching for DecOupled Neural Architecture learns to identify the decoupled blocks by learning the gating parameters similar to gradient-based architecture search algorithms, such as DARTs. The empirical experiments demonstrated the effectiveness of SEDONA on CIFAR and TinyImageNet using various ResNet architectures. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in the way that satisfied the reviewers. The ideas in this paper are interesting and are broadly applicable. Additional experiments / discussions on the tradeoff between initial search cost and accuracy should be included in the final version."
"This paper presents a student-teacher framework, where the teacher network can be used to select and prioritize the relevant properties of the given dynamical system that should be learned by the student. <sep> Pros: <sep> (significance) I think the presented framework is a powerful one, and has a potential to be applied broadly to many real-world problems. <sep> (quality) The development of the method is sound and well-motivated. The method was tested on a toy example and then applied to tasks with varying degrees of challenges. <sep> Cons: (mostly on clarity) <sep> What confuses me the most is the way the authors frames their work. From the current title ""Learning to interpret trajectories"", and the abstract, it was not really clear to me what the paper is about; I am not sure if the population of people who stop at the title would be the same as the population that finds the contents most interesting. Specifically: <sep> Why is ""trajectory"" a central keyword for this work? The word trajectory can be used in many different contexts, I don't think it was made clear anywhere in the paper what is the defining features of the ""trajectory-ness"" that the authors want to emphasize. <sep> What do you mean by ""interpret""? This word is being used in a very loose fashion without a clear context; it is empty at best, and misleading at worst. <sep> I get that the proposed framework avoids the problems of model-based and model-free methods, but I am having difficulties identifying what advantages of the two methods that the framework is incorporating. <sep> One of the central questions that is raised in the introduction is this: ""What is the right way to incorporate information from different trajectories?"". But I am not sure how this work solves the problem of incorporating different trajectories specifically. <sep> Additional comment: <sep> The core concepts from the previous works that the work is based on, such as learning using privileged information or the meta-gradient approach, are not clearly introduced. Even a brief, ~1 sentence description would be helpful. <sep> The ideas of model-based and model-free methods are reinforcement learning concepts, and may not be clear to people who are not in RL. Again some brief description would help. <sep> Overall, I have a mixed feeling about this paper and I currently stand between scores 5 and 6. Whereas I find the proposed method interesting, I feel that the lack of clarity, and the confounding of messages in the framing, make this paper rather short of the standard for the conference. <sep> UPDATE: <sep> My major concerns were addressed in the revised version of the paper.","The paper proposes a new teacher-student framework where the teacher network guides the student network in learning useful information from trajectories of a dynamical system. The proposed framework is inspired by the Knowledge Distillation method. The teacher learns what information should be used from the trajectories and distills this information for the student in the form of target activations. In a nutshell, the framework allows the student to interpolate between model-based and model-free approaches in an automated fashion. Experimental evaluation on both the hand-crafted and simulated tasks demonstrate the effectiveness of the proposed framework. The reviewers had borderline scores in their initial reviews and raised several questions for the authors. The reviewers appreciated the rebuttal, which helped in answering their key questions -- I want to thank the authors for engaging with the reviewers during the discussion phase. The reviewers have an overall positive assessment of the paper, and believe that the proposed teacher-student framework is novel and potentially useful for many real-world problems. The reviewers have provided detailed feedback in their reviews, and I would like to strongly encourage the authors to incorporate this feedback when preparing the final version of the paper."
"Summary: <sep> The authors introduce a framework for sufficient conditions for proving universality of a general class of neural networks that operate on point clouds which takes as input a set of coordinates of points and as output a feature for each point, such that the network is invariant to joint translation of the coordinates, equivariant to permutation of the points and equivariant to joint SO(3) transformations of the coordinates and output features of all points. Notably, this class contains Tensor Field Networks (TFN). The authors accomplish this by writing the network as a composition of an equivariant function from a class F_feat and followed by a linear pooling layer. When the F_feat class satisfies a ""D-spanning"" criterion and the pooling layer is universal, the network is universal. For a simple class of networks and for TFNs, the authors prove D-spanning. Linear universality of the pooling layer follows from simple representation theory. <sep> Strengths: <sep> It is useful to know whether prevalent classes of neural networks are universal <sep> The authors use a general construction for proving universality of equivariant networks for the point cloud group, rather than being specific to certain architectures. <sep> Reading the proofs along with the main text, the argumentation is clear and relatively easy to follow for me as reviewer, unfamiliar with similar universality proofs. <sep> Weaknesses: <sep> The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2 <sep> In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper. <sep> As the authors of the TFN paper note: in practice not all higher order irreps of the tensor product of the filter and the features are computed. This seems to indicate a big difference between the theoretical analysis - which includes all irreps and thus is computationally intensive even when modelling low order polynomials – and the practical application of TFNs. It would be interesting to know how expressive such practical low order TFNs are. Another difference between the described networks and practical TFN is that in the described networks, all relevant parameters are in the pooling layer, which sums a large number of terms (looking at the proof of lemma 2, exponential in D), while in practical TFNs, the parameters are in the filters. <sep> Recommendation: <sep> The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper. <sep> Suggestion for improvement: <sep> Make the big picture clearer by providing more intuition. <sep> Comment on the differences between the class of networks described and TFNs used in practice. <sep> Minor points/suggestions: <sep> P3 Add definition of W^n_T as n direct sums of W_T <sep> P3 ""where W_feat is a lifted representation of SO(3)"", what does lifted representation here mean? Just any rep? <sep> I get a bit confused by the wording in Def 1. Unless I am mistaken, it appears like the quantifiers are reversed. Should it mean ""for every polynomial …, there exists f_1, … in F_feat and linear functionals Lambda_1, …, : W_feat -> R ""? <sep> Around Eq 5, perhaps the authors could clarify the clarify the domain of the Q functions, which I suppose is Q^r : R^3n -> T_T, where T=||r||_1 <sep> Around Eq 7, are X_j and x_j the same? <sep> In lemma 4, is A_k any linear map or an equivariant linear map? <sep> In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1? <sep> In the proof of thm 1, it says ""p: R^{d \\times n} \\to W_T"", should that be W_T^n? <sep> In the proof of lemma 2, it says ""we see that that exists a linear functional"" <sep> Post rebuttal <sep> I thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas. My previous rating still applies.","The paper presents a theoretical analysis of the expressive power of equivariant models for point clouds with respect to translations, rotations and permutations. The authors provide sufficient conditions for universality, and prove that recently introduced architectures, e.g. Tensor Field Networks(TFN), do fulfil this property. <sep> The submission received positive reviews ; after rebuttal, all reviewers recommend acceptance and highlight the valuable paper modifications made by the authors to clarify the intuitions behind the proofs. <sep> The AC also considers that this paper is a solid contribution for *CONF*, which will draw interest for both theoreticians and practitioners in the community. Therefore, the AC recommends acceptance."
"This paper presents a new CNN module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. To achieve that, the authors propose to divide feature channels into several sub-dimensions (called channel tensorization) and then perform group convolutions at each sub-dimension sequentially to improve channel interactions. An SE-like attention mechanism is also applied to further enhance feature representation. The proposed approach achieves competitive results on Kinetics400 and Something-Something, compared to some existing SOTA results. The paper also provides detailed ablation studies on the approach. <sep> My understanding is that the main idea of the paper essentially performs channel shuffling followed by group convolutions at each sub-dimension. From this perspective, the idea is similar to ShuffleNet, which applies channel shuffling to enhance interactions between channel groups. While this provides interesting technical questions from the algorithmic perspective, from the point of view of the novelty, the paper does not appear as a strong technical contribution. <sep> Another downside of the proposed approach is that it is not sufficiently validated by experiments. Firstly, the contribution of TE is not separated in Table 3 and 4, so it remains unclear whether the performance improvement is due to the enhanced channel interactions or channel attention. Secondly, the approach seems to be only effective when the number of dimensions is low, leaving it difficult to fully justify the advantages of high-dimensional channel tensorization claimed in the paper. <sep> The FLOPs of the proposed approach in Table 4 is somewhat misleading. As indicated in the work of X3D, the performance gain from using more clips (i.e. >5x10) in evaluation is small. Nevertheless, most approaches are evaluated with 30 clips on Kinetics. I would suggest sticking to this general practice for fair comparison. <sep> Table 3 and 4 miss some existing SOTA approaches such as X3D [1], CorrNet [3] and TAM [3], which should be listed for reference. <sep> Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 200–210, 2020. <sep> Heng Wang, Du Tran, Lorenzo Torresani, Matt Feiszli, Video Modeling With Correlation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020 <sep> Fan, Q.; Chen, C.-F. R.; Kuehne, H.; Pistoia, M.; and Cox, D. 2019. More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation. In Advances in Neural Information Processing Systems, 2261–2270.","The paper focuses on the task of learning efficient representation models for video classification. To avoid the excessive computational cost of performing 3D convolutions on video, the authors propose to break the channel dimension of video representations into sub-dimensions that are treated separately. This cuts down on computation and improves classification performance over many methods in the literature. Extensive experiments were run on well-known benchmarks to justify the claims of the model. Such backbone architectures can be very useful in the realm of video understanding. The authors should be commended for the amount of work they did in the rebuttal period to address the comments and inquiries brought up by the reviewers. Extra experiments were done and more in-depth analysis was made possible."
"Most neurons in the brains are either excitatory (E) or inhibitory (I) - sometimes referred to as Dale's law.  Practically Dale's principle is often left out of Artificial Neural Networks (ANNs) because having the E and I separation often impairs learning, although this has not been well documented in the literature (probably due to that this is also interpreted as a negative result). In this paper, the authors propose a new scheme to construct and train the feedforward E/I network by incorporating several ingredients, including feedforward inhibition and E/I balance among others. It is shown that this particular kind of E/I networks (DANNs) trained on MNIST and variations of MNIST could achieve a level of performance that is comparable to those without E/I separation. <sep> Quality: I think this is an interesting submission of good quality, with some novel ideas and promising preliminary results. <sep> Clarity: The writing is generally clear. <sep> Originality: As far as I can tell, the results are original. <sep> Significance: Although the results are promising, I have reservations about the significance of these results as the performance of the models are still worst than the standard ANNs. <sep> Pros:1.To my knowledge, this is the first E/I network that could achieve comparable performance with the standard ANN model on MNIST task (although at the same time, I have to say that not too many papers have studied and reported this issue). <sep> 2. The ingredients in the proposed model is well motivated in neuroscience, such as the feedforward inhibition, and E/I balance, as well as no connections between I neurons across the different layers. <sep> 3.   The results on the MNIST and its variations look promising. <sep> 4.  The paper is fairly well written and the basic ideas are clear. <sep> Cons: <sep> 1.The role of the subtractive and divisive components need to be better explained. Are both of them necessary for getting the results shown later? <sep> 2. The authors assume the number of E neurons is far larger than that of the I neurons. This is not quite true in physiology. The E/I ratio reported is often around 4:1. The authors assumed 10% of neurons are I neurons- this is on the smaller end. Another related concern is that, in cortex, despite of a smaller number, I neurons are often responsible for controlling the dynamics/computation due to the dense connectivity from I to E neurons. I am a little bit worried that the paper is studying a quite different regime, in which the E neurons are dominating. Also, would adding more I neurons decrease the performance of the network? If that is the case, that would be concerning. <sep> 3. The initialization of E/I network has been carefully studied previously in the context of training balanced E/I recurrent neural networks (e.g., Ingrosso & Abbott, 2019, which the authors cited). How does the authors scheme different from the previous work? <sep> 4. The method assumes inhibitory units are linear units. Several questions arise. First, is this a mathematical issue or a numerical issues? Second, does this imply the firing rate of inhibitory neuron can be both positive and negative? <sep> 5. In fig4, DANN performs significantly worse than LayerNorm and BathNorm. <sep> 6.The algorithms is not tested on slightly more challenging benchmark datasets such as CIFAR10 or ImageNet. Relatedly, would DANN scale up to larger networks? <sep> Questions to be clarified: <sep> *Are their connections between the I neurons within the same layers? <sep> *page 4, ""Unlike a column constrained network, a layer in a DANN is not restricted in its potential function space. "" - It is unclear what this sentence means… <sep> *Between Eq 4 and Eq 5, the authors mentioned the exponential family. What particular distribution was used? Gaussian or any exponential family distribution would produce similar results? <sep> *The authors wrote: ""As a result, inhibitory unit parameters updates are scaled down relative to excitatory parameter updates. This is intriguing given the differences between inhibitory and excitatory neuron plasticity…including the relative extent of weight changes in excitatory and inhibitory neurons (McBain et al., 1999). "" I think these comparisons to the neuroscience literature are too vague and potentially mis-leading. To make this useful, it would helpful to make the comparison more specific and clear. <sep> *I am worried that the experiments for the ColumnEi model was not treated fairly. In section 5.1, it is mentioned that 50 columns are negative. Did the authors try to make increase this number to see if the performance would be improved for the ColumnEi model? <sep> *****updated after rebuttal period <sep> I still consider this as an interesting contribution, and stand with my original rating. <sep> It would be useful if the discrepancies and similarity between the connectivity structures in the model and the anatomy could be more carefully discussed in the paper.","This paper was unanimously rated above the acceptance threshold by the <sep> reviewers. While all reviewers agree it is worth accepting, they <sep> differed in their enthusiasm. Most reviewers agree that major <sep> limitations of the paper include that the paper provides no insight into why <sep> Dale's principle exists and the actual results are not truly <sep> state-of-the-art. Nevertheless there is agreement that the paper <sep> presents results worth publicizing to the *CONF* audience. The comparison <sep> of the inhibitory network to normalization schemes is interesting. <sep> Also, please reference the Neural Abstraction Pyramid work."
"This paper studies the label solicitation strategy in active learning. In particular, it focuses on the expected loss reduction (ELR) strategy, analyzes its problem, and modifies the original ELR method to make sure the active learner converges to the optimal classifier along learning iterations. The paper provides theoretical guarantees on the new method's convergence. In the experiment, the proposed method is evaluated on synthetic data and UCI data. The improvement margin over the existing method is very limited. <sep> Strong point: <sep> The paper's finding on the existing ELR method is interesting and novel. <sep> The theoretical analysis of the convergence of the proposed method seems to be sound. <sep> Weak point: <sep> The experiment is conducted on low-dimensional data and the proposed method's performance is not very competitive. <sep> The notation in this paper can be confusing to readers, especially the use of (*). Star usually means the ""optimal"". <sep> The main paper does not have an algorithm. <sep> There is no validation in experiments for the theory. In the synthetic experiment, it should be possible to simulate a case with ground truth optimal classifier and verify whether the proposed method actually converges to the optimal. <sep> My major concern is the practical impact of the proposed method. Therefore, I recommend a weak reject for this paper (5). <sep> Additional questions and suggestions: <sep> I think the paper would be improved if there is a discussion on how the proposed method can be extended to deal with high-dimensional data and/or using deep learning models. <sep> It seems to me the proposed weighted method is not the only way to guarantee convergence. But I am not sure about that. It would be nice to have some discussion about that. <sep> The one-step-look-ahead strategy involving expectation model change or expected loss reduction usually suffers from the large class space for computing the expectation. The experiments are mostly conducted in a small class space. It would also be good to have a discussion about the complexity in terms of class space. <sep> MES usually suffers a lot from noise (experiments in the appendix also show that). ELR methods are usually more robust to noise. I was wondering whether different noise level has been tried and how the proposed method compare with ELR on that. A similar question is also, for certain data there is a larger gap between the proposed method and ELR (in appendix). What would be the reason? <sep> The visual presentation can be improved for the paper, as well as the explanations. In figure 1's explanation, I got very confused by the ""side"". What does that mean? <sep> The results should be shown with error bars if experiments are conducted multiple times. <sep> ================ <sep> Update after rebuttal: <sep> I increased the score to 6 and appreciated the revision of the paper. The readability is improved. However, I also have different opinions with the authors in terms of how empirical evaluation of algorithms should be regarded in active learning research. So I would further encourage the authors to apply their method on high-dimensional large scale data, even it may take a lot of computing resources or require actual sample acquisition. <sep> I agree that the goal of active learning is to reduce the burden of labeling data. But it does not conflict with the requirement of dealing with high-dimensional (feature space) data. Also, I see a lot of active learning works focusing on theoretical analysis but cannot be easily put into real-world applications, which actually undermines the significance of the theory to some extent. In the real world, a lot of assumptions would be violated. As the authors also mentioned that, it is ""expected"" that different feature space and data quality affects the performance. Therefore, I think the theory does not spare us from justifying our methods in practice. <sep> Last but not least, actual sample acquisition is not unrealistic if given real-world problems. So I encourage the authors to further demonstrate the nice properties of the proposed algorithms in more realistic settings in the future.","The paper proposed weighted-MOCU, a novel objective-oriented data acquisition criterion for active learning. The propositions are well-motivated, and all reviewers find the analysis of the drawbacks of several popular myopic strategies (e.g. ELR tends to stuck in local optima; BALD tends to be overly explorative)) interesting and insightful. Reviews also appreciate the novelty of the proposed weighted strategy for addressing the convergence issue of MOCU-based approaches. Overall I share the same opinions and believe the paper offers useful insights for the active learning community. <sep> In the meantime, there were shared concerns among several reviewers in the readability (structure and intuition), lack of empirical results on more realistic active learning tasks, and limited discussion on the modeling assumptions. Although the rebuttal revision does improve upon many of these points, the authors are strongly encouraged to take into account the reviews, in particular, to further strengthen the empirical analysis and discussions, when preparing a revision."
"Summary <sep> This paper presents a graph neural network model for learning to model PDE dynamical systems. Given its graph structure and its continuous time formulation (employing the adjoint method for differentiation/backpropagation), this method allows the usage of samples placed arbitrarily in space and time. <sep> Pros <sep> The continuous-time nature of the model allows for the usage of irregular time sample points. <sep> Previously proposed methods either would not work on continuous time, or unstructured grids, or would not be applicable to settings with unknown governing PDEs. This work combines all these features. <sep> The graph-based representation used makes the proposed method invariant to translations and rotations in the spatial domain. <sep> Cons <sep> Similar graph-based methods that used continuous-time to model differential equation dynamics had been previously presented, e.g. GNODE. The novelty of the proposed method might be limited. <sep> The test cases are simple and the experimental details are somewhat lacking for a full evaluation of the results (more details below in the additional comments). <sep> Reasons for score <sep> [Edit: Score updated, see discussion below] <sep> Overall, given the ""cons"" described above, notably the potential lack of strong novelty in the proposed method, and the lacking experimental description and results, I am for now classifying this paper as marginally below the acceptance threshold. <sep> On the positive side, the method seems to perform favorably when compared to other baselines, in comparisons that are actually favorable to the other methods (e.g., using regular grids). Moreover, the method performs well on the tasks it is tested on. <sep> However, I'm concerned with some uncertainties I have regarding the experimental section and the presented results. These are discussed below in the comments. <sep> Moreover, the proposed method centers around using message-passing neural networks to model the differential equation dynamisc. As mentioned above, previous methods had already proposed the usage of graph neural networks with continuous time for the learning of differential equations, and I am not sure that the addition of spatial mesh information to such a graph neural network constitutes a significant enough modification at this point. <sep> Despite the concerns above, I am open to reading the authors' responses and the reviews/comments and changing my opinion depending on how those affect my current uncertainty. <sep> Additional comments <sep> I believe an important element that is missing from the description of the experiments is a clearer of how much do train and test set actually differ? This would be important to understand how hard the tasks being performed are. Clearly, if training and test set are too similar, the results lose a lot of power. <sep> Morever, since we also dont see any traning vs test plots, it is also hard to see how much performance is different between these two are. (I am not claiming such a plot would be necessary, but merely that given the otherwise lack of information in this direction, it would be helpful information.) I am aware that the appendix includes a description of how the initial conditions for the data are generated, but lacking more information these are hard to grasp intuitively to be able to judge the tasks. <sep> Moreover, the error in model rollouts over time seems to spike at the beginning and then quickly flatten out. It seems strange that errors would spike up initially and then not compound significantly over time. Do the authors have any intuition as to why this is the case? Is it maybe a consequence of the data samples reaching a sort of steady state after some time? If so, wouldn't this weaken the case being made for a continuous-time model? <sep> How would the Delauney triangulation being employed deal with possible obstacles present in the spatial domain? For example, an airfoil might have its opposing boundaries connected by edges (since they are close in space), even though that would supposedly be a solid. Would these types of solids have to be manually specified when extending this method to such scenarios? (This is not a ""drawback"", of course, it would be expected of most methods that such object boundaries would have to be defined.) <sep> What integrator is used for the experiments?","This paper proposes a new method for learning a model for spatio-temporal data described by an (unknown) spatio-temporal PDE. The model learns a continuous time PDE using the adjunct method and uses graph networks to perform message passing between different discrete time steps on a grid obtained with Delaunay triangulation. <sep> The method initially 3 favorable and 1 unfavorable ratings, but convincing responses to some of the raised issues led to unanimous recommendations for acceptance (not all reviewer feedback after the rebuttal has been made public, but feedback has been made to the privately AC on these issues by different reviewers). <sep> The reviewers appreciated novelty of the method and numerous ablations. <sep> Initially perceived weaknesses were some key experiments on generalization over different grid discretizations; the simplicity of some experiments, and links to different prior art - many of these points have been dealt with by authors in their response. <sep> The AC concurs and proposes acceptance."
"This paper proposes the signal propagation plot (spp) which is a tool for analyzing residual networks and analyzes ResNet with/without BN. Based on the investigation, the authors first provide ResNet results without normalization with the proposed scaled weight standardization. Furthermore, the authors provide a bunch of models that are competitive to EfficientNets based on RegNetY-400MF,  which seem to be highly tuned in terms of architecture design. <sep> Pros) <sep> This paper is well written and easy to follow. <sep> The proposed SPP seems to be a good tool for analyzing a model. <sep> Cons) <sep> The authors seem to have failed to provide any reasons for needing normalization free ResNet over the original BN-ResNet. <sep> It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks (e.g., object detection). <sep> I don't think it was necessary to show competitive results with EfficientNets using the other baseline - RegNet.  Especially, the proposed models (which are compared with EfficientNets) are highly-tuned trying to surpass EfficientNets' accuracy. This type of paper would be better to be focused on investigating the characteristics of a network. <sep> Comments) <sep> The major problem of this paper is none of the advantages of NF with SWS are highlighted over BN, so it is hard to find any reasons for replacing BN with NF-SWS. I mean non of the disadvantages of BN are addressed by the proposed method. <sep> SPP could enlighten that an unusual ReLU-BN-Conv ordering would have some benefits but why should a network without normalization mimick the SPP trend of ResNet? It is unnatural that a network without normalization should follow the ResNet's behavior. <sep> From the equation x_{l+1}=x_l+ a * f (x_l/b_l) at p.4, the proposed approach eventually normalizing the feature even if a or b_l is fixed. Moreover, gamma in eq. (3) additionally scaling up or down the weight which ultimately gives an effect on the computed feature. <sep> How did the authors compute gamma in eq.(3) in a training phase? Why the provided code contains a learnable gamma? <sep> Why ""zero padding"" at p.5 affects the variance decay in the rightmost graph in Figure 2? <sep> Please clarify why RegNetY-400MF chose it as the baseline. It is not clear that the authors pick RegNet and tune it highly and compared with EfficientNets. If the authors decided to use RegNets, then it is natural to use NF-SWS-RegNets are compared with the original RegNets without any big modifications as shown in the model details in the Appendix. <sep> Did the authors use trained ResNetV2-600 or randomly-initialized model? <sep> How does SPP goes on with the post-activation network which uses the original bottleneck block consists of Conv-BN-ReLU? <sep> How can the resolution downsampling block in a ResNet affect averaged channel mean and variance? <sep> Why the ResNet experiments are done with weight decay of 5e-5? The common knowledge of training ResNet is with weighing decay of 1e-4, so one may unconvinced the result because of the tuning. <sep> Comparing EfficientNets that are trained without CutMix and Mixup (but used randaug or autoaug) with the proposed models with cutmix and mixup seems to be not fair.","This paper analyses the signal propagation through residual architectures; then suggests a scaling method which, together with weight standardization, allows to train such networks to high accuracy with batch-norm; it demonstrates that the method performs better than previous methods (Fixup, SkipInit), and can be used on more advanced architectures. <sep> The reviewers initially had several concerns, but after the author's revision, these concerns were addressed and most reviewers recommended acceptance. One reviewer did not respond, but I think these concerns were addressed. I think it will help to further convince the readers on the usefulness of the method readers if the authors would check the sensitivity to the learning rate with the current method and compare with other methods (SkipInit, Fixup, BN). The reason I'm suggesting this is that I think one of the main reasons BN is still in popular use is that it commonly tends to make training more robust to changes in hyper-parameters, such as the learning rate (while other methods, like SkipInit and Fixup, require more hyper-parameter tuning). <sep> Overall the analysis and the suggested method seem useful, especially at a small batch size and the writing is mostly clear, so I recommend acceptance."
"The authors propose a stronger lottery ticket hypothesis in this paper – the multi-prize lottery ticket hypothesis. In particular, the new hypothesis seeks answer to the required amount of over-parameterization for a randomly initialized network to become able to compress to a sparse untrained binary subnetwork with on-par accuracy. The authors prove the existence of such subnetwork and show the bounds on over-parameterization. The paper proposes new methods to get the binary-weight tickets and the binary-activation tickets, where binary-weight tickets are subnetworks with weights as binary, and binary-activation tickets have the activation function in the forward propagation as binary. As binary networks can largely reduce the computational complexity for inference, this work has practical importance especially for applications with constraints for memory and power. The paper has many simulation results to support the theoretical guarantees, and the proposed approach on binary-weight networks has advantages over existing methods. <sep> The paper has sufficient and novel contributions, both for the theoretical results on binary subnetworks and the empirical evaluations that reveal the efficiency of the algorithm on binary-weight subnetworks, so that I recommend this paper for publication. <sep> Here are some minor concerns. <sep> [a] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1. Compared with Malach 2020 ""Proving the lottery ticket hypothesis: pruning is all you need"", the authors can highlight what key differences are needed for proof of the binary network. <sep> [b] For the comparison of full precision network and the MPT from this paper, it can be useful if the computational complexity is shown, where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network. <sep> [c] The MPT 1/1 seems not to perform as well compared to trained binary activation network, which may reduce the quality of paper. It is good that the authors mention future work for MPT 1/1 network to attain state-of-art results of binary activation network. <sep> [d] Some of the references seem to lack information, e.g. Malach 2020, Orseau 2020, both of which do not have the venue or journal names. <sep> [e] A question: for the binary weights subnetworks, what does it mean by 20 million parameters? The weights can only be -1 or 1, so the network has 20 million of -1 or 1, but they do not require any multiplication at the inference stage?","The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. The results are important in the general context of the ""strong"" lottery ticket hypothesis, and are of both theoretical and practical interest. Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel. Some further concerns of clarity and novelty were addressed by the authors."
"The authors suggest a way to backpropagate through neural network with an embedded optimization problem. <sep> They propose to perform the estimation of the gradient of the optimum of the embedded problem with respect to its parameterization by the differentiation of one step of a stochastic search algorithm. <sep> Their suggested NOVAS approach together with the deep FBSDE method allows for solving optimal control problems efficiently and with low memory due to not unrolling multiple optimizer steps during the backpropagation. <sep> The authors demonstrate the feasibility of their approach via an example with a structured prediction energy network and two optimal control tasks (cart-pole swing-up and portfolio selection). <sep> I do not recommend to accept the paper. <sep> The results are interesting but require a more compelling presentation and in depth analysis. <sep> At the current state it is hard for me to understand how their approach for differentiation of an embedded optimization problem is principled and why it should be better than other approaches. <sep> It is also hard to evaluate whether the presented approach really is better from the given computational experiments. <sep> For a locally strictly convex optimization problem of the form x^star = argmin_x f(x, theta) <sep> the computation of the Jacobian dx^star / dtheta analytically requires the inversion of the full Hessian matrix via the implicit function theorem: <sep> [partial x^star / partial theta] = -[partial (df/dx) / partial x]^{-1} [partial (df/dx) / partial theta] <sep> where [partial (df/dx) / partial x] is the Hessian of the optimization problem at the optimum. <sep> If x \\in R^n then in the locally strictly convex case the Hessian has full rank n. <sep> Backpropagation through only one gradient step contains only information of rank 1. <sep> So the backpropagation of one gradient step generally only contains a small part of the sensitivity of the optimum with respect to theta. <sep> When starting the optimization from different points and repeating throughout multiple outer iterations we potentially gather enough sensitivity information to properly represent the shape of the loss function around the optimum in expectation. <sep> But the approach certainly adds variance compared to having an exact gradient or unrolling many optimization steps. <sep> Additionally, the authors suggest a stochastic search method that due to using the log function trick has low sample efficiency compared to using actual gradient information. <sep> The stochastic search method proposed by the authors gains information about the loss function shape by sampling locally in the input space. <sep> These types of gradient estimators have much higher variance for truly high dimensional loss functions due to the curse of dimensionality. <sep> See Ben Rechts criticism of the policy gradient / REINFORCE gradient estimator that has the same issue: <sep> https://www.argmin.net/2018/02/20/reinforce/ <sep> So backpropagation of one gradient step with a high variance gradient estimate compared to the true derivative which has rank n curvature information should be inefficient in theory for truly high-dimensional optimization problems. <sep> Perhaps the authors could add at least some analysis of the variance of their gradient estimator compared to other gradient estimators for embedded optimization problems for different examples and show how the variance behaves depending on the difficulty of the optimization task (dimensionality, curvature) taking the above perspective into account. <sep> Since the authors explicitly treat nonconvex optimization problems they could also make more explicit that a nonconvex optimization problem does not necessarily have a unique optimum. <sep> The argmin is therefore a set-valued map and not differentiable in the classical sense (even when the optimum is unique almost everywhere in the parameter space it is possible that the optimal value is discontinuous in the parameter). <sep> For non-global optimizers (such as gradient descent) starting in such a way that we do not end up in the wrong local optimum is crucial. <sep> It seems problematic that the comparison against the backpropagation through unrolled gradient descent is with an example where the latter finds a different (wrong) local optimum then the suggested NOVAS method. <sep> I don't see how the comparison results with respect to convergence, computational time and inference can be considered conclusive without making the tuning effort for the baseline to perform the desired task properly. <sep> The authors refer to computation time as ""inner-loop convergence rate"" (Page 6) but convergence rate cannot be inferred from computation time if e.g. sampling gradients using the likelihood ratio trick is much faster than computing a backpropagation gradient. <sep> Reverse mode autodiff gradients are only faster than finite difference / forward mode / sampling gradients for a non-zero constant number threshold of input dimension (how many depends on the efficiency of the autodiff module but it can be e.g. 100-200 input parameters). <sep> The computation time for truly high-dimensional problems can thus look very different than medium dimensional problems (n = 100). <sep> Regarding the violin plots for the portfolio optimization I am not sure how much they benefit the paper. <sep> The goal of the paper should be to demonstrate the superiority of the suggested method over baselines. <sep> The violin plots are relevant to people who are interested in the outcome of that specific optimal control task. <sep> Someone not knowledgeable about the control problem will not benefit much from knowing the terminal wealth of the different strategies. <sep> More analysis about the quality of the gradients obtained by the suggested method compared to other methods would improve the paper: <sep> How many unrolling steps do what to the gradient variance? <sep> What is the difference of using backprop through gradients of the loss vs stochastic search gradients (log likelihood trick) gradients for different dimensions of the optimization variable? <sep> Another small suggestions: <sep> On Page 4 ""desirable properties of optimization"" seems rather vague, perhaps it should be stated more specifically (most likely what is meant is smoothness of the resulting objective function).","The authors propose an intriguing alternative to IFT or unrolled GD as a method for optimizing through arg min layers in a neural net, by using a differentiable sampling-based optimization approach. I found the general idea in the paper to be intriguing and thought-provoking. The reviewers generally seem to have also appreciated the method, and many of the reviewers' concerns were addressed by the authors during the rebuttal. Although the paper does have a number of flaws -- in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered, -- I think in this case the benefits outweigh the downsides. The work is far from perfect, but the ideas that are presented are interested and valuable to the community, and I think that *CONF* attendees will appreciate learning about this work. I would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the camera-ready, and to take reviewer comments into account insofar as feasible. I'm also not sure how much I buy the ""overfitting to hyperparameters"" argument for unrolled GD, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits. That's not necessarily a bad thing, but I think making such a big deal of it is a bit strange. It's probably fair to say at this stage that the actual benefits of this approach are a bit modest (though improvements in runtime are a good thing...), but the idea is interesting, and may spur future research."
"Summary of paper: A series of empirical observations are made about the influence of scale of init on generalization (in particular, that a continuum of generalization performance from random to very good can be generated by varying only the scale of init) , and these effects are explained in detail for different activation functions. The authors also propose a measure of gradient alignment which they show correlates with generalization performance <sep> Pros/strong points: <sep> detailed explanations for each activation function provide nice insights solid experiments <sep> Cons/weak points: <sep> overall clarity and presentation of information is the largest weakness in my opinion, although the writing is generally good. I think it just needs a few more passes, with an eye to making sure things are accessible/understandable/flow. Could be improved substantially just with formatting/subsections or something, e.g. italicizing key insights or making sub paragraphs where each gives a particular insight some small things in related work <sep> Summary of review + recommendation: Overall I think this is a good paper, and could be a very good paper with some ""tightening up"" and clarifications. The combination of things is too much for me to recommend acceptance out of the box, but the things are relatively small and I think easy to address, and I'd be happy to increase my score. <sep> Detailed review and Specific questions/recommendations: <sep> unclear what ""large scale training"" means <sep> ""engendering"" is an unnecessary word there observations about overparameterized models should be cited, e.g. Zhang et al.  and Arpit et al. <sep> Chizat & Bach further observe (not observes), same incorrect pluralization with many citations (suggest checking the whole document) <sep> background work portion of the intro misses works, some poorly explained / relationship to current work not discussed, and overall feels rushed. The Related Work section does a good job mostly though. I suggest moving the 3rd p of the intro into the related work, moving the first section of it about scale of init to the first sentence of Contributions. Merging them should get you some extra space for more experiments/larger figs. <sep> Related work on inits should cite lottery ticket works (e.g. Frankle et al) <sep> Geiger et al reference you describe what they do but not what to take away from it extreme memorization should be bolded since it's a term you're defining (and make clear if you're proposing this term and if not, where it is from), but you then  define memorization the same way you define extreme memorization, making this term (""extreme"") seem unnecessary. <sep> ""from verylittle overfitting to perfectly memorizing the training set while making zero progress on test error"" this sentence is unclear, makes it sound like ""while..."" applies to both of the 2 extremes. Suggest rephrasing. <sep> I find 2nd bullet of contributions unclear about what is expected vs. what happens and what we learn from that in 3rd bullet briefly summarize the alignment measure. I suggest using a different/more precise term for this measure (e.g. gradient alignment) since just ""alignment"" means so many things already <sep> In ""related statistics"" you don't mention if Chatterjee has a measure for coherence of gradients (I skimmed that paper and it seems not, but I'm not sure). If not, then maybe calling this a measure of gradient coherence would be appropriate? I googled it quickly and it seems that coherence means something specific in linear algebra and signal processing: (https://en.wikipedia.org/wiki/Mutual_coherence_(linear_algebra), https://en.wikipedia.org/wiki/Coherence_(signal_processing)) - maybe not necessary to comment on in the paper, but if the authors are familiar with this use of the term I'd appreciate a clarification of how it relates to the mentioned measures of alignment I'd be interested <sep> ""related statistics"" should also mention Arpit et al critical sample ratio (which does take class information into account) and comment on differences (or if it's too different to include here, I'd appreciate an explanation of why) <sep> Mention computational cost of the different measures of alignment homogeneity is repeatedly mentioned without explanation (just a brief 1-line would do) <sep> ""fix the scale"" ambiguous whether this means fix in place (at a particular value) or fix as in correct seems obvious to me that the scaling would affect relus (especially in the absence of bias as your experiments say); if the scale is larger, fewer values are initialized near the non-linear region of the relus, meaning there are more 'dead relus' near the beginning (which I would guess up to a point could provide regularization, but past that point would just make learning slow and even unstable). Could the authors comment on this; do you think it's correct/relevant? How does it fit with the argument about homogeneity? <sep> State important equations in words as well as math for clarity and ease of reading (as is done for eq.6; make sure this is done consistently, especially important for your proposed measures of alignment). <sep> Conclusion discusses the results strangely, without mentioning the actual results (e.g. ""making it particularly interesting"" - why/how is it interesting, what are the implications for people using sin?, ""the loss function plays a crucial role"" what role? what things are good for what, what should I look out for?). The conclusion should stand on its own and summarize results, not reference them in a way that requires me to have read the whole paper to understand.","This paper provides a clear and useful empirical study of how the initialization scale and activations function affects the generalization capability of neural networks. Previous works showing the effect of the initialization scale (Chizat and Bach (2018), Geiger et al. (2019), Woodorth et al. (2020)) had a more limited set of experiments. Moreover, here an extreme case is shown, wherewith sin activation function no generalization is possible at a large init scale (there the kernel regime is useless for generalization since the hidden layer output becomes very sensitive to any small perturbation in the input). Lastly, two alignment measures are suggested, which are correlated with the generalization across several architectures and initialization scales. <sep> All the reviewers argued for acceptance, and one strongly so. I agree that the paper is sufficiently interesting and clear to be accepted. However, despite the high scores, I only recommend a poster and not spotlight/oral: I think the novelty of the empirical study is not groundbreaking, given the experiments in previous works, and the usefulness of the suggested measures are not completely clear without a thorough comparison against previously suggested measures."
"In this paper, the authors proposed a method to learn efficient representations of discrete tokens. They took a two step approach: in step 1, they learn ""full fledged"" embeddings for a subset of anchor tokens. In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. This two-step approach reduced the overall number of parameters. The sparse matrix T can also encode domain knowledge (e.g. knowledge graphs). In the experiment section, the authors showed that their approach has good performance on several language tasks, with far fewer parameters. <sep> In general the paper is well written and the flow is easy to follow. I find the main idea plausible and ingenious. For language tasks and word embeddings, anchoring method has been shown to be effective in several tasks already (e.g. [1] http://papers.nips.cc/paper/8152-the-global-anchor-method-for-quantifying-linguistic-shifts-and-domain-adaptation). The authors took two steps forward: 1) instead of in [1] where the entire vocab is used for anchoring purposes, the authors used a subset of tokens which reduces the amount of parameters. 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. Although this paper is probably related to other strains of research (e.g. leaning manifolds for IR/NLP where anchoring is also a key concept, which the authors could have admittedly surveyed more), I particularly liked the fact that the two-step procedure decomposes two tasks that are often mixed together for embedding tasks: learning representation vs learning relations. <sep> While the authors claimed that they can further impose domain knowledge in the learning process (which I think this is at least a good attempt), this part in general feels a bit less convincing. To be specific, there can be a variety of knowledge (like related, is a subset of, analogy, etc.). It is not clear how the distinction of different types of knowledge can be incorporated. What the authors proposed is lumping them into the notion of ""positive pair"" and relax constraints on them. This may or may not suffice (for the purpose of adding domain knowledge), but on paper, there is a chance that some finer structures of the domain knowledge may get lost. It's not clear how much gain (especially the experiment section, for fair comparison purposes where other methods do know use domain knowledge in particular) is from incorporating domain knowledge; an ablation study might help. <sep> Another question is about training. It's not obvious to me how to guarantee that for every row of T, there is at least 1 non-zero element. Is some specific tuning needed for rows corresponding to rare words? How the regularization strength λ on T is selected? <sep> The reduction of parameters is while keeping task performance is illustrated quite well in the experiment section. Their method does not reduce the theoretical complexity (still linear w.r.t. vocab size, as T must have at least one element per row), but in practice the reduction (which mostly comes from savings of dimensionality) is quite obvious.","This paper proposes a method to cope with large vocabulary sizes. The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. They give an end-to-end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process). They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. Finally they give a variety of experiments, particularly in language and recommendation tasks. The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users. <sep> This paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis. One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (e.g. if the ideal number of anchors changes over time)."
"Summary: <sep> The authors theoretically study the prediction performance of pre-conditioned gradient descent/flow with linear models and squared loss aligning in the setting of least squares regression and non-parametric regression. For parametric least squares, the predication performance of the limiting solution for preconditioned gradient flow i.e. time goes to infinity, is studied in an asymptotic regime where both the number of samples and dimension go to infinity in proportion to one another. Meanwhile for non-parametric regression, source and capacity assumptions are leveraged to achieve finite sample guarantees. Experiments are also conducted on neural networks in a student and teacher setup. <sep> Summary of main Contributions: <sep> A1) In the case of parametric least squares, an asymptotic characterisation of the test risk is utilised to study the limiting solution of preconditioned gradient flow. Preconditioning with the inverse Fisher information matrix (covariates population covariance) is shown to achieve the optimal variance among preconditioned updates (Theorem 1).  Meanwhile for the asymptotic bias, the optimal pre-conditioner depends upon the covariance of the ground truth parameter. In a mis-aligned case, where the ground truth covariance is equal to the inverse of the population covariates covariance, the optimal pre-conditioner for the bias aligns with the inverse Fisher information matrix (Theorem 2). <sep> A2) In the case of an Isotropic covariance for the ground truth parameter, it is found that the Bias and Variance can be traded-off by interpolating between the two aforementioned pre-conditioners (Proposition 4). <sep> A3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered. Mini-max optimal statistical rates are achieved with a number of iterations that grows logarithmically in the data set size i.e. linear convergence (Theorem 7). <sep> A4) Experiments for neural networks are conducted in support of A1). Specifically, gradient descent pre-conditioned with the Fisher information matrix achieves better generalisation performance when the noise is large or the model is misaligned (Section 5). <sep> A5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6). <sep> Pros: <sep> B1) I feel contribution A1) in conjunction with A5) is novel and offers a precise interpretation of when pre-conditioning with the inverse Fisher information matrix can yield an improvement in performance. <sep> B2) Contribution A2) is also interesting and can point towards understanding and controlling the implicit bias of gradient descent through the pre-conditioner i.e. taking a linear combination of two pre-conditioners. <sep> B3) Contribution A4) supports the findings in A1) in a setting beyond least squares. <sep> Concerns: <sep> C1) The authors do not compare their theoretical results for non-parametric regression (contribution A3) ) to prior work within the literature. Specifically, reference [1] where the generalisation performance of a pre-conditioned gradient method is considered. To remedy this, I feel the authors should discuss how their theoretical results and proof method differ from [1] as well as the novelty of their approach. <sep> C2) The theoretical results and discussion focus on a particular type of pre-conditioner: the inverse population covariates covariance and transforms thereof. This limits the applicability of the insights as this quantity is often not known in practice. Similarly, the experiments are in a setting where Fisher information is estimated accurately using 100,000 samples while training uses 1024 samples. In contrast, prior work for non-parametric regression considers pre-conditioners involving estimates of the population covariance [1]. To remedy this, I feel the authors should include a discussion on how their insights i.e. A1), A2) are impacted when the population covariance is swapped for an estimate (using unlabelled data). <sep> C3) The manuscript can be difficult to read. For instance, the authors start with a time varying pre-conditioner while all pre-conditioners considered are constant in time. Tools from random matrix theory and regularity assumptions for non-parametric regression are introduced with little discussion. Section 3.3 ""Misspecficiation \\approx Label Noise"" considers misspecification that is independent and gets interpreted as additional noise. It is not clear what this brings to the manuscript in terms of insights and introduces another layer of complexity. <sep> C4) For parametric least squares regression the results focus on three cases for the ground truth covariance: well-aligned (where it equals the covariates population covariance), mis-aligned (where it equals the inverse covariance population covariance) and Isotropic. Whereas the theoretical results allow for a more general ground truth covariance to be considered. It would be natural to follow the source conditions from non-parametric regression and investigate natural gradient descent when the ground truth covariance is not fully well- or mis-aligned. <sep> General Comments: <sep> -Remark on page 4 states ""we demonstrate generalisation properties only possed by the population Fisher"", clarify which properties are /only/ held the population Fisher versus Sample Fisher. <sep> -In Proposition 4 possibly change the description ""interpolating preconditioners"" as all the preconditioned methods are interpolating the data, and thus, can be confusing. <sep> -Proposition 4 states for pre-conditioners (ii) and (iii) the bias is monotone for α in some range depending upon the covariates population covariance. What is the range of α and is the risk increasing or decreasing? What conclusions are we to draw from this part of the result? <sep> -In Figure 6 and Figure 23 how is ""geometric"" and ""additive"" interpolation defined ? <sep> -More discussion around Proposition 6 would be helpful. For instance, in the statement of the result what is choice of P ? The analysis is described as difficult, although no details are provided into how this result was obtained. Within the proof why is the ratio of Eigenvalues \\overline{\\lambda}{min}/\\widehat{\\lambda}{min} is bounded, and how many iterations are required until NGD is below, say, standard gradient descent? <sep> -In Section 5, the misalignment experiment in Figure 7 is conducted for MNIST but not CIFAR-10, with no discussion in the main body of the manuscript for why this is. Although, paragraph ""Misalignment"" in Appendix C.3 states the phenomena of NGD outperforming GD in the misaligned case is ""... difficult to observe in practical neural network training on real-world data"". The authors then go on to state that, in short, this is due to (see Appendix A) NGD moving parameters further from initialisation, and thus, no longer well described by a linear model i.e. NTK.  Is there a link between this discussion within the Appendix and the experiments? <sep> -A Summary at the start of Appendix A to describe contents of A1-A4 would improve readability.  Similarly, for Appendix C and D. <sep> -In the proof of Theorem 2 (Appendix D.2) some details on how to get from (ii) to (iii). <sep> [1] - Rudi, A., Carratino, L., and Rosasco, L. ""Falkon: An optimal large scale kernel method"",  Advances in Neural Information Processing System 2017. <sep> POST REBUTTAL EDIT: <sep> I thank the authors for providing detailed answers regarding my concerns. I have updated my score in light these comments. Below are some additional comments in response. <sep> Response to comments regarding C1) and C2): <sep> While early stopping with pre-conditioned updates differentiates this work from (A. Rudi et. al 2019), the analysis still requires the knowledge of the population covariance. Indeed, while the authors have included a section (Appendix A.3) showing that the operator norm of the population and the inverse regularised empirical covariance can be controlled, it would be insightful to discuss to what extent this allows the analysis for the pre-conditioned gradient descent to be extended to an approximated population covariance. <sep> Response to comment regarding C3): <sep> I am inclined to agree with reviewer 3, in that the manuscript is difficult to read due to the larger number of fragmented results. In this regard, I feel the authors should focus on a single phenomenon that is supported by both the parametric and non-parametric aspects of the paper, for instance, how pre-conditioning helps against misalignment. <sep> Response to comment regarding different prior on ground truth (point 4. third bullet point): <sep> Note that some concurrent works have studied the case of different priors on the ground truth [2,3], which are likely relevant in this case. <sep> Minor Comment: The pre-conditioned updates for non-parametric regression (4.1) use notation α where as Appendix D.8.1 uses notation λ, with the discussion then switching back to using α and λ being used in reference to the regularisation used within FALKON. The switching of notation is possibly confusing here. <sep> [2] - D. Richards, J. Mourtada, L. Rosasco ""Asymptotics of Ridge (less) Regression under General Source Condition"", arXiv:2006.06386 (2020) <sep> [3] - Wu, D. and Xu, J. ""On the Optimal Weighted ℓ2 Regularization in Overparameterized Linear Regression"" NeurIPS 2020","The paper provides a study of the impact of preconditioning/second-order methods on generalization by giving a precise analysis in tractable regression settings. <sep> It illustrates conditions under which preconditioning might be useful for better generalization. <sep> The readability issues raised by the reviewers have been taken into account, as well as some missing references, except <sep> Wu, D. and Xu, J. ""On the Optimal Weighted Regularization in Overparameterized Linear Regression"" NeurIPS 2020, raised by reviewer (though it is a really recent reference). <sep> Overall the contributions are significant enough to accept the paper for publication."
"Quality and Clarity <sep> The paper is generally well written barring a few typos (see Queries and Suggestions below). The problem is clearly described and the solution is well motivated. The main theoretical result (Theorem 2) might be a bit hard to parse for readers unfamiliar with the theoretical results in this space but the overall explanation is clear. <sep> Originality and Significance <sep> The main approach for converting biased compressor to unbiased compressors appears to be novel and flexible. The theoretical analysis of distributed compressed SGD using unbiased compressors can also be useful to the community since the derived upper bound can be easily applied to any unbiased compressor. <sep> Strengths <sep> The approach for converting biased compressors to unbiased compressor does not seem to have been stated in this fashion before and provides an elegant framework for combining previous approaches. <sep> The convergence analysis of unbiased compression is general and can be applied to any unbiased compressor and thus should be useful to the community as a whole. <sep> The proposed approach appears to be more flexible than biased compression + EF and the authors show how it can be extended to Federated Learning settings, and also admit techniques like variance reduction and acceleration using momentum. <sep> Weaknesses <sep> Neither the theoretical analysis (comparison of upper bounds) nor the experiments (comparisons for some specific biased and unbiased compressors) in this paper convincingly show that the proposed approach will always be theoretically and practically better than Error Feedback. <sep> The right choice of unbiased compressor C2 for a given biased compressor C1 is not clear. The authors show that if δ2=δ1 then the upper bound of Theorem 2 will be tighter for the induced compressor C than for C2. However it is not clear if that will be enough to outperform EF since there will still be some extra variance. Indeed in the experiments the authors consider several unbiased compressors and while Top-a + Rand-(K-a) seems to beat Top-K + EF no general guidelines are offered. <sep> The flexibility of the proposed approach which makes it amenable to the extensions  in Section 4 is not illustrated in the experiments which are limited to a direct comparison of the proposed approach with EF. <sep> Queries and Suggestions <sep> Why do both C1(g) and C2(e) need to be communicated if C is unbiased? (paragraph after Theorem 3) <sep> While I understand that it is probably not possible to demonstrate that the proposed approach will outperform EF in every single scenario, I believe including some guidelines about choosing biased and unbiased compressors appropriately will greatly increase the impact of this work. For eg. Are all combinations of biased and unbiased compressors acceptable? Should one just care about ensuring that δ1=δ2? What are good choices of unbiased compressors for some popular biased compressors? If it is possible to answer these questions, the paper will definitely be more useful to readers. <sep> I would also recommend adding experiments on at least one of the extensions from Section 4 to highlight the flexibility of the proposed approach. <sep> Typos: <sep> i) Missing 'of' in Section 2, line 4. <sep> ii) Last term in RHS of first equation in C.1 should be +||x||2 <sep> iii) Missing '2L' in 3rd inequality in C.2 <sep> iv) Superscripts and subscripts have been interchanged in some terms in the proofs of Lemmas 6 and 7. <sep> v) Second term in LHS of first equation of C.3 should be C2(.) <sep> Overall, while I am not quite convinced that the paper achieves its stated goal of showing that the proposed unbiased compression approach always outperforms biased compression + EF, I believe it adds enough to the discussion in this space to merit acceptance. <sep> Comments after Author Response <sep> I thank the authors for their response. My opinion of the potential of this paper to encourage further discussion in this area is unchanged. I can already see from the response on choice of biased and unbiased compressors to combine that there is plenty of scope for future work that builds on this idea. Regarding the comparison with EF, I appreciate the additional intuition provided in the author response on the drawbacks of EF and hope to see improvements to EF or more exhaustive comparisons between EF and the approach proposed in this paper in future work. As I had already recommended acceptance I am leaving my score unchanged.","I agree with the reviewers' comments. The technique proposed in the paper is very interesting, and although the method itself is not particularly surprising (it's ""just"" chaining two compressors), it's a really nice way of framing and studying the problem. On the other hand, the experiments are relatively weak, and I think there is significant potential for improvement here (especially with an added 9th page of text). I encourage the authors to add some more convincing experiments in future versions of the paper."
"This paper contributes to a recent line of researches linked to Neural Ordinary Differential Equations (Chen et al. 2018) and variants. <sep> This new family of deep neural networks (NODE) generalize ideas from Residual Networks and consider continuous dynamics of hidden units using an Ordinary Differential Equation (ODE) specified by a neural network. Computing in such networks consists in taking x=h(0) as input and define the output layer h(T) to be the solution of an ODE initial value problem at time T. The main ingredient which makes learning possible is the backpropagation algorithm through the last layer ODE solver, that relies on the adjoint sensitivity method (Pontryagin et al., 1962). <sep> This work considers Delay Differential Equations instead of ODE, which allows to implement more complex dynamics and thus achieve estimation of more complex functions. The contributions of the paper are the following: <sep> (1)    a novel deep continuous -time deep neural network defined by the following equations <sep> For a given delay \\tau (never discussed), Neural Delay Differential equations define dynamics of the form: <sep> dh_t\\dt = f(h_t,h_{t-\\tau}, t ;w),  t \\geq 0, <sep> h(t) = \\phi(t), t \\leq 0 <sep> In this way the network can take into account a former hidden layer. <sep> (2)    the derivation of the adjoint sensitivity method for delayed equations, which seems relatively straightforward and which is backed by two proofs in the appendix. <sep> (3)    A novel learning algorithm that implements the forward for h and the backward pass for h,lambda (the augmented variable) and dL/dw (L is the loss) by a piece-wise ODE solver, dealing with the different delayed states. <sep> (4)    Experiments on 2D toydatasets and on classic differential equations such as Mackey-Glass are shown to exhibit the ability of DDE to cope with those dynamics, in constrast to ODE. <sep> (5)    Experiments on image datasets <sep> (6)The strong point of this paper is of course the proposal of the new variant of NODE, which comes with a novel algorithm and overcomes some limitations of NODE. I was interested by the examples of functions not covered by NODE and covered by NDDEand easily convinced by that. <sep> There are some weak points in this work. Some of them could be easily improved I think, others call for further  work. <sep> Relatively Minor points <sep> The paper is not self-content and does not a very good job in explaining the context of Neural ODE. I suggest that you more clearly describe NODE, as a hypothesis class and then as a learning algorithm. <sep> The two uses of NDDE concern modeling time-series  or implementing a classification/regression function. Can you each time precise inputs/outputs, samples you use in training. Just for clarity, the reader guesses of course but its rather slows down the reading this absence of notation and formalization ? <sep> Major points and questions to the authors: <sep> More importantly, as a novel algorithm is introduced, I expect to see a complexity in time analysis. As for NODE I understand that the complexity in space is favorable. <sep> An associated question is the role of tau, the delay. How is it chosen ? I imagine that if tau converges towards 0 we find the behaviour of NODE ? what was its value in the two real-world experiments? Was it selected by cross-validation ?  Is it too computationally heavy to consider multiple delays ? <sep> Eventually, I have doubts and questions about the real opportunities for using NDDE in the two real word datasets: yes the divergence of NODE poses problems : in these experiments no doubt that NDDE has a more stable empirical behavior, with a small variance. However, in fine, the average performance is nearly of the same order, a significant difference only on MNIST but with a very bad score considering this is an easy problem. I won't qualify these results as exceptional and I kindly engage the authors to remain modest. <sep> Now can we cope without delays by introducing new states in the way augmented  Neural ODE are built (Dupont et al.) ? <sep> For what kinds of classification problems NDDE could be more interesting than NODE – The question is certainly difficult. <sep> In conclusion, the paper tackles a very interesting topic and I had pleasure to discover it (with the help of literature). Although the idea to consider DDE instead of ODE is incremental, its relevant and novel, and certainly promising and worth to be explored because it addresses some of the issues of NODE. <sep> However I stay on my hunger on different points, there are pending questions that require to be answered before acceptance.","This paper is a variant of the large growing class of Neural ODEs, and adds dependency on a time delay to the baseline, which allows to model a larger class of physical systems, in particular adding the possibility of crossing paths in phase space. <sep> After initial evaluation, the paper was on the fence, with 2 reviewers providing favorable reviews, and 2 reviewers recommending rejection. A particular important issue raised was positioning with respect to prior art, [Dupont 2019], with some substantial overlap between the papers; requests of theoretical discussions of the class of studied systems and its properties. <sep> Most of these remarks have been addressed by the authors, in particular positioning and experimental comparisons. <sep> The AC judged that the paper had been sufficiently improved and recommends acceptance."
"Summary: <sep> The paper contributes a novel method, Drop-Bottleneck (DB), for discretely dropping input features that are irrelevant for predicting the target variable. Key idea is to instantiate the compression term of the information bottleneck framework with learned term that sets irrelevant feature dimensions to 0. To this end, a drop probability is learned for each dimension. Dimensions that have a lower probability than 0.5 (a fixed threshold) of being relevant are set to 0. <sep> Strong Points: <sep> The paper is well-written and easy to understand. <sep> Experiments show that DB works better than VIB in VizDoom and DMLab when a noisy-TV noise is added to the input images. Different noisy-TV noises are considered: changing the image when the agent performs an action, adding random noise to the TV, and adding random noise when the agent performs an action. <sep> Experiments also show that the obtained representation is more robust against l_2 and l_inf attacks in ImageNet. Furthermore, the experiments show that the approach can drop many ImageNet features while almost preserving the accuracy of a ResNet. <sep> The paper comes with code in the supplementary material. <sep> Weak Points: <sep> I found the reinforcement learning experiments not convincing since only a fixed region of the input is modified by noise (ie. the noisy TV). Hence, the approach essentially identifies irrelevant pixel locations. Such a problem could be solved by a simple pre-processing step. The method won't work if the location of the noise changes. In general, limitations of the work are not discussed. <sep> The experiments on ImageNet are more interesting. However, the fact that individual dimensions (i.e. specific pixel locations) are identified as irrelevant is still a limiting factor. Furthermore, the experiments do not fit to the focus of this paper on reinforcement learning. <sep> The paper does not discuss connections of the presented approach to prior works for (discrete) feature selection. It only discusses connections to prior bottleneck methods. <sep> The paper does not perform experiments on datasets with meaningful features where a feature selection makes more sense than for specific pixels in images. <sep> Additional feedback: <sep> It could be stated explicitly that H refers to the entropy. Currently, it is only implicitly defined in Eq. 6. <sep> I think it would be better to also cite Jang et. al (Categorical Reparameterization with Gumbel-Softmax) for the Concrete relaxation of the Bernoulli distribution. <sep> Figure 1 should either be improved or removed. I don't see much additional insights that can be gained from this figure. <sep> Of course, it would be very interesting to see if the drop probabilities correlate with the location of the noise inputs. It would be great if such an analysis could be added. This could replace Figure 1.","This paper proposes to enhance the robustness of RL and supervised learning algorithms to noise in the observations by dropping input features that are irrelevant for the task. It relies on the information bottleneck framework (well derived in the paper) and learns a parametric compression of the input features that sets them to zero if they are not relevant for the taskn. The method is extensively evaluated on several RL tasks (exploration in VizDoom and DMLab with a noisy ""TV"" distractor) and supervised tasks (ImageNet or CIFAR-10 classification with noise). <sep> Reviewers have praised the idea, derivation and writing, as well as the extensive experiments on RL and supervised tasks. Critique focused on: <sep> the contrived nature of the TV noise (localised always in the same corner of the image -- a standard evaluation according to the authors), <sep> lack of comparison with other feature selection methods, <sep> lack of comparison with Conditional Entropy Bottleneck (done during rebuttal), <sep> more general noise than just specific pixels (clarified by the authors as being the features coming out of a convnet) <sep> Given that the reviewers' comments were largely addressed by the authors, and given the final scores of the paper, I will recommend acceptance."
"------Overall------ <sep> This paper utilizes the memorization effects of deep models and aims to improve their robustness to noisy labels before early stopping. As the deep models fit training data with clean labels in the early stage of training, the authors propose a novel method to identify those more important parameters for fitting clean labels. They then deactivate the unimportant parameters to reduce side effects brought by noisy labels, which enhances the fitting to clean labels implicitly. I think this paper is interesting and makes sense. The major comments and issues are as follows: <sep> ------Major comments------ <sep> Different from other complex methods for learning with noisy labels , this work discusses that standard cross entropy loss can achieve competitive performance with early stopping. We can therefore focus the training stage before early stopping to handle noisy labels. The authors skillfully allow the optimality to be checked by a scalar, and then judge the importance of the parameters by analyzing the influence of the parameters on this scalar. The idea of this paper is novelty and meaningful. <sep> The paper is very well-written. The description of its motivation and technical details is clear and flows smoothly, which makes it easy for readers to understand the core idea of this paper and follow its implementation details. <sep> The experimental results are convincing. The authors provide a very detailed description of experimental settings. Besides, this paper exploits multiple methods for comparison and considers various noise settings to verify the effectiveness of the proposed method. The experimental results on synthetic and real-world datasets are convincing. The authors also perform an ablation study to present the proposed method is insensitive to the estimation of noise rate. <sep> ------Issues------ <sep> I only find the illustration of comparison between CE and CDR in the case of noisy CIFAR-100. This paper aims to reduce the side effect of noisy labels before early stopping, thus CE is an importance baseline in this paper. Can the authors add illustrations of the experimental results in other cases like Figure.2? <sep> The baselines and experimental results are sufficient. Could the authors add some introduction for the baselines and more detailed discussion for experimental results. <sep> The authors may need add some explanation for Eq.(2) and Eq.(5). The proposed method makes use of the memorization effects of deep models. The authors directly write \\tilde{S} rather than S in Eq.(5). However, this may be easy to misunderstand. I suggest that the authors can emphasize it or change it. <sep> Some typos need to be corrected. (1) ""The underlying issue of directly using the gradient of......""; (2) ""Robust positive update uses the gradients to update the critical ones......"". <sep> Some minor comments. (1) The experimental results in Table 1 are too dense. (2) The figures are not readable, especially the title is small for me. This makes it a little hard to match the figures with specific cases. (3) The parameter (noise rate) τ still needs to be estimated, which may be challenging. It will be promising to automatically set this parameter during training. Thus, a more in-depth analysis is worthy of further learning. <sep> I hope the authors can address these issues carefully to improve this work.","The paper presents a novel method for learning with noisy labels based on an interesting insight into the learning dynamics of deep neural networks. <sep> Reviewers unanimously vote for acceptance. I agree with their assessment, and it is my pleasure to recommend the paper for acceptance. <sep> If I can draw attention to one comment, I strongly agree with R1 that the criterion in Eq. (3) is somewhat poorly motivated. I believe the paper would benefit from a clearer exposition of this part. <sep> Please make sure to address all reviewers' remarks in the camera-ready version. Thank you for submitting your work to *CONF*."
"Pros: <sep> This paper proposes new scene, where a meta model is pre-trained on some datasets, and transfer the learned meta feature onto other datasets to do fast adoption. This scene can be applied in variety of domains. <sep> The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance. <sep> The paper is well-organized, the paper structure is clear. <sep> Cons: <sep> The three parts of your model are of little novelty. The dataset encoding part is just borrowed and there is no improvement to adapt NAS tasks. Similar graph decoder is proposed in previous NAS works [1] and performance predictor are proposed more times. It seems that the proposed framework is just putting existing models together. <sep> No comparing to other methods on fast adaptation by NAS such as [2]. Besides, meta-learning methods may also be compared. <sep> According to Figure 8 and Figure 9, it is likely that your graph decoder can only generate one type of edge connections. Your graph decoder may fail. Since your framework needs other methods (GDAS / NAS-Bench-201) to provide training material. These materials are all good architectures. It is possible that your framework just gives architectures the same as those good architectures rather than using meta features. Your experiment should prove that your model can generate variety of architectures. <sep> Overall Review: <sep> This paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta-learning. The paper proposes a framework to generate good architectures according to the datasets. However, the model may need to improved and more experiment need to be done to solve the problems mentioned above. <sep> [1]Does unsupervised architecture representation learning help neural architecture search? NeurIPS 20' <sep> [2]Fast neural network adaptation via parameter remapping and architecture search *CONF* 20'","The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset-to-Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset-architecture latent space learned with amortized meta-learning. Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta-learning of a ""dataset-conditioned architecture generation"" framework can enable fast generation of a good architecture without training on the target dataset. The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real-world problems. I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS-Bench-201 search space to show the strength of their proposed method in generating good architectures. While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn't show that it can successfully meta-learn to produce better architectures for a new task from an existing pool of good architectures. <sep> We believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (e.g., R3, in his last post, seems to suggest that his review should be updated but it has not happened)."
"The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality. <sep> Strengths: <sep> The idea is simple and leads to a notable improvement compared to MA in terms of likelihood <sep> The background and method is presented very clearly. <sep> The evaluation is done on a wide variety of tasks, ranging from standard 1D regression of GP samples to pendulum trajectory prediction tasks. <sep> Weaknesses: <sep> The evaluation is missing an important baseline model, which are (A)NP models that have self-attention in the encoder for processing the contexts (c.f. model figure in ANP paper (Kim et al., 2019b)). Contrary to the NP/CNP baselines that are compared against in the paper, the ANP with self-attention in the encoder does not give uniform weights to each context point - the self-attention allows the model to assign varying importance to the different context points (despite using mean-aggregation after the self-attention), which is presented as a key motivation for the BA mechanism introduced in the paper. Hence for the experiments, I strongly suggest comparing against CNP/NP/ANP with self-attention in the deterministic/latent/latent path of the encoder. For completeness, if would be nice to also compare against models that have both deterministic and latent paths, since BA can also be applied to these models. At the same time, I understand that BA would be more interpretable for showing which observations have little/high effect on z compared to the approach of using self-attention in the encoder, but it would still be very informative for the reader to be able to compare the two approaches. Also these two approaches can be combined to have self-attention in the encoder + BA, which might also yield improved performance. <sep> The claim that ""BA includes MA as a special case"" doesn't seem to be true. Using a ""non-informative prior and uniform observation variances"" leads to constant sigma_z and mu_z being linearly proportional to mean(r_n) (i.e. sum_n r_n / N), which is not quite the same as MA - MA allows sigma_z and mu_z to be non-linear functions of mean(r_n), hence is strictly more expressive than this special case. <sep> In Equation (7), it seems as though the context points (x_n,y_n) only affects r_n via the variance, which seems unnecessarily limiting. Why not have the mean also depend on r_n? e.g. p(r_n|z) = N(r_n| z + mu_{r_n}, diag(sigma_{r_n}^2) where mu_{r_n} is also computed as a function of (x_n,y_n)? This will still give a closed-form posterior p(z|r_{1:N}) since the mean of p(r_n|z) is still linear in z, creating a model that's strictly more expressive with very similar efficiency. It would be informative to see how this changes the experimental results. <sep> I'm guessing the VI objective was used to train the ANP. Given the clear advantage of training with the MC objective, shouldn't the ANP also be trained with MC? <sep> The latent variable models were not evaluated on 2D image completion tasks because ""architectures without deterministic paths were not able to solve this task"". Why not then add a deterministic path to these latent variable models to allow them to train? <sep> Other points <sep> In the text, it says that the model is also compared against ANP to show that BA can compete with SOTA. This is arguably incorrect since ConvCNP models are SOTA among models of the NP family, showing a significant improvement over ANP. Hence to achieve the goal mentioned in the text, it would make sense to compare with ConvCNP models as part of the evaluation against other deterministic NPs. <sep> Overall the paper is presented very clearly with a simple yet effective idea tested on a wide variety of tasks. However it's missing an important baseline that uses self-attention in the encoder, along with several other baselines that would be informative to compare against. I am willing to increase my score should these results be included in the revised version of the paper. <sep> ================= <sep> Score raised to 6 after inclusion of MA + SA results in rebuttal.","The authors present a Bayesian approach for context aggregation in neural processes based models. The article is well written, and provides a nice and comprehensive framework. The reviewers raised some issues regarding the lack of comparisons to proper baselines. The authors provided additional comparisons in the revised version. The comparisons were found satisfactory by some some reviewers, who increased their scores. Based on the revised version, I recommend acceptance."
"This paper studies the problem of maximizing empowerment in the context of RL, where the aim is to maximize the mutual information between some latent variable and future outcomes (e.g., future states). The paper first observes that a procedure proposed in prior work [Gregor 16] is biased and hence does not recover a (latent-conditioned) policy that maximizes mutual information. The paper then proposes a new method, based on learning the transition dynamics. that does recover the optimal (mutual information maximizing) policy. Experiments on a few simple tasks show that the proposed method outperforms prior methods. <sep> Significance: Empowerment remains one of the main methods for autonomous skill discovery. Thus, a better understanding of how to optimize empowerment would be an important contribution to this area. This paper identifies a limitation (a biased objective) in a commmon formulation of empowerment [Gregor 16], and proposes a method to correct for this. I think the significance of this paper hinges on (1) how large this bias is for reasonably complex tasks, and (2) if this type of bias might occur in other RL objectives, besides empowerment. The paper only convincingly shows that removing this bias is useful for maximizing empowerment on small scale problems. <sep> Novelty: To the best of my knowledge, this limitation of VIC has not been discussed in prior work. <sep> Experiments <sep> It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods. <sep> It'd also be good to include explicit VIC as a baseline, even though it requires pre-specifying the number of skills. <sep> The experiments are limited to very simple gridworlds and tree domains. <sep> Clarity <sep> The derivation of the variational bias in S2 is pretty hard to follow. I'd recommend including a bit more discussion of what implicit VIC is and how it differs from explicit VIC, before continuing with the formal derivation. <sep> S3 and especially S4 are also hard to follow. I'd recommend moving most of the derivation to the appendix and just stating the final objective as an equation. Theoretical guarantees can then be stated as Theorems/Lemmas with proper proofs. <sep> Overall, I give this paper a score of 5 / 10, primarily because of (1) a lack of clarity and (2) the limited experiments. I would increase my score if the clarify of writing were (greatly) improved, if experiments on higher dimensional tasks were added (e.g., see those in [Achaim 18, Eysenbach 18]), and if additional visualizations of the (suboptimal) behavior of implicit VIC were added. <sep> Questions for discussion: <sep> How significant is the bias in implicit VIC [Gregor] in more complex tasks? Is it significant enough to warrant the additional complexity of the proposed approach? <sep> Is the GMM approach in S4 just a special case of the model learning approach in S3, where the model is taken to be a GMM? <sep> Does explicit VIC have the same bias as implicit VIC? <sep> Minor comments <sep> ""methods for measuring it"" -- This makes it sound like Arimoto + Blahut proposed methods for measuring empowerment. I'd revise to ""along with methods for measuring it based on Expectation Maximization [Arimoto + Blahut]"" <sep> ""This can severely degrade the empowerment"" -- Clarify what this means. <sep> ""This type of option differs..."" -- Aren't there two differences? (1) Closed loop options depend on the state at each time step, and (2) these options include a termination condition. <sep> "" which hinders the maximal level of learning"" -- Add a citation. <sep> ""implicit VIC which defines the option as the trajectory until the termination"" -- This sentence is confusing without knowing about the method apriori. <sep> ""becomes possible to learn the maximum number of options for the given environment"" -- Add a citation. <sep> ""achieve the maximal empowerment"" -- It'd be good to formally define what the ""maximal empowerment"" means. <sep> ""environment dynamics modeling incorporating the transitional probability"" -- Grammar error. <sep> ""is the inference to be trained"" -> ""is the inference network/model to be trained"" <sep> Eq 4: Where are \\tau_t and s_{g | \\Omega} defined? <sep> In S3, it might be clearer to use q(...) instead of p^q(...). <sep> In Eq 15, it's unclear how \\log p_\\rho^p depends on \\theta_\\pi^q. <sep> ""when the cardinality of the state space is unknown"" -- Where does the cardinality of the state space show up as a dependency? Perhaps what is meant is that the method is most readily applicable to discrete settings, where the distribution over future states can be approximated exactly, without sampling. <sep> S4: I'd recommend providing some intuition for why this alternative method is being derived. Is it going to address a limitation of the method in S3? <sep> ""Gaussian Mixture Model (GMM) (Reynolds & Rose, 1995)"" -- I think GMMs existed before 1995. E.g., see early work by Karl Pearson. <sep> ""extreme gradient"" -- What is an extreme gradient? <sep> ""revisited the variational"" -> ""revisited variational"" <sep> Update after author response:  Thanks to the authors for answering my questions during the rebuttal paper and for incorporating the feedback into the paper! My original concerns were about clarity, high-dimensional experiments, and visualizations. Since the paper has been revised to include nice visualizations and improve the clarity, I am increasing my score 5 -> 6. <sep> I think the experiments on HalfCheetah are a great proof-of-concept of the method! I'd encourage the authors to include some comparisons against baselines for that task.","This paper revisits the under-explored ""implicit"" variant of Variational Intrinsic Control introduced by Gregor et al. They identify a flaw that biases the original formulation in stochastic environments and propose a fix. <sep> Reviewers agree that there is a [at least a potential, R4] contribution here: ""even the description of what implicit VIC is trying to do is a novel contribution of this work"", in the words of R2, and ""the derivation has theoretical value and is not a simple re-derivation of VIC"", in R4's post-rebuttal remarks. Several reviewers raised significant concerns around clarity, which were addressed in an updated manuscript, which also provided new visualizations and new experiments which reviewers found compelling. All reviewers agreed that the revised manuscript was considerably improved. <sep> R4's score stands at the 5, with the other reviewers all standing at 6. R4's main concerns are around whether the missing term in the mutual information identified by the authors is a problem in practice on non-toy tasks (echoing somewhat R3's concerns re: high-dimensional tasks). While this is a valid concern, the function of a conference paper needn't necessarily be to (even attempt) to provide the final word on a matter. Identifying subtle issues such as the one brought forth in this manuscript and re-examining old ideas is a valuable service to the community, and this paper will serve as a beginning to a conversation rather than an end. The AC also considers themselves rather familiar with the original VIC paper, and found the results herein somewhat surprising and noteworthy. <sep> I recommend acceptance, but encourage the authors to incorporate remaining feedback in the camera-ready."
"Summary <sep> Using backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients. This offers a possible explanation of the empirically observed positive effect of (relatively) large step sizes on generalization performance. The paper further contests previous findings based on a vanishing step size assumption. <sep> Rating <sep> Similar to several recent works, this paper tries to explain certain aspects of stochastic gradient descent using a continuous time approximation. In contrast to existing works, it explicitly accounts for the effect of finite step sizes, which I think is a very interesting direction and surfaces several interesting aspects. I also welcome and endorse the critical discussion of prior work based on infinitesimal step size assumptions. Overall, the paper was interesting and pleasant to read. To the very best of my knowledge, all mathematical derivations are technically correct. <sep> However—as the authors themselves note in their critique of SDE approximations to SGD—the devil is in the details with continuous time approximations. In my opinion, that makes is absolutely crucial to discuss the scope of the results carefully and transparently, including a critical discussion on assumptions made and simplifications that go into the continuous-time model. In my opinion, this paper fails to deliver that, which is why I recommend rejection. Below, I am asking for clarification on various points and would encourage the authors to respond to the major points in the rebuttal phase. <sep> Major Comments <sep> The main result says that the expected SGD iterate after a single epoch lands close to the path of a gradient flow ODE on a modified loss. Unless I am missing something, this fundamentally fails to capture the behavior over multiple epochs. The analysis only guarantees that, from any given starting point ω0, the expected iterate after one epoch of SGD ends up close to the ODE path starting from ω0. Unless I am missing something, this does not imply that two epochs of SGD starting from ω0 end up on that path. We can not simply chain two epochs together: The first epoch only stays on the path in expectation, but any realization of that random variable will deviate from the path, which affects the initial condition of the next epoch. Intuitively, one needs to get a handle on the variance of the iterate as well in order to give guarantees for multiple epochs. Is this understanding correct? If so, to what extent can insights about a single epoch of SGD be transferred to practical settings? <sep> Comment (1) hints at a larger (but vague) point that the paper is trying to characterize a stochastic optimization procedure with a solution of a deterministic gradient flow ODE. It does so by focusing on the expectation of the iterate, which might be an approach to highlight certain aspects, but it will never give a full picture. Why wouldn't we also be interested in the covariance of the iterates? The limitations of this characterization should be discussed thoroughly in the paper. <sep> In Section 2, the composition of the minibatches is assumed to be fixed and the randomness only comes from their ordering. The paper says: ""It is standard practice to shuffle the dataset once per epoch, but this step does not affect our analysis and we omit it for brevity."" I don't think that statement is justified with respect to the result in Eq. (1), given that the modified loss depends on the minibatch composition. Therefore, would we reshuffle the dataset after each epoch, the modified loss would change from one epoch to the next. Later, in Section 3, the expectation is additionally taken over the composition of the batches. Why is the result presented in these two distinct steps? None of the key findings of the paper seems to rely on the intermediate fixed-composition result. It also doesn't reflect the common practice of reshuffling the entire dataset and then traversing it, which simultaneously randomizes the composition and ordering of batches. So why not give the result of Eq. (22) directly? It is also the more intuitive result, invoking the trace of the gradient covariance matrix, which also appears in prior work on continuous time approximations of SGD. <sep> While the analysis tries to account for finite step sizes, it still seems to assume step sizes that are orders of magnitude smaller than those used in practice. In particular, when going from Eq. (12) to Eq. (13), each minibatch cost function is equated with its second-order Taylor approximation around the starting point ω0. This is a drastic approximation and I don't see any justification for why this should be anywhere near accurate for practical settings. For large datasets and moderate batch sizes, the number of updates in one epoch will be in the thousands. For realistic step size choices, a second-order Taylor expansion around the starting point will probably be rather poor after a handful of SGD updates, no? <sep> The paper strongly emphasizes the assumption of sampling data points without replacement. While sampling without replacement is indeed the usual setting in practice, most of the stochastic optimisation literature builds on the assumption of sampling with replacement. And to my knowledge, no major differences (in terms of generalization performance) have been reported in the literature between the two approaches. <sep> a) Can the analysis presented in the paper be extended to setting of sampling with replacement? It seems to me that this should be straight-forward. Equations (12) and (13) should hold also when each minibatch is obtained from sampling with replacement. In that case, the expectation of the second-order correction term should directly give a result akin to Eq. (22). If that is in fact possible, it should definitely be added to the paper. <sep> b) If that is not possible, what prevents the application and is this a technicality or would you actually expect substantially different behavior in terms of generalization? <sep> c) It would also have been nice to see the experiments repeated with sampling with replacement to check empirically whether the findings hold in that case? <sep> Something that bugs me from an optimization perspective is that the smoothness properties of the problem do not enter this analysis at all. For example, you write (near the bottom of page 4) that ""our analysis assumes mϵ=Nϵ/B is small."" However, any given loss function C(w) can be rescaled by a constant M≫1 while scaling the step size with 1/M. This leaves the behavior of SGD unaffected while making the step size arbitrarily small. Why does that not enter into the analysis? It probably relates to my comment (4), seeing that the step sizes are assumed to be so small that they are not restricted by the smoothness of the function. <sep> Minor Comments <sep> The paper derives the implicit regularizer and provides empirical evidence that it can partially explain the benefits of large step sizes for generalization. However, very little attention is given to the regularization term itself and to the question why this regularizer might be beneficial. The only comment speaking to that is that the regularizer penalizes ""sharp"" regions. I would like to see this discussion expanded and connected to the recent literature. <sep> At the end of page 6, you write about the large batch size regime and say that the ""we expect the optimal learning rate to be independent of the batch size in this limit."" It would have been great to substantiate that conjecture with an experiment and/or to refer to specific experiments done in prior work. <sep> You repeatedly use the phrase ""small but finite learning rates"". If my understanding is correct, that has phrase has a very precise meaning in the context of this work, namely that terms of order O(ϵ3) are vanishingly small while terms that a quadratic or linear in ϵ can not be ignored. (This is in contrast to prior work that also ignores quadratic terms.) Maybe this could be stated clearly the first time you use this phrase. <sep> Typos / Style <sep> I think you should capitalize references to sections, equations, figures, et cetera. <sep> The bib file could really need some love. You are citing the arXiv versions for several papers that have been published in peer-reviewed venues. Capitalization in paper titles is messed up (e.g., ""sgd""). <sep> Edit after Rebuttal <sep> I thank the authors for their engagement with my review. Many of my comments and questions have been resolved and, consequently, I have increase my score and recommend accepting this paper.","Dear authors, <sep> all reviewers found many interesting contributions in your paper and also pointed out some minor/major issues. In your rebuttal discussions, you addressed most of them to their satisfaction and I hope you will incorporate them carefully also in your final submission. <sep> I hence recommend accepting this paper"
"Summary <sep> The paper provides an extensive empirical analysis of Pruning-at-Initialization (PaI) techniques and compares it against two pruning methods after (or during) training. This comparison sheds some light on why pruning at initialization is inherently hard. Furthermore, the comparison among PaI methods with various ablations shows some inherent properties that are common to PaI methods and the benefits/drawbacks of certain methods. With these experiments, certain conclusions are reached among them an important one is that PaI methods only determine what is the fraction of weights to be pruned in each layer rather than which weights to prune. <sep> Strengths <sep> Quantitative comparison of PaI methods is an important contribution to the community. In addition, the paper extensively analyses them with various ablations (eg, shuffle the layerwise masks, reinitialization) which uncovers various properties of PaI methods previously not conveyed clearly in the respective papers. <sep> Extensive analysis shows that there is a performance gap of PaI methods compared to Pruning-after-Training (PaT) methods (understandably) and attempts to explore the potential reasons. This analysis uncovers some inherent difficulties of PaI. The analysis provided in this paper could serve as a guide to better understand and improve PaI methods. <sep> Overall the paper is clearly written and sufficient details are provided in the appendix. <sep> Weaknesses <sep> The main weaknesses of the manuscript in my opinion are as follows: <sep> Some conclusions of this paper are unsubstantiated or should be rephrased: <sep> First, the experiments mainly convey the inherent difficulties of PaI rather than the issues with specific methods themselves. To address this concern, the methods (SNIP, GraSP, SynFlow) are performed during training and showed that they perform inferior to LTR in Fig. 6. However, this comparison is unfair as LTR has additional information which is obtained after training (pruning mask is obtained after training but applied during training) and those PaI methods are not specifically designed to be performed during/after training (even so some perform reasonably well). In short, it is not clear that the performance of PaT can be matched by PaI given only the information at initialization. Note that, having access to the pruning mask after training defeats the purpose of PaI since given an already trained network, one might as well simply prune at the end (no need to retrain from scratch). I believe, Sec. 7 has some discussions about this (not complete) but should come early in the paper and emphasized. Please clarify and rephrase certain parts (especially in the introduction). <sep> One of the main conclusions: ""PaI methods only determine what is the fraction of weights to be pruned in each layer rather than which weights to prune"" should be rephrased. In fact, it is possible to have a pathological case (eg, disconnected layers at a given pruning ratio) where knowing the optimal pruning ratio is not sufficient to obtain matching accuracy. These pathological cases are not observed in practice due to the random component (initialization or shuffling) and it should be emphasized. Also, since the weight initialization is iid Gaussian, there may not be sufficient information for PaI methods to select each weight individually but rather select them as a group in each layer or whole network. <sep> Another hypothesis that the performance gap between PaI and PaT is due to the robustness of PaI methods to shuffling, reinitialization, etc, are also not clear. I understand that there is a correlation exists that some PaT methods are not robust to such variations. But it is NOT clearly demonstrated that being robust to such variations is necessarily a bad thing for PaI. In fact, one would think it is a good thing that PaI methods are robust to such variations given that there is not a lot of information available at initialization (note initialization is iid) to perform effective pruning and it seems these methods are robust and perform competitively to unpruned networks. <sep> Inconsistency of SNIP being independent to initialization: <sep> In the last paragraph of page 5, it is mentioned that SNIP is independent to what initialization is used. However, there is a follow-up work of SNIP showing that it could be beneficial for SNIP if the network is initialized to have good signal propagation [a]. Please discuss this in the context to avoid any misinterpretations. <sep> Issue with random shuffling: <sep> The random shuffling experiment needs some refinement in my opinion. I believe PaI methods such as SNIP, GraSP or SynFlow might have some unpruned weights which are not updated during training (ie, they are disconnected in the signal propagation path). In my personal experience, I observed that there are about 1-2% of unpruned disconnected weights in the network (in particular layers this value could be up to 10%) for SNIP (meaning they can be removed) depending on the network. This means the effective sparsity is slightly lower than what is observed in SNIP (not mentioned in the original paper though) and presumably in other methods as well. This observation would be applicable to random shuffling as well. I mean, after random shuffling there might be some unpruned disconnected weights and they could be removed before training, leading to higher effective sparsity. Furthermore, the existence of unpruned disconnected weights could be the reason for similar behaviour even after random shuffling in each layer. <sep> I found the experiments in this paper to be thorough but I think the deduced conclusions are slightly off with respect to the observations in the experiments. I believe this paper will be a good contribution to the pruning literature if the conclusions are tightened. <sep> Minor Comments <sep> Related work: SNIP is not a follow-up of LTH but rather they are concurrent works published in *CONF* 2019. <sep> Abstract: I think ""undermines"" might be a strong word given that there are questions regarding deduced conclusions. <sep> References <sep> [a] Lee, N., Ajanthan, T., Gould, S. and Torr, P.H., 2020. A signal propagation perspective for pruning neural networks at initialization. *CONF*.","The paper analyses several approaches to pruning at initialization, compared to after training. There was a large gap in reviewers appreciation of the paper, but I think that the pros outdo the cons as the paper show a lot of insights overall. I recommend accepting the paper."
"Pros <sep> Audio-visual sound source separation is an impotant task. The paper pushes the boundary from specific domains (e.g. speakers, musics, etc) to generalized open-domain, which is crucial and far from trivial. <sep> The authors introduced a new, large-scale, open-domain dataset for on-screen audio-visual separation. The videos span 2500 hours, 55 of which are verified by human labelers. The dataset will definitely be very useful for the community as it is way more diverse than before. <sep> Cons <sep> Related work <sep> To my understanding, Owens and Efros (2018) did not assume fixed number of speakers. While they validate their method under such setting, there is actually no limitation in their model that prevents them to have multiple on-screen sources. Therefore, I'm not sure about the first contribution, except the ""open-domain"" part. <sep> Model <sep> In terms model architecture, (maybe I have missed something but) I didn't see much novelty in the current state. To me, the proposed model is simply a composition of multiple exisiting modules from previous work. Please note that I'm not saying building upon the sucess of previous efforts are wrong by any means. I just had the feeling that the authors are piecing various building blocks together w/o providing much intuition. Maybe there is some novelty lying within the current design. For instance, the authors may have developed a novel routing/module drawing inspiration from a certain observation; the combination of xxx and yyy is based on deliberate choice. It is, however, not clear to me at this point, at least the writing does not reflect it. <sep> Furthermore, if the network is the key player in this paper, the authors shall provide more evidence. While the authors do show conduct some ablation studies on the losses and data, there aren't any analysis regarding the importance of each module (e.g. how critical is the attention design?). It is thus difficult for readers to understand which part of the network is crucial for the success, and what is the major novelty within the architecture. The current form provides very litte intuition and take away. <sep> Objectives <sep> Eq. 4 and Eq. 6 looks very similar to me. Aren't they equivalent if the A in Eq. 6 is the same as A that minimizes Eq. 2, since the assignment in Eq. 4 comes from Eq. 2? On the other side, if the two A are different, what's the intuition of exploiting different A for different loss? <sep> Writing/Presentation <sep> The flow of the model architecture section can be improved. The authors did not provide any high-level context. Instead, they simply dig into the "" details"" of each module. I'm not aware of the connections among while reading the text. Instead, I need to constantly check the figure and infer these. <sep> I also don't know what is the input/output of the model and what representations they are using. Shoudn't these be explained at the very beginning? These are not explicitly defined and I need to infer them myself. For instance, is the output of masking network a M x T soft mask with values lying within [0, 1]? do they exploit waveform (fig. 2) or spectrogram (fig. 1) for audio? I figured/inferred a lot these out after I moved to the experiment section. But from two cents, these are related to the model. <sep> Experiments <sep> Currently the authors only evaluate the model on their own dataset. How does the model work on existing datasets? For instance, AudioSet, MUSIC, FAIR-Play, etc. It seems that there is nothing preventing them from applying their model to those datasets. Without these results, it would be hard to justify if the proposed ""open-domain model"" can generalize to ""a specific domain."" I think at least  direct inference (generalization) or train from scratch is required. <sep> Furthremore, the authors did not compare with any baseline. It seems to me that quite a few prior art [Owens and Efros (2018), Hang et al (2018), etc] can serve as baselines with minor modification. Take Owens and Efros (2018) for example. While they may not be able to decompose each sound source within the on-screen mixture, one can still leverage it to evaluate the on/off-screen separation. The authors thus shall be able to report SI-SNR too. Otherwise its very difficult for people to do an apple-to-apple comparison of this work and prior work. <sep> The authors should report more performance at more percentiles. The most illustrative way is to show the cumulative plot - how many % of data have error less than x. <sep> Is there an intuition of why only pre-training part of the model? Why not pre-train the separation network too? <sep> Minor comments <sep> How do the authors define the diversity (Sec. 5.1) of the videos? Do they make use of the tags provided by YFCC100M? Also, whats the statistics of those filtered data? Would be great to provide more details so that we know its indeed covering a wide range of semantic categories. <sep> Some relevant literatures are missing. For instance, [1] also associates the visual information with speeching signal. The subjectives (eg person, dog, birds) in the paper can be viewed as on-screen audio, while prepositions can be seen as off-screen voice. There are definitely a lot more on this direction, but this paper pop out my head right away. <sep> [1] Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input. ECCV 2018.","This paper presents a new, large-scale, open-domain dataset for on-screen audio-visual separation, and provides an initial solution to this task. As the setting is quite specialized, the authors proposed a neural architecture based on spatial-temporal attentions (while using existing learning objective for audio separation). The reviewers were initially concerned that, while reasonably motivated, the architecture seemed some arbitrary. The authors then provided extensive ablation studies to evaluate the significance of each component with existing datasets, and these efforts are appreciated by reviewers. The authors may consider re-organizing the paper and moving some ablation studies to the main text. On the other hand, the reviewers believe that the dataset will be very useful for the community due to its diversity in content and label quality."
"Summary <sep> AdaSpeech is a paper on practical TTS custom voice adaptation with the aim of reducing the amount of adapted parameters per voice to allow cloud serving of a large number of custom voices while maintaining high adaptation quality and similarity. The novel piece that enables this is the conditioning of layernorm in the model on the speaker embedding. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. Descriptions of the model, experiments, and analysis of results are well done. <sep> Recommendation <sep> Weak Reject <sep> I believe this paper is not novel enough / too applied / focused on experimental results for this conference. There is little discussion on the theoretical side of the acoustic condition modelling, such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition. Instead, the strengths of this paper are entirely through the strong numerical results. I think this paper would be a solid accept for a more specialized conference like ICASSP or Interspeech. <sep> Positives <sep> Well written, great analysis of results and ablation studies. <sep> Negatives <sep> What is the loss used to train the phoneme level acoustic predictor? MSE? <sep> How is it determined that acoustic conditions such as loudness or room conditions are actually captured by the utterance- and phoneme-level acoustic condition modelling? My intuition would be that your phoneme-level predictor is trained only with phoneme hiddens (textual information only) (do these phoneme hiddens include speaker embedding information?), so at most it models some pitch or prosody information. The utterance-level can definitely model the rest, but where is the evidence? It could end up modelling only one very specific dimension and still improve the MOS. <sep> Similarly, I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information. What happens when using a speaker embedding with the utterance-level vector extracted from a reference speech for a different speaker? <sep> The paper would be better with a discussion on controllability. As a reference utterance needs to be provided, does this mean the synthesized speech will take on the prosody in the reference? What happens if you want to synthesize a prosody for a speaker that's not present in any of that speaker's reference utterances? I understand this is a big topic with ongoing research which is why it would be a big bonus if this paper can make any kind of progress in that area. <sep> I am curious how your phoneme-level predictor would compare with a VAE-based setup, although I understand this can be difficult to set up so no action is required here. <sep> Misc <sep> 2.3 Pipeline of AdaSpeech: ""we do not use the two matrices in each conditional layer normalization"" -- which two matrices? <sep> 4.2 Method Analysis: What does it mean to remove conditional layer normalization? Then you don't have any adaptation parameters, so is it not equivalent to Baseline (spk emb) case?","The paper is about adapting a voice generation model to new speakers with minimal amount of training data. The key insight in this paper is that the voice can be adapted using a small set of variables -- the bias and the variance associated with the layer that normalizes the mel-spectrogram associated with the decoder. Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation. <sep> The strength of the paper are: <sep> Simplicity of the approach <sep> Empirical evaluation that demonstrates its effectiveness <sep> The weakness of the paper are: <sep> analysis of what the crucial parameters of the model represent <sep> lack of clarity that is obvious from several back-and-forths between the reviewers and the author. <sep> A few examples include: <sep> ""There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me."" <sep> "" it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. (Both the normalization parameters and the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK."""
"UPDATE: <sep> The authors have consistently improved their argumentation over the course of the review and addressed my concerns sufficiently. I have increased my score and recommend acceptance. <sep> ORIGINAL REVIEW: <sep> The authors propose three elements: <sep> A way to approach model heterogeneity across clients with different resource constraints, a 'masking trick' for approaching non-iid-ness and a modification to BatchNormalization in the Federated Learning setting. <sep> In order to receive differently sized models that still allow to be aggregated systematically on a central server, the authors propose to deterministically prune away neurons/feature maps of NNs to effectively create networks of different widths. While a more powerful end-device receives, computes with and updates all feature maps, a less powerful device computes only with a fraction of those feature maps. This idea seems novel and interesting to me and I commend the extensive experimental study. <sep> For the proposed 'static BN', the authors propose to not worry about running estimates across all clients until convergence of the model. I disagree with both, the proposition that 'static BN' is something new compared to normal BatchNormalization, nor that it inherently solves the issue with BN in the federated setup. Firstly, BN in section 3.1 of the original paper suggests using moving averages only as a means to estimate test-time performance during training. At convergence, the final model requires re-computing statistics on the whole dataset, identical to what is proposed in this work. Secondly, apart from the problem of finding a global statistics estimate at the end of training, the usage of mini-batch statistics during training is used as an alternative to the global estimates since the global estimates are too expensive to compute during training. In centralised training, a random mini-batch represents the global data-set well enough. In non-iid data settings, a random mini-batch from a client does not serve as a good estimate of the global data statistics, only for the statistics of its local data-set. <sep> I therefore do not understand how the proposed solution of 'sBN does not track running estimates and simply normalize [sic] batch data' addresses the issue that a client-level mini-batch does not represent global statistics. <sep> A common approach to dealing with BN is to simply replace BN with, for example Group Normalization (https://arxiv.org/pdf/1910.00189.pdf) (Also see Section 5 for a discussion of the issues of BN in FL). <sep> I am therefore asking the authors to explain the exact differences of sBN to normal BN with respect to re-estimation of global statistics at the end of training and to elaborate on the issue of using mini-batch statistics as an estimate of global data-set statistics. If indeed there is a difference to normal BN, I would like to see explicit experimental results that compare normal BN to sBN and, ideally, to GroupNormalization as a way to circumvent the BN issue. <sep> Maybe I am misunderstanding in the sense that the 'Masking Trick' also somehow alleviates the non-iid data issues with BN. If that is the case, I would like to see an explicit ablation study that distinguishes between the two. <sep> With respect to non-iid data and the proposed 'masking trick' the authors cite Zhao et al. stating that the weight divergence mostly occurs in the last classification layer of networks. Inspecting Figure 2 of that paper, this conclusion can be drawn only for one of the three experiments at display. I agree that this is a minor point and the proposed trick sounds reasonable and interesting to me. In oder to see its effectiveness, however, there needs to be an ablation study with and without that trick. I cannot find such an experiment in the paper. Furthermore it should be stated that label skew is just one of the possible sources of non-iid-ness in FL. Lastly, the authors mention that the masking trick 'allows local clients to switch to another subtask simply by changing its mask...'. I have troubles understanding what is implied here. From a client's perspective there is only its local label-distribution. If a client is assumed to be new to the federation of clients, the new client would receive the un-masked global model presumably. In which setting would a client require a new mask (from another client?) <sep> Experimental Evaluation: <sep> The authors present a large range of experiments for different scenarios and levels of heterogeneity between clients. <sep> I do have issues understanding the results precisely though. The authors do not mention what the x-axes in Figure 2 represent. Is the y-axis local or global accuracy? In combination with Tables 1 and 2, I am confused. If Standalone and FedAvg have 633K parameters respectively, how does a 100% model a (first row in Table 1) have 1.6M parameters? Presumably, experiments were conducted with the same full, 100% CNN model architecture. Alternatively, the hyper parameters in Table 4 in the Appendix do not make sense to me. An alternative interpretation would be that these are the amount of parameters communicated until convergence - but then again the hyperparamters are inconclusive and additionally, the space requirements of 100% model a should still be the same as FedAvg. Or does this column describe the amount of communication at 32bit float precision? <sep> If the authors chose a different architecture for their baselines, then the results are inconclusive. <sep> I am assuming that 'Standalone' refers to no communication between clients, but that needs to be specified! <sep> Also I am assuming that 'Local' assigns zero probability on p(y=c|x) for those classes c that are not present on a client during training. Again, this is not explicitly specified. Are the reported results averaged across clients? Are they weighted by the amount of data in the local test-sets?  In the conclusion, the authors state that their method achieves better results with fewer number of communication rounds. I can no-where see a comparison of communication rounds. <sep> The authors mention two scenarios: Fix and Dynamic and explicitly say 'We annotate Fix for experiments with a fixed assignment of computation complexity levels and Dynamic for local clients uniformly sampling ..."". I cannot find this annotation anywhere and am therefore confused which setting the results correspond to. <sep> I am missing one axis of evaluation: In a heterogeneous (Fixed) setting with, for example, 50% a and 50% level e, how is the average local performance on devices with model a and model e separately. In the text, the authors describe 100% model e achieves 77.09% accuracy and a 50-50 mix with model a achieves 89%. But that does tell me nothing about how much the (weak) clients with model e improved through the increased power in these other 50% devices. In the next sentence the authors claim that 'HeteroFL can boost client's performance with low computation and communication capabilities'. But reporting the average could also allow for the conclusion that only clients with higher compute power and a larger model achieve higher performance. <sep> Conclusion <sep> The paper proposed three elements, a heterogeneous modelling approach, sBN and the masking trick. Apart from confusion in motivation and explanation, the experimental section requires most attention in my opinion. Things are simply very unclear to me.  Terms, axes and results need to be properly discussed. Furthermore, the effects of HeteroFL, sBN and the masking-trick need to be independently studied, otherwise no conclusion can be drawn on the effectiveness of the individual ideas. I would recommend the authors to re-focus their paper on the heterogeneous training idea alone and leave sBN, which I don't understand at the moment, and the non-iid remedy trough masking to another paper. <sep> I believe that the idea of dynamically adjusting the model width to the local compute capabilities in the way the authors present it is promising. However I need to be convinced that less powerful clients can meaningfully contribute to the global model and have higher performance compared to training a small-sized global model in the first place. I encourage the authors to revisit their motivation for elements of this work (HeteroFL, sBN and masking), refocus, and fix their experimental discussion. I believe that there is enough merit to this idea and paper to be accepted with major effort during the rebuttal.",The reviewers had a number of concerns which seem to have been addressed by the authors in the discussion phase. All the reviewers are in favor of accepting the paper. The paper provides an interesting/novel idea for federated learning with heterogenous clients/devices.
"== SUMMARY == <sep> This is a well-written paper that discusses how to learn disentangled representations for the learning from demonstrations (LfD) task in robotics. It is shown that using weak-supervision on top of unsupervised learning frameworks (that use the variational autoencoder for instance) can work well in this case. These disentangled factors of variation in the data are shown to correspond well to the 'abstract concepts' of the human demonstrations. This is shown in the example of the PR2 robot dabbing demonstrations, including visual data as well robot trajectories. <sep> == QUALITY & CLARITY == <sep> The paper is written very well, in fact most papers contain a lot of spelling mistakes but this paper was a joy to read in this regard :) The concepts are also explained clearly. However I would have expected better and more detailed coverage of related & past work. <sep> == ORIGINALITY & SIGNIFICANCE == <sep> Unfortunately, I think the paper suffers from lack of originality, at least with respect to ML. From a robotics point of view, I would have accepted it as a very good application paper, if the authors had also presented real-robot results that show the learned model in action (generating actual trajectories for the robot). The authors however show only the prediction performance for (held-out) test demonstrations. <sep> == VERDICT == <sep> I would like to thank the authors for a very well written paper. Overall, I think the paper needs some improvements such that it can be accepted in a revised version or most likely, in another conference. The paper needs to present real robot results and cover related work in more detail. Comparing the method to related work (DMP, or ProMP-variants, or any other competitive method) in the real-robot experiments would also be crucial. <sep> === NOTES & SOME MINOR COMMENTS === <sep> No need to mention the link in the abstract <sep> ""For example, the concept of pressing softly against a surface manifests itselfin a data stream associated with the 7 DoF real-valued space of joint efforts, spread across multipletime steps."" -> For the PR2 robot? Either remove 7 DoF or add for which robot. <sep> ""However, the essence of what differentiates one type of soft press from another nearby concept can be summarised conceptually using a lower dimensional abstract space"" -> Nearby concept sounds vague, please be more explicit or give examples. <sep> In Figure 2, the image encoding 'i' does not affect y. Why? <sep> Related Work: ProMPs are not represented as dynamical systems <sep> ""To fully close the loop, the trajectories which we sample from the model could further be executed on the physical robot through a hybrid position/force controller (Reibert,1981). However, such evaluation is beyond the scope of the paper."" -> I don't think so. The real proof of concept is the actual robot experiments!  Without real robot experiments to show how the generated conditioned trajectories actually perform, in my opinion the paper is an application paper without any significant ML contributions. The paper as of now I fear only confirms the fact already acknowledged in previous ML papers, that disentanglement can be achieved through <sep> (semi)supervised learning [see Locatello et al. and papers citing this work) <sep> equivallent -> equivalent force-relate -> force-related","The paper considers the problem of learning interpretable, low-dimensional representations from high-dimensional multimodal input via weak supervision in a learning from demonstration (LfD) context. To mitigate the disparity between the abstractions that humans reason over and the robot's low-level action and observation spaces, the paper argues for learning a low-dimensional embedding that captures the underlying concepts. The primary contribution of the paper is the ability to learn disentangled low-dimensional representations that are interpretable from weak supervision using conditional latent variable models. <sep> The paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper. The paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over high-dimensional multimodal observations. As the reviewers note, the use of variational inference to learn low-dimensional interpretable representations from weak supervision is compelling. The primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow. The authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks. However, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear. The paper would be strengthened by the inclusion of experiments in a less contrived setting (and one for which the concepts are not necessarily disjoint) as well as a clearer discussion of the primary contributions."
"This paper proposes a new gradient attack method named R-GAP. R-GAP decomposites the DNN gradient attack problem into subproblems for each layer, and recursively solves each of them. The subproblem of each layer is formulated as a least-square reconstruction problem. <sep> The authors further point out the rank of network weight matrix is (non-surprisingly) correlated with the difficulty of input recovery. Based on this finding, they design a metric based on the weight matrix rank to estimate the feasibility of fully recovering data. <sep> Experimental results on MNIST and CIFAR10 show that the proposed method R-GAP is comparable or superior to the classic DLG method. The authors also claimed the proposed method to be much faster than DLG baseline. <sep> Strengths: <sep> Gradient attacks raise data privacy concerns in many applications such as federated learning, making it an important problem. <sep> As an analytical method to solve the input reversion problem, R-GAP should have its intrinsic advantage over previous optimization-based gradient attack (O-GAP) methods such as DLG. For example, in my assumption (and also claimed by the authors), R-GAP can be much faster than O-GAP. <sep> R-GAP has much better performance than DLG on full-rank CNN6 networks, as shown both visually and numerically in Figure 3 and Table 1. This shows that if the attacked model satisfies the full-rank condition, R-GAP can be both faster and more effective than DLG. <sep> Weakness: <sep> Insufficient experiments. <sep> My largest concern is over the lack of necessary experiments to show the advantage of R-GAP. <sep> i. Why only showing results on a self-designed CNN6 network? In order to fairly compare with DLG, and also to show the general effectiveness of the proposed method, the authors should also consider comparing with DLG on some standard network, such as the LeNet benchmarked in many previous gradient attack works [1,2]. <sep> This is very important also because we need to see whether the popular deep models such as LeNet, VGG, ResNet, etc. satisfy the full rank condition required by the proposed method. If not, the proposed R-GAP will have limited application scenarios. <sep> ii. Why not compare with more recent gradient attack methods such as iDLG [2], which has been shown to also outperform the original DLG [1]? <sep> iii. The authors claimed the proposed method is much faster than DLG. Although I agree this is intuitively true, I think it necessary to report the numbers in the paper. For example, how much time/FLOPs does it take to attack a single image for each method? <sep> Additional tricks used without detailed description in the Method section. <sep> In Table 1, the authors show that R-GAP is largely outperformed by DLG on the rank-deficient network CNN6-d. However, according to the authors' vague descriptions, simply adding a smoothing operation can largely improve R-GAP performance. (I assume H-GAP = R-GAP + image smoothing?) Is the image smoothing the main technique making the proposed method effective? <sep> This is really confusing since neither H-GAP nor image smoothing is mentioned in the method/related work sections. I suggest the authors to provide more descriptions about the H-GAP method and also provide explanations why it largely outperforms R-GAP. <sep> I'm willing to increase my score if these concerns are properly tackled during the rebuttal period. <sep> Other comments: <sep> The artifacts of DLG reconstruction images are mainly located on the corner of the images (see Figure 2), while the artifacts of DLG are evenly distributed on the whole images (see Figure 3). Is this a general trend? If yes, any explanations or intuitions behind this? <sep> The proposed RA-i is only using matrix rank to predict the hardness of input recovery. In my view, it might be better to consider using matrix condition number as the metric. This is because the sub-problem at each layer is basically a least-square regression problem, and two least-square regression problems can have different difficulties when the matrix have identical ranks but different condition numbers. In other words, condition number contains more information than rank, and thus might be more useful. (Please point out if I'm wrong here.) <sep> Reference <sep> [1] Deep leakage from gradients. <sep> [2] iDLG: Improved Deep Leakage from Gradients. <sep> Update: The authors have addressed my concerns and now I vote for acceptance.",The major criticism of this paper after the initial reviews was a lack of experimental results on deeper and more modern architectures that include skip connections. The authors added results to the paper to address these issues.
"Summary： <sep> In this paper, the authors intend to propose an efficient automated reinforcement learning (RL) framework. To achieve this goal, they integrate three technologies, i.e., evolutionary RL for hyperparameter search, evolvable neural network for policy network design, and shared experience replay for improving data usage. The paper uses a case study on MuJoCo to demonstrate the claimed advantages over baselines. <sep> Some pros: <sep> The motivation is good. The automation of reinforcement learning is beneficial to the research community, especially for researchers who are not familiar with RL but in need of it. <sep> The framework is general and friendly to users. This framework is general and can be regarded as a plug-in module for a series of reinforcement learning methods. Besides, as we all know, both of the autoML and RL have a heavy computational burden, the adaption of evolvable neural network and shared experience replay greatly alleviate this dilemma. <sep> The organization of this paper is easy to follow. We can follow the authors from why they want to deal with the problem, to how they are inspired by existing work, and then to how the algorithms are design based on the questions to be answered and the existing technologies. <sep> Some cons: <sep> The experiment is far from enough. Actually, this paper only has a case study. First, the author claims that the framework can optimize arbitrary off-policy RL algorithms, why only try on TD3? From the perspective of robustness, the authors need to compare more off-policy algorithms with and without the proposed method. Second, the paper claims there is no directly comparable approach for efficient AutoRL, which I do not agree with. In its own related work, many AutoRL baselines are listed, e.g., H. L. Chiang et al.  'Learning navigation behaviors end-to-end with autorl', F. Runge et al. 'Learning to design RNA'. For these baselines, they can either take the same exploration steps and compare performance with the proposed method, or compare the performance/computational cost when reaching the same performance. Anyway, the readers expect to see more comparisons with more baselines from more perspectives. Third, the authors claim that to tackle the non-stationarity of the RL problem, existing studies can substantially increase the number of environment interactions, implying the proposed framework has advantages on non-stationarity RL environments, but still, no experimental results are given. <sep> Some technical details are missing. The logic is clear, the solution is reasonable, but the details are ignored. It is a good idea to keep the writing compact, but the lack of details may harm the readability of the paper. For example, since this is a general framework, how should we design hyperparameter settings of SEARL in initialization for different algorithms? In the training part, why should individual be trained for as many steps as frames have been generated in the evaluation phase, and why the training time could be reduced by using only a fraction j of the steps?","This paper tackles a very important topic in deep RL, namely automatic (non-differentiable) hyper-parameter tuning. It does so by combining ideas from genetic algorithms and neural architecture search with shared experience replay in order to obtain the key property of sample efficiency. The proposed solution is communicated clearly, and the results are compelling (often 10x improvements), as well as qualitatively interesting. <sep> Unfortunately for the authors, their original submission contained only part of the intended results, hence the borderline scores by some reviewers. In the meanwhile, a second suite of experiments have been added, which I think are compelling enough evidence to validate the paper's approach."
"This paper studies the asymptotic convergence properties of (population-level) policy gradient methods with two-layer neural networks, softmax parametrization, and entropic regularization, in the mean-field regime. By modelling the hidden layer as a probability distribution over the parameter space, the training dynamics of policy gradient methods can be written as a partial differential equation. Under certain regularity conditions, the paper shows that if the training dynamics converge to a stationary point, this limiting point is a globally optimal policy. The paper also presents results for finite-time convergence of the training dynamics for neural networks to the mean-field limit. <sep> The optimization landscape and convergence properties of policy gradient methods have drawn attention in RL theory for a long time, and it is nice to see a work that studies this problem from the perspectives of mean-field limit of neural networks, albeit being completely asymptotic. Overall I think this makes an interesting contribution, and I appreciate the sketch of proof ideas in the simpler bandit case. Technically, it seems that the main results are built upon existing frameworks of Mei et al., (2018), Chizat and Bach et al., (2018), etc. But the author also pointed out an interesting technical novelty, which is the use of density arguments when the problem structure is in lack of the hidden convexity used in other works. <sep> On the other hand, it appears to me that one major weakness of the result is that the theorem holds true only when the dynamics converges to a stationary point. Can the authors provide conditions under which this can happen? For example, would it be possible to establish some compactness under additional regularity conditions and use it to show the convergence of a subsequence? If the convergence does fail to happen in certain regimes, how will the dynamics behave? Will it convergence to a limiting cycle or diverge? Are there some natural counter-examples? It would be helpful if the authors could provide more discussions on this condition. <sep> Additionally, it seems to me that the paper actually shows that (due to entropic regularization) the limiting point is the Boltzman policy induced by the optimal Q function (at temparature τ), instead of the optimal Q function iteslf. If that is the case, this needs to be stated clearly in the theorem.","This paper takes a step towards understanding the role of nonlinear function approximation--- more specifically, function approximation via (two-layer) neural nets---in some variants of the policy-gradient algorithms. The authors borrow the mean field analysis idea recently popularized in studying shallow neural nets, and investigate the mean-field limits of the training dynamics in the current RL settings. The results and analyses are interesting as they nicely complement another line of linearization-based analyses (i.e., the one based on neural tangent kernels) towards understanding non-linear function approximation. As suggested by a reviewer, it would be nice to add discussions in the revised paper regarding when the dynamics can be guaranteed to converge to a stationary point."
"########################################################################## <sep> Summary: <sep> This paper proposes a simple yet general approach for exploration in discrete-action problems. The proposed approach, called ez-greedy, combines randomly selected options with the well-adopted e-greedy exploration policy to achieve temporally-extended e-greedy exploration. The paper overviews the publicized exploration methods from the perspective of their inductive biases, and clearly states where the inductive bias of ez-greedy would be better suited over e-greedy. The paper reports results in tabular, linear, and deep RL settings, on numerous domains ranging from classic toy problems to Atari-57. The results are interesting, and the analysis aligns and supports nicely the narrative of the paper. <sep> ########################################################################## <sep> Reasons for Score: <sep> Overall, I vote for accepting this paper. The idea is simple (a generalization of e-greedy) and the discussions nicely illustrate the main properties of an ideal generally-applicable exploration method. The experiments clearly show where ez-greedy exploration would be useful. Also, they show that the inductive bias of ez-greedy does not hurt much the performance in simpler dense-reward domains while more specialized algorithms suffer significantly. <sep> ########################################################################## <sep> Pros: <sep> See ""Reasons for Score"" above. <sep> ########################################################################## <sep> Cons: <sep> The results in Atari are based on a deterministic version of Atari (i.e. not using ""sticky actions""). Also, in DeepSea the deterministic version of the task is used. Ideally, I would've liked to see empirical results in stochastic domains as well. More importantly, I'm not sure why only deterministic domains are used? <sep> The literature on action-repeats are discussed briefly. But it's hard to know how the former related works were different in their formulation and use of action-repeats. Also, could you clarify how sticky-actions are positioned w.r.t. ez-greedy (beyond that the purpose behind sticky-actions was to induce stochasticity in the environment as opposed to being used explicitly for exploration)? For instance, do sticky-actions actually improve learning performance in the same domains were ez-greedy improves performance? <sep> The rainbow + e-greedy vs. Rainbow + ez-greedy Median and Mean plots do not show significant findings. I think a bar-plot should be added to show per-game relative human-normalized improvements for these versions. The same should be done for R2D2 (e-greedy) vs. R2D2 + ez-greedy as well. I think what this could reveal is symmetric bars over the 57-Atari games (i.e. number of games in which ez-greedy outperforms and underperforms e-greedy are the same). Also, the extent of improvements on average is the same as shown in the Mean plot of Figure 8. <sep> To clarify, I don't see an issue with this outcome (i.e. if the bars are symmetric; meaning overall there are as many games in Atari-57 that would benefit from ez-greedy over e-greedy as there are games in which the opposite is the case). This does not go against the narrative of the paper which makes it clear that they each have an inductive bias that suits some tasks over others. But I think this should be made super clear in the results section, through such bar plots. For the same reason, I think the Mean plots should also be brought to the main text and shown next to the Median curves. <sep> Why only 5 random seeds in DeepSea? I suggest showing results for 30 randoms seeds like in the other toy problems. <sep> ########################################################################## <sep> Questions during the rebuttal period: <sep> Please address and clarify the ""Cons"" above. <sep> ########################################################################## <sep> Minor comments: <sep> It would be useful to replace ""Rainbow"" with ""Rainbow (NoisyNet)"" in Figure 3 so as to emphasize the difference between ""Rainbow"" and ""Rainbow + e-greedy"". Similarly, for ""R2D2"" it'd make it easier for the reader if the Figures show ""R2D2 (e-greedy)"". <sep> Table 1: ""Algorithm (@200M)"": M doesn't need to be italicized (to be consistent with ""Algorithm (@30B)""). <sep> It'd make it easier if ""(100%)"" is added to the y-axis of Median/Mean plots.",This paper proposes a simple generalization to epsilon-greedy exploration that induces temporally extended probes and can leverage options. The idea and analysis are trivial. Computational results demonstrate when this sort of exploration is helpful. The paper is well written and the authors offer a fair assessment of when these ideas do or do not address challenging exploration tasks. A range of computational results support and offer insight into the concepts.
"########################################################################## <sep> Summary: <sep> This paper presents a novel approach/perspective for improving data efficiency and robustness, other than existing research progress achieved by scientists, from model, optimizer and data perspective. <sep> In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters --- Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation.  To valid its findings, the authors develop designed a set of comprehensive experiments for evaluation and comparison purposes, while making the best effort to not introducing variations from other angles, such as keeping the same data for training and testing and introducing adversary information consistently among all labeling representations. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, I vote for accepting. <sep> I like the idea of approaching image classification problem from a new angle that are not well explored yet. <sep> My major concerns are: <sep> The clarity of the study on the underline true set of characteristics that contribute to the improvement, from speech label, shuffled-speech label, Gaussian-composition label, besides high dimension and high entropy. <sep> The logic behind the adversary image generation, target vs non-target. <sep> And the evidence/thinking process behind the pre-selected threshold of 3.5 <sep> Hopefully the authors can address my concern in the rebuttal period. <sep> ########################################################################## <sep> Pros: <sep> The paper provides a novel perspective for classification performance improvement, rather than from data, algorithms, or optimizers perspective. It shows the importance of label for supervised learning problems, can also come from the ways that we represent them, just only label quality.  For me, this approach is new and potentially expend to other applications, besides image classification <sep> The experiment design is also quite comprehensive as it covers all potential perspectives and variations. <sep> This inspiration of the idea to me is also quite natural and understandable, as for most of us, when recognizing an image, we express it not just in writing and can also in speech format. <sep> ########################################################################## <sep> Cons: <sep> Although the proposed representations have shown better performance in image classification problem, with evidence to support its out-performed robustness to adversarial attack and data efficiency -- achieving comparable accuracy with less data in training, I would still suggest the authors to conduct the following studies to enhance the quality of the paper: <sep> It could be valuable to future investigate the inherent property that contributes to the improvement, besides high dimensionality and high entropy. <sep> What's the performance with high dimensional and high entropy label representations, comparing to text label, for other kind of the classification problems, such as NLP problems. <sep> For speech label, in model evaluation, what's the performance for the model, if we choose a speech to text process to obtain its ground truth, beside the two approaches mentioned in the paper -- ""nearest neighbor"" and a validated loss threshold. <sep> ########################################################################## <sep> Questions during rebuttal period: <sep> Please address and clarify the cons above","This paper proposes to use high dimensional representation for labels to strengthen the adversarial robustness of deep neural networks. Experimental results demonstrate that the proposed method improve adversarial robustness. All reviewer agree that the authors propose an interesting idea and this direction deserves further exploration. On the other hand, the reviewers also raise a serious question: There is a lack of explanation of why high dimensional representation of labels improve adversarial robustness. Therefore, it is not clear if the proposed method can defend refined attacks tailored to such dimensional label representation. The authors are highly encouraged to conduct deeper analysis, especially on the robustness against finer attacks."
"Summary <sep> This paper considers the problem of adapting a pre-trained model for few-shot learning in case there is a shift of distribution from the meta-training set. If the new tasks significantly differ from the meta-training distribution the model might need to be retrained from scratch but this is not always possible, so the authors propose to ""repurpose"" the model under the assumption that the support set can be used to re-calibrate the pre-trained model to the new shifted distribution. <sep> Following two intuitions/hypotheses: 1) if the model uncertainty of specific parameters is high, then the step size should be small and 2) high uncertainty on input gradients require more adversarial training to improve robustness. <sep> In practice, the uncertainty of the parameters is computed by using deep ensembles with perturbed MAML checkpoints rather than random initialization. High variance components are moved with lower step sizes. Moreover, If slightly perturbed models from the deep ensemble disagree on parts of the input gradient, then it means that they might be more prone to adversarial attacks, hence they need to be robustified with stronger adversarial training. <sep> Questions <sep> Q0a: maybe I'm missing the point, but am I wrong or the procedure that you propose can be also applied to any other supervised learning model, not necessarily few-shot? Let's consider you are not only observing new classes, but also new domains. Does the method apply to that situation? <sep> Q0b: also the use of deep ensemble and the proposed UFGSM maybe can be useful in the supervised learning scenario to improve robustness. I would suggest to try some experiments in that direction. <sep> Q1: from plots in figure 1 it seems that batchnorm parameters are the most uncertain. This is expected since BN capture the statistics of the activations from the meta-distribution and it is the most sensitive to the domain shift. Are you using batch-norm in a transductive setting, i.e. you don't use the running stats, but you always use the batch statistics also at test time? <sep> If no, and you use running stats, then a simple baseline would be to use test time statistics, so I would add this comparison to your experiments. <sep> If yes, I would also try not to update the BN scale and shift parameters in the inner loop since they seem very sensitive. Another alternative would be to have per-step running stats and per-step bn weights as proposed in Antoniou 2019. <sep> Q2: the improvement wrt to the SGD baseline can be more appreciated with 5 shots, rather than only 1 sample. I would have expected the opposite because of the limited number of samples. How do you interpret this? <sep> Q3: what happens if you try PGD instead of simply using FGSM? <sep> Considerations <sep> I think the paper is well written and motivated. The idea of repurposing a FSL model without retraining from scratch is timely and interesting. The experimental campaign is carefully performed as well as the ablation. I recommend acceptance, but I want to first to hear the author's response to my doubts. <sep> Bib <sep> Antoniou et al 2019 How to train your maml","This paper considers a new and practical setting of meta-learning for out-of-domain task adaptation where a pretrained model exists but the original meta-training data is not available. The authors incorporate several ideas including deep ensembles, adversarial training and uncertainty-based step sizes, and achieve competitive performance under this particular setting. <sep> The combination of various methods appears complicated, but the authors provide detailed ablation study to show the effectiveness of each component empirically. During rebuttal and discussion, they addressed many of the concerns from the reviewers. As pointed out by a reviewer, their proposed method would have a value in the domain adaptation area beyond meta-learning. <sep> The remaining concern is on the somewhat ad-hoc combination of multiple methods and lack of a clear single solution for addressing the OOD few-shot learning problem. Nonetheless, the proposed methods show a convincing empirical improvement on the vanilla MAML baseline in the experiments."
"Verdict: <sep> Recommendation to REJECT; Please consider for *CONF* Special Journal Issue with modifications. <sep> (Unique situation, please read fully) <sep> ########################################################################## <sep> Summary: <sep> The paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by neural networks. The authors show how complex tasks can be modularly formulated thus yielding a joint monolithic learning possibility. They also show that such modularity can be used to interpret simple algorithms thereby also leading to their joint learning. <sep> The main contributions of the authors are the following: <sep> showing that ""the two layer neural network can jointly learn the task coding scheme and the task specific functions without special engineering of the architecture"" <sep> ""systematic theoretical investication of the extent of this ability"" (ability = single network can successfully be trained to perform a wide variety of tasks) <sep> ""...primarily interested in the extent to which different tasks may interfere,..."" (in a multitask setting) <sep> ########################################################################## <sep> Reasons for recommendation / score: <sep> The paper is composed of a vast amount of very good research work. The research results seem significantly novel and definitely not incremental or based on other similar contemporary works. The rigorous mathematics and the attention to many details is laudable. <sep> However, the content in the paper literally and logically coerces the reader to constantly look at the supplementary material. I could also dare to say that the paper, strictly, without the supplementary material almost seems incomplete or as a compilation of claims, making for a choppy read. This is clearly an effect of the authors trying very hard to squeeze a lot of content into the 8-page limit. This must have taken a lot of efforts and I definitely can see the work that has gone into writing this concisely. <sep> The paper would read much better with all the appendices and supplementary material introduced in the appropriate places, in proper continuum. I could see this paper be re-written almost as a proper tutorial paper in this topic of research. I would in this context, recommend some extra experiments and discussions (commented below also), to make the work more thorough. <sep> For these reasons, I must insist that the venue for submitting this work should be a suitable journal such as JMLR, ML, or specifically be rewritten for the Special Journal Issue @ *CONF* 2021. It is not suitable to be accepted as a conference contribution, by the sheer magnitude of work and the style it is presented in. <sep> Another possibility I see, but do not recommend, is that the main motivation of the paper be modified to a solely task solving angle rather than the science of comparison and analysis. Then the theoretical rigour could be reduced and the focus can land on experimental results. This, in my opinion, could be suitable and a definite ACCEPT for *CONF* conference. <sep> ########################################################################## <sep> Note about the reviewer: <sep> My area of research is Bayesian non-parametrics applied on to multitask learning. I am not very familiar with the mathematical support provided for the theorems in this work. I cannot promise a critical verification of the correctness of the proofs. <sep> Please also keep this in mind when considering my recommendation above. <sep> ########################################################################## <sep> Pros: <sep> The research quality and quantity are exceeding requirements for acceptance! <sep> The language is clear and crisp when introducing the research area and placing it in context with its related works. I especially like the delineated ""Our Results"" section. The authors clarify and discuss the topic with respect to two landmark papers very well. <sep> The flow of thought is clear and makes the reader comfortable with the presented paper structure. <sep> All the assumptions are made evident and clarified beyond any doubts, there are no hidden assumptions or simplifications. The scope of the focused research is also well clarified. <sep> There is thorough mathematical justifications, case studies of monolithic formulations, guarantees on bounds and learnabilities in the supplementary material. (I have tried to not give it attention as it is not a necessary part of the submission) <sep> ########################################################################## <sep> Cons: <sep> The title, abstract and some parts of the conclusion suggests a tone of comparison. This makes me expect a more involved discussion about the topic ""Modular versus Monolithic Task Formulations"". The authors have a lot of insight in this matter, however, when it comes to presentation, they fall short to guide the reader through them. <sep> The authors have carefully cherry-picked the theorems and balanced the extent to which they explain them so that it reads with completeness on the whole. I would argue that the details are important and without the proofs and mathematical involvement the scientific reader is forced to question ""why?"" or ""how?"" quite often. <sep> I would have liked the authors to place themselves better in the research context. I would have liked to know the findings of the authors in the lists of references (in Sections 1.1 and 1.2) with respect to the title of this work. <sep> The writing in many middle sections where details are needed, are overtly compressed. This is an effect of trying to squeeze in too much in too little space. The authors do direct the reader to the supplementary material many times. <sep> The authors do not answer the grander questions they begin the paper with. They analyse other attributes which are aligned in the same directions as these questions. Eg. Is modular construction better than the monolithic ones? When should we use which construction? <sep> The authors talk about multitask learning in the same context as [Caruana 1997]; That is, they reduce the scope of their analysis to inputs of the same size for all tasks or even same inputs to different functions (or tasks) to be learnt. Is this the general case of ""any"" multitask learning scenario? <sep> The experiments themselves do not seem statistically thorough. There should be more than 3 trials to draw conclusions, especially when the experimental setup is based on parameters drawn from a uniform distribution. <sep> The authors need to address the few inconsistencies in the graphs they have shown in Fig.2. I definitely would have liked more authors' insight on the observed statistics. <sep> In Fig.3 are the numbers significant? They are reported in the third decimal position for the Test R-squared values. Some more explanation is needed. <sep> Is the monolithic formulation of multitask learning effectively: joint learning of the switching function and the task function? It would be nice to read some more of the authors' explanation of how and if they are doing something different. <sep> ######################################################################### <sep> Suggested Presentation Changes: <sep> There could be more figures explaining the schematics of the networks, explaining the setup etc. Especially Sec.2.3 and Sec.2.4. <sep> There could be more figures and clearer captions with simpler explanations. This is where the reader looks first. It would be nice to suggest what to expect from such a graph and then highlight any results. <sep> The details in Table 1 can be made more readable. It is unclear where the focus lies. <sep> I feel it is important, the authors highlight that the Simple Programming Cosntructs part of their research derives from their novelty in the formulation of modules in terms of mathematical functions! <sep> ##########################################################boarman############### <sep> Suggested Small Corrections: <sep> Generally, try to break down longer sentences into shorter multiple sentences. <sep> Sec. Abstract: remove or replace word ""underlying"" <sep> Sec. Abstract: ""... trees over some task-code attributes."" Change 'some' --> 'certain' <sep> Sec. Introduction: ""As techniques, such as neural networks, for learning with relatively rick classes have been developed, it is ..."" <sep> Please see if you can move the references to the end of sentences than in the middle. It makes for better readability. <sep> Sec 1.1: move NTK references to end of the sentence. <sep> Sec 2.2 theorem 2: for omega(1) far A...A subspaces, if A ... <sep> Sec 2.2 theorem 2: for omega(1) separated c...c prototypes, if ||c ... <sep> Fig.3. caption: 1-(Test R-squared). Parentheses helps understand the subtraction from unity. <sep> ######################################################################### <sep> Updates:","This paper shows how multiple tasks can be encoded in a single neural network without the need for explicit modular construction for each task. The idea is very interesting and the research work presented is of high quality. <sep> All the reviewers underline their interest in the presented work. However, there is a deviation in the reviewers' score with half voting <sep> for acceptance and the other half for rejection. The main concern of the fellow reviewers with the below acceptance threshold score was the difficulty in grapsing the theory of the research presented due to the lack of important content from the main manuscript due to space limitations. The authors have an extended supplementary material that covers the whole magnitude of their work. <sep> I understand the reviewers' concern on how such a dense presentation does not do justice and harms the presented effort itself. However, given the edits the authors added to address the issue rasied and the interest and potential of this work - acknowledged by all the reviewers and myself I recommend acceptance. This is a work of a quality I would like to keep seeing in *CONF*."
"This paper proposed a model of navigation based on grid cells and the successor representation (SR). <sep> Quality: This submission contains some interesting ideas. However, it feels that several key statements in the paper need to be clarified, and some statements need to be toned down. Overall, I feel this is a good submission, but the quality did not quite reach the bar in its current form. <sep> Clarity: The writing requires some improvement. The key idea is not very clear. What does the ""sense of direction"" mean exactly? Also, it is mentioned that the model is capable of predicting the effect of arbitrary directed transitions, irrespective of local details, e.g., obstacles. This seems to suggest the grid cell responses are not affected by the obstacles. It that's indeed the case, it would be inconsistent with experimental findings. (Am I misinterpreting this?)  Also, it is not clear exactly what are the inputs and outputs of the grid cell system. <sep> Originality: Some key ideas presented in the paper (e.g, eigendecomposition of the transition matrix, successor representation) is fairly standard in previous literature, although the authors show that there is a way to unify some of the previous models with these notions. <sep> Significance:  It is difficult to judge how much this work really adds to the previous literature. <sep> Pro: <sep> the paper shows that a simple model based on grid cells and SR representation can perform navigation in some simple environments. <sep> the paper attempts to unify various kinds of grid cell models, which is interesting. <sep> the model is mathematically quite elegant and simple. <sep> Cons: <sep> Some key ideas in the paper, e.g. eigen-decomposition of transition matrix, SR representation, are not new. <sep> The ""sense of direction"" added to the model is not clearly described. It is unclear how it is implemented in the network model, and how this information can get to the grid cells. <sep> It would be useful if the novel contributions could be more clearly stated. For example, Section  2 seems to be pretty standard materials. The paper would benefit by highlighting the true innovations (assuming there are some). <sep> Furthermore, the claim of unifying previous models is an over-statement and potentially misleading. Clearly, the proposed model is related to many of the previous models, but to go one step further and say that it unifies these previous models, that would seem to be a over-claim in my view.  For example, the proposed model does not have any recurrent connections- it is difficult to see how this unify the CAN models which crucially study  the role of recurrent computations. <sep> Concerns: <sep> It is claimed that the proposed model generalizes across  different environments. Does that predict that the grid cells are shared across the different environment? Do the local cues such as barriers and reward locations influence the firing pattern of grid cells? Are the predictions of the model consistent with neurophysiological data? <sep> A key quantity in generating ""a sense of direction"" is the quantity s_G in Eq. (9). However, I didn't find anything about how it is defined. Sorry if I am missing something. How is S_G encoded in the network? How would the grid cells have access to this information? <sep> The paper claims that this model also unifies the OI models. Notably, various experimental results have argued against the OI models, which now lacks experimental support. Thus, it is unclear how much this really adds. <sep> How does the model update the SR information when environment changes (e.g., inserting a barrier)? <sep> It is stated that  ""a computational role for the neural grid codes: generating a ""sense of direction"" (eq. 9) even in new or bounded environments, via utilising a Fourier basis for a larger toroidal pseudo space"". What specific predictions does this model make for neurophysiology? Is there a way to falsify the hypothesis/model? <sep> A few more clarification questions: <sep> How does Fig F shows ""SR values can be used for gradient-based navigation""? <sep> How is the barrier implemented using ""wind"" exactly? <sep> What does ""intuitive planning"" mean? <sep> I am puzzled how theta phase precession could arise from the model. Apologize if this is obvious…Could the authors flesh out the ideas a bit more?","This paper was controversial amongst the reviewers. There is clear utility to the *CONF* community: a new model of grid cells based on well-known technique (SR) used frequently in ML; good science---careful analysis showing the proposed model exhibits key properties and useful in synthetic navigation domains; such work reminds of the important concerns in natural learning systems which is relevant to those that wish to simulate and build intelligence. Two of the reviewers with subject matter experience in the area advocated for acceptance. <sep> On the other hand, many readers of *CONF* may find the paper confusing and unsatisfying as some of the reviewers did. The empirical work was limited to small domains and mostly in the form of demonstrations---a typically *CONF* reader would expect a performance improvement claim or a scientific hypothesis tested by each experiment. Presented as a new algorithm for ML the paper might appear too limited and simple (e.g., relying on state aggregation). The reviewers with neuro background found the paper clear and well organized, while the ML reviewers found it confusing. The relevance of the work will be limited to a smaller subset of researchers---but this is true of many ML works also. Finally, ML readers might be more familar with neuro work which propose computational models and then validate those models against real neural activity data from brains. This is work is not like that, rather using synthetic data to demonstrate important properties and explore empirical conjectures about the model. <sep> In the end the paper is boarder line: the subject matter experts both listed issues that should be addressed (e.g., band cells issue), while the reaction of the ML reviewers suggests the impact of the work might be reduced at *CONF* (compared to other venues). Additional text clearly articulating the scope and managing reader expectation could mitigate this concern, but it's not a small task to change the tone and pitch this way. Scientific conferences are about insights and understanding, this paper provides both. Please consider the suggested edits to maximize the impact of your work at *CONF* this year."
"Summary: This paper proposes a method for obtaining probably-approximately correct (PAC) predictions given a pre-trained classifier. The PAC intervals are connected to calibration, and take the form of confidence intervals given the bin a prediction falls in. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. Experiments in both of these cases show improvements in speed-accuracy or safety-accuracy tradeoffs, as compared to baselines. <sep> I'm recommending acceptance since the idea seems useful and well-argued both conceptually and experimentally. However, the paper needs some work in terms of clarification of the key ideas – with this clarification I can raise my score. <sep> Strong points: <sep> The proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem. <sep> This approach is a simple but good idea, seems grounded in a good motivation and the explored use cases are informative and interesting. <sep> Experimentally, the method shows improvements over a naïve baselines, and demonstrate that it can obey a given error or safety threshold in practice, an important property <sep> Weak points + Clarifications: <sep> I am confused about the application of this method to safe planning. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. However, this will not be the case in the safe planning setting as I understand it, since the observed trajectories are drawn from a different policy than the one which will be implemented in the world <sep> With respect to these test set questions, it would be nice to see a little discussion in the paper of how these guarantees transfer from training set to the test distribution <sep> I would like to see more explanation of the proofs in the appendix, right now they are a little too compact for me <sep> Experimental baselines raise some questions for me. First, I need more explanation on the histogram binning baseline beyond the one sentence given. Second, the authors state this baseline ""does not satisfy the desired error"" – but I'm not sure why we would expect it to, since that baseline was not tuned to any sort of error level. Finally, I would like to see naïve threshold baselines in the safe planning setting for more cautious values than 0.5 – since that is more aligned with the goals of safety. <sep> In your PAC definition, we could just always output [0, 1] to satisfy it. Therefore, when framing the goals of your method, you should be a little more clear about exactly what you want from a PAC prediction. <sep> A number of notation errors throughout which are important to fix for clarity and neatness, and a decent amount of lack of precision in language throughout which is important to fix for clarity. See Other Feedback <sep> Line below Eq 5 confuses me: you say you exit at m if \\hat{y}_m correctly classified an example also correctly classified at \\hat{y}_M. But how can you know this without doing inference to the last layer? <sep> Is there a reason why the greedy approach to Fast DNN inference you take is desirable? Be more clear about why you chose this and if it is optimal somehow or not <sep> Other feedback: <sep> Some precision in language could be improved throughout – for instance ""by using the accurate model only if the confidence of the accurate one is underconfident"" on p2 doesn't really make sense <sep> -p2: ""a naively trained DNN is not reliable"" what does reliable mean here? <sep> -above eq 2, should this be \\kappa_x ? <sep> -Defn 1: should this be a nearness constraint rather than equality? We have multiple examples x in a bin each with their own p-hat, and so not all of them can be equal to c(x) <sep> -Is there a reason why these intervals should be defined as continuous rather than possibly a disjoint set? <sep> -should make it clearer from the start that this is defined for post-hoc classifiers, you're not learning these intervals directly <sep> Eq 4 – the bold theta-hat here is different from the one defined in the line above <sep> Need more explanation on ""The following expression is equivalent due to the relationship between the Binomial and Beta distributions"" p. 4 <sep> In the definition of C-hat, you're overloading x on the right side of the given sign. Can you use x' or something? <sep> In the ""important case"" below Thm 1, can you clarify – this is the mean right? <sep> Problem formulation in Fast DNN Inference: should the \\hat{y}_i be \\hat{y}_m? I feel like i is not scoped here <sep> You define d_m below the problem formulation but it isn't in the formulation itself. <sep> Top of p5, you train the network in the ""standard way"" – this is not clear. Do the gradients at the lower levels flow back through the earlier layers? Or are they stopped and the only gradients are from the final prediction task at the last layer? Either could make sense to me <sep> Should define more carefully what the ""composed classifier"" refers to <sep> Clarify how rollouts work – will you observe unsafe states? Sometimes in safe planning you assume you don't actually observe the unsafe states in training but it looks like you need that <sep> Bottom of p6: what is Z'? Not sure what Z is – is it ordered pairs? Why is the second element always 1? <sep> P13: why is this an upper bound? It's hard to parse but it looks equal to the expression in 10 at first glance – it's the appendix so please explain further. You can also note that the E_t are disjoint by definition <sep> Also please add comments about the appendix figures? I have no idea what they are <sep>","The paper provides a method for constructing PAC confidence scores for pre-trained deep learning classifiers. The reviewers were all positive about the paper. <sep> Pros: <sep> Has provable guarantees on the reliability of the prediction. Such guarantees are quite desirable in practice. <sep> The problem of neural network uncertainty is important and timely problem, especially in safety-critical applications. <sep> The method is simple and well-motivated. <sep> Strong empirical performance. <sep> Interesting applications to fast DNN inference and safe planning. <sep> Cons: <sep> Lack of generalization guarantees-- the guarantees in the paper only hold on the training set; but in practice, performance in test is what's important. <sep> Only a handful of baselines tested against, most of which (if not all) were naive."
"The paper considers the problem of creating spatial memory representations, which play important roles in robotics and are crucial for real-world applications of intelligent agents. The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama. Given the relative pose between frames, the representation from the previous frame is transformed via forward warping (using known depth values) to the viewpoint of the current frame. The proposed approach has no learnable parameters. Experiments on a wide range of tasks show that the proposed approach outperforms baselines such as LSTM and NTM. <sep> On the positive side, the approach is positively simple, in the sense that it relies on known techniques (EKF, forward warping, etc.) that ensure that it is easy to implement while achieving good results in the experiments. Up to Sec. 4, I found the paper easy to follow, although some design choices could be better motivated (e.g., I assume that diagonal covariances are assumed for simplicity). <sep> The paper evaluates the proposed approach on multiple tasks and in various configurations, which is another strength of the paper. <sep> While I like the proposed approach, I also see multiple significant weaknesses: <sep> I found the experimental evaluation nearly impossible to understand. My main problem is that I don't understand what the different method that are evaluated are: <sep> Given the abbreviation ESMN introduced in the abstract, I assume that ESMN is the proposed approach. ESM seems to be a variant of ESMN, but I am not sure how ESM and ESMN differ as the difference is never clearly described (or if it is, I seem to have missed it). Sec. 4.1.1 mentions training with a convolutional encoder in the context of ESMN, Sec. 4.1.3 and Sec. 4.1.4 only evaluate ESM but not ESMN, while Sec. 4.2 states that ""ESM represents map-only inference, while ESMN includes convolutions for both the image-level and map-level inference"". Unfortunately, the term ""map-level"" inference is not well-defined. Overall, I don't understand the difference between ESM and ESMN. As a result, it is unclear to me why ESM performs worse than ESMN in Tab. 1 for DR-Ego-S but comparable for all other tasks in the table, or why ESM and not ESMN is used for some of the experiments. Similarly, what is the ""ESM-DepthAvoid"" baseline? Does it only use depth and no features? <sep> Tab. 1 contains a baseline called ""PO"" and I don't understand how it works. The abstract introduces PO as an abbreviation for partial observability, but that does not seem to be an explanation for a baseline. The PO baseline performs similarly well as ESM and ESMN in Tab. 1, which makes me wonder whether Tab. 1 really shows the superiority of the proposed approach. <sep> I am confused by the statement ""In contrast, ESM by design stores features in the memory with meaningful indexing. The inclusion of relative cartesian co-ordinates in the memory image also effectively aligns each pixel with an associated relative translation."" since the inclusion of such coordinates is never mentioned before. I assume ESM uses some form of handcrafted features? <sep> Some of the statements made in the paper seem too strong: <sep> I don't see how the claim ""Our memory is much more expressive than these 2D examples, with the ability to represent detailed 3D geometry in all directions around the agent."" (Sec. 2.2)  holds. I agree that being able to store information in 2.5D (panorama and depth) is more powerful than storing only top-down 2D maps. However, the allocentric maps of the references can store larger and more complicated scene parts. E.g., if an agent would turn around a corner, ESMN would essentially be forced to forget about everything that is not directly visible anymore as it is occluded and thus not included in the memory structure anymore. As such, a point can be made that ESMN is much less expressive than for example Henriques & Vedaldi. Given that the latter evaluate on significantly more complex scenes compared to the ones used in this paper (which do not have strong occlusions) strengthens this impression. <sep> Regarding the statement ""Although the most recent depth frame is also a strong signal for local obstacle avoidance, we show that the avoidance based on the full ESM geometry results in fewer collisions when tested on a variant of the drone task with the inclusion of 25 obstacles."": Looking at Tab. 2, it seems to me that the standard deviation is so large for both ESM and ESM-DepthAvoid that it is unclear to me whether one is consistently better than the other. <sep> The CodeSLAM approach from Bloesch et al. also provides a form of memory representation and I don't understand why the paper, and its follow-up (Zhi et al., SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded <sep> Scene Representations, CVPR 2019), is not discussed in the related work section. <sep> Overall, I believe that the paper has potential. My main criticism is that I do not feel able to properly understand the experimental evaluation. As such, it is hard to recommend acceptance. However, I am willing to increase my score if the information necessary to understand this part of the paper is provided. <sep> After rebuttal phase <sep> The answers provided by the authors and the revised version of the paper sufficiently address my concerns. As such, I recommend to accept the paper. I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines. Still, I feel that in the current form, the paper can be accepted.","In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego-centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping. <sep> The main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense. Two subsequent revisions of the paper have dramatically improved the paper. <sep> Given the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance."
"Summary of review: <sep> This paper considers solving rank-constrained convex optimization. This is a fairly general problem that contains several special cases such as matrix completion and robust PCA. This paper presents a local search approach along with an interesting theoretical analysis of their approach. Furthermore, this paper provided extensive simulations to validate their approach. Overall, the paper provided solid justification for their approach. <sep> Approach: <sep> The proposed approaches, namely greedy and local search, iteratively add a rank-1 update to the current solution, where the rank-1 update comes from the maximum singular value and eigenvector of the current gradient. After performing the rank-1 update, an inner optimization problem is performed and operates in a low-rank (compared to the dimension of the input) space. <sep> In the greedy approach, the rank-1 update is simply added to the iterate. <sep> In the local search approach, the iterate goes through an additional truncation step, which reduces its rank by one before adding the rank-1 update. <sep> Theoretical analysis: <sep> For both approaches, this paper proves that the iterative procedure converges with a solution within ϵ to the optimum in r⋆κ2log⁡O(1/ϵ iterations, where r⋆ is the rank of the optimal solution of the minimization problem and κ is a certain rank-restricted condition number. This result is quite interesting because it provides an explicit upper bound on the rank of the converged iterates. The arguments in the analysis look sound to me. <sep> Validation: <sep> The authors went on to validate their proposed approaches in matrix completion and robust PCA. <sep> For matrix completion, the authors compared their approach to SoftImpute (Mazumder et al 2010), which is a well-known approach in this literature. They showed their approach outperforms SoftImpute in simulations. Then, the authors compared their approach to NMF and SVD on the MovieLens datasets and showed their approach achieves comparable test loss while speeding up the computation by several factors. <sep> Writing: <sep> Overall, the writing is clear and easy to follow. I have several detailed comments below. <sep> Questions: <sep> It would help improve the reviewer's understanding if the author(s) address the following questions in their rebuttable. <sep> --- Note: Properly addressing the questions below is required for the reviewer to better appreciate your results. <sep> (i) In Table 2 and 3, could you add a comparison to SoftImpute and Alternating Minimization with L2 Regularization (say you choose the rank via cross-validation)? How would your results compare to these approaches? <sep> (ii) Specific to the MovieLens benchmark, could you discuss how your approach compares to more recent approaches (as opposed to 2009, https://paperswithcode.com/sota/collaborative-filtering-on-movielens-10m)? Particularly the runtime improvements since these look like the key claims of this experiment. While these approaches may not apply to generic constrained convex optimization, it is still worth discussing? <sep> (iii) How large should I expect the rank-restricted condition number to be in Theorem 2.1 and 2.2? For example, how large are they in the setting of Figure 1 and 3? <sep> Detailed comments: <sep> Section 2.1 and 2.2: these two theorems and the algorithms came out all of a sudden with no description. Could you add some explanation and describe some intuition? <sep> Typo, P2: ""we will found useful"" -> we will find useful",Please clarify as early as the abstract that you refine the analysis of the algorithm proposed by Shalev-Shwartz et al (which is a great contribution given the importance of the problem).
"Paper summary: <sep> Building on previous work on neural operators, the paper introduces the Fourier neural operator, which uses a convolution operator defined in Fourier space in place of the usual kernel integral operator. Each step of the neural operator then amounts to applying a Fourier transform to a vector (or rather, a set of vectors on a mesh), performing a linear transform (learnt parameters in this model) on the transformed vector, before performing an inverse Fourier transform on the result, recombining it with a linear map of the original vector, and passing the total result through a non-linearity. The Fourier neural operator is by construction (like all neural operators) a map between function spaces, and invariance to discretization follows immediately from the nature of a Fourier transform (just project onto the usual basis). If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization. Experiments demonstrate that the Fourier neural operator significantly outperforms other neural operators and other deep learning methods on Burgers' equation, Darcy Flow, and Navier Stokes, and that that it is also significantly faster than traditional PDE solvers. <sep> Strengths and weaknesses: <sep> Much of the theoretical legwork for this paper, namely, neural operators, was already carried out in previous papers (Li et al.). The remaining theoretical work, namely writing down the Fourier integral operator and analysing the discrete case, was succinctly explained. The subsequent experimentation was extremely thorough (e.g. demonstrating that activation functions help in recovering high frequency modes) and, of course, the results were very impressive. I liked the paper a lot, and it's definitely a big step-forward in neural operators. I'm assigning a score of 8 (a very good conference paper), and I think that the paper is more or less ready for publication as is. I've included a few questions below (to help my own understanding), as well as some typos I spotted whilst reading the paper. <sep> Questions and clarification requests: <sep> Section 4, The Discrete Case and the FFT – could you explain the definition of bounds in the definition of Z_{k_{max}}? <sep> Section 4, Parametrizations of R, sentence 2 – could you explain the definition R_{\\phi}? At present I can't see how the function signature of R matches the definition given. <sep> Typos and minor edits: <sep> Page 3, bullet point 3 – ""solving Bayesian inference problem"" -> ""solving Bayesian inference problems"" <sep> Section 1, final paragraph, sentence 2 - ""approximate function with any boundary conditions"" -> ""approximate functions with any boundary conditions"" <sep> Section 4, The discrete case and the FFT, final paragraph, last sentence - ""all the task that we consider"" -> ""all the tasks that we consider"" <sep> Section 4, Parametrizations of R, last sentence - ""while neural networks have the worse performance"" -> ""while neural networks have the worst performance"" <sep> Section 4, final sentence – ""Generally, we have found using FFTs to be very efficient, however a uniform discretization if required."" -> ""Generally, we have found using FFTs to be very efficient. However, a uniform discretization is required."" <sep> Section 5, final paragraph, sentence 2 – ""FNO takes 0.005s to evaluate a single instances while the traditional solver"" -> ""FNO takes 0.005s to evaluate a single instance while the traditional solver"" <sep> Section 6, final sentence – ""Traditional Fourier methods work only with periodic boundary conditions, however, our Fourier neural operator does not have this limitation."" -> ""Traditional Fourier methods work only with periodic boundary conditions. However, our Fourier neural operator does not have this limitation.""","Pros: <sep> Provides a practical technique which can dramatically speed up PDE solving -- this is an important and widely applicable contribution. <sep> Paper is simultaneously clearly written and mathematically sophisticated. <sep> The experimental results as impressive. <sep> Cons: <sep> There were concerns that the paper lacks novelty compared to Li et al 2020b, where the underlying theoretical framework was developed. The primary novelty would seem to be: <sep> using Fourier transforms as the specific neural operator <sep> the strength of the experimental results <sep> Overall, I recommend acceptance. I believe the techniques in this paper will be practically useful for future research."
"After the discussion, my concerns were fixed. The paper explores the interesting relations of Mix Up and Uncertainty, which is useful and will be the right fit for the conference. <sep> Summary: <sep> The work studies how a better calibration of individual members of an ensemble affects the calibration of an ensemble. It is demonstrated that i) better calibration of individual members of the ensemble may lead to the worse calibration of the ensemble predictions ii) this is the case when mix-up / label smoothing are used during training. <sep> To fix the issue Confidence Adjusted mixup Ensembles (CAMixup) is proposed. The CAMixup is an adaptive mixup data augmentation based on per-class calibration criteria. The core idea of CAMixup is to use powerful (unconfidence encouraging) mixup data-augmentation on examples of overconfident classes, and do not use mixup for the under-confident classes. The confidence criteria are computed once in an epoch. <sep> The empirical results are provided in- and out-of-domain for CIFARs(C) and ImageNet(C). <sep> The concerns: <sep> ECE is a biased estimate of true calibration with a different bias for each model, so it is not a valid metric to compare even models trained on the same data [Vaicenavicius2019]. In other words, the measured ECE has no guaranty to have something to do with the real calibration but reflects the bias of the measured metric. Other metrics, that are based on histogram estimates have the same problem. Please put extra attention to this concern. <sep> What I suggest is the following: <sep> a. Mesure NLL for in- and out-of-domain data. It seems to be still an adequate (indirect) criterion of calibration, and is an adequate criterion of uncertainty estimation. According to [Ashukha2020], the NLL needs a pre-calibrated model with temperature scaling for in-domain data (called calibrated NLL / calibrated LL). <sep> b. To use the squared kernel calibration error (SKCE) proposed in [Widmann2019] along with de facto standard, but biased ECE. The SKCE is an unbiased estimate of calibration. There might be some pitfalls of this metric that I'm not aware of, but the paper looks solid and convincing. Also, please put attention to Figure 83 in the arХiv version. <sep> Yes, ECE is the standard in the field, but it is the wrong standard that prevents us from meaningful scientific progress, so we should stop using it. <sep> The standard deviation needs to be reported everywhere. Especially the differences between close values like (97.52, 97.52, 97.47 in Table 2) may appear not statistically significant. The same touches Fig 5 and other figures that are reported. Otherwise, it is impossible to stay how solid results are. <sep> Minor comments: <sep> Maybe it worth to provide plot mean-λi vs epoch to illustrate this ""Notice that λi is dynamically updated at the end of each epoch.""? <sep> Figure 4(a) is done slightly disorderly. <sep> In the paper, ECE is measured in percentages. As far as I can tell ECE is dimensionless quantities. It is not clear what is intended. <sep> Final comment: I put the ""marginally below acceptance threshold"" score, but I'm willing to increase it after the update with corrections (and hope that these corrections will be done).  I like the direction and CAMixup, but in-domain results are not very consistent (see Fig 5 (d)), the ECE has uncontrollable model-specific biases that ruin all the presented results. <sep> [Widmann2019] Widmann D, Lindsten F, Zachariah D. Calibration tests in multi-class classification: A unifying framework. In Advances in Neural Information Processing Systems 2019 (pp. 12257-12267). https://arxiv.org/pdf/1910.11385.pdf <sep> [Ashukha2020] Ashukha A, Lyzhov A, Molchanov D, Vetrov D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. *CONF*, 2020.","This paper analyses the interaction between data-augmentation strategies and model ensembles with regards to calibration performance. The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble. They propose a simple solution. The paper merits publication."
"Summary <sep> The authors present ""HyperDynamics"", a novel method for system-identification and learning of flexible forward models that can be used in planning tasks. The presented methods is generic and is shown on both locomotion and pushing tasks with different simulated robots. <sep> I enjoyed reading this work a lot and I hope it gets accepted. It's a clever idea and most flaws that I'm about to point out are easily addressable by the authors. <sep> Strengths & Weaknesses <sep> Strengths <sep> The method is generic (shown to work across tasks and environments). <sep> The baselines are strong. When I started reading your paper, I thought that DensePhysNet and some form of MAML would be good candidates for this to compare again and it turns out these were indeed included. <sep> Figure 1 + caption as well as the introduction to section 3 do a great job at introducing the architecture in a way that would allow the reader to create a basic implementation. <sep> Code was included. I didn't run it but it's clean and seems functional from what I can tell. <sep> Weaknesses <sep> You really, really need to be more clear in the main paper on the implementation details. You can't move the amount of training data to the appendix and it's not good practice to only include the network architecture by name in the main paper. And what are all your losses? You make the method looks super simple but then you train on shapenet, some 2D reconstruction, something about cropping, and there's a GRU in there too (full backprop vs truncated backprop?). The appendix shines a little light on this but you need to be way more specific in the main paper. That has to stand on its own. <sep> You don't motivate all the nitty-gritty implementation choices. Why did you add the decoder? What's the performance if you remove it? What about the cropping? What if you don't do an object-centric feature map, but instead a few CNN layers? What are the individual contributions of all these details? <sep> DensePhysNet, Visual Foresight, and many other works in this domain use (simple) real-robot experiments to demonstrate that their method can handle realistic robot noise. Obviously there's a global pandemic happening at the moment, so I won't require you adding this for the rebuttal, but I think in order to really establish this method (maybe before putting it on Arxiv), you'd have to add some real-robot experiments. This can be as simple as a 180USD RealSense and a 500USD robot arm plus a few objects and a playfield. It's become a standard for system-identification-style works and it's justified in my opinion since your method isn't inherently useful in simulation where the user has access to all the information and can arbitrarily reset/reposition the model. And since you don't have ShapeNet data for many real-world objects (which you seem to need for pretraining), could you at least add a sentence or two detailing how this would transfer to real-world problems? <sep> TL;DR my main requests: (2) Motivate implementation details (add ablations if you have any) and (1) be more explicit about them in the main part of the paper. <sep> Impact & Recommendation <sep> Despite that there seem to be a lot of hacks that make this method in the specific settings, I think the general idea behind it is sound. And I think the authors show that it performs better than the sota, at least in simulation. Therefore I'd recommend acceptance given that the authors add the requested information. In its current shape, it's a 6 for me but if my main concerns are addressed, I'm happy to up this to a 7 or if major improvements are made and my questions below are answered, to an 8. <sep> Questions, Nitpicks, Comments <sep> Kudos for not making another acronym method <sep> There are a lot of typos and orthographic errors, would recommend a spell-checked or getting this proofed. Examples: section 2 ""poinclouds"", section 2 ""properties in hand"" -> ""properties at hand"" <sep> Maybe start the introduction with an example, e.g. how children are able to chew on a block of wood to assess its hardness and then build towers with it. <sep> important Introduction: when you go over (i-iv), that feels a bit too long and lit-reviewy and misplaced in the introduction. I would recommend the following changes: (a) trim this severely, only mention that there are model-based methods that usually do only one environment and there's meta-learning and how your method is more adaptable than either, (b) move this into the literature section, where you have to come back to it anyway, (c) move the Hypernetworks section from the literature into a separate ""Background"" section and develop it a bit further, since it's less ""competing method"" and more ""you should know about this to understand out method"". <sep> Also in the introduction, you present (i-iv), and you mention how your method is better/different than (i-iii) but you never address (iv). <sep> It's become a standard to summarize the contributions again at the end of the introduction, ideally as bullet points. Please add these. <sep> In equation (1), why is the ordering O-T-N for the sums? I feel like ONT would be more natural, no? <sep> When reading the method, my main question was if the method would work on ""dense"" trajectories or on before-and-after photos like DensePhysNet. This is only answered a few pages later but I think this belongs in 3-Overview or 3.1. Just to be clear, you're gathering trajectories of length 4s, i.e. 5 frames of 800ms where you do NOT retract the robot arm when pushing, right? (Compared to DensePhysNet, where the arm is never visible because they take photos before and after complete standstill). If that's the case, how do you deal with occlusion from the arm? <sep> Do you encourage object-object interactions in any way or do they just occur randomly? Or do you only ever experiment with single objects? <sep> The object orientation vs state section isn't super clear? You're subtracting an object's absolute starting position+orientation from it's future trajectory points? <sep> In 3.2: Why a GRU, why not LSTM? Why k=16 (and similarly why k=5)... This ties into the main criticism from above. Please motivate your choices. <sep> In 4.1: I think this is a typo, but it says you added beds to your experiment table. I think they'd be a bit too large, no? :D <sep> 4.1: specify the random mass+friction range, please! <sep> 4.1: same with the total amount of training data/frames <sep> And since you won't have ShapeNet <sep> 4.2: I think it's a half-cheetah, not a cheetah. <sep> 4.2: I don't understand why it's unrealistic to assume arbitrary resetting in simulation. That's one of the benefits of running simulations and common practice. <sep> 5: What do you mean ""predicting both the structure and parameters of the target dynamics model""? Parameters is clear (mass, friction, etc.) but what's the structure here?","This paper proposes ""HyperDynamics"" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. These parameters are fed into a forward dynamics model, represented as a neural network, that is used for control. <sep> Pros: <sep> addresses an important problem (adapting dynamics models to ""new"" environments) and provides strong baselines <sep> well written and authors have improved clarity even further based on reviewers comments <sep> Cons: <sep> I agree with the reviewer that it is currently unclear how well this will transfer to the real world <sep> The idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as I know). The authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist) <sep> (1) Preparing for the Unknown: Learning a Universal Policy with Online System Identification"
"[Summary] <sep> Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). An extra self-attention model is also stacked over the trajectories to trade off the length of prediction and the possible explaining away issue.  The whole model is trained via a canonical MARL objective while the trajectory prediction model utilizes direct supervision collected from the environments. Experiments on several toy MARL benchmark demonstrates the effectiveness of the proposed method. <sep> [Strengh] <sep> +) The idea of communication with imagined intention is motivated properly with rich psychological background and also technically sound. <sep> +) The paper is overall clear and well-written. I found there are enough technical details to reproduce the main results of the main approach. <sep> +) Still limited though in terms of the converted domains, the empirical evaluations deliver impressive results over a reasonable collection of counterparts. <sep> [Weakness] <sep> The main concerns I have with this submission lies in the overall novelty and the evaluation of the proposed method. After reading this paper, indeed I find the authors failed to capture some important research in this narrow area and it's still not clear how does the proposed method really works and whether it is sensitive to some specific implementing factors. <sep> -) The idea of intention sharing based on prediction is essentially not novel esp. in the MARL domain. In the CogSci & AI community, theory-of-mind (ToM) and its application in multi-agent execution (either collaborative or zero-sum games) have been well studied for years [1, 2, 3]. Although I may agree that there are few prior works [4] on sending these predictions as messages to other agents, it does not really introduce that many new ideas to how collaborative or adversarial agents could benefit from such ToM-based intention prediction. However, the authors failed to capture these counterparts in their discussion and evaluations. Specifically, I would like to see how does the proposed method formulate its intention prediction differently than the prior work and whether it could enjoy advantages in performance with such differences. <sep> -) The proposed method is essentially quite complex (stacked prediction, transformer, etc) than its Bayesian counterparts, while the authors only provide an overall evaluation against several MARL baselines. It will be critical to also conduct a serious ablation study given the overall complexity of the proposed method, i.e. whether to use the transformer, the architectural choice of the transformer (num. of heads, etc), and the length of predicted trajectories (H). Also, the disagreement between the predicted trajectories and actual observation and its relation to the performance should also be investigated. <sep> -) Although the selected tasks are all canonical to MARL, given the fact a growing number of recent MARL learners have been moved on to more challenging tasks, I feel it would be necessary to include some mini MOBA games or other tasks with vision-based observations. <sep> [1] Bayesian models of human action understanding <sep> [2] Theory-based Social Goal Inference <sep> [3] Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution <sep> [4] Machine theory of mind <sep> [Suggestions&Questions] <sep> (1) Add ablation studies on the use of self-attention model and the length of prediction (H). <sep> (2) Visualize&compare the predicted and observed trajectories, add some discussion on how such disagreement would affect the performances. <sep> (3) (minor) Try the proposed method on mini MOBA games or tasks with vision-based observations. <sep> (4) Add citations to the related work on ToM and its application in MAS. <sep> [Post-rebuttal] <sep> I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4's concern on the significance of results is valid. Therefore, I will keep my initial justification as is.","The authors study co-ordination in multi-agent systems. Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents' actions, they then use this to form a plan which forms the agents' intention which is then communicated to the other agents. <sep> The major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest. During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test. While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication"
"This paper considers the problem of per-instance model adaptation for neural data compression, and proposes a new method for end-to-end finetuning the model that is quantization-aware, by introducing an additional term that measures the compression cost of model update to the typical rate-distortion loss. Evaluation on the UVG dataset shows encouraging performance, with an average distortion improvement of approximately 1 dB for the same bit rate compared to the naive baseline (without fine-tuning). <sep> Pros: <sep> The paper is well written and concepts are clearly explained. <sep> The method is sound, and incorporating the entropy cost of model update during fine-tuning offers a conceptually appealing (and likely more performant, though not empirical verified (see below)) approach compared to previous methods (Lam et al., 2020, Zou et al., 2020) that tackles model update quantization after fine-tuning. <sep> Cons: <sep> The experiment section is the weakest point. Particularly: <sep> It's unclear from the description if the evaluation on UVG actually ""adapts the entire model to a single data instance"" (i.e., for each image) as claimed, or amortizes the model update cost over a batch of all the images in a video. The paper claims that ""In this paper we consider the extreme case where the domain of adaptation is a single instance, resulting in costs for sending model updates which become very relevant"", but this would highly misleading if all the experiments were conducted in a batch compression setting. <sep> If the experiment did perform per-instance model adaptation, then it would be much more convincing to evaluate on standard datasets like Kodak and Tecnick from the image compression literature, instead of frames of UVG videos. <sep> Since the paper's contribution is about improving the existing fine-tuning strategy that tackles model update quantization after fine-tuning (e.g., Zou et al., 2020), the proposed method should then also compare to these baselines to really assess its performance. <sep> It would also be interesting to compare with approaches that optimize the encoded latents (e.g., Yang et al., 2020), which also achieve close to 1 PSNR improvement at equal bitrate without the overhead of decoder updates. <sep> Questions: <sep> Can the author comment on how ""the quantization bin width t and standard deviation σ of p[\\bar δ]"" (Sec 4.3) are chosen? How sensitive is the compression performance to their choice, e.g., is it possible to discretize so finely that no amount of RD improvement can overcome the model update cost? <sep> The use of the continuous density for the M (model update cost) term in Eq 2 is established in the Appendix A by showing that the gradient of the discrete cost \\bar M has the same gradient (up to first order) as that of -log p(δ) based on the density p(δ). Did I understand this correctly?  But M = -log p(δ) doesn't actually give an estimate of the cost after discretization \\bar M = -log p[\\bar δ]. Instead, the typical thing to do in literature (due to Balle et al.) is to actually minimize -log p[\\bar δ], where \\bar δ = round(δ), and the rounding can be either approximated by uniform noise injection or STE.   Can the authors comment on this choice of their method? <sep> Typos and minor mistakes/fixes: <sep> p. 2, under eq (1): The R-D loss is equivalent to the negative ELBO in VAEs; <sep> Does Figure 3 bottom show the histogram of bit allocation for \\bar δ? If so then the caption can just say ""Bottom: histogram of bit allocation for \\bar δ"" as it's clearer. <sep> Update after author response: <sep> I have increased my score in light of the substantial improvement to the manuscript and experiments.","The paper suggests a procedure to efficiently adapting a learned neural compression model to a new test distribution. If this test distribution has low entropy (e.g., a video as a sequence of interrelated frames), large compression gains can be expected. To achieve these gains, the method adapts the decoder model to the new instance, transmitting not only the data but also a compressed model update. Experiments are carried out on compressing I-frames from videos, while comparisons comprise baseline approaches that finetune the latent representations of videos as opposed to the decoder. <sep> The paper's main contribution is very timely and relevant. While it was well-known in the classical compression literature that model updates could be sent along with the data (e.g., as already done in ""optimized JPEG""), this is the first time the idea was implemented in neural compression. The experiments are arguably the paper's weaker part and were originally a concern, but they have been significantly improved during the review period such that all reviewers voted for acceptance. We encourage the authors to further strengthen their experimental results by adding more challenging baselines on well-established tasks (e.g., image compression)."
"The paper suggests that a major factor for increasing few-shot performance in the few-shot object detection task is the number of categories in the base training set used to pre-train the few-shot model on a large set of data before it is adapted to novel categories using only a few (or even 1) examples. This effect is measured by the authors by trying out the existing Siamese few-shot detector on 4 datasets: PASCAL, COCO, Objects365, and LVIS showing that the gap in performance on the seen training and the unseen (novel) testing categories is reduced when the base dataset has more classes (e.g. on LVIS where there are more than 1K classes, this ""generalization"" gap is shown to be minimal). The authors also quantify empirically the effect of increasing the model size and of prolonging the training schedule on this gap. As well as testing on COCO classes while training on LVIS. <sep> Pros: <sep> number of base classes is indeed an important factor in few-shot methods performance (not just in detection) <sep> the paper is easy to follow and generally well written, the message conveyed is clear and the experiments are useful <sep> Cons: <sep> the positive effect of increasing the number of base classes on few-shot performance is long since known, and numerous works even in the few-shot classification literature have noted this fact, so nothing seems to be new here the effect of increasing backbone size and prolonging the train schedule does not seem to indicate a strong correlation to the number of train classes, the original gap is maintained, while the gains of the tested modifications seem to be relatively similar up to some noise <sep> I might be wrong, but it seems the authors mostly target few-shot localization, assuming the target object (given by the reference image example) is always present in the image. This is opposed to what I understand by few-shot detection, wherein a test episode there are several target objects, each accompanied by its support example and query images can have an arbitrary mix of these target objects or none at all. <sep> To summarize: I like the paper, yet I fear it does not meet the plank of what I would consider a paper fitting *CONF* standards in terms of novelty. There is nothing wrong in not proposing a new algorithm and instead - uncovering an important fact that was so far overlooked, but as I noted above this is not the case, the paper highlights a well known fact.... I would be happy to monitor other reviewers responses and authors comments on this issue of novelty and would be happy to be convinced otherwise.","The reviewers have ranked this paper as borderline accept. On the negative side, the main claim of the paper (the more categories for training a one-shot detector, the better) has already been observed in several works and very intuitive. However, the paper has done significant experimental work to support this claim. The paper is very well written, it carefully explores the existing setups for one-shot detection and highlights their weaknesses. The paper also gives advice on how to construct better datasets for one-shot detection (the conclusion ""add more diverse categories"" is somewhat obvious but the paper demonstrates how important that is)."
"Summary <sep> The authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). In my understanding, the knowledge of the environment dynamics is assumed. The authors tried to validate the proposed idea on objectworld (Levine et al., 2011) <sep> Quality <sep> The quality needs to be improved in the sense that a clear theoretical link between the target problem and MC-EM cannot be found in the submission. For example, the main objective (3) is optimized through (4) and (5), but the relation is unclear. There are lots of such things in the submission. <sep> Clarity <sep> The readability of the submission is poor and needs to be improved. Lots of terms are unclear to me (e.g., succinctness, robustness, transferability of rewards). At some part of derivation, I couldn't understand the motivation. Experiment settings are unclear, and the results are not confident and seem irreproducible with given information. <sep> Originality <sep> Exploiting the distribution of reward is considered in Bayesian IRL. I think the probabilistic view was originated from Bayesian IRL (e.g., uniform prior on rewards may cover the idea of this work). The submission only sets MaxEntIRL as its baseline, but I think Bayes IRL should have been considered. <sep> Significance <sep> There seems to be a minor contribution <sep> Detailed comments <sep> (p.1, Abstract) expert demonstrations may be optimal for many policies <sep> I feel this statement is weird since we haven't defined the optimality of expert demonstrations. <sep> (p.1, Abstract) we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. <sep> SIRL tries to solve the inherent issue of IRL problem, not generalize IRL. Also, since Bayesian IRL also recovers the reward distribution, I couldn't get the major advantage of the SIRL from this statement. <sep> (p.1, Abstract) The solution is succinct, robust, and transferable <sep> Definitions of these expressions seem ambiguous to me. <sep> (p.1, Abstract) a global viewpoint <sep> Again, ambiguous. <sep> (p.1, Introduction) <sep> It would be better to write it in a more abstract way and separately write down the Related Work section. <sep> References should be much clearer: LaTeX commands like \\citet{} and \\citep{} should both be used. <sep> (p.1, Introduction) if the model dynamics are known <sep> Recent works on IRL such as adversarial IRL (Fu et al, 2017) didn't require the knowledge of model dynamics. <sep> (p.1, Introduction) The recovered reward function provides a succinct, robust, and transferable definition of the learning task succinct, robust, and transferable: Ambiguous <sep> (p.1, Introduction) First paragraph <sep> Lots of words from Abstract seem to be repeated. <sep> (p.1, Introduction) In a real-world scenario, experts always act sub-optimally or inconsistently, which is another challenge. <sep> The sentence seems abrupt. The terms like sub-optimal and inconsistent here are awkward. <sep> (p.1, Introduction) imposes regular structures of reward functions in a combination of hand-selected features <sep> GAIL (Ho et al, 2016) doesn't require a hand-crafted feature. <sep> (p.1, Introduction) hand-selected by experts <sep> A word experts here seems to imply a reward designer, not an expert on target tasks. I'd rather use a different word here. <sep> (p.1, Introduction) based on demonstrations respectively respectively seems inappropriate. <sep> (p.1, Introduction) Influenced by the work of Finn et al. (2016a;b) <sep> How these references affected AIRL needs to be mentioned. <sep> (p.2, Introduction) because the MaxEnt approach is equipped with the ""transferable"" regular structures over reward functions. <sep> In Ziebart et al., 2008, transferability wasn't mentioned. <sep> I believe the statement -- MaxEnt itself gives transferable reward feature -- is wrong but you should share the correct reference if this is true. <sep> (p.2, Introduction) The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem <sep> This explanation seems insufficient to understand the meanings of ""succinctness"" and ""robustness"". <sep> (p.2, Introduction) Benefits of the class of the MaxEnt method, <sep> Thanks to the benefits of the class of the MaxEnt method? <sep> (p.2, Introduction) Since of the intractable integration in our formulation, <sep> Due to the intractable integral in our formulation? <sep> I think the intractability of the mathematical derivation didn't need to be mentioned in Introduction. <sep> (p.2, Introduction) in a model-based environment when model dynamics is known? <sep> (p.2, Introduction) In general, the solutions to the IRL problem are not always best-fitting in the previous approaches because a highly nonlinear inverse problem with the limited information is very likely to get trapped in a secondary maximum in the recovery. <sep> I couldn't understand what the authors wanted to emphasize. <sep> It seems like they intended to emphasize the problem of local optima, but I don't know if such a problem is exactly what's happening in IRL. <sep> (p.2, Introduction) global exhaustive search <sep> What does global imply? Knowledge of dynamics? <sep> (p.2, Introduction) theoretically convergent demonstrated by pieces of literature is theoretically convergent? <sep> How the theorem in the references (Caffo et al., 2005, Chan and Ledolter, 1995)  is applicable to the proposed idea should be much clearer since this is one main advantage that the authors argue. For example, what kind of assumptions are required to acquire global optimality? What is the algorithmic assumption of MC-EM for optimality? How are those assumptions linked with IRL setting? <sep> (p.2, Introduction) is also quickly convergent converges quickly? <sep> How can we guarantee the convergence speed? Empirically or theoretically? <sep> (p.2, Introduction) the preset simple geometric configuration over weight space in which we approximate it with a Gaussian Mixture Model (GMM) <sep> preset -> predefined? <sep> approximate it -> approximate <sep> (p.2, Introduction) We generalize the IRL problem <sep> It seems the objective is not a generalization. <sep> (p.2, Preliminary) T:=P(st+1=s′|st=s,at=a) <sep> T(s′|s,a):=P(st+1=s′|st=s,at=a) <sep> (p.2, Preliminary)  a sequential of state-action pairs a sequence of state-action pairs? <sep> (p.2, Preliminary) The estimated complete MDP yields an optimal policy that acts as closely as the expert demonstrations. <sep> The discount factor should be considered as well. <sep> (p.3, Regular Structure of Reward Functions) N <sep> I'd rather use a different letter since N is used to indicate Gaussian distribution in Section Second Stage. <sep> (p.3, Regular Structure of Reward Functions) ϕi(s,a)i=1 <sep> ϕi(s,a)i=1M? <sep> (p.3, Problem Statement) MDP∖R:=(S,A,T,γ) <sep> The definition doesn't match with one without the discount factor γ in Preliminary. <sep> (p.3, Problem Statement) ϕi(s)i=1M <sep> ϕi(s,a)i=1M? <sep> (p.3, Problem Statement) weights W <sep> The definition should be provided. <sep> Either W=(α1,…,αM) (for linear model) or the weights of neural network (for non-linear model)? <sep> (p.3, Problem Statement) more likely generates weights to compose reward functions as the ones derived from expert demonstrations <sep> Is this only a special case of Bayesian IRL? <sep> (p.3, Problem Statement) Suppose a representative trajectory class ~ <sep> The explanation should be clarified. In my understanding, CϵE is a class of sets of trajectories. <sep> Why do we need to care such a class with \\epslion threshold? <sep> (p.3, Problem Statement) Integrate out unobserved weights W <sep> What does unobserved weights mean? <sep> Integrate out -> Marginalizing out? <sep> (p.3, Problem Statement) trajectory element set O assumes to be uniformly distributed for the sake of simplicity in this study <sep> I don't fully understand what's the advantage of considering a representative trajectory class and why it is required. <sep> The section Note: tries to explain it, but more explanation or theorems seems to be needed. How can we theoretically guarantee that using a representative trajectory class doesn't affect our estimation? It seems to me that we cannot guarantee the optimality with this class is the same as the original optimality. <sep> (p.3, Problem Statement) fM <sep> How this quantity is related to reward weights is unclear to me. The relationship between weights and fM for both linear and non-linear models should be specified. <sep> (p.3, `Note:) <sep> Instead of using a separate section, I'd rather put these statements in the middle of Problem Statement for a clearer explanation. <sep> (p.4, Two-stage Hierarchical Method) <sep> Why do we need to use two-stage method instead of single-stage method (joint optimization over Θ1 and Θ2)? The advantage of two-stage methods should be briefly mentioned when it first appears for readability. <sep> How does the iterative update rule (4), (5) guarantee the optimization of (3)? It's unclear to me due to the expectation in (4) and (5). My guess is that direct optimization of RHS of (3) is not possible and (4) and (5) might be either lower or upper bound of (3) due to Jensen's inequality. <sep> (p.4, Initialization) ~in each learning task <sep> Do we care about multi-task learning or multiple reward weights only? I believe the latter case. <sep> (p.6, Experiments) since almost only objectworld provides a tool that allows analysis and display the evolution procedure of the SIRL problem in a 2D heat map, we skip the typical invisible physics-based control tasks for the evaluation of our approach, i.e. cartpole Barto et al. (1983), mountain car Moore (1990), MuJoCo Todorov et al. (2012), and etc. <sep> I think this makes the contribution weaker. At least a few classic control tasks should have been considered. One way of evaluating the quality of rewards is retraining the agent with acquired reward, which is already widely used in the literature. <sep> (p.6, Objectworld) <sep> One figure for illustration will enhance readability. <sep> (p.7, Evaluation Procedure and Analysis) DSIRL <sep> DSIRL abbreviates Deep SIRL but wasn't mentioned. <sep> (p.7, Recovery Experiments) <sep> How many runs were used? How's the mean and confidence interval of the empirical result? <sep> (p.8, Robustness Experiments) <sep> I couldn't understand how the robustness of reward is related to the proposed experiments. How the robustness is defined and its relation to the experiment should be clarified. <sep> (p.8, Hyperparameter Experiments) <sep> How is the range of hyperparameter search for all methods? Currently, only the results for SIRL and DSIRL are given. <sep> (p.8, Conclusion) <sep> It seems like both succinctness and transferability were not discussed in the main part of the submission. <sep> References <sep> Levine et al., 2011, ""Nonlinear inverse reinforcement learning with gaussian processes"" <sep> Fu et al., 2017, ""Learning robust rewards with adversarial inverse reinforcement learning"" <sep> Ho et al., 2016, ""Generative adversarial imitation learning"" <sep> Ziebart et al., 2008, ""Maximum Entropy Inverse Reinforcement Learning"" <sep> Caffo et al., 2005, ""Ascent-based Monte Carlo expectation-maximization"" <sep> Chan and Ledolter, 1995, ""Monte Carlo em estimation for time series models involving counts""","This paper describes a method called 'stochastic' inverse reinforcement learning. It is somewhat unclear how this differs from other probabilistic approaches to IRL. In particular Bayesian approaches have been used in the past to obtain distributions over reward functions. However, SIRL tries to estimate a generative model over such distributions. All the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed. There are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf. work on multi-task IRL). The experiments also seem to be insufficient."
"This paper investigates adversarial feature augmentation for improving the generalizability of graph neural networks. The authors adopt an existing augmentation algorithm and apply on the nodes of each training graph, and use the perturbed graphs for training. The focus of the paper is extensive experimentation in various tasks and settings to illustrate the effectiveness of adversarial augmentation in graph-based tasks. The experiments provide new, non-trivial insights, such as the effect of the number of network layers on the effectiveness of augmentation. The paper is well-written and easy to read. <sep> Nevertheless, there are a few drawbacks: <sep> Even though the method is completely adopted from prior work and there are no novelties, the authors claim they propose a new solution for graph data augmentation. It is true that adversarial feature augmentation has not been studied for graph neural nets, but it is a straightforward idea to apply an existing feature augmentation method on graph nodes, which are represented using feature vectors just like other types of data. <sep> Even though the performance improvement is consistent across tasks, it is not significant. Hence, my takeaway from the paper (which is in fact a valuable takeaway) is that adversarial augmentation is not considerably effective for graph neural nets, no matter what dataset and network architecture is used. A truly ""free"" augmentation is even less effective, as shown in table 2. To be clear, this is not a weakness of this paper. However, the authors are encouraged to acknowledge it and use a more neutral and objective language when describing the performance. <sep> Some conclusions are not reliable enough. For instance, the impact of biased perturbation is statistically insignificant in table 2, and those limited numbers are not sufficient to make such a deduction. Moreover, section 6 is not convincing. Just because augmentation hurts the performance of MLP on images but improves on graphs doesn't mean data distribution is the primary determinant of the efficacy of augmentation. It is similarly insufficient that augmentation improves the results on a discrete graph but doesn't improve when noise is added. Moreover, the authors do not elaborate ""how"" the data distribution affects the augmentation. Just proving that data distribution has an effect is a trivial and unhelpful fact. Furthermore, the authors do not support the claim that model architecture does not affect the efficacy of augmentation. In fact, they prove otherwise by figure 1 (left), where the network depth has a direct effect on the performance gap. <sep> Table 5 is confusing, as it does not demonstrate any relevant information. The two bottom rows of both tables are a repetition of results from table 1, and only the first row is new, which only shows that GAT performs worse without BatchNorm or Dropout. The table does not show how FLAG performs without BatchNorm and Dropout, and hence there is no new takeaway from this table that is relevant to the purpose of this paper. <sep> In sum, although the paper provides new and valuable findings, the experiment analysis and conclusions that are made are not strong enough for a purely empirical study. Hence, I recommend improving and resubmitting the paper, either by adding methodological novelty, or by bolstering the analysis sections to make more reliable and useful conclusions. I also encourage the authors to clarify any part that I may have misunderstood, as I am keen to adjust my rating accordingly. <sep> ######## Post-Rebuttal Updates: <sep> I appreciate the authors' response, but my main concerns were not addressed. Particularly, I still believe the novelty of this method is extremely limited, as it directly applies an existing method on graph node embeddings. The main novelties are biased perturbation and unbounded attack, which are both very simple modifications, and were not adequately studied in experiments. Although the authors show extensive comparison on various datasets, their comparison does not particularly show the contribution of biased perturbation and unbounded attack. They only show the effect of biased perturbation using a single number in Table 2, which is not convincing.","This paper studies the problem of adversarial training for graph neural networks. The proposed method is build on the free training approach, and more specifically FreeLB, with some additional tricks including bias perturbation (for node-classification) and unbounded attacks. While these additions are potentially useful, there are only limited investigation into their effect. Putting aside the technical distinctions of the method with prior work, this paper can also be viewed as an empirical study of adversarial training techniques on graph data with various GNN architectures. It is worth noting that overall the conclusions on ""adversarial training"" are positive, we do see consistent improvement over a variety of architectures and tasks. The issues however, is that it is unclear whether these improvements can be similarly achieved using prior technique like FreeLB (oblation is only done on one single task, where biased perturbation is shown to lead to minor improvement). The paper also provides some results showing the effect of depth of the network as well as different training strategies such as batch norm, dropout with general adversarial training. These results are interesting to see but do seem to be limited in both scope and depth. It appears that the authors have two goals in mind, one is to propose FLAG and demonstrate its usefulness, and the other is to provide a better understanding of how adversarial training works for GNNs in general. Given the limited novelty of FLAG compared to prior methods, the main contribution actually comes from the later part, which unfortunately is somewhat underdeveloped."
"This paper presents an interesting approach for training neural networks with a small dataset. The main idea is to train the model from the early layers to the deeper layers step-by-step, with different types of inputs (i.e., patches, cropped images, or full images) sampled from the given training set. Experimental results show great performance compared to training from scratch. <sep> Pros: <sep> This paper presents a good idea for training with small dataset. The proposed method is technical valid, and it is clear that step-wise training can help improve the performance. From my point of view, it is more like applying intermediate supervision on each layer using different types of training inputs, By doing so, we ensure that the early layers and the deeper layers are forced to learn to find the specified low-level and high-level semantics, respectively. Thus, the resultant model could behaves like the large-scale pre-trained deep CNNs we analyzed in the literature. <sep> In experiment section, the authors present the key results on two datasets showing the advantage of the proposed method. <sep> The paper is easy to understand. Also, the writing is very concise. <sep> Cons: <sep> Though this paper presents a really focused contribution on training with small dataset, one can see that the paper lacks of in-depth analysis on either the target task or the proposed algorithm. I would suggest that the authors could conduct more experiments to better validate the target task (i.e., training with small dataset). It would be great to add a transfer learning baseline (i.e., pre-trained on ImageNet, and then fine-tuned on the target dataset), and show that it does not work for your research problem. The readers could better understand the difficulty of your research problem. <sep> My another question is more related to the problem definition, or more specifically the importance of the addressed problem. (1) Why we need to deal with small datasets? If the target problem/application is important, it should be easy to enlarge the dataset at scale. For example, there is a scene classification dataset built by MIT in 2009. It is a small-scale dataset which is used in this paper. In 2015, due to the importance of the task, MIT people have scaled the dataset and the new one is called Places, which has 2.5 millions of images with scene-category labels. (2) Why it is small? There are two possible reasons I could think of: (i) it may be difficult and expensive to collect training labels due to extreme labor efforts or privacy concerns (i.e., pixel-wise labels, medical images). (ii) the applications are newly emerged so all data are sparse but surely it will be scaled up in the nearly future. But in the paper, the presented tasks, including scene classification and quality assessment, are well-established and they should not be so difficult to obtain training data. In general, I think the presented experiments are toy examples and the small-scale setting may not be convincing enough. I would suggest that the authors could include more examples and results of small-dataset scenarios, which could add values to the paper. <sep> ################################## <sep> Post-rebuttal: <sep> The idea is good, but the experiments and analysis are not enough to validate the proposed idea. The paper is not ready for a publication.","All reviewers agreed on the major shortcomings of this submission, the most important of which is that the contributions are insufficiently evaluated. There was no author response."
"Summary <sep> This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises. <sep> Pros <sep> (1) The topic is interesting and important as the P and M pathways are both crucial elements of the efficient and robust human visual system but their computational models are much less studied. <sep> (2) The proposed model is new and overall simple to implement. <sep> Cons <sep> (1) The model's design decisions are arbitrary and poorly justified. <sep> (Imitation learning) It's unclear why CoarseNet activations must mimic FineNet activations, since only FineNet activations are ultimately used for inference (not CoarseNet activations, which are further transformed and used by the FineNet). To justify the necessity of imitation learning, the authors should present full FineNet+CoarseNet results (not just Fig 3) using all 3 losses versus using only FineNet's classification loss. <sep> (Binary masks) The setting that the CoarseNet can use clean binary masks of objects as inputs doesn't make sense at all in my opinion. Segmentation itself is a complex task that often requires high-level vision so it's unclear to me why the authors assume that PRGCs can provide such information. <sep> (SMA) It's unclear why the SMA's u and v are implemented as memory buffers (that are updated per 2 epochs, which seem arbitrary and not biologically plausible either) following [Orhan, 2018], instead of end-to-end learned parameters (that are consistently updated every iteration with the rest of the network using backprop gradients). More generally, the authors should present solid reasons why association should also produce results that mimic FineNet activations. <sep> (DMA and feedback) It's unclear why an RBM is required as an additional component to introduce dynamics, since extending the feedback loop dynamically (i.e. more loops) should also be able to achieve similar results using existing components (which seems more biologically plausible in terms of resources). <sep> (2) The experimental results are weak and incomplete, and comparisons against related work are missing. <sep> (Noise robustness) As the key benefit claimed in this paper, the noise robustness of the proposed model is however weak (still roughly 20% accuracy drops for all types of noises) and only better than simplified or similar variants of the model. In addition, using only FGSM (targeting the FineNet) to generate adversarial noises doesn't fully test the robustness of the model, since more recent techniques [1, 2] can easily generate smooth adversarial examples that will likely severely affect the CoarseNet (unlike FGSM). <sep> (Related work) Although the authors argued that existing models are conceptually different, it doesn't mean that architecturally similar models [3, Hou et al.], SOTA in adversarial defense [4, 5], and most importantly other brain-inspired models, targeting robustness [6, 7] or not [8, Tang et al.], shouldn't be compared against to properly prove the value of this work. Also, as the field has started to more directly use neural data to guide better network design [8], it's unclear why the authors seem to have completely omitted this approach. <sep> (Rough-to-fine processing) The value of this approach is unclear since the accuracies of training both networks using the subclass labels (CIFAR-100) are missing. <sep> (Backward masking) Visual results alone (Fig 5 and 12) don't properly support the claim that this model ""can explain visual cognitive behaviors that involve the interplay between two pathways"". Please consider providing more detailed statistical analyses (e.g. R^2) if the authors want to make this claim. <sep> (3) The clarity needs improvement. <sep> The clarity of this paper is substandard as many key details are ambiguous or completely missing. For example, how does the SMA memory buffer store features? Random sampling over the training set? Is the model also trained on noisy data? How does a CN+SMA model (Fig 4) even work? What exactly are the backward masking stimuli used in the experiments (frame by frame)? <sep> Recommendation <sep> I recommend rejection of the paper given the following two major cons (see details above). <sep> (1) The model's design decisions are arbitrary and poorly justified. <sep> (2) The experimental results are weak and incomplete, and comparisons against related work are missing. <sep> Questions <sep> Please address the cons listed above. <sep> Additional Feedback <sep> (1) Although using an L2 loss for imitation learning is straightforward mathematically, the authors' arguments regarding how the brain may implement imitation learning aren't very convincing (Sec 3.2). For example, is there direct evidence of synchronized oscillation supporting the transfer of neural representations and thus imitation learning? <sep> (2) Speed seems to be a major potential benefit of the proposed model, which however was not clearly discussed or benchmarked. Please consider adding speed comparisons against SOTA networks in terms of inference speed. <sep> References <sep> [1] Low Frequency Adversarial Perturbation, UAI, 2019 <sep> [2] SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations, WACV, 2020 <sep> [3] U-Net: Convolutional Networks for Biomedical Image Segmentation, MICCAI, 2015 <sep> [4] Feature Denoising for Improving Adversarial Robustness, CVPR, 2019 <sep> [5] Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks, ICCV, 2019 <sep> [6] Brain-inspired Robust Vision using Convolutional Neural Networks with Feedback, NuerIPS-W, 2019 <sep> [7] Biologically Inspired Mechanisms for Adversarial Robustness, arXiv, 2020 <sep> [8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019","This paper explores a network that has a parvo (fine, detailed, slow) <sep> and magno (low-res, quick) stream. The ideas are interesting and the <sep> results intriguing, and one reviewer is in favor of acceptance. <sep> Several reviewers criticized the clarity of the paper. and the lack of <sep> details for, explanations of, and critical evaluation of, the design <sep> decisions. For example, how do the results depend on certain design <sep> decisions? I think that with a bit more work, this paper has potential to <sep> be a very impactful paper. I would encourage the authors to follow the <sep> detailed suggestions and resubmit the work to a high-impact conference or <sep> journal."
"—Summary: <sep> The authors propose a generalized neighborhood message aggregation function for GNNs. The proposed choice of generalized aggregation functions is SoftMax and PowerMean, which generalizes Max and Mean functions and interpolates them. Additionally, they propose a variant of these two methods, which can also encompass the Sum function. By making the components of these generalized aggregator functions differentiable, the GNNs can choose an approximate instantiation of the aggregation function that best optimizes the task. <sep> —— <sep> Pros: <sep> A straightforward and elegant solution <sep> The Paper is well written and is easy to follow. <sep> —— <sep> Concerns: <sep> (i) It is not clear how generalized aggregation functions to aid training deeper GCNs <sep> Also, from the experiments, the baseline, ResGCN model's performance also increases with layers and is falling short of the proposed model only by a small margin. <sep> (ii) Missing model wise results on OGB benchmark (Table 2) <sep> A detailed study is done only on one dataset, and in all other datasets, the best of the models alone is reported. <sep> For all the datasets, results for the following compared models from Table1 is required to understand the achieved performance improvement. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. <sep> - baselines: ResGCN and ResGCN+ with mean, max, sum, <sep> - Fixed simple variant: Softmax, PowerMean, Softmax-sum, PowerMean-sum <sep> - Learnt variants: Softmax-sum and PowerMean-sum <sep> (iii) Benefits of the proposed model is inconclusive: <sep> Comparison with ResGCN+ is reported only for two datasets, and out of which, one dataset, the arXiv dataset, has results reported only for one of the aggregation functions. From the other protein dataset, the avg score for Max-ResGCN+ is 85.09, Learned PowerMean, and SoftMax based models scores are 84.56 and 85.22. There is both a drop and gain in performance with the adoption of the generalized aggregation functions. Hence, it is not clear whether both the generalized functions aid in improving performance without seeing the results on other datasets for all the four or two versions of the learned generalized aggregation functions (as in Table 1.d) <sep> (iv) Need a statistical significance test report. <sep> The results are very similar among models and are not clear whether there is any significant difference in choosing one over the other. <sep> —— <sep> Questions during rebuttal: <sep> Kindly address the concerns raised above. <sep> --- Post rebuttal: <sep> I've read the author's response and there is no change in my scores. <sep> The given argument regarding better-generalized aggregation function aid in training deep GCNs is not clear and convincing. <sep> I agree that doing a detailed ablation study on a large dataset is expensive. In which case experiments on either synthetic or other smaller real-world datasets would be helpful.","One referee supports acceptance, whereas three referees lean towards rejection. All referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened. The rebuttal addresses R1's concerns about novelty and unfair comparisons, R2's concerns about computational efficiency of the methods, R3's concerns about motivation of the proposed approach and some missing baselines, and R4's concerns about motivation. However, the rebuttal does not address the reviewers' concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with SOTA. I agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring. However, after discussion, the referees agree that further work should be devoted to strengthen the contribution. I agree with their assessment and hence must reject. In particular, I would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following OGB guidelines to report the std of the results and including a proper comparison with the state of the art."
"Summary of this paper: In this work, the authors propose a method to learn to generate long-range video sequences. The general idea is starting from a prior work (Video Textures) and extending this work with a learning framework. Specifically, during training a model is used to learn the transition probability between different video segments. During inference, long-range video synthesis is achieved through iterative sampling of new video segments. To guarantee the smoothness of the transition between different segments, an existing interpolation method is used to connect these video segments in a sequential order. <sep> Pros: <sep> quality: This paper is overall easy to read. The motivation behind this work is clearly presented, i.e., to synthesize long-range video sequences. In the introduction part, the authors present the basis of their work (Video Textures) and present a comprehensive comparison with previous works. The authors also present sufficient qualitative results to demonstrate the superiority of their work. <sep> clarity: The pipeline of the proposed method is clearly presented in the method part. The general framework is very straightforward. The analysis of the quantitative and qualitative results is convincing and logical. <sep> Cons: <sep> originality: This work is more like a simple extension of the previous work (Video Textures) with limited novelty. I am confused about the difference of the basic formulation of the core  transition probabilities (Eq. 1 and 2). Are they different from the work (Video Textures) or not? It is highly recommended that the authors could present more comparison with the previous baselines in both general idea and model details. Moreover, the video interpolation is directly borrowed from previous work without further improvements, where I think is still challenging and worth to explore. The audio conditioned video synthesis part also seems like an extra module which does not influence the completeness of the whole model if not included. <sep> significance: I have carefully checked the quality of the generated video sequences, which are not so satisfying. First, these are noticeable discontinuity between sampled video segments. Second, this method seems to be example-specific, which needs retraining if fed a new video sequence. The scalability of this method is limited. Third, the video content is directly sampled from seen sequence, where the diversity is constrained to the given video.","The paper initially had mixed reviews (4,5,6). The main issues raised were: <sep> limited novelty (re-using/integrating components) [R2]; <sep> limited generalization ability since the model needs to be retrained on every video [R2, R3]; <sep> limited applicability - experiments limited to certain domain of video, while results on videos with large motion are not convincing [R2, R3]; <sep> missing ablation studies / experiments [R3, R4]. <sep> The author response partially addressed some concerns, but the main points 1-3 are still problematic. In addition, the AC noted that the technical aspect was lacking: <sep> Training with contrastive loss on a single video may likely overfit the embedding to the video, which leads to a meaningless embedding where all non-neighboring segments are orthogonal in the embedding space. While changing the softmax temperature can yield higher entropy transition probabilities, the induced probability distribution is probably highly noisy. It would be better to train this on a large video corpus, which will prevent overfitting. Also contrastive loss is typically used to build a discriminative embedding space for classification/recognition, not a smooth embedding space for generation (where distances between embedding vectors are strongly correlated to similarity). Thus some other embedding smoothness terms could be added during contrastive learning. <sep> The learning is only on the transition probabilities, while the video generation is separate. It would have been more convincing to learn the transition probabilities with the video generation process in an end-to-end manner. Perhaps a discriminator could be placed after the video generator so that the transition probabilities could be learned so as to better mimic real video. Other loss terms based on video temporal smoothness could also be added ensure smoother transitions between clips (e.g., motion consistency). <sep> The negative reviewers remained unconvinced by the author response, and the AC agreed with their concerns. Thus, the paper was recommended for rejection."
"The authors have created a well written paper for a new watermarking method that addresses an important challenge in security intellectual property rights for deep learning models. They claim their method has resistance to l2 attacks within a certifiable bound, and show experimental results that the method is also resistant to other forms of attack. The method can be used as a black-box watermark (does not require model parameters to verify),  however, the certification bounds only apply to a white-box use case in which the verification can perform inference and test accuracy for a set of trigger images for multiple smoothed versions of the parameters. <sep> Pros - <sep> The paper is well written and organized. <sep> The paper provides a useful survey of prior art and good motivation for their approach. <sep> Unlike prior methods, the method provides a resistance bound for attacks. <sep> Cons - <sep> The bound only applies to l2 attacks. <sep> The bound only applies to white-box verification. <sep> The bound is relatively small. <sep> The method reduces accuracy of the trained model. <sep> The paper does not provide any direct comparisons to other watermarking methods. <sep> The bound is based on empirical estimates which have some uncertainty, so is not actually a true bound. <sep> Cons 1, 3 can be seen as acceptable limitations given this is a step towards certifiable watermarks. Con 4 is par for the course with any watermarking scheme, although the reduction 89.3->86% accuracy for CIFAR-10 is concerning, as that much accuracy loss is a significant deterrent to use of the method and the trend from MNIST CIFAR-10 makes me wonder if larger and more realistic images may show even greater reduction in accuracy. Con 5 is of particular concern for a conference of this tier. If published metrics comparable to the experiments shown in the paper are available, these should be included for side-by-side comparison. <sep> Update: I appreciate the authors response and their hard work in preparing this submission. I also understand that comparisons to prior art are often difficult to obtain. However, I still think further comparisons are warranted to prove out the benefits of this method against other art and whether it can achieve the stated goals for more realistic datasets. The authors did not rebut many of my negative concerns. Most critically, I feel that the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper. For the limitations I mentioned in the review I am leaving my rating score unchanged, but I encourage the authors to continue to develop their approach, which shows promise.","While it's commonly acknowledged that the paper is well written, the reviews are a bit split: R3 and R1 are mildly positive/negative, respectively, R2 and R4 both voted for reject. R2 asked many questions regarding experiments, which were addressed in the details in the rebuttal. R4 raised 6 questions regarding the bound, and the authors only answered some of them in the rebuttal. R4 felt ""the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper"". Both R1 and R4 pointed out the proposed algorithm is not practical as expected, especially the results on larger scale such as ImageNet are missing. <sep> The AC cannot agree with the authors' argument that the contribution of the paper is ""a conceptual framework that it is possible to certify a watermark for neural networks"" in responding to such criticisms. It's indeed very important for this conceptual framework to be proven valuable through thorough experiments and solid comparisons."
"The authors describe an automated system for co-designing neural architectures and HW accelerators. The system is able to find the best solution under latency and chip-area constraints.  A highly parameterized (commercial) edge accelerator defines the hardware search space.  Results are compared to MnasNet, platform-aware NAS and EfficientNet. <sep> This is an interesting area and the authors demonstrate clear advantages of their approach. <sep> A claim is made that ""although previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly."". Does the work outlined in the ""co-design"" paragraph of section 2 not attempt to do this?  Some other examples that could have been discussed include: <sep> ""FPGA/DNN co-design: An efficient design methodology for IoT intelligence on the edge"", Hao et. al. DAC'19 <sep> ""Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator"", Abdelfattah et al, DAC'20 <sep> When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design https://ieeexplore.ieee.org/abstract/document/8839421 <sep> It is a little unclear why the papers by Jiang and Yang and the papers above are dismissed? It would be good to understand better how the author's NAS approach improves on these previous works? <sep> Limited information is provided regarding the architecture of the accelerator. What fraction of the HAS search space is unavilable due to restrictions imposed by the compiler? Perhaps these design points are all uninteresting? <sep> Is there anything to learn by a discussion of the good hardware configurations that were found? Are these surprising or unexpected? How do they differ from the baseline configuration? <sep> The work is interesting but the claimed contributions perhaps need to be clarrified.","This paper considers the problem of searching over the joint space of hardware and neural architectures to trade-off accuracy and latency. <sep> Reviewers raised some valid questions about the following aspects: <sep> Low technical novelty <sep> Prior work on hardware and neural architecture co-design, and closely related work are not addressed <sep> Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory) <sep> One additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints. <sep> Overall, my assessment is that the paper requires more work before it is ready for publication."
"The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as 'importance'. However, there is nothing in the formulation of this concept which requires that this is an importance and could in fact be any form of weighting. The paper evaluates the new metric - but only agains the Balanced Accuracy metric (which seems quite restrictive). <sep> Some major comments on the paper: <sep> The proposed evaluation metric appears to be to show whether machine learning approach A is actually better than machine learning approach B. As such one can use the metric to give a value which can be used to compare different approaches. However, in order to judge if a particular approach is better or worse than another you need some way of showing that your metric is correct. The paper lacks a demonstration that your approach actually does give a more appropriate ordering of the machine learning approaches. <sep> In the abstract you talk about 'importance', however, this concept is not clear at this point. Later in the introduction you explain what importance refers to. Some of this in the abstract would help. The abstract also seems to terse making it difficult to follow. <sep> The second paragraph in the introduction seems odd. You are claiming in the previous paragraph that importance need not be the inverse of rarity, however, the example in the second paragraph seems to re-enforce the idea of importance being the inverse of rarity. The text on Uber also doesn't seem to fit the paper at all and could easily be dropped. <sep> Figure 1a needs more discussion in the text. <sep> Paragraph starting 'Let us illustrate the problem' - this talks about a dataset in the most abstract sense. It would seem this is potentially a concocted example without a real dataset behind it. It would be much better to indicate what dataset this is based on - perhaps detailing the scenario in the appendix. <sep> The related work on evaluation metrics seems a little short. How do the other proposed approaches compare to your work? <sep> ""As we discussed with examples in previous sections, in many real-world classification problems,"" - you only give one example and the 'real-world' case that this refers to is not provided. Stronger evidence is needed to support this statement. <sep> Most sentiment analysis approaches are not based on RNNs - can you justify why you used this approach? <sep> The conclusions are rather short and say little. You also claim in the introduction that you will discuss future directions but don't. <sep> Some more minor comments: <sep> If a majority is the larges group, what is the 'greatest majority'? Surely it's just the majority? <sep> 'adopt' -> 'adapt' <sep> There is no punctuation around the equations - For example there should be a full-stop after equations 1 and 2. <sep> As only equation 6 is referred to in the text why are the others numbered? <sep> Something going odd in the quotes in "" not inflated due to high-frequency classes' results dominating over the others'. "" <sep> ""must be rewarded higher scores"" -> ""must be rewarded with higher scores"" <sep> Is equation 7 really needed?","This paper proposes a weighted balanced accuracy metric to evaluate the performance of imbalanced multiclass classification. The metric is based on a one-versus-all decomposition from multi-class to binary, and then aggregating the metrics on the binary classification sub-problems in a weighted manner. The authors hope to argue that the new metric is more flexible for evaluating classifiers in the imbalanced and importance-varying setting. <sep> The reviewers agree that the proposed framework is simple and applicable to an important problem. Somehow the novelty and significance of the work is pretty limited, as many related metrics (e.g. micro/macro-averaged metrics) exist in the literature. The authors are encouraged to think about stronger reasoning on how useful the ""new"" metric is. The experiments are also not convincing nor complete enough to verify the benefits of the proposed metric."
"Response to authors: After reading the authors' response, I have decided to maintain my original rating. The authors have not adequately addressed my main concerns. <sep> Novelty: The work here, as indicated by the authors, is largely an incremental improvement over an existing work MINE. The authors' response did not alleviate this concern and in fact reinforced it. <sep> Citations and comparisons to other work: The authors did not agree to even include citations to important literature in this area. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions. <sep> Theoretical work: The authors simply pointed to the theoretical work for MINE. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). More theoretical work is needed in this area to justify the use of these estimators over others. <sep> Some responses to other comments that may help the authors with further revisions: <sep> (1) The presentation of MINE should occur in the main paper as this is crucial for understanding the paper. <sep> (2) This was not clear. Perhaps the authors could include similar pointers in the paper with each of these issues. <sep> (3) The bias I'm referring to here is the actual statistical bias of the estimator. From Theorem 6, it seems that the drift problem does seem to create some bias but it would be useful to quantify that, which could then lead to a bias correction approach. <sep> (7) The way this is currently worded, it sounds like you are saying that training with a larger batch size is bad. This part should be clarified to avoid this. <sep> Original Review: <sep> This paper presents a modified version of a neural network-based MI estimator. They investigate a few of the issues of this specific estimator and propose a regularization to help with one of them. MI estimation is an important and difficult topic. Improvements in this area are of definite interest. <sep> Pros: <sep> The paper appears to be technically correct. The experiments are somewhat supportive of including the regularization, especially when the MI is higher which is a known issue with some MI estimators. <sep> Cons: <sep> There are some interesting ideas here but the paper feels unpolished. The presentation of the ideas is somewhat unconventional. Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them. The paper could benefit from a bit more focus in this regard. In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement. But it's not clear how much of a problem this drift really is. The authors show that it causes a bias but they do not present how much bias it adds. <sep> In addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons (see the references below for some examples, which all have strong theoretical results). <sep> The theoretical work is also weak with regards to the convergence rates of the proposed estimator as well. While empirical results can confirm that an estimator can be useful in practice, they are easy to cherry-pick and ultimately theoretical guarantees are needed to know an estimator's general performance. Thus the results would be a lot stronger if convergence rate guarantees were given. <sep> Other comments/questions: <sep> The authors should define the MINE estimator in this paper. <sep> The second bullet point on page 2 says that ""training with larger batch size reduces the variance of the MI estimate"". Isn't this a good thing? That would lead to better convergence. <sep> In Section 3.1 the notation is technically incorrect. Instead of stating I(X;X) it should be written as I(X1;X2) where X1 and X2 are i.i.d. The former suggests that you're comparing the same random variables. <sep> On page 4, it's suggested that joint samples are sparse with reduced sample size. Why aren't joint samples simply included together during training? <sep> Does regular L2 regularization help with the drift problem? <sep> [R1] Moon et al.""Ensemble estimation of mutual information,"" ISIT, 2017. <sep> [R2] Moon et al., ""Information theoretic structure learning with confidence,"" ICASSP, 2017. <sep> [R3] Moon et al., ""Ensemble Estimation of Information Divergence,"" Entropy, 2018. <sep> [R4] Singh and Poczos, ""Exponential concentration of a density functional estimator,"" NeurIPS, 2014. <sep> [R5] Kandasamy et al., ""Nonparametric von Mises estimators for entropies, divergences, and mutual informations,"" NeurIPS. 2015.","This paper is a study in optimizing the Donsker-Varadhan lower bound on mutual information focusing on a ""drift"" problem. The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together. They propose a fix for this problem. The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance. The paper does not address the variance (convergence) issues with the DV bound. <sep> We have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection. Other reviews are not very enthusiastic. I will side with rejection."
"Summary: <sep> This paper discusses an approach to augment a medical imaging dataset using images from another modality. The images in the other (i.e. source) modality should have been originally collected & labeled to perform the same discrimination or regression task as the target modality. A network consisting of a prediction network for the mixture of augmented and target images & another network based on CycleGAN for the image translation network are jointly optimized using end to end training. <sep> Reasons for score: <sep> The objective of the paper (i.e. addressing small training set size in medical imaging) is quite important, and the approach is interesting. However, the evaluation, discussion, and generalizability of the approach to other tasks require additional clarification, simulations, and extra information. <sep> Detailed review: <sep> Is physiological age in the clinic estimated from a single slice in a CT volume, or an entire volume? If latter, the entire evaluation strategy should change to include the set of images from the volume rather than single images. Otherwise, the task selected will not have any bearing on the actual clinical task of estimation of physiological age. This would also impact the approach which uses a 2D image to image translation, as it will likely mean that a 3d to 3d image translation should be used. <sep> The authors have not provided sufficient information about the distribution of attributes of the different datasets. This information is critical in assessing the generalizability of results, as well as whether the experiments were set up in a meaningful way. <sep> a) What is the distribution of actual age in the different datasets from the different modalities? <sep> b) What are the image acquisition / reconstruction attributes of source/target datasets? Images from the same patient will look very different and contain varying degrees of anatomical detail depending on slice thickness, dose in CT & imaging sequence in MRI, reconstruction algorithm, etc. When doing image to image translation, does e.g. slice thickness across the modalities need to match each other? <sep> In medical imaging we often have severe class imbalance, where disease positive samples are much more rare compared with disease negative samples. Given that datasets from different modalities will likely have different ratios of positive to negative samples, how will this affect the overall training strategy? E.g. would the authors only augment the positives in this way , or both positives & negatives? Otherwise, if images from source modality have different class balance than the one in target? <sep> The proposed approach has been described for a medical imaging task that involves macro anatomical features only. It is therefore not clear whether this approach would generalize to a task that pertains to more micro features (e.g. tumor classification/detection, or tumor segmentation). Based on Fig2, it appears that the MRI images converted to CT contain obvious anatomical inaccuracies (e.g. Fig 4 shows consistently thick skulls in the generated CT images, which would affect a brain volume estimation task). Have the authors used this approach for other tasks that require more micro level anatomical precision? If not, this should be explicitly stated as a shortcoming of the current approach. <sep> The selection of hyper-parameters, experimental setups, and split of data into train/test using different approaches requires more explanation: <sep> a) It is not clear how the lambda values were selected. Also, for eq 3 the authors state that a lambda of 0.001 was used, which seems to severely favor the GAN loss rather than the prediction loss. Given that the prediction task is the more important among the two, no justification has been provided on why the weight of the corresponding loss value would be so small compared to the GAN loss. <sep> b) What was the stopping criteria for the different scenarios in Table 2? In the Appendix, the authors have the number of training epochs for each approach (which is different for different experiments), but it is not clear what determined the end of training. This is important, since all approaches should be trained using the same rule to ensure a fair comparison. <sep> c) On p5, the authors state that they randomly select images from the CT cases to split the data into train/val/test buckets; Does this mean that images from same patient are mixed into both train/test? This would not be appropriate, since the different slices of CT data from the same patient share anatomical similarity, which means that the train/test data are likely cross-contaminated. <sep> d) In Table 2, what error is being shown? Is it l1 norm, l2 norm, or absolute error of predicted age relative to annotated age? <sep> e) What is the actual loss function in eq 2? The loss for L_adv has been described on p6 but not the loss in eq2. Also, predicting age is a regression task, and not a discrimination task. Why is eq2 described as showing a discrimination task? <sep> In Table 3, why is the cyclegan result  significantly worse than other methods for MRI (it actually deteriorates the performance compared to baseline)? On the other hand for PET, cyclegan is better than other methods; The opposite trend is happening for domain adaptation (i.e. worse for PET and better for MRI). Is this related to the amount of training data? Otherwise, this would seem to indicate that perhaps cyclegan did not train properly due to implementation issues, etc. <sep> In Figs2-4, visual comparison of the translated images to real CT images have been provided. However, since unpaired data was used, it is not clear how anatomically correct the translated images are. It would be best if for a small number of samples that have paired data (i.e. patient was imaged using both CT and MR, or CT and PET), the authors show a comparison of the translated images from MR or PET compared to the actual patient images in CT. Such a comparison would show beyond doubt that the translated images are anatomically correct or not.","The paper proposes a method for data augmentation by cross-modal data generation. While the reviewers agree that the paper addresses a relevant and important problem in medical imaging, they also agree on that the paper has limited novelty over the state of the art. Also the setup of experimental validation to comparison methods is questioned."
"The paper introduces an algorithm for mitigating disparate impact of private learning (DP-SGD) on different groups of a given population. In each iteration of DP-SGD, instead of using a uniform gradient clipping threshold for all groups, the proposed Fair DP-SGD algorithm uses an optimal clipping threshold (one that minimizes the bias-variance tradeoff) for each group separately. The authors include experimental results to show how well their algorithm performs compared to state-of-the-art algorithms. <sep> While the problem considered by the authors is very interesting and has impact on real world, I recommend rejection. The major concern I have with this work is that it lacks a formal (differential) privacy statement. I am not even entirely sure that the proposed algorithm is actually differentially private because the step that finds the optimal clipping thresholds seems to use the non-noisy mini-batch gradients without any privatization (please clarify if my understanding is not correct). In any case, if the proposed algorithm is claimed to be (epsilon, delta)-DP then there must be a rigorous proof for it. Also, in the experiments I don't see any reported values for epsilon? Are different methods compared with the same value of epsilon? <sep> Other comments: <sep> -In addition to a formal privacy statement, the authors should formally define the notions of ""privacy"" and ""fairness"" that they use in the paper. Overall, I believe this work can have a better formalization. <sep> -As mentioned earlier, I cannot find the values of epsilon in the experiments. The authors could for e.g. use moments accountant to find the total privacy loss in their experiments. <sep> -When the model is logistic regression (which is the adopted model for 2/3 of the datasets in the experiments) and if the input data is normalized, then the Lipschitz constant L of the (logistic) loss function is a small constant. So in this case clipping the gradients is not necessary because the norm of gradients is always bounded by the Lipschitz constant L which is small and the added noise can be calibrated to L. I think in the case of logistic regression, the authors should also compare their method with a private SGD algorithm that simply adds noise with scale ~ L without any clipping. <sep> -I'm not sure if I understand the optimization problem given in 6a, 6b and how the algorithm is solving it. In particular, the constraint set of the problem seems to be all models with optimal risk (absent any fairness, privacy). But are you actually solving this problem? I.e., does the model output by the algorithm fall into this constraint set? <sep> -The gradients are sometimes denoted by g^t := \\nabla L and other times by \\nabla f (see for e.g. section 2.2). Is f the same as the loss function L? It would be better if a consistent notation was picked for gradients.","This paper proposes an algorithm to address the disparate effect that DP has on the accuracy of minority/low-frequency sub-populations. Unfortunately the work does not actually guarantee or analyze the resulting privacy guarantees. In particular it may provide much worse privacy (or no privacy at all) to the minority subpopulation. <sep> The paper also calls their algorithm ""fair"" without using an accepted term or a careful discussion of what an algorithm needs to satisfy to be considered ""fair"". Using a more technical term such ""reducing the accuracy disparity"" would make much more sense."
"This paper addresses the problem of training GNNs in a distributed environment (e.g., with multiple machines communicated through a network). In such a setting, the training of GNNs for node classification problems substantially differs from the training of other neural networks, because if the graph and node data are partitioned and distributed across machines, the data held by each machine may not be enough to compute a local gradient. <sep> The proposed solution is simple: augment the data held by each machine in order that they are enough for computing the local gradient. Specifically, the augmented data are nodes that do not belong to the current partition but are within multi-hop neighborhoods of the nodes in this partition. If the augmented data do not all fit in memory (GPU memory in the authors' work), sample the neighborhoods. <sep> This idea is a practical and effective solution. However, the authors' work leaves a lot to be desired. In what follows are questions and suggested work that may help enrich the contribution. <sep> Section 2.2. It is not entirely clear what ""approximated graph"" means. Did the authors reduce the training set, cut the graph to a smaller one (but keep the training set), sample multi-hop neighbors for each node in the training set, or do something else? If the approximation means using a smaller training set (which is suggested by the caption of Figure 3), this is not the same as sampling neighbors but keeping the training set. In other words, how does the motivation suggested by Figure 3 support the proposed method? <sep> Section 3.1. Why breadth-first and uniform sampling? How about nonuniform sampling (e.g., according to node degrees)? How about deterministic sampling (also according to node degrees but done in a deterministic order)? <sep> Section 3.1. Evenly spreading the overlap across partitions appears too naive. Since the augmentation is done only once at the beginning and does not involve repeated communication, why not adding nodes as many as needed, regardless which partition they come from? <sep> Section 3 overall. An important point the authors have neglected is load balancing. Since the proposed distributed training is synchronous, load imbalance may be a critical issue. Have the authors considered a more elegant augmentation that encourages load balance, in the sense that each partition may include a different number of additional nodes? <sep> Section 5.2. The authors mention that the observed speedup (with respect to time) is restricted by ethernet connection, which offers a much smaller bandwidth than infiniband. However, network bandwidth is not the only factor that affects speedup (for example, load imbalance may play a role also). It is important to understand how much each factor contributes. The authors are encouraged to conduct a thorough investigation, convey findings, and design mitigations. <sep> Figure 5. What does Figure 5 look like if the horizontal axis is epoch? Figure 6 in the appendix partly answers the question, but the ""single machine"" baseline is missing therein. <sep> Section 5.2. The authors state ""KW-GCN is less resilient to input approximations than GraphSAGE is."" Can the authors articulate the reason? <sep> Section 5.3. What happens if the GNN has more than two layers? Is the proposed method still effective, facing a much larger neighborhood? How would prediction accuracy be affected? The authors are encouraged to conduct investigations. <sep> Section 5.4. The local time complexity per machine is not the most important factor in distributed computing. Load imbalance, communication, and synchronization costs should all be factored in. <sep> Additional question to consider for enriching contribution: When a vast amount of computing resources is available (e.g., 100 machines), do the authors recommend using them all, using as few as possible, or somewhere in between? What are the trade offs between overlap size, communication/synchronization cost, and monetary cost?","The paper proposes a new distributed training method for graph convolutional networks, using subgraph approximation. The reviewers raised multiple challenges, such as novelty, validity of experiments, and some technical issues. The authors did not respond to the reviewers' comments. The AC agreed with the reviewers that the paper, in the current form, is not ready for publication."
"The submission proposes a meta-learning algorithm attuned to the hierarchical structure of a dataset of tasks. Hierarchy is enforced in a set of synthetically-generated regression tasks via the data-sampling procedure, which is modified from the task-sampling procedure of [1] to include an additional source of randomness corresponding to which of a set of cluster components task parameters are generated from. The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; if tasks belong to the same cluster, the correspond task-parameters receive the same update at that step (in particular, the update direction is averaged). It is assumed that there are increasingly many clusters at each step, so that task-specific parameter updates are increasingly granular. <sep> Strengths: <sep> Clarity: The experimental setting and exactly how the data-generating process relates to the proposed algorithms are clearly described. <sep> Significance: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; that it learns more efficiently than a MAML in terms of the cumulative number of datapoints observed; and that both MAML and {Fixed|Learned}Tree MAML outperform a naive baseline. <sep> Weaknesses: <sep> Significance: Since the evidence provided in favor of the proposed algorithm is in the form of an empirical evaluation on a synthetically generated dataset, the present impact of the algorithm is limited. In particular, there is no evidence that (i) the algorithm works for larger and/or more complex datasets; and (ii) that natural datasets of interest to the community exhibit a hierarchical structure analogous to the synthetic datasets presented in the submission. <sep> Novelty: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3]. <sep> Clarity: Specific details surrounding the relationship between Algorithm  1 and Algorithm 2 are insufficiently discussed: <sep> i) Algorithm 2 as it appears in the text is very similar to Algorithm 1 (The OTD algorithm) in [2] with the exception of the new hyperparameter ξ, and introduces new symbols that do not appear elsewhere in the text. It is therefore not sufficiently adapted for clarity in the context of this work. <sep> ii) Whether Algorithm 2 acts as a strict subroutine of Algorithm 2 is not stated. I believe it is not because the clustering decision for a new task relies on tree structures that are ""generated for a training batch,"" although what a ""training batch"" refers to is not clear. Similarly, how the ""online""/""offline"" distinction in the context of the clustering algorithm fits into the training/evaluating setup borrowed from [1] is not made clear. <sep> iii) How exactly the task-similarity approach of [3] is employed in Algorithm 2 is not made clear. The only mention of the use of [3] is briefly around Eq. (8) before the main algorithm (Algorithm 1) is introduced, and Algorithm 2 only refers to a generic ""similarity metric"" (as in the original work, [1]). <sep> References <sep> [1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" arXiv preprint arXiv:1703.03400 (2017). <sep> [2] Menon, Aditya Krishna, Anand Rajagopalan, Baris Sumengen, Gui Citovsky, Qin Cao, and Sanjiv Kumar. ""Online Hierarchical Clustering Approximations."" arXiv preprint arXiv:1909.09667 (2019). <sep> [3] Achille, Alessandro, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. ""Task2vec: Task embedding for meta-learning."" In Proceedings of the IEEE International Conference on Computer Vision, pp. 6430-6439. 2019.","The paper proposes a variant of MAML for meta-learning on tasks with a hierarchical tree structure. The proposed algorithm is evaluated on synthetic datasets, and it compares favorably to MAML. The reviewers identified several significant weaknesses, including: (1) the experimental evaluation is limited, and it only includes small synthetic datasets; (2) the proposed algorithm is incremental over MAML. The reviewers agreed that the paper cannot be accepted in its current form. I recommend reject."
"This paper tackles a very important and under-studied problem: reducing the cost of training NLP models. The authors present a method that builds on the lottery ticket hypothesis (LTH). The authors first identify redundant structures early during training, then prune these structures, which leads to faster training. The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance. <sep> Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered. I cannot recommend accepting this paper in its current form, but am looking forward to reading the authors' response which might clarify things. <sep> Detailed comments: <sep> Training (especially pretraining) costs have been going wild in AI and NLP more particularly, which leads to large costs ([1]) as well as potential environmental problems ([2]). Reducing these costs could have a very high impact on the field, allowing many more researchers to participate in state-of-the-art research [3]. <sep> As a result, this paper has a great potential, and its results seem very promising. <sep> Nonetheless, the current paper leaves too many open questions regarding the validity of the experiments. <sep> First, much of the improvement (I think) comes from reducing the number of epochs and/or the number of steps. For fine-tuning, the authors run their model for 2.2 epochs, while their baseline model runs for 3 epochs, roughly 30% more which accounts for much of the reduction observed in Table 2. Similarly, for pretraining, the model runs 80% of the training steps (20% reduction), which accounts much of the training time reduction reported on section 4.3. Running a baseline model that runs for the same amount of time is essential to appreciate the contribution of this work (e.g., repeat the same analysis in Figure 3 for the vanilla BERT). <sep> Second, the authors argue for a large reduction in runtime, but are very cryptic about how they actually measure this reduction (footnote 2), while reporting number in ranges of 5%. As the main contribution of this paper is the increased efficiency of the proposed approach, it must be clear how efficiency is measured. Finally, writing in general can be made clearer: <sep> Last sentence of intro: ""without scarifying accuracy"" seems like an inaccurate description of the results presented in this paper. around 1% might be reasonable for 30-40% reduction in training time, but it is certainly a reduction in accuracy. <sep> Some description of Network sliming would help <sep> The term ""intermediate neurons"" (section 3.2) was unclear to me. <sep> Section 3.3: how is mask difference defined? <sep> Figure 1 was unclear to me. What do the axis represent? The authors say ""the axes in the plots are the number of training steps finished."" so why do you need two of them? <sep> The ""Non-trivial Sub-network"" paragraph feels like it should be part of the Experiments section. <sep> Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well? <sep> ""Since we observe that the randomly pruned models do not competitive performance ..."": how uncompetitive? I would have liked to see these results (also, please fix grammar in this sentence) <sep> ""Reducing it to 80% seems to be a sweet point with the best balance between performance and efficiency."" -> I would disagree. The graph indicates that for MNLI and QNLI 60% seems like a better choice. <sep> Questions: <sep> ""We observe empirically that if pruned globally, the attention heads in some layers may be completely removed, making the network un-trainable."": in this case, couldn't the authors remove a full layer? <sep> The difference in the ablation results seem quite small (tables 3 and 4). Are they statistically significant? <sep> Minor: <sep> Missing period at the end of the first paragraph in the related work section. <sep> Second paragraph of related work: McCarley et al. (2019) appears twice with different descriptions, is this intentional? <sep> The authors say ""we focus on larger datasets from GLUE (MNLI, QNLI, QQP and SST-2), as it is less meaningful to discuss efficient training"", but then report and analyze results from other GLUE datasets as well. <sep> ""Hon downstream tasks with smaller learning rate"" -> Do you mean smaller datasets? <sep> References: <sep> [1] Sharir, O., Peleg, B., and Shoham, Y. (2020). The cost of training NLP models: A concise overview. arXiv:2004.08900. <sep> [2] Strubell, E., Ganesh, A., and McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. In Proc. of ACL. <sep> [3] Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. (2019). Green AI. arXiv:1907.10597.","It is important to develop efficient training methods for BERT like models since they have been widely used in real-world natural language processing tasks. The proposed approach is interesting. It speeds up BERT training via identifying lottery tickets in the early stage of training. We agree with the authors's rebuttal that autoML is not that related to the work here. Our main concern on this work is its worse-than-BERT performance showed in Table 2. The performance gap is significant. Sufficiently more training steps would fill the performance gap but the proposed method may have no advantage any more over the normal training procedures. To make this work more convincing, we would like to suggest to include experiments on comparing different methods under similar prediction performance. In addition, since the main claim of this work is for training efficiency, it will be helpful to show the advantage of this method by directly presenting the training curves/ results of different methods. Overall this paper is pretty much on the boundary. We encourage the authors to resubmit this work once these issues are well resolved."
"This paper presents a deeply supervised few-shot learning model via ensemble achieving state-of-the-art performance on mini-ImageNet and tiredImageNet. The authors first studied the classification accuracy on mini-Image across convolutional layers and found the network could perform well even in the middle layer. Therefore, they added classification headers on the selected layers, so that these layers can directly output predictions. The final result is the ensemble of all the select layer predictions, called the Multiple Representation Emsemble. To improve the result, they further average the results of two models with different network backbones, called Multi-Model Emsemble. The results show this method can achieve state-of-the-art performance on the two datasets. <sep> Advantage: <sep> The motivation and idea in this paper are clear and simple, so the reader is easy to understand it. <sep> Figures 2 and 3 are nice, which are clearly demonstrate the motivation and algorithm. <sep> The find in Figure 2(a) is very interesting. The middle layer has a better representation than the end on the few-shot image classification task. <sep> The results are positive. <sep> Disadvantage: <sep> The idea in the paper is not very novel. The main contribution of this paper is doing a deep supervision ensemble. However, people have studied deep supervision learning for a while on image classification [1], segmentation [2], and depth estimation [3]. Specifically, [2] [3] also fuse the multi-layers' outputs. <sep> The authors only show the ensemble results via averaging scores over the models. It will be good to study more ensemble methods. For example, the deep layer has higher accuracy than the shallow layer. Is it possible to assign a different ensemble weight for each layer based on the accuracy? <sep> In figure 2(a), why the middle layer performs better than the last layer? It will be good to show some analysis? <sep> In table 1, since the proposed model has done a model ensemble, it cannot directly compare with CAN and CTM. Should add the result without ensemble in table 1. If I put the third-row result ""64.03"" in table 2 to table 1, the improvement would be marginal. <sep> Both mini-ImageNet and tired-ImageNet are the subsets of ImageNet. To verify the generalization, it will be good to add CIFAR, meta-iNat [4], or CUB [5] results. <sep> Minor mistakes, <sep> Equation 1, should add the superscript n to r. <sep> Figure 1, the characters are not evenly spaced. <sep> Figure 2 (a), the axis label is too small. <sep> In section 4.1, the sentence ""The model can be pre-trained ......Dtrain or Dval.)"" is redundant, which is common sense. <sep> In section 5.1, ""After pre-training, we added shift and scaling parameters for the convolutional layers in the encoder and trained the parameters by the MTL approach used in"". Might add more details about the shift and scale, so that the reader does not have to read another paper. <sep> Table 1, the standard deviations in ""our"" results are not aligned. <sep> ----- post rebuttal ---- <sep> The authors haven't addressed my questions. I would keep my score unchanged. <sep> One more comment: I suggest the authors compare to a related baseline SimpleShot [6] that is arguably less complicated. <sep> Overall, given that the novelty and improvement are minor, I think this paper might be not ready at this time. <sep> [1] Lee, Chen-Yu, et al. ""Deeply-supervised nets."" Artificial intelligence and statistics. 2015. <sep> [2] Xie, Saining, and Zhuowen Tu. ""Holistically-nested edge detection."" Proceedings of the IEEE international conference on computer vision. 2015. <sep> [3] Chang, Jia-Ren, and Yong-Sheng Chen. ""Pyramid stereo matching network."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. <sep> [4] Wertheimer, Davis, and Bharath Hariharan. ""Few-shot learning with localization in realistic settings."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. <sep> [5] Wah, Catherine, et al. ""The caltech-ucsd birds-200-2011 dataset."" (2011). <sep> [6] Wang, Yan, et al. ""Simpleshot: Revisiting nearest-neighbor classification for few-shot learning."" arXiv preprint arXiv:1911.04623 (2019).","This paper introduces an ensemble method to few-shot learning. <sep> Although the introduced method yields competitive results, it is fair to say it is more complicated than much simpler algorithms and does not necessarily perform better. Given that ensembling for few-shot learning has been around for a while, it is not clear that this paper will have a significant audience at *CONF*. <sep> Sorry about the bad news, <sep> AC."
"This paper studies gradient descent dynamics of two-layer neural networks with ReLU activation function. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. The paper proposes that training happens in two steps: The first phase resembles the learning of a random feature model (RFM) where only the output layer coefficients are changed. The second phase can activate or deactivate neurons and decreases coefficients to favor sparse solutions if the target function admits a sparse solution. The dynamics are further compared the setting of mean-field scaling, which shows a different learning dynamic. <sep> Pros: <sep> The paper presents experiments on dynamics of gradient descent with the goal to give new insight on the important and unsolved phenomenon of implicit bias observed in deep neural networks. <sep> By starting in a (simplified) controlled setting, a large scope of experiments is conducted that studies different aspects and different training regimes. The setting and the experiments are all well-described and clearly presented. Interesting behavior of the learning dynamics can therefore be shown. <sep> With the largely overparameterized setting fairly well-understood, the current paper approaches the medium-overparameterized regime where the learning dynamics are more complicated and more close to neural networks in practice. Therefore, the paper targets an important knowledge gap. <sep> While some interesting observations can be made, I consider the presented results and gained knowledge as limited in scope and hence slightly tend to suggest to reject. <sep> Cons: <sep> In particular, no clear conclusion can be made from the results. In fact, there is no conjecture of how the results may manifest in practical neural networks. (In case I missed such a conjecture, then I wonder why the conjecture was not tested in a practical setting.) <sep> The paper is purely experimental. Since it considers largely simplified settings (specific target functions, data sampled uniformly from the unit sphere, in most experiments the true solution can be found with a sub-network consisting of only one hidden neuron) and since the networks and optimization method (gradient descent) are rather simple, one could expect that at least some theoretical contribution or explanation could be given, which the paper lacks entirely. <sep> While there is a large scope of experiments conducted, some results are only partial and draw conclusions without further investigations. It is hard to tell from the presented results how characteristic they are for other settings or behave under small changes such as the input dimension, target function, loss function, etc. <sep> Further more detailed remarks on the specific contributions and the presentation are below. <sep> For the author's reply, I would appreciate a clarification of the exact contributions summarizing the findings and its possible implications for the training of practical neural networks. <sep> Comments on the specific contributions: <sep> The first contribution points out the existence two phases. In the first phase, the weights of the first layer do not change and the model behaves like a random feature model. This phenomenon seems to be limited to a few iterations where it is an almost trivial observation considering that the second-layer weights are initialized at zero (which implies a vanishing gradient for first layer weights). <sep> The third mentioned contribution is peculiar since the only change is a factor of 1/m to the network function, which is equivalent (for the considered squared loss) to a scaling of the target function. The stated contribution is therefore that this scaling changes the observed dynamics significantly, which casts doubt when the observations generalize to other settings. <sep> The observed behavior aims to explain the implicit bias of neural networks. However, this behavior can also be observed in overparamterized networks and the observed dynamics differ in the considered regimes of under-and overparameterized networks. This poses the question in which way the implicit bias is explained by the observations. <sep> The paper suffers from its presentation. The list of contributions mentions an observed transition (what kind of transition?) and consider undefined terminology of quenched neurons. The terminology of quenched neurons is only loosely explained much later on page 4 as neurons that are ""quenched"" in the sense that their outer layer coefficients eventually start decreasing and then keep decreasing. (Shall quenching neurons have coefficients that converge to zero? If neurons can still pop up at a later stage, are they still considered quenched neurons? Are neurons with constant output layer coefficients but norm-decreasing inner layer coefficients also quenching (this difference cannot be distinguished in the plots)? It would be good to make precise what the authors mean by quenched neurons. <sep> More detailed remarks: <sep> First line page 3: citation seems misplaced, as I was unable to find the result in the paper. Please update reference or show a proof. <sep> How many iterations does the first phase consist of where the model neural network behaves like a RFM? <sep> Is it correct that the two phases in the loss development can not consistently be matched with changes in the parameters? <sep> Is it correct that in the most realistic setting 3.1.2, there is barely a quenching behavior visible? <sep> I was wondering about the notion of overparameterization. Why would a network be underparameterized just because the width is small even if the target function can be learned with a single neuron?It sounds reasonable to consider the setting of training polynomials of maximal degree 30 to learn a linear function as an overparameterized problem, no matter how many samples are considered for training. <sep> The consideration to compare m being proportional to n against m being proportional to n/(d+1) has little meaning without fixing or comparing the respective constants (m=10^9n/(d+1) vs m=10^-9n also satisfies the stated proportionality), and most importantly without experimenting with different dimensions d. Since it is not investigated how the behavior changes with changes of d, there cannot be made any conclusion about the different regimes. Also the consistency claim in 4.2 is meaningless when changing the constants. It would be consistent if in 4.1 one would consider m=n and m=n/(d+1) instead of these terms scaled by constants (For a single d, suitable constants can always be found.) If no experiments for changing input dimension are carried out, then all we need to care about is the quotient m/n. <sep> Typos: <sep> Figure 6: legend not entirely visible <sep> Conclusion: „the the""","This paper empirically investigates the gradient dynamic of two-layer network nets with ReLU activations on synthetic datasets under L2 loss. The empirical results show that for a specific type of initialization and less overparametrized neural nets, the gradient dynamics experience two phases: a phase that follows the random features model where all the neurons are quenched and another phase where there are a few activated neurons. As pointed out by Reviewer 1, this paper lacks mathematical support and did not distinguish between random features model and neural tangent model. Reviewer 3 and Reviewer 4 also complained that the paper is purely experimental. Therefore, this paper may benefit from proposing an at least heuristic or high-level conjecture/interpretation/argument that tries to explain the empirical results."
"=== Summary <sep> This paper proposes a benchmark that aims to systematically evaluate models' ability in learning representations of high-level variables as well as causal structures among them. The authors introduce two benchmarking RL environments: <sep> One is in a physical domain where an agent is pushing blocks of different weights. <sep> Another one is to simulate a chemistry environment, where the state of an element can cause changes to another variable's state according to the underlying causal graph. <sep> The authors evaluate several representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning. <sep> === Strengths <sep> This paper targets an important problem of assessing models' ability to automate the inference and identification of the causal variables from high-dimensional inputs like images. <sep> The construction of the benchmark allows building causal graphs with varying complexity, such as the size of the graph, the sparsity of the graph, and the length of cause-effect chains. <sep> The authors have evaluated several baseline models on the benchmark, including two typical monolithic models (autoencoders and variational autoencoders) and two models with explicit structure: graph neural networks (GNNs) and modular networks. <sep> They have made several interesting observations, e.g., modular networks hold better scalability than other baselines, suggesting the benefits that explicit structure and modularity bring for causal induction in MBRL. <sep> === Weaknesses <sep> My primary concern of this paper is that the dataset is a bit too contrived, which makes it hard to know whether the observations from this benchmark can generalize to more complicated real-world scenarios. <sep> For example, in the Physics Environment proposed in this paper, only heavier objects can push lighter ones, not the other way around. I understand the authors' desire to make the underlying graph a directed acyclic graph (DAG), but it does not reflect what will happen in the real world. One could imagine sliding a lighter object to collide with a heavier one; the motions of both objects are likely to change, where the interaction between them is bi-directional. <sep> Also, in the Chemistry Environment, a few objects are connected by a randomly generated DAG, where interventions can change the color of the target and the subsequent blocks. In chemistry, a molecule is a group of atoms held together by chemical bonds, which are also bi-directional relationships. Are there any specific examples in chemistry where the graph is a DAG? I would appreciate it if the authors can elaborate on the connection between the design of the environment and ""chemistry."" <sep> In terms of the difficulty of the tasks, the results shown in Figure 6 suggest that the Greedy algorithm can achieve a near-perfect performance on the tasks, which consistently outperforms all other baselines. If a simple greedy algorithm can solve the tasks, does it mean that the benchmark may be a bit too simple, where a good understanding of the underlying causal structure may not be necessary? It would be better if the authors can discuss the necessity of causal induction in these tasks, and how is the ability to perform causal inference correlate with the metrics used in the benchmark. <sep> Overall, I feel the environments proposed in the paper are a bit too artificial, which does not reflect what's likely to happen in the real world. While I like the goal of this paper, I think a set of more realistic environments could greatly improve the significance and potential impact of this paper. <sep> === Other comments <sep> The font size of the image caption may be a bit too small. <sep> Typo: Section 2.1, ""Impotant"" --> ""Important"" <sep> === Post rebuttal <sep> The authors' rebuttal addressed some of my concerns, but my primary concern still remains that that benchmark may be a bit too contrived, where the observations made in this paper may not generalize to more complicated real-world situations. The authors also made some far-fetched arguments in the rebuttal by claiming some concurrent works [1, 2] as ""the 'real-world' version of the environments used in the paper,"" which, to be honest, further lowers the rating of the paper on my side: why is this paper worthy of acceptance if there exist more realistic benchmarks? <sep> I also agree with R1 and R3 that there are no new methods proposed in the paper, and the insights derived from benchmarking a set of existing methods may not be considered novel from the point of view of the *CONF* audience. As a result, I keep my rating the same. <sep> [1] Physically Embedded Planning Problems: New Challenges for Reinforcement Learning, https://arxiv.org/abs/2009.05524 <sep> [2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296","This paper proposes a suite of benchmark visual model-based RL tasks to evaluate causal discovery approaches under systematically varying causal graphs. Despite some disagreement on this point among reviewers, I would come down on the side of saying that a better-executed version of this paper would have been a good fit at *CONF*. However, its current drawbacks make this a borderline reject. The most important of these drawbacks is: it is unclear to what extent results on these simple environments translate to more realistic complex ones. <sep> Reviewers have also pointed to omitted relevant work that could be discussed in future versions, such as PHYRE. Another relevant benchmark in this vein: https://arxiv.org/abs/1907.09620"
"This paper proposes a method to detect adversarial examples. The detection scheme is based on the observations that typical adversarial attacks generate adversarial examples on the decision boundary, so if we use a ""counter attack"" <sep> on the adversarial example, it will be easy to change its label. <sep> A main weakness of this paper is that the proposed approach does not include an adaptive attack for evaluation.  If the proposed detection scheme is known to the attacker, the attacker can still generate visually indistinguishable adversarial examples that the detector fails to detect. This can usually be done by adding the detection objective to the loss function for attack.  Many heuristic adversarial example detections and defense methods have been broken by stronger and adaptive attacks [1,2], and the use of adaptive attacks is crucial [3]. <sep> Additionally, although the paper claims to detect zero-day, or unknown attacks, <sep> in evaluation the selection of attacks are quite limited. For example, it only includes gradient based attacks, but not decision based attacks or evolutional adversarial attacks. <sep> The paper attempts to make several theoretical justifications, but these theorems are too simple (e.g., based on direct application of triangle inequality) and do not significantly improve the contribution of this paper. <sep> As a conclusion, I cannot support the acceptance of this paper because the novelty of the proposed method is limited and evaluation is insufficient. <sep> [1] Athalye, Anish, Nicholas Carlini, and David Wagner. ""Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples."" arXiv preprint arXiv:1802.00420 (2018). <sep> [2] Carlini, Nicholas, and David Wagner. ""Adversarial examples are not easily detected: Bypassing ten detection methods."" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017. <sep> [3] Carlini, Nicholas, et al. ""On evaluating adversarial robustness."" arXiv preprint arXiv:1902.06705 (2019). <sep> [4] Brendel, Wieland, Jonas Rauber, and Matthias Bethge. ""Decision-based adversarial attacks: Reliable attacks against black-box machine learning models."" arXiv preprint arXiv:1712.04248 (2017). <sep> [5] Cheng, Minhao, et al. ""Query-efficient hard-label black-box attack: An optimization-based approach."" arXiv preprint arXiv:1807.04457 (2018). <sep> [6] Alzantot, Moustafa, et al. ""Genattack: Practical black-box attacks with gradient-free optimization."" Proceedings of the Genetic and Evolutionary Computation Conference. 2019.","Reviewers liked the concept of the zero-day attack and yet raised different concerns about the other parts of the paper. In general, Reviewers wanted to see more thorough experimental evaluations (e.g., against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses. AC encourages authors to incorporate Reviewers' comments when preparing the paper for elsewhere."
"Review: <sep> [Summary] In this work, authors address the problem of how to achieve a more feasible data augmentation not only from the samples but also considering the accompanying labels. Authors solution is framed on a framework called 'AutoLabel', in which they leverage on already existing concept including: mainly on expected calibration error, and AugMix (in general mixup principles). <sep> [Cons] <sep> -- The problem of adjusting labels for augmented data is indeed of interest for the community. <sep> -- The motivation of the work is good. <sep> [Pros] <sep> -- Several strong statements are not supported, and therefore, the level of novelty is hard to appreciate. <sep> -- There is a lack of formalism to transmit the idea whilst there is empirical evidence of a few statements- there is not enough theoretical background to support the authors findings. <sep> Detailed comments for authors: <sep> -- [Novelty of the work] The proposed method strongly leverages the widely used  expected calibration error along with principles of mixup. Therefore, it is hard to appreciate the significance of the proposed method. Moreover, central observations are unsupported: <sep> A major observation of the authors is: ""Our key insight is that the confidence in the labels associated with the augmented data likely depends on how significant the transformation is.""  This is the definitional concept behind when generating pseudo-labels (-examples). That is, one seeks to get a high confidence (high probability to belong to the k class) to produce usable augmentation -- even from a set of feasible augmentation one can further average on the confidence produced on the label.  Authors should further elaborate on this idea as this is reported as highlighted in the paper. <sep> The author's statement ""assumption of label-preservation holds in practice"" this also echoes the previous point, a major consideration is a 'feasible augmentation'. This is a major area not only in the supervised setting  but also in semi-supervised learning -- how to define these δ augmentations (perturbations) is far from being trivial not only in the sense of the augmented labels but also samples. Authors should formalise the assumption in the work including a 'feasible augmentation' of the samples (not the labels) as it is highly dependent on the augmented label; strong and weak augmentations how these concepts hold in the paper? <sep> Author central contribution is given in (1) and the following sections revise concepts and connections with the principles of mixup. Authors should focus on giving a further interpretation of (1) along with the  assumptions that hold and how a certain modelling hypothesis fails this expression. <sep> -- What are the connections with SOTA augmenters? For example RandAugment, and CTAugment. <sep> [*] Cubuk, Ekin D., et al. ""Randaugment: Practical automated data augmentation with a reduced search space."" CVPRW. 2020. <sep> -- From [**] the randomisation test where the true labels were replaced by random labels yet deep nets easily fits them raised the question of -- Are augmented labels really  better than search for feasible augmented samples (instead of labels)?  If one defines a feasible augmented sample then the label preserving will hold. <sep> [**] Zhang, Chiyuan, et al. ""Understanding deep learning requires rethinking generalization."" *CONF* 2017. <sep> [Experimental Results]  There is a lack of strong discussion and findings that somehow weakens the paper. One would like to see connections with other augmenters such as RandAugment, CTAugment, Cutout etc.","The paper introduces a simple and interesting method that adaptively smoothes the labels of augmented data based on a distance to the ""clean"" training data. The reviewers have raised concerns about limited novelty, minor improvement over baselines, and insufficient experiments. The author's response was not sufficient to eliminate these concerns. The AC agrees with the reviewers that the paper does not pass the acceptance bar of *CONF*."
"Update after author response: I have read the other reviewer's comments. My take is that at a high level the contribution of this paper is above the bar of *CONF*, but the experiments aren't controlled enough so I vote for a weak reject. <sep> Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class. Going further, Hendrycks et al claim ""but we find that even with OE, classifiers with the reject option... are not as competitive"". As I understand, Hendrycks et al are basically saying the (simpler) method in this paper does not work as well - but Hendrycks et al don't provide experimental evidence for this claim, it's merely stated. The original paper might in fact discourage practitioners from trying this approach, and instead using the high entropy approach. In fact, this was a question in my mind when I read Hendrycks et al last year. <sep> Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. Conceptually, the approach in Hendrycks et al also seems more brittle and there are distinctions between these two methods (e.g. see Vernekar et al 2019, ""Analysis of Confident-Classifiers for Out-of-distribution Detection""). <sep> So what's the potential practical impact? If this paper didn't exist, I suspect practitioners would use the method in Hendrycks et al, and not try the reject class method, because of that paper's claims. But with this paper, practitioners might use this method or try both, which seems like a good impact. <sep> So barring problems with the experimental setup, I'd give the paper a 7 / accept, and so I'd encourage the authors to continue on with this work. <sep> To me the decision hinges on the quality of the comparisons. I am inclined to agree with R1 on the quality of comparisons. Taking a closer look at their paper, they have no detailed discussion about the hyperparameters and experimental setup, which is key when the main contribution is a fine-grained comparison with Hendrycks et al. R1 raises a question about fine-tuning vs trained from scratch, and it does look like this paper trains from scratch whereas outlier exposure fine-tunes. The outlier exposure set is also different. While it is a smaller set in this paper, Hendrycks et al say ""experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models."". <sep> Overall, I agree that the best thing would probably be for them to do a more careful  and controlled comparison, include all these details, and submit to the next conference. In my review I did mention that their comparisons were unclear, but they didn't take the chance to update their paper and misleadingly responded that ""The OE method is closest to ours, so we are able to match their training regimen well"", but as R1 points out there seem to be salient differences. <sep> ######################################################################### <sep> Summary: <sep> This paper tackles the problem of out of distribution detection. Concretely, they have an in-distribution set of inputs D_in and the goal is to reject inputs from an unknown set D_out as out of distribution, while accepting inputs from D_in as in-distribution. Like in the outlier exposure paper, they assume a known set of proxy out of domain distribution inputs \\tilde{D}_out. The approach is to classify inputs in D_in into one of K classes, while classifying inputs from \\tilde{D}_out into a ""K+1""-th class. On a few vision and NLP datasets, they show better performance than outlier exposure and other methods. <sep> ######################################################################### <sep> Reasons for score: <sep> The main technical difference compared to outlier exposure is that they classify inputs from \\tilde{D}_out into a K+1-th class, whereas outlier exposure enforces that the prediction over the K classes have high entropy if the inputs are from \\tilde{D}_out. 1. One main concern is that they have other (hidden) differences from Hendrycks et al, for example I looked at their code and they use Mixup and a different \\tilde{D}_out, so it is unclear if the gain is actually coming from the K+1-th class, or e.g. more modern data augmentation practices. They also don't test on the most challenging image datasets from Hendrycks et al, in particular D_in = CIFAR-100, D_out = CIFAR-10 (they have distinct sets of classes). 2. This method was also proposed by Vernekar et al, ""Analysis of Confident-Classifiers for Out-of-distribution Detection"". <sep> I generally believe that this is the right thing to do, and seems simpler and more sound than enforcing high entropy, and Vernekar et al only have toy experiments. However, since the contribution is a small change to OE, to make this complete I believe they should have more complete experiments. In other words, I'm holding them to a higher bar experimentally, than if say their idea was novel or they had conceptual insights into the problem, because without more complete experiments the contribution to the community is limited. Note that unlike most works on OOD detection, they use more data, \\tilde{D}_out, similar to outlier exposure. This is completely fine, but that's why I think simply having SOTA results is not quite enough to push this over the bar since there's only about 1 paper on this precise setup. <sep> On the plus side, this work should at least convince practitioners to try both methods, K+1-th class and enforcing high entropy. <sep> ######################################################################### <sep> Pros: <sep> Simple idea, and does better than competing methods on most baselines. <sep> Combination of vision and NLP datasets. <sep> Well written, clear and simple. <sep> ######################################################################### <sep> Cons: <sep> I looked at this paper's code, and Mixup is an additional difference compared to Hendrycks et al's outlier exposure. There could be other differences I did not spot (e.g. training procedure? Optimization method). It's great that the overall method does better, but I'd like to see at experiments on some datasets investigating what happens if you use the same procedure as OE except K+1-th class instead of entropy. <sep> Results on CIFAR-10 -> CIFAR-100, and CIFAR-100 -> CIFAR-10 would be good. Since the main contribution is experimental, it would be particularly compelling if a couple more datasets were added. E.g. the original OE paper is substantially more comprehensive. E.g. one combo could be Places365 -> ImageNet. <sep> No conceptual reason for why the K+1-th method does better (I am still happy to vote to accept without this, but this would make the paper more compelling). The earlier work by Vernekar et all doesn't quite explain things for high dimensional data like images. <sep> ######################################################################### <sep> Questions and things to improve: <sep> If the comparisons to Hendrycks can be re-run with matching training procedure, and they show results on a few more cases (e.g. CIFAR-10 -> CIFAR-100, and CIFAR-100 -> CIFAR-10), I would seriously consider leaning towards an accept. <sep> #########################################################################","This paper proposes a method for out-of-distribution (OOD) detection by introducing a K+1 abstention class for outliers, in addition to the in-distribution classes. While the method has shown promising performance compared to the Outlier Exposure (OE), the novelty is limited given the idea is almost identical to an AAAI'20 paper (Mohseni et al. 2020). In addition, several reviewers have raised concerns regarding the lack of fairness in the experimental setting. Authors are encouraged to address them in a future submission. <sep> The AC believes an interesting and valuable contribution to the community would be showing conceptual and theoretical reasoning for the pros and cons of using K+1 class vs. entropy regularization. Currently, the tradeoff between these two types of training objectives is not well understood. <sep> Overall, three knowledgeable reviewers in this area have indicated rejections. The AC discounted R2's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion among reviewers. <sep> Lastly, the AC would like to greatly thank R1, R3, and R4 for the active engagement and participation in the paper discussion period. It was very helpful for the decision-making process."
"########################################################################## <sep> Summary: <sep> In this manuscript, the authors propose a novel way of performing counterfactual inference in time-series in the presence of hidden confounders. For this, they employ neural ODEs as a latent time-series model, which they augment with additional latent variables. They test their approach on synthetic and real-world data and demonstrate improved performance in comparison to the state-of-the-art. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, I vote for weak rejection (5) in its current form. This is mainly due to the following reasons: <sep> Marginal theoretical contribution <sep> Analysis of the method is lacking. How critical is the introduction of dynamic variables for the performance? How much is the performance increasing by only increasing the number of parameters of a vanilla neuralODE? How significant are the results, e.g., in figure 2? <sep> ########################################################################## <sep> Pros: <sep> The paper is well and concisely written and is understandable to a broad audience. <sep> The authors apply an interesting model to an interesting problem. <sep> To the best of my knowledge, neuralODEs have not been applied to counterfactual inference before, but it is hard to keep track of those two fast-evolving fields (neural ODEs and counterfactuals). I believe, however, that this approach is very promising. The idea of introducing additional dynamic variables to neuralODEs to account for the bias introduced by hidden confounders is also novel. <sep> The presence of clusters in the inferred auxiliary variable states is an exciting finding. <sep> ########################################################################## <sep> Cons: <sep> The theoretical contribution is marginal. The authors only slightly extend the model without further studying the implications of the model. <sep> I would have expected to see the following analysis: <sep> A)    Auxiliary states vs. history-dependent (non-Markovian) neuralODE <sep> B)    Auxiliary states vs. additional parameters of neuralODE <sep> This is due to the following reasons: A) In prior work, a similar approach has been employed in various prior works to model hidden confounders, e.g. [Nodelman, U., Shelton, C. R., & Koller, D. (2012). Expectation-Maximization and Complex Duration Distributions for Continuous-Time Bayesian Networks. UAI], or see ""Mori-Zwanzig"" formalism, in order to model non-Markovian behavior. I would have liked to see some discussion on auxiliary variables vs. non-Markovian neuralODEs (f(x,u,t) vs f(x_1,…x_t,t))?. What is the better approach? To introduce auxiliary variables into the neuralODE, or to extend the neuralODE also to have the time-series' history as input? B) My kneejerk reaction is that decreased bias and increased variance can be expected when introducing auxiliary variables, solely because this increases the number of parameters of the neuralODE. I would suggest performing a dedicated experiment that allows for disentangling the number of parameters from the presence of auxiliary variables. <sep> Further, the results in fig. 2 should include the variance of results. The curves are all close together, and it would be interesting to see whether the improvement is significant. <sep> How would I determine k in practice? <sep> ########################################################################## <sep> Questions during the rebuttal period: <sep> Please address and clarify the cons above. <sep> ######################################################################### <sep> Some typos: <sep> •    Algorithm 1,2,3 ""ODESlove""","There are some interesting ideas raised on continuous-time models with latent variables in machine learning. However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed."
"Summary and contributions <sep> This paper tackles the problem of the impact of mini-batch size on the variance of the gradients of SGD. Unlike most work on the topic, it does not study the variance of the gradient conditionned over the last iteration, but rather the absolute variance of the gradient conditioned only on the initial point. <sep> The paper restricts itself to linear models, either with least mean square regression or deep linear model with 2 layers (which would be non convex but still linear). <sep> The paper shows two results. For linear models, the variance of the gradients conditionned on the initial point is decreasing with the batch size b. <sep> For a deep linear model, the result shows that the variance of gradient is a polynomial in 1/b with no constant term. Therefore, for b >= b0 for some b0, the variance is decreasing with b. <sep> Review <sep> This paper tackles the problem of the variance in SGD from a novel angle, namely the total variance conditioning only on the initial weights, and not on the previous iteration as usually done. <sep> The result on linear regression seems very natural, and the proof is done elegantly (I checked up to the proof of Theorem 1). <sep> I haven't checked the proof for the deep linear network. The result is more interesting than in the linear regression case because the 2 layers linear network is non convex, and therefore, one can imagine having multiple local minima and 2 different trajectory starting from the same point to diverge at some point, which could lead to drastically different gradients. This is not really possible for least mean square with a full rank hessian, as there is a single optimum, and in any case all trajectories will end up at the same place. <sep> Having a small total variance conditioned only on the initial point means that somehow the trajectories for different samplings cannot diverge too much. Of course, if the batch size goes to infinity, one is doing gradient descent, and all the trajectories are exactly the same which is compatible with theorem 4. I think the proof technique is interesting to be able to bound or study deviations between trajectories. However, in its current form it is non practical as the bound is very complex and in particular can increase as the iteration increases. In essence, the results says that for any iteration, as B increases, trajectories get closers, but it does not say that they will stay close for any iteration number. <sep> I must say that while the theoretical part is doing a good job, the experiment part in Section 4 is quite poor. <sep> The author tries to open up to the problem of generalization. <sep> Speaking of figure 1 b), the authors say that the validation loss improves with the batch size, i.e. the generalization ability is better with large batch sizes. This however contradicts previous work that have been mentioned by the authors in their very introduction. The authors do not comment on this contradiction. My own conclusion is that deriving general results on generalization from MNIST is a perilous exercice, if not plain wrong. <sep> Conclusion <sep> Overall, I would say that this paper is just above the acceptance bar, because the theory holds up well and could be of interest for finer analysis of the dynamics of SGD, and in particular of different trajectory starting from the same point (how quickly will they diverge?), although doing so would require significant extension to the present work. <sep> The authors try to open up to the problem of generalisation but fail to a proper theoretical link with their own work, while their experimental results are obtained only on MNIST and therefor subject to caution (and in fact contradict previous work). <sep> Notes and remarks <sep> In the abstract, the authors write ""for deep neural network with L2 loss, we show..."". This is not true, the result is only for two layers deep linear network. <sep> Definition 1: the definition of the degree is a bit hand wavy, there should be some minimum over all possible decomposition of M. This is especially needed if for some X, both X and X^{-1} belong to \\mathcal{X}. <sep> page 18, at the top: ""We denote A = \\sum ..."", A has already been introduced before end of page 17, but with x_i x_i^T instead of C_i (both are equal of course). <sep> Section 4, comments on the loss of Figure 1: phrasing ""loss is decreasing with b"" is ambigous or wrong. For the training, the loss increases with the batch size. I guess the authors meant that the loss worsen with b (as low is good), but worsen != decreasing.","This work analyses the impact of mini-batch size on the variance of the gradients during SGD, in the context of linear models. It shows an inverse relationship between the variance of the gradient and the batch size for such models, under certain assumptions. Reviewers generally agree that the work is theoretically sound. However, all reviewers believe that the contributions of this work are limited. This concern was not adequately addressed during the discussion phase and led to the ultimate decison to reject."
"The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework. The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. Then, to avoid label leakage, a masked label prediction strategy is employed. <sep> Pros: <sep> The presented method shows strong empirical performance on the open graph benchmark dataset. <sep> The whole framework is simple and the idea is easy to follow. <sep> Cons: <sep> What is the label leakage problem? It is not clear to me (1) why label will be leaked during the joint learning process and (2) what the outcome does label leakage bring. <sep> The writing of this paper is poor. The authors are suggested to polish their paper. Please see minor comments below. <sep> Although the proposed UniMP achieves state-of-the-art performance, the experiments are not convincing enough. <sep> Experimental results are not consistent, c.f. Table 4~6 and Table 7. It seems that the standalone Transformer even surpasses UniMP on ogbn-products and ogbn-arxiv. <sep> It seems that the hyper-parameter specifications vary greatly across the three datasets. To me the residual connection is helpful when stacking many layers (Li et al., 2019; Chen et al., 2020), while in this paper the number of layers is relatively low. A sensitivity analysis on the network depth is necessary to demonstrate the impact of the residual connection. <sep> When compared with GAT, the main differences are (1) different implementations of self-attention mechanisms and (2) whether to adopt gated residual connections. However, no ablation studies are provided to demonstrate the impact of these two independent components. Especially, as shown in Table 7 and Figure 3, given X,A,Y^, transformer outperforms GAT. The authors are expected to elaborate on which component (self-attention implementation or gated residual connection) brings the improvement. <sep> Minor comments: <sep> Abstract: we adopt a Graph Transformer jointly [using] label embedding? <sep> Abstract: UniMP ... and be empirical powerful -> is empirical powerful <sep> Page 2: there are different -> they are different <sep> Mathematical notations are severely abused; for example, hidden_size should be represented by f, and how Y^e is transformed from Y^ is not clear.","This paper proposes a semi-supervised graph classification technique that unifies feature and label propagation techniques. The resulting algorithm is a simple extension that attains strong performance. Reviewers were divided on this submission. Some reviewers felt the proposed algorithm did not constitute a sufficient technical contribution given that it was a simple combination of existing techniques. I tend to agree with other reviewers that the simplicity is a benefit. However, despite the methods simplicity there was significant confusion about the details of the method and multiple reviewers flagged that the paper was difficult to read and understand. It further could benefit from additional discussion and some clarification/cleanup of the experimental results. Finally, multiple reviewers asked for better situating of the proposed method with respect to prior work. Given these concerns, I do not think the paper is ready for publication. I would recommend the reviewers do a thorough re-write of the paper to address these concerns and consider resubmitting."
"########################################################################## <sep> Summary: <sep> The authors proposed CNV-Net, a deep learning-based approach for copy number variation identification. They encoded mapped DNA sequences into a pileup image that captures reference sequence, sequencing coverage, and mapped reads. Then, they used CNNs to classify it into deletions, duplications, or non-breakpoints. They benchmarked CNV-Net with two whole-genome sequencing datasets and claimed to obtain more accurate and efficient results than current tools. <sep> ########################################################################## <sep> Major comments: <sep> While the paper has its own merits, unfortunately, it has several issues that need to be addressed. <sep> Although the authors claimed that CNV-Net is the first tool to use a CNN to detect CNVs, this is not true. Several previous works used CNNs for CNV detection. Furthermore, I think other machine learning and deep learning-based works should also be acknowledged. I'd recommend authors properly cite and compare previous works with the proposed method. Some of the previous works include the followings: <sep> (1) Zhang, Yun Xiang, et al. ""DL-CNV: A deep learning method for identifying copy number variations based on next generation target sequencing."" Mathematical biosciences and engineering: MBE 17.1 (2019): 202-215. <sep> (2) Cai, Lei, Yufeng Wu, and Jingyang Gao. ""DeepSV: accurate calling of genomic deletions from high-throughput sequencing data using deep convolutional neural network."" BMC bioinformatics 20.1 (2019): 665. <sep> (3) Hill, Tom, and Robert L. Unckless. ""A deep learning approach for detecting copy number variation in next-generation sequencing data."" G3: Genes, Genomes, Genetics 9.11 (2019): 3575-3582. <sep> (4) Pounraja, Vijay Kumar, et al. ""A machine-learning approach for accurate detection of copy number variants from exome sequencing."" Genome research 29.7 (2019): 1134-1143. <sep> The main contribution of the paper would be using a pileup image of mapped reads and a CNN to detect CNVs. However, in my view, the novelty of the paper is quite limited. As stated in the introduction, the pileup image encoding and CNNs have already been used in a couple of previous works for SNV detection. I could find any significant methodological differences in CNV and SNV detections; it seems like rather a straightforward application of previous methods on another similar problem. Otherwise, please clarify the differences between the two problems and what authors have done to overcome the new obstacles. <sep> The core issue I have with this paper is that I do not think the experiment settings are realistic. As stated by the authors, CNV-Net must know the candidate CNV positions. I think this is a serious issue that must be handled rather than leaving it as a limitation of the work. Currently, the CNV-Net is evaluated with mapped and pre-preprocessed reads with CNV centered breakpoints. Compared to the experiments conducted in the previous works, the experiments of the proposed work seem limited, unrealistic, and biased in favor of the proposed method. <sep> In my view, the authors left out too much information. For examples, it is quite difficult to understand how they used other tools for the experiments; Do they only use the CNV breakpoints that passed the quality control filter as CNV-Net? Or do they use all the mapped reads? What tool-specific arguments did the authors use for each tool? Currently, it is extremely difficult to reproduce the results presented in the paper. <sep> ########################################################################## <sep> Minor comments: <sep> How did the authors choose the specific numbers to encode individual base into R channel (e.g. A with 250, G with 180) <sep> In the results section, the authors stated that they only used CNVs passing the quality control filter. Please provide more details for the filter explaining the filtering criteria and how they chose them. <sep> In Table 2, how did the authors obtain the metrics for the multi-class problem? Please state whether they are macro or micro averages of scores. <sep> ##########################################################################","Four knowledgeable referees have indicated reject. I agree with the most critical reviewer R4 that the model design lacks a clear and transparent motivation and that the experimental setup is not convincing, and so must reject."
"Summary: <sep> The authors proposed to first extract a subgraph Gu for each node u. Then use GNNs to extract the hidden representation hu=GNN(Gu). The subgraph extraction uses PPR-Nibble with is a conductance based local clustering method. <sep> Pros: <sep> The model seems to be more efficient compare to GNN with global clustering method such as Cluster-GCN. <sep> Using local clustering method to determine the subgraph for each node might be better than random neighborhood sampling in some cases. <sep> Cons: <sep> The intuition on using short random walk is problematic. (See detailed comments) <sep> The experimental results are insufficient to verify the proposed method. <sep> Insufficient related work on local clustering and PageRank-based methods. <sep> While the authors claim that they have theoretical analysis, all stated theorems are from the other papers. I do not aware of any theoretical contribution as claimed by the authors. <sep> Detailed comments: <sep> The main weakness of the paper is the claim that short random walk is sufficient to extract topology information from graph. This contradicts to both [1] and [2]. In [1], the authors used Personalized PageRank (PPR) to build their GNN (APPNP) which shows that large propagation step is helpful (K=10−20). Notably, they show that using K≥10 gives significant improvement on Cora and PubMed dataset for node classification problem compare to using K≤5. On the other hand, the authors of [2] study the generalized PageRank method for local clustering (seed-set community detection) problem. They show that using large step propagation (K≥10) leads to better local clustering performance compare to small steps. These works show that using merely short random walk is insufficient to fully extract the topological information from graphs and thus the claim by the authors seems questionable to me. <sep> The other weakness of this paper is their experimental results are insufficient to verify the proposed method outperforms the sampling-base method. For example, in Table 2 the test accuracy of LCGNN-SAGE is actually lower than GraphSAINT (SAGE aggr). Even if we compare LCGNN-GAT with GraphSAINT(GAT-aggr) the gain is not obvious. The model proposed by the authors that has the best performance is LCGNN-Transformer. However, it is not clear whether the performance gain is due to the local clustering procedure or the transformer. <sep> Minor comments: <sep> As the authors mentioned, the local clustering methodology is only reasonable for graphs that has low conductance with respect to all ""cluster"" (nodes with same label). This is exactly the ""Homophily principle"" which is true for most of the popular benchmark datasets such as citation networks (Core, Citeseer and PubMed). However, as pointed out in [3], there are also practical graphs that are heterophilic or low homophily. Although this is not the main theme of this paper, the existence of heterophilic graph should not be ignored. <sep> Reference: <sep> [1] ""Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"" Klicpera et al., *CONF* 2018. <sep> [2] ""Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection,"" Li et al., NeurIPS 2019. <sep> [3] ""Geom-GCN: Geometric Graph Convolutional Networks,"" Pei et al., *CONF* 2020.","The paper considers using local spectral graph clustering methods such at the PPR-Nibble method for graph neural networks. These local spectral methods are widely used in social networks, and understanding neural networks from them is interesting. <sep> In many ways, the results are interesting and novel, and they deserve to be more widely known, but there are several directions to make the work more useful to the community. These are outlined in the reviewer comments, which the authors answered partially but not completely satisfactorily. Much of this has to do with explaining how/where these (these very fundamental and ubiquitous) methods are useful in a particular application (GNNs here, and node embeddings below). An example of a paper that successfully did this is ""LASAGNE: Locality And Structure Aware Graph Node Embedding, E. Faerman, et al. Proc. 2018 Conference on Web Intelligence."" (That is mentioned not since it is directly relevant to this paper, but since it provides an example of how to present the use of a method such as PPR-Nibble for the community."
"Summary: <sep> The author extends generative models with multi-generators by restricting the generators to share weights and all bias to be regularized in order to enforce that the inverse maps of the generators can be represented by a single encoder. The regularizer proposed minimizes an upper bound the the sum of the bias variances. This extension is evaluated on a set of visual datasets (MNIST, 3DChair and UT-Zap50k) with respect to density estimation (evaluated with the FID score) and disentanglement (evaluated with their own disentanglement score). <sep> Strengths: <sep> The method proposes a regularizer which is easy to understand and implement. This makes it easy to apply to existing generative models. <sep> The evaluation showed that sample diversity w.r.t. FID score is comparable/superior to baseline generative models. <sep> Weaknesses: <sep> For me, the main problems are the motivation of encoder compatibility, the clarity of the paper and limited evaluation: <sep> Motivation of encoder compatibility: I don't understand why the inverse of all generators needs to represented by one (!) single encoder? Why can't the generators be in general invertible and thus there exists a mapping for each observed variable to the latent space? What is the advantage of one encoder? <sep> The paper needs to improve its preciseness and clarity. I found it at parts confusing to understand because it introduces terms without really explaining or justifying them, e.g., ""encoder compatibility"", ""good generalizability"", ""equivalence relation"". Further, background is lacking to give details about the model that this paper extends on. This also hinders readability of the section where the authors introduced their own method. Further, assumption of continuity and generalizability have been made without defining these terms or reference. <sep> The evaluation is limited to generators w.r.t. generator architecture (fully-connected layers only). Further, FID score show high standard deviation and thus are not necessarily superior compared to baselines and the introduced disentangle metric are only reported for specific attributes of MNIST and 3D-Chair. <sep> More detailed comments are below. <sep> Overall assessment: <sep> Unfortunately, I do not believe this paper is ready for acceptance. Therefore I am recommending a rejection. I do believe it is an interesting approach. And with improvement in terms of clarity and evaluation, I am happy to improve my score if the authors can make the necessary improvement. <sep> Detailed comments and questions: <sep> ""since discrete features are both common and combinatorial"" (Introduction): Common to what? <sep> ""they usually have the same generic structure since the underlying continuous features since the underlying continuous features remain the same"" (Introduction): Has this been shown in any work before (if so, please cite)? I can also imagine this being depending on the specific learning task. <sep> ""it induces a generalizable equivalence relation between data, and the manifold structure of out-of-sample data can be derived by taking the quotient of this relation"" (Introduction): What is a generalizable equivalence relation between data? Is the quotient of this relation defined somewhere? <sep> ""Since deep encoders usually exhibit good generalizability"" (Introduction): What is the generalizability considered with respect to? Can you elaborate? <sep> Multi-generator scheme (Sections 2.2, 3): Can you elaborate on the actual model used for Quotient Manifold Modelling (QMM)? The paper mentions that a multi-generator scheme is used but there are no more details than that. It would be helpful to know what the learning objective is, how the (multiple) generator (and discriminator) are used. <sep> ""Let H be a set of encoding maps (X→Z) that can be represented by a deep encoder"" (Section 3.1): Why is being represented by one encoder important? <sep> ""this binding is meaningful only when H has a certain property"": Which one? Is that the generalizability that is mentioned in the next paragraph? <sep> ""its elements--deep encoders--have good generalizability"": Similar to the claim above, what does that mean to have generalizability? How can generalizability be quantified? <sep> High standard deviation of FID scores of proposed methods: The proposed method have high standard deviation. Can you explain why this is? Is this an optimization problem? <sep> Proposed disentanglement metric: There is a vast literature of evaluation for disentanglement, why did you choose to propose your own disentanglement metric? How does it compare to the existing disentanglement metrics (advantages, similarity, etc.)? <sep> Table 1 (disentanglement): Can you also report overall disentanglement of the dataset? For disentanglement specifically you can also use existing benchmarks and datasets from Locatello et al. (2018). <sep> Minor: <sep> Definition of generative maps fG(i):Z−>M[...]iA (Section 3.1): I believe A was not defined before, is that just the number of generators? <sep> Post-Rebuttal: <sep> Unfortunately, the authors neither did update their paper nor addresses my comments. Therefore, I'm keeping my recommendation of rejection.","This paper presents an interesting method dubbed quotient manifold modeling to handle the ""multi-manifold"" structure of natural data and generalize to new manifolds that arise from novel discrete combinations. While some of the methods and ideas were appreciated by reviewers, there were a number of experimental and clarity concerns. The authors's did not submit a rebuttal, and the many unaddressed concerns (especially around experimental baselines) lead me to recommend rejecting this work."
"########################################################################## <sep> Summary: <sep> The paper studies policy optimization in multidimensional action spaces. They consider atomic factorization of the action space (i.e. action space is factored into sub-action spaces, one per action dimension). In this setting, the authors consider two well-known policy representation techniques: 1) independent sub-policies (diagonal-covariance policies over the sub-action spaces) and 2) sequential/autoregressive policies (an ordering of the sub-action spaces is assumed a priori and the sub-policies receive as input the state and the selected sub-actions for the preceding sub-action spaces). The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks. <sep> ########################################################################## <sep> Reasons for score: <sep> I believe this paper fails to position itself appropriately with respect to the literature. This makes it difficult to assess what the main contributions are and whether sufficient new methodology is proposed. Additionally, the paper misses on several fundamental experiments for the results to be convincing. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? Is it the improvements of the discrete versions over the continuous versions of the methods that are interesting (in which case, the results for the continuous versions on the MuJoCo tasks should be reported)? <sep> ########################################################################## <sep> Pros: <sep> Factorizing action spaces is an interesting approach for scaling to high-dimensional action spaces. Also, they have been shown in several recent studies to enable discrete-action methods to outperform numerous continuous-control methods. So studying them more extensively is useful. <sep> The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. <sep> ########################################################################## <sep> Cons: <sep> The paper generally does not position itself appropriately with respect to the literature. Below is my overview of the paper with additional related works (some are suggestions that would enhance the paper): <sep> Branching Q-learning (BQL) in Ref. [I] is similar (if not the same) to the independent critic of FSAC. Using this approach they scale to domains with 33^17 discrete actions. <sep> Autoregressive critic of FSAC is the same as that in Metz et al. (2017) (also stated in the paper I believe). <sep> Independent versions of numerous PG methods have been previously developed and studied extensively by Tang and Agrawal (2020). Also, the original TRPO paper was used with independent discrete sub-action policies in Atari tasks (if I remember correctly). Therefore, there is nothing new about FPPO-independent (also stated in the paper I believe). <sep> Other methods for learning the independent critic in FSAC are also possible but are not discussed in the paper (see e.g. Ref. [II]). Moreover, Ref. [II] uses independent policies with A2C. <sep> When independent policies are used in PG, a better baseline can be used which provably reduces variance [III]. Discussion of such a baseline would be very useful in this paper which somewhat serves to summarize deep RL in factored action spaces. <sep> Ref. [IV] combines autoregressive and independent proposal policies for approximate Q-maximization in Q-learning. In this way, they scale to very high-dimensional action spaces. This work is able to handle hybrid action spaces without discretization. A discussion and positioning with respect to this paper could be valuable. <sep> Experiments fall very short in my opinion. Below are some of the experiments that I think are necessary to include: <sep> Figure 1a,b: Why not run FSAC? <sep> Figure 1c: I believe the autoregressive FPPO and FSAC are reported in Platform. But why not independent? Why not report the standard (non-factored) baselines of PPO and SAC? <sep> Figure 2: Compare against the continuous versions of SAC and PPO on MuJoCo. <sep> Figure 2: Why not evaluate autoregressive policies in MuJoCo? Metz et al. did that with DQN. <sep> No new factorization schemes are explored in this paper. For instance, Ref. [V] explores a mixture of independent and sequential action-value function representations. A discussion of the spectrum of other possible factorizations would be interesting. <sep> [I] Action Branching Architectures for Deep Reinforcement Learning, AAAI 2018. <sep> [II] Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement Learning, arXiv 2017. <sep> [III] Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines, *CONF* 2018. <sep> [IV] Q-Learning in enormous action spaces via amortized approximate maximization, arXiv 2020. <sep> [V] Inferring DQN structure for high-dimensional continuous control, ICML 2020. <sep> ########################################################################## <sep> Questions during the rebuttal period: <sep> In light of new related works together with those included in the paper, I believe the novelty of the paper currently is in developing the policy optimization updates for autoregressive policies. The rest (independent policy optimization updates, independent critic, and autoregressive critic) are not novel as far as my assessment goes at this point. Is this correct? <sep> Does ""without autoregressive"" (in Figure 1) imply ""independent""? Or does it refer to the standard (non-factored) versions of PPO and SAC? If not, I need clarification on what it exactly means. <sep> This paper considered stochastic policies. I'm curious about any potential use for extending it to deterministic policies; e.g. could it be useful to use an autoregressive policy network with DDPG? <sep> I think that the manual ordering of the sub-action spaces in an auto-regressive policy could introduce bias (i.e. I think there could be a setting that some orderings would make it impossible to learn the optimal policy). Can you comment on this? <sep> ########################################################################## <sep> Minor comments: <sep> The title is too broad in my opinion. Unless the paper also incorporates analysis and results for purely action-value methods (e.g. DQN-based agents such as Sequential and Branching Q-learning), the paper should specify that policy optimization methods are of focus. Also, stating atomic factorization in multidimensional action spaces instead of only ""factored"" could clarify the kind of factorization that is the subject of this paper. <sep> ""FPPO was trained only for 2 days on 4 CPU cores while IMPALA was trained with 150 CPU cores."": <sep> This is not very useful without providing training-time of IMPALA. <sep> The word action is frequently used instead of action dimension and sub-action. <sep> ""With this transformation, both factored agents reach completion in a few time steps, see Figure 1c"": <sep> Saying ""a few time steps"" for a plot that goes to 90k time steps is somewhat strange. <sep> This work claims to develop techniques for dealing with hybrid action spaces. But in the experiments, this is not demonstrated. In fact, the action space in Gym Platform features discrete and continuous sub-action spaces, but they are discretized. As such, I think this paper should state dealing with discrete action spaces or continuous ones via discretization.","This paper studies two techniques for handling high dimensional action spaces in deep RL, namely selecting action components independently or selecting components sequentially in an autoregressive approach. The methods are developed for two deep RL algorithms (PPO and SAC) and tested on multiple domains. <sep> The reviewers recognized the significance of this research topic but found significant problems in the presentation. The reviewers raised concerns on the relationship to prior work in the literature (R2), baseline comparisons that are missing in the experiments (R2, R4), and a lack of clarity in the intended message of the experiments (R3). The authors responded favorably to the reviews, answered clarification questions, and acknowledged the limitations of the submitted work. The authors expressed their intent to release a stronger paper sometime in the future. The reviewers acknowledged the author response and were in consensus that the submission needs more work. <sep> Three reviewers have indicated reject for the reasons described above. The paper is therefore rejected."
"==== Sumary of the problems considered and paper contribution <sep> This paper studies the problem of sign recovery for sparse mean estimation and sparse linear regression. They show that a median based divide and conquer algorithm has high utility (as measured by power, false discovery rate, and positive and negative false discovery rates) and robustness properties. They discuss the privacy implications of these robustness properties, rigorously showing that the deterministic algorithm they define satisfies random differential privacy, where the probability (over the choice of dataset) of not satisfying privacy tends to 0. They show experimentally that their algorithm has low communication and performs well, even when compared to algorithms with no robustness properties. <sep> ==== Comments <sep> The paper touches on some interesting questions around robustness and privacy. They essentially design a robust algorithm, subdividing the data to produce averages, reducing the signal by reporting on the sign of each answer, then using the median to give the final answer. Their privacy guarantee amounts to showing that this process is very stable to outliers. The authors remark at one point that this stability implies that they could likely use a procedure like propose test release to give an actually differentially private version. I wonder why they didn't do this? I'd be very interested to see how it performed. <sep> I think the strength of the stability statements Proposition 1, Proposition 2 and corollary 1 gets a bit lost in the vagueness of the wording. It should be made clear that the randomness is over X, and X' is any worst case neighbouring dataset. This is significantly stronger than if the randomness was over the pair. This stronger version means that the algorithm is stable against outliers, including those caused by unclean data, or malicious participants. This is particularly interesting in Corollary 1, which discusses group privacy. Also ""random differential privacy"", which has appeared in the literature previously, seems like the notion the authors are looking for. However, it seems like a little bit of a stretch to call this ""roughly (0,0)-DP"". <sep> The paper is well written, it clearly states its theoretical guarantees and discusses intuition. In particular the privacy guarantees are clearly stated, and their difference to pure DP highlighted.  I thought Section 2.2 was especially well-written. I'm not an expert on sparse DP algorithms but it seemed to discuss prior work well, and highlight how this work is different, as well as why previous work did not immediately imply a solution. <sep> It looks like the pooled mean does really well for false discovery rates, why is this? <sep> ==== Presentation <sep> The paper is well written. I think the privacy aspect would be more compelling if the authors ran the propose test release version, which would actually be DP. It seems like this experiment would be interesting whether or not the propose test release version did well. <sep> Small comments: <sep> Definition 1 should be all measureable subsets, not just all subsets. <sep> Typo: second sentence in intro: ""large quantities of sensitive data are.."" should be ""large quantities of sensitive data have been.."" <sep> It would have been helpful to have the definition of the sgn of a sparse vecctor in the introduction, I was a little unsure exactly what was meant. <sep> When defining the distribution space in (4), I think it would be helpful to state why the condition is required, not just that it is mild. It's for Berry Esseen, yes? <sep> It would be nice to see a discussion of the privacy implications of the five-fold cross-validation. <sep> The second sentence of the abstract: it is not just common sense that randomness is required, unless the function is constant, randomness is provably required.","The paper considers a problem of weak mean estimation under a differential privacy like constraint. Specifically, estimating the signs of a (sparse) mean, and not the actual values. <sep> The reviewers brought up a number of concerns, including the weak privacy guarantee (a type of average-case privacy). Other lesser concerns include inaccuracies in comparisons with the literature and lack of interest in the algorithm/method itself. <sep> As there was no response from the authors, there was little further discussion afterwards, and the reviewers remained in their opinion to reject the paper."
"This paper builds on recent work characterising deep neural networks in terms of Neural Tangent Kernels and Neural Path Features. Over the past few years, a number of papers have developed the theory of Neural Tangent Kernels, which can be used to interpret infinite width deep neural networks in the context of a particular type of kernel. A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths. For a fixed set of network weights, we can consider each path to give rise to a feature, corresponding to whether this path is active (i.e., is not switched off by one of the gates on the path). Then, the output of the neural network can be viewed as a weighted sum of active paths, equivalently the dot product of the neural path feature vector and a neural path value vector. Lakshminarayanan and Singh showed that under certain assumptions, a kernel defined in terms of the neural path feature is approximately equal to the neural tangent kernel (up to a constant). Specifically, they show that the value of the neural tangent kernel matrix tends to a constant multiple of the neural path kernel matrix as the width of the network goes to infinity. This suggests that the key component in a deep neural network with RELU activations is the gating structure, which defines active subnetworks, as opposed to the values. <sep> As far as I can see, this work extends the analysis of (Lakshminarayanan and Singh, NeurIPS 2020) in two ways. Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). Secondly, they provide an interpretation of the neural path kernel as a composite kernel composed of layer-wise kernels, giving rise to the title of the paper. <sep> I was not very familiar with the work on neural tangent kernels and encountered (Lakshminarayanan and Singh, NeurIPS 2020) for the first time when reviewing this paper. As such, there were things which I didn't fully understand and may have misunderstood in my review. <sep> I have one question regarding the theoretical results in both papers. Theorem 5.1 (in both papers) relates the neural path kernel to the neural tangent kernel by showing that the neural tangent kernel for a network in which the gates have been fixed tends to a constant multiple of the neural path kernel as the width of the nerwork goes to infinity. This felt counter-intuitive to me at first reading, as fixing the gates and growing the width to infinity seem to be mutually exclusive. Is it correct to interpret the result as follows? For any fixed gating structure, there is a relationship between the neural path kernel matrix and the neural tangent kernel matrix for a network with that gating structure (i.e., one in which we are only learning the neural path values). As we allow the width go to infinity this relationship tends to one of equality (up to a constant multiple). <sep> I also have a number of questions regarding the empirical results in this paper. <sep> In the experiments where we have fixed the weights, is the model learning parameters in a network in which the gating structure is fixed or is it learning the neural path value vector as part of a linear model? <sep> The discussion mentions performance when we fixed the input gram matrix to be a constant in the definition of the neural path kernel (and hence define the neural path kernel in terms of the gating structure only), but does not include numerical results for this case. I understand that this may be due to a desire to keep the paper within the recommended 8 pages, but don't see why these results could not have been added in the appendix. <sep> The discussion mentions performance when we permute the layers of the model and claims this is robust to permutation of the layers, but but does not include numerical results for this case. As above, I understand that this may be due to a desire to keep the paper within the recommended 8 pages, but don't see why these results could not have been added in the appendix. Moreover, I wasn't not sure what being robust to permutation of the layers means for the case where we are learning the both components of the DGN. Is this claiming that the results do not change if we permute the layers after training in the DL regime? <sep> Additional comments <sep> LS2020 is used in several places to refer to (Lakshminarayanan and Singh, NeurIPS 2020), the recent paper on which this builds. These should be corrected to match the format of the other citations. <sep> Table 1 contains the information flow for the FCNN. Arguably, this is the simplest of the three architectures and an illustration of the information flow for a CNN could be more useful here. At the very least, the authors should direct the reader to Appendix A, where the CNN is described in more detail. <sep> The authors refer to this work and the previous work of (Lakshminarayanan and Singh, NeurIPS 2020) as a ""paradigm shift in understanding deep learning"". While this work seems to be an interesting and promising line of research, I think it is fair to say that we will need to wait to see if it really does provide a paradigm shift in how we understand deep learning. <sep> In gener`al, I think the neural tangent kernel is an interesting and promising line of research in the study of deep neural networks. The recent work of Lakshminarayanan and Singh (NeurIPS 2020) seems to add to this and this paper provides a relevant follow-up up that and as such is likely to be of interest to the *CONF* community. However, I was did not check the proofs in the appendix or the appendix of (Lakshminarayanan and Singh, NeurIPS 2020), on which the results in this paper depend, and hope another reviewer more familiar with this line of work was able to do so.","This paper provides a new perspective on deep networks by showing that NPK is composed of base kernels and their dependence on the architecture is explicitized. It is further shown that learning the gates can perform better than random gates. <sep> While the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it. On the architectures considered such as FC, ResNet and CNN (btw, it seems restricted to 1-D), it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning (or get very close to). It is debatable whether drawing such a nontrivial insight alone warrants publication at *CONF*, while ""nontrivial"" itself is a subjective judgement. I understand people differ in their opinions, and the NTK paper has been impactful. Unfortunately since there are quite a few other papers that are stronger, I have to recommend not accepting this paper to *CONF* this time."
"This paper proposes an advanced masking strategy for CutMix augmentation based on the low-pass filter. The authors provide an interesting mutual information analysis for different augmentation strategies to describe their motivation. The experiments include many vision tasks (CIFAR-10, CIFAR-100, Fashion-MNIST, Tiny-ImageNet, ImageNet, Bengali datasets) and language tasks (Toxic, IMDb, Yelp). <sep> Pros <sep> + The mutual information analysis provides us a new perspective to understand different data augmentations. <sep> + Various experiments. <sep> Cons <sep> [Contradictory results between mutual information and performances] <sep> If we believe the VAE experiments in section 3, we have another paradox: the mutual information measurement and the real performance are not related. <sep> Table 1 shows that in terms of mutual information, MixUp < Baseline < CutMix (and < FMix with a very small gap). <sep> However, many experiments in this paper show that baseline < mixup < cutmix in terms of the performances. <sep> This paper cites information bottleneck theory to justify the deceases shown by Mixup, but it is still contradictory to the performances. <sep> It makes me confused to understand the meaning of mutual information. What is good for an augmentation method if we have high or low mutual information? It is still unclear to me. <sep> A similar comment also can be applicable to the ""adversarial robustness"" experiments. Aside from that mixup is hard to say ""adversarial training"" (what is the threat model in this scenario?), I feel that this result is irrelevant to FMix motivation. <sep> [Weak logical connection between the motivation and the method] <sep> In my opinion, the connection between the analysis in the motivation and the proposed method is too weak. This paper proposes a CutMix variant where the mask is sampled by a low-pass filter. Why the low-pass filter approach can solve the motivation, i.e., enhancing mutual information between input and augmented images? There could be other possible variants as discussed in my ""related works"" comment <sep> [Related works] <sep> There are a few CutMix variants that employ a non-random masking strategy. Especially, I believe these two variants, which have similar motivation, should be compared: <sep> Walawalkar, Devesh, et al. ""Attentive Cutmix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification."" ICASSP 2020 <sep> Kim, Jang-Hyun, Wonho Choo, and Hyun Oh Song. ""Puzzle mix: Exploiting saliency and local statistics for optimal mixup."" ICML 2020 <sep> where Attentive CutMix uses CAM to extract masks, and PuzzleMix employs an optimization problem to optimize masks. <sep> If it is possible, please provide more comparison between these two papers. <sep> [Too small performance gap, less convincing experiments] <sep> In Table 2, the performance gap between FMix and CutMix is too small, usually less than 0.3%. Note that the performance gaps are almost neglectable in these tasks. <sep> Furthermore, FMix is often worse than CutMix in many tasks (Table 2 TinyImageNet, Table 3 ImageNet-A, Table 4, CIFAR-10H Table 6). I wonder what is the advantage to use FMix comparing to CutMix if FMix shows worse performance than CutMix. <sep> Especially, I believe Table 3 is problematic. This paper argues that ""Mixup uses 1024 batch size and CutMix uses 300 epochs"". However, in PuzzleMix Table 5, CutMix-trained ResNet50 (top1 err 22.92) outperforms baseline ResNet50 (top1 err 24.31) with only 100 epochs. Thus, to me, this table is not convincing enough. <sep> [Potential issues in VAE analysis] <sep> The mutual information analysis is heavily relying on the learned VAE model. I wonder the quality of the generated images by VAE, in terms of both qualitatively (please provide generated samples in the supplementary) and quantitatively (e.g., FID). <sep> If the VAE is not optimized well, the analysis will not be convincing enough. <sep> Minor comments <sep> Why CutMix experiments are missed in Table 5? <sep> I suggest avoiding using the words, ""clear"" and ""clearly"". <sep> Post-rebuttal update <sep> My main concerns in the initial review were three-folds: <sep> Potential flaws in the analyses based on VAE and adversarial attacks <sep> Unclear connection between the MI analysis and the proposed method <sep> Small performance gap, and even sometimes worse performance, compared to the baseline methods (Mixup, CutMix) <sep> After having discussions with the authors, I will keep my initial score because: <sep> I am still confused about the MI-based analysis conclusion. The authors mentioned ""We make no claim that increasing or decreasing the mutual information measure will have a strong impact on performance. Instead, we contend that MixUp works by forcing the model to ignore sample specific features (thus learning compressed representations – the reason for discussing the information bottleneck theory) and that CutMix works by mimicking the real data whilst preventing example memorization."" in the rebuttal, but these two conclusions are not trivial to me (by the MI analysis). <sep> Even if we ignore the first part, my second concern still remains. The authors mentioned ""That is the problem FMix tries to solve by removing the horizontal and vertical edge artefacts from cutmix. Our belief is that cutmix biases models towards these edges as they are a guaranteed feature of the data and learning about them would reduce the loss since these edges can tell you how much of each source image is present in the input (a key part of the objective)."". But if this paper assumes that the rectangle masking strategy of CutMix makes bias, then I think other CutMix variants such as AttentiveCutMix or PuzzleMix should be considered as the comparison methods. Hence, I disagree with this statement ""A comparison to masks generated using additional models (and, thus, significant additional computation) does not seem fair to us."" <sep> For my last concern, the small performance gap, the authors claimed that this method ""was also used by the second place team in the BengaliAI Kaggle competition"". It is good evidence that FMix can sometimes offer benefit to real-world applications, but I think more evidence that FMix can really solve problems of previous MSDA in a certain scenario, e.g., the edge bias as pointed by the authors.","The paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, FMix, based on a new masking strategy. Reviewers point to the fact that FMix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded. This is a really borderline paper but I see the issues as more important than the benefits, so I recommend rejection."
"This paper aims to study how GCN will behave under spectral perturbations/manipulations. The empirical numerical analysis on three benchmark datasets (cora, citeseer, pubmed) show that most of the necessary information is contained in the low-frequency domain. Based on that, the author propose to expand the node feature matrix with the eigenvectors corresponding to low-frequency domain and apply MLP on this new feature matrix. Experimental results show that the proposed method outperforms vanilla GCN and achieve comparable results on pubmed with other baselines. <sep> I think the paper has a pretty good start point: understanding how the spectrum of adjacency matrix will affect the behavior of GCN. But I think the manuscript is loosely written and I can hardly follow it. I do have a lot of confusion about this paper and hope that the authors can clarify them. <sep> In general, there are too many typos and grammar errors which make the manuscript hard to read. <sep> Sec 1: in the 1st paragraph, what does 'graph principles' mean? I cannot recall a clear definition of this term. I think the authors could clarify it before using it formally. <sep> Other contributions in Sec 1: <sep> (a), I think the empirical results in the paper show that retaining a small portion of low-frequencies is enough for achieving good classification accuracy. From the results and your analysis, I can hardly find a clear conclusion that links to it (i.e., the very first eigenvector is most informative); <sep> (b) and (e) are somehow connected since both are mentioning about GCN's behavior over manipulating high frequencies; <sep> (c), I actually did not quite understand where the clear link is. I would suggest the authors clarify it more clearly in the manuscript; <sep> Just a minor thing, what exactly does 'informative' mean, greatest change in loss function or greatest change in test accuracy? Better make it clear. <sep> Sec 3: <sep> Notations are quite messy. A matrix can be denoted using italic lowercase letter, italic uppercase letter, calligraphic uppercase letter, bold italic uppercase letter. It is very confusing when reading the paper, especially when some scalars are also denoted using the same convention. <sep> I did not find any framework in this section, which contradicts the last paragraph in Sec 1. <sep> Sec 4: <sep> Pubmed should have only 1 connected components. In the following sentence, what does 'not necessarily fully supported on ...' mean? <sep> In Sec 4.1, is there any intuition or theoretical justification for the projection operation used in band-pass filter? <sep> In Figure 2, it seems that Cora has a very sharp performance drop (~0.82 -> ~0.5) when x-axis is nonzero, is there any insightful explanation on that? <sep> Does the content of 'MLP ablation' correspond to your proposed method? This is the only place I can find clues about your method. I understand that the goal of this paper is show a MLP can perform better or comparable with GCN that perform message passing. But I believe you still need topology information to get those eigenvectors. Is there any insight or theoretical concern about simple concatenation instead of linear transformations like eTex?","Though the observation regarding the importance of the low end of the spectrum is interesting in its own right, the paper would be better substantiated by experiments on more datasets and a more thorough characterization of the paper novelty/contrast to state of the art."
"The problem studied in this work is of interest in the quantum machine learning community, as the power of small and noisy quantum computers for machine learning problems is far from being understood. Therefore, it is important to study the expressivity of quantum neural networks as function approximators. This work uses the model introduced by Tensorflow Quantum, where different neurons can be implemented on either quantum or classical computers. <sep> However, it is unclear how this result applies to current topologies of QNN/variational circuits used in current literature. From my knowledge of quantum variational circuits, the architecture proposed is different. To my understanding, this work addresses specifically the model proposed for TensorFlow quantum, and this should probably be made more explicit, as it's somehow different from current literature, where quantum neural network architectures are the sole computational node, and no classical computation is performed classically (besides the optimization of the parameters of the variational circuit). The model described in this work is called the ""prologue, acceleration, epilogue"". <sep> If I understood the work properly, the role of quantum computers is to evaluate on a quantum computer the ""binary"" part of a Binary Polynomial Neural Network (this is what the authors call the acceleration phase), after in a prologue part the data is loaded as initial quantum state with a log-depth circuit.Then, in the epilogue phase, a nonlinearity, like a a ReLU function is applied classically, <sep> It is really interesting the comparison with the approximation function of neural networks in classical computers. Perhaps more (recent) literature review on quantum expressiveness of other variational circuits can be added. <sep> I checked some of the proofs in the manuscript, and they are correct. The paper is nicely written, but perhaps might benefit some more clarity, especially in the proofs in the appendix. <sep> Overall, the submission would have benefited from experiments, showing that a QNN built with the architecture proposed in this work can achieve high accuracy in classification/regression tasks. This can be either done on small quantum computers, or even simulated in GPUs or large classical computers. Also, it would have been beneficial to write more clearly section 3.3, perhaps with an example, on how the classical optimizer is meant to choose the parameters of this circuit in a machine learning problem, i.e. how this architecture is meant to be used in practice. <sep> Other remarks are the following: <sep> Please use a consistent notation for multiplication. If my understanding is correct, In proposition 3.2 and the subsequent lines, you use the notation x×y, xy, and xy˙ to denote the same operation. <sep> In section 4.1 it's not clear to me what this sentence means: <sep> At the end of these operations, both zero states |0⟩ in Q1 and Q2 are y, and the |0..⟩ state in the combination of these two systems, Q1,2 will be y2. How can the zero register be y2? <sep> What is the p in the Discussion section, when discussing the result of Yarotsky? <sep> I think in some parts of the paper the authors use n for the number of qubits, and then log⁡n. <sep> Figure 1 could be split into two figures (left and right), and the notation in the figure could be better explained as the figure is referenced many times in the paper. <sep> Is written in the section where the  prologue phase is described ""As pointed by Bravo-Prietoet al. (2020), unitary matrix A can be decomposed to the quantum circuit with gate complexity ofO(logn), where logn is the number of qbits."" This is only true if the matrix A is of the kind specified before, i.e. it's just one single column. <sep> The fact that the depth of BPNN in hybrid quantum-classical computing can be of O(1) is a strong result that perhaps should be compared more with the literature on the power of quantum shallow circuits or constant depth circuits. <sep> Also, I think the work should conform to the widespread and standard notation of using qubits and not qbits. <sep> Some typos: <sep> Proof Proposition 4.2. ""the Of"" should be ""the of"". <sep> After lemma 3.3 ""which is introduced in the following texts"" -> which is introduced next (or in the following section). <sep> ""To take the advantages of high-parallel in quantum computing, we made an observation on the network structure of BPNN as described in the following Property."" Might be improved. It might be changed into ""high-parallelism"" and rephrased the whole sentence. <sep> Section 4.2 <sep> ""d input variables and f has weak derivative"" should be derivativeS. <sep> . This brings flexibility in implementing functions (e.g., ReLU), while at the same calls for interface for massive data transfer between quantum and classical computers. Perhaps you wanted to say ""at the same time calls for fast interfaces""","This paper provides approximation results for functions that can be represented by hybrid quantum-classical circuits. It is felt that venues such as QIP would be a more suitable venue, and perhaps some experiments/simulations could be added."
"Summary <sep> The use of permutation decoding improves the performance of some types of structured channel codes. Still, the question remains on which permutations (out of the vast set of possible ones that most of these codes allow) select to decode, in a way that we increase the chances of a successful recovery of the transmitted codeword. Obviously, we could just pass all the possible permutations through the decoder, but this is clearly extremely inefficient and hinders the usage of these methods on real-world scenarios. In the current manuscript, the authors propose a method to select the best possible permutation for every received corrupted codeword y. This allows increasing the performance in terms of error rate, and preserve the efficiency of the decoding process, up to the overhead introduced by the permutation selection procedure. The methodology is based on a permutation classification procedure (trained beforehand), where each permutation is encoded into an embedding vector obtained using the concept of self-attention, to account for the geometric similarity between transformations. The pipeline presented allows improving previous permutation decoding methods by several dBs at almost no extra cost. Hence, it renders an interesting alternative to improve further the decoding of powerful and widely used coding schemes such as Polar codes, that also accept permutation decoding. <sep> Strong and weak points <sep> Honestly, the use of self-attention for this specific task sounds quite convoluted. Still, there is no doubt it actually works, and the performance is significantly outperformed. It is unclear how this attention looks between the permutation vectors, but in the end what matters is the embedding obtained, and how it captures the similarity between them, and also, I believe, some relations with the syndrome. Still, the most important advantage is that it actually should not decrease significantly the efficiency, as the forward pass required during test could be run in parallel for all permutations, as the authors suggest. This is extremely important, and it is one of the most positive aspects of the present methodology. Besides, the paper is nicely and clearly written, with all the important details explained and all the required background information, which allows perfectly understanding the methodology. <sep> Still, there are some important aspects, concerning the results, that may require, from my point of view, some further comparison/study (more details in the section ''Questions and additional evidence''): <sep> First, I do consider the comparison of GPS + (W)BP vs rand quite unfair. Is there not any other, more advanced, methods for selecting at least a subset of permutations? IN BPL, although it is applied to Polar codes, the authors finally make use of only 5 different permutations, which is not that ineffective, and enable them to report a great performance. <sep> The previous concern also takes me to Fig. 3a, where we can observe how there is still a lot of room for improvement, as taking the 5 top permutations still yields some extra considerable improvement. <sep> When performing the ablation studies, it is not clear to which of the codes those degradations correspond. I mention this because the performance degradation, for all ablations, looks quite large and pernicious, but it is difficult to assess their impact without a proper comparison. Hence, it would be interesting to see a plot showing the actual SNR values at a BER of 10−3 for some of the most harming ones, such as the exclusion of s′. I consider depicting such comparison quite important because, in some cases, it seems that the degradation may take the model to worse performance than rand-WBP. <sep> From my point of view, the aforementioned problems hinder the comparison of results, therefore impeding a full assessment of the scheme used, the decisions taken, and its performance compared to other similar approaches. And this is for me the weakest point of the present manuscript. <sep> Decision, and key reasons <sep> Accept, after discussing and further elaborating some of the previous concerns. Despite the previously described issues, I believe the paper presents an interesting method to advance the current state of permutation decoding. Thanks to the possibility of freely generating training samples on these schemes, it is possible to achieve very satisfactory training for permutation embedding and classification. Once that is achieved, this model can be applied at almost no extra overhead, which indeed renders the current pipeline much more useable on real-world scenarios. Additionally, the paper is nicely written and justified, easing the understanding of the methodology and the need for such an approach. <sep> Questions and additional evidence <sep> Nevertheless, I believe the authors should elaborate more on the following concerns: <sep> Would it be possible to propose a more fair comparison? If not, perhaps please comment on the selection of permutations done in approaches like BPL, and how that selection criterion won't be applicable here. This may help to alleviate the feeling that comparing against rand seems a bit unfair. <sep> The results of Fig. 3a are quite revealing, and the room for improvement is still pretty large. Could you discuss a little bit more about the source of this error in the selection of the permutation? <sep> Also, and as part of some supplementary material, I would appreciate a more visual comparison of the different SNR values when performing the ablation studies. And also, discard subsets of them at the same time, etc. This will help to understand better the most key components of the pipeline, as currently it just seems everything is extremely essential, and changing any single aspect will degrade significantly the performance. <sep> I would like to understand why you take the absolute value of the LLR. I understand that this is the only way to proceed when you train with all 0 codewords, but is it not harming your training? <sep> Finally, I am curious about the Block error rate, as I can imagine that sometimes, when a permutation is quite wrongly chosen, it might push y to a different c, hence leading to s=0, but still resulting in a wrong decoding. Could you provide some more insights about this? <sep> Extra feedback <sep> I just want to conclude with a suggestion for a correction and a typo: <sep> In Section 2, second paragraph, the sentence ''Codes with good decoding performance are represented by graphs with cycles'' is a bit confusing, as it could be understood that indeed graphs are helping with the performance. Although you clearly explain this in the following sentence, I would rephrase that sentence. <sep> In page 4, when indicating the dimensions for the matrices to learn, I believe the dimensions are wrong, as they should be Q,K,V∈Rdp×dw.","The reviewers positively valued the proposed idea of performing permutation selection in permutation decoding via combining node embedding and self-attention, which seems to be of high originality. I found that this paper is mostly clearly written, except Section 3.2 as AnonReviewer5 commented. The main concern among the reviewers is regarding applicability of the proposal beyond the BCH codes. <sep> Pros: <sep> The proposal of utilizing self-attention for permutation selection in permutation decoding is novel and interesting. <sep> Computational complexity in the decoding phase is only slightly increased compared with random permutation selection, and is far smaller than performing decoding for all permutations. The GPS classifier can be parallelizable to further reduce latency. <sep> The proposal should be applicable beyond the BCH codes to those with decoding based on the Tanner graph, including polar codes. <sep> Cons: <sep> Only the BCH codes were considered, whereas in the authors responses they will add a short analysis on polar codes. <sep> It seems that systematic enumeration of the PG is required, which would limit applicability of the proposal. <sep> There is a room for improvement in presentation: <sep> In Section 3.2, the description of ""positional encodings"" was unclear to me, in that the ordering of the codeword entries is arbitrary, unlike typical sequence transduction problems to which attention mechanism is being applied. <sep> Dependence of the input vector sequences of the attention head on the permutation π is not clearly explained. <sep> Performance of the proposed method might depends on choices of the parity-check matrix, which is however not discussed in this paper at all. <sep> Based on the above concerns, the paper is not yet ready for publication in its current form. <sep> Minor points: <sep> In page 3, line 14, ""that"" should be deleted. <sep> In references list, ""hdpc"" should be in capital. ""reed-muller"" should be ""Reed-Muller""."
"===============Update after rebuttal period================ <sep> The connection between the contrastive learning objective and discriminative learning is made via ""resemblance"". And the author claims the ""resemblance"" as a theoretical contribution, which the first reason I vote for a clear rejection. This issue has not been addressed by the authors. The second reason for my rejection of the paper is the paper requires an effort to make it self-contained, especially for the experimental section. I remain my score of clear rejection. <sep> ======================================================= <sep> This paper connected contrastive-learning and supervised learning from the perspective of the energy-based models. Then, the authors combine both objectives and evaluate the presented method on various datasets and tasks. <sep> Strengths: The paper attempts to connect supervised and contrastive learning. I like the attempt. But unfortunately, I don't think it is valid. See explanations as follows. <sep> Weakness: <sep> I feel the claim in the paper is too strong. The approximation from equation 12 to 13 is very crude. Specifically, the approximation states that the infinite integral (for the normalization constant) can be replaced by a finite sum, which is generally not true. <sep> Even if we assume the above approximation is fine, the connection with contrastive learning is very unclear. Precisely, the approximation is for modeling p(x|y), yet the contrastive learning is modeling p(x_1|x_2) with  x_1 and x_2 being the outcomes from correlated data. The authors do not discuss or compare between p(x|y) and p(x_1|x_2), and hence it makes the connection very vague. <sep> The resulting objective (eq. 15) is a combination of the discriminative and generative modeling, which has already been studied. <sep> On page 4, ""the representation captures important information between similar data points, and therefore might improve performance on downstream tasks."" This sentence is super vague, and I can't understand what the ""important information"" is and why if we capture ""this important information"", we ""may improve performance on downstream tasks."" The author should spend time polishing the presentation. <sep> The main complaint of the presentation is the overclaim for the experimental section. I understand the contents are too much, and hence the author must move some experimental sections into Appendix. The author claims that the proposed method is performed on adversarial modeling and generative modeling, while these two sections only appear in the Appendix. In the last few lines of page 5, the author seems to rush the remaining experimental sections into the Appendix and asks the reviewer/reader to read themselves. The author should spend time arranging the contents and make sure the paper is self-contained. <sep> ================================== <sep> Summary of the reasons why I vote for rejection: <sep> The main contribution of the paper by connecting supervised learning and contrastive learning is overclaimed. The approximation of the intractable normalization term is not appropriate. The connection with contrastive learning is not solid. <sep> The paper doesn't seem to be ready for submission. The content is not organized well and some ambiguous wordings should be avoided. <sep> [1] Representation Learning with Contrastive Predictive Coding by Oord et al. <sep> [2] On Variational Bounds of Mutual Information by Poole et al.","The paper proposes hybrid discriminative + generative training of energy-based models (HDGE) building on JEM. By connecting contrastive loss functions to generative loss, HDGE proposes an alternative loss function that reduces computational cost of training EBMs. <sep> The reviewers agree that this is an interesting idea and that the empirical results look promising. <sep> However, multiple reviewers raised concerns that the theoretical justification was incomplete and felt that some of the claims about the equivalence between the two, as well as some of the practical approximations introduced, need more justification. <sep> I encourage the authors to revise the paper and resubmit to a different venue."
"Summary <sep> In this paper, the authors propose a novel approach for quantifying the statistical significance of binary masks predicted by a subclass of deep neural network (DNN) models for image segmentation problems. <sep> In brief, the manuscript considers the particular setting where a model has been pretrained to produce a binary output of the same dimension as the input, which can be interpreted as a binary attention mask / image segmentation output. In this scenario, the main contribution is an approach to test the null hypothesis that a linear contrast of the image, depending on the segmentation outputs, equals zero. In particular, the specific formulation put forward in the paper tests the null hypothesis that the average value of pixels classified as 1 (e.g. in the mask) is equal to the average value of pixels classified as 0 (e.g. not in the mask). <sep> Since the linear contrast is a function of the input, in this case, a nonlinear function implemented by the pre-trained DNN model, classical statistical inference (such as e.g. a two-sided t-test) does not apply. Instead, the authors propose to leverage recent advances in the field of selective inference to derive the null distribution of the test statistic conditioned on the DNN's output (plus additional constraints to get rid of nuisance parameters). <sep> In a nutshell, to accomplish this, they: <sep> Assume that images follow a multivariate normal distribution with known covariance matrix, <sep> and <sep> Restrict the class of DNN models to those whose internal representation in every layer can be expressed as a piecewise affine function of the input. Namely, this excludes activation functions other than piecewise linear functions (e.g. ReLU), which the authors propose to approximate using piecewise linear functions. <sep> The modelling assumption (1) together with the model constraints (2) permit the problem to be cast in a form for which the celebrated Polyhedral lemma (Lee et al. 2016) applies. <sep> Finally, the authors also tackle the algorithmic problem of efficiently integrating over the resulting truncation regions. To this end, they propose two approaches: one based on over-conditioning, inspired by Lee et al. 2016, and another based on the homotopy method, possibly inspired by Liu et al. 2018. <sep> Experimental results are provided for a synthetic toy problem with ""images"" that follow a multivariate normal distribution with no correlation between pixels. The ground-truth attention mask is a large square region in the center of the image, and pixels within this region have a larger mean than those outside. DNN models consisting of 2 convolutional layers are pretrained on data generated under the same distribution, with the entire ground-truth binary segmentation masks used for training for each ""image"". Results suggest that, under these conditions, the proposed approach succeeds in controlling the type I error while achieving non-trivial statistical power. The supplementary material shows that type I error remains controlled for other distributions such as Laplace or Student-t with 20 DoF. <sep> Finally, results are also shown for a real-world brain image dataset, where type I error appears to be only sensibly larger than the tower and power non-trivial as well. <sep> High-level assessment <sep> From a methodological perspective, the authors have identified a novel, interesting application of existing work, namely, the now seminal work of Lee et al. 2016 and recent improvements over those ideas presented by Liu et al. 2018, to the problem of image segmentation using certain DNN models. Moreover, they also proposed a sound algorithmic implementation of these ideas targeting the application at hand and provided some nascent, preliminary results that suggest the proposed approach might be of use in certain applications such as medical imaging. <sep> All in all, this represents an original contribution of relevance to the field that I wish to see published eventually. However, I believe the current version of the manuscript falls short in certain areas, and should be improved prior to publication. Mainly: <sep> I have strong reservations regarding the adequacy of the modelling assumptions required by the selective inference framework for this application. <sep> The experimental results are substantially lacking, both in terms of breadth and depth. <sep> The paper is also lacking in clarity. Many key, low-level aspects such as model architecture, (pre)-training procedure or assumptions between the relation of pre-training and testing data are either not clearly stated or mentioned without proper justification or discussion of its implications and impact on the overall results. Likewise, I feel that some of the limitations of the proposed approach are understated and that its generality is overstated, especially in the abstract and the introduction. <sep> Because of this, I am as of now leaning towards recommending rejection, as I believe the paper would greatly benefit from a strong, non-incremental revision. <sep> Major points / suggestions for improvement <sep> In order to frame the problem within the assumptions of the Polyhedral lemma, the authors postulate that images in the dataset follow a multivariate normal distribution, with unknown mean but known covariance matrix. <sep> Unfortunately, while selective inference is known to abound on strong parametric assumptions, I believe this assumption is just too strong and hard to justify in the context of image data relative to, say, interpreting the coefficients of a linear model as done in more ""typical"" applications of selective inference. <sep> While it might be unrealistic to expect the authors to overcome inherent limitations of the current state of the art in selective inference, I believe they should at least carry out a much more thorough investigation of the extent to which these assumptions are applicable to real-world data, and what are the consequences of violations for the inference process. <sep> The robustness results provided in the supplementary material are a good step in this direction, but keep many unrealistic assumptions, such as unimodality or simplistic / inexistent correlation structures that remain far from representative of natural or medical images. <sep> The proposed approach relies on pre-training the DNN model on a separate dataset, since the authors have made no attempt to account for the impact of training itself on the null distribution. <sep> This can be an important limitation, especially in applications where data does not abound, which unfortunately might have a strong overlap with applications for which statistical significance might be of special relevance. Moreover, the need to split the data into ""development"" and ""inference"" sets will introduce randomness of the inference results, apart from incurring a loss in statistical power. <sep> To this end, I believe the authors should: (1) discuss in much greater clarity how the pre-training of the DNN models interacts with their proposed approach; (2) study the extent to which the resulting inferences are robust to data splitting & retraining of the DNN model and (3) the impact that data splitting has on the resulting statistical power. <sep> The paper is substantially lacking in clarity, leaving the reader to second-guess many key aspects of the proposed approach. <sep> One such example is precisely the issue of pre-training. It is never clearly stated in the main manuscript how the DNN models are (pre)-trained, and what the relation between the training and inferences datasets is exactly. <sep> Other, related aspects are (i) which type of supervision the models require for pre-training and (ii) which type of output layers are required in order for the null hypothesis to be relevant. For example, I had to look at the code in order to know that, in their simulation experiments, models are trained with the segmentation masks as supervision targets. To the best of my knowledge, this is nowhere stated in the paper. Similarly, I am uncertain of how the models for the real-world brain image dataset were trained. Given the change in architecture reported in the supplementary material, I could guess that in this case only tumor/no tumor labels were provided, but this is neither stated nor, as far as I could see, shown in the code provided. Also, in the latter case, I imagine that in order to keep the null hypothesis relevant, the model must rely on a global average pooling layer prior to classification since, otherwise, a null hypothesis defined in terms of the difference of pixel intensity values would not capture well the behavior of the output layer(s). More generally, the choice of architecture for the models is not justified in any way. <sep> As a final example, it is also never precisely stated how exactly the output representation is thresholded in order to define the binary attention mask, how the threshold parameter is supposed to be chosen by a user and what its impact is in overall performance. <sep> All in all, in my opinion, at the moment the manuscript does not provide sufficient information for a reader to accurately reproduce its results from the text alone. <sep> In my opinion, the abstract and introduction currently overstate the generality of the proposed approach. I believe that claims such as ""testing the reliability of DNN representations"" implicitly imply the approach can be applied to any kind of DNN model/problem. However, the target application for the framework here developed is narrower and applies only to image segmentation problems and perhaps to other strongly related settings (e.g. problems where the target representation has the same size as the input and can be interpreted as an attention mask of sorts). <sep> To clarify, I do not intend to imply the author's contribution is insufficient. I believe proposing an approach to quantify the statistical significance of image segmentation masks is on its own a worthwhile contribution. However, I do believe it would be preferable that the abstract and introduction were written in a way that was more specific to what the manuscript truly proposes, implements and tests. <sep> The experimental results, while providing some encouraging preliminary results supporting the proposed approach, in my opinion fall short in multiple aspects. <sep> Firstly, as mentioned in point 1. above, I believe the experiments on synthetic data as they stand now are barely a ""sanity-check"" for the model. Instead, I suggest that the authors explore ways to create more challenging synthetic datasets capturing characteristics of real-world data, perhaps (but not necessarily) by exploiting deep generative models trained on natural / imaging data. In brief, the goal should be to exhaustively characterize (1) the extent to which the modelling assumptions required by selective inference are applicable to real-world data and (2) which failure modes / limitations the proposed approach might have. <sep> Secondly, the models used by the authors are too small and not representative of those deployed in the target applications. I would encourage the authors to show results pertaining larger models (e.g. ResNet-based architectures with > 10 layers). <sep> Finally, related to the previous point, it would be of interest to explore the effect of model size & architecture (depth, number of units per layer, presence of residual connections / normalization) on the number of intervals encountered, false positive rate and statistical power. <sep> Minor points (not related to the manuscript's rating) <sep> In my opinion, Example 1 would be much more informative if it included inactive units rather than the special, much easier case where all hidden units are active. <sep> Personally, I do not find the definition of power in the supplementary material to be sufficiently clear. I would encourage the authors to define mathematically what # detected and # rejected are in their context. <sep> To the best of my knowledge, the code does not currently set the seed for the PRNG, which might be detrimental for reproducibility. <sep> I would recommend having the submission proof-read for English style and grammar issues.","Four reviewers evaluated your work and provided a detailed review with many suggestions. I also think that there is an interesting idea and encouraging results but there is a lack of numerical results and still some parts are still unclear and need to be polished. Consequently in its current form, the paper can not be accepted for publication. I would advise you to carefully follow the remarks of reviewer 1 to improve the paper."
"Summary of Contributions <sep> The paper explores adversarial perturbations in deep RL, providing a new thread model where the perturbation is computed based on a single state. The paper explores the impact of various types of perturbations between states in the same environment (state transferability), and between states in different environments (environment transferability). <sep> Review <sep> I think the paper does a good job at capturing the extent/prevalence of the issue of adversarial perturbations. I liked the breadth in the types of perturbations considered, and how they map to scnarios that could happen in practice (e.g., restricted adversaries, etc.). <sep> One thing that I think could improve the paper is teasing apart properties of environments or learning algorithms that are telling of the transferrability of the perturbations. For example, beyond suggesting that the offset pushes things beyond the decision boundary, checking things like how action gaps change may be insightful. Some of the action manipulation may further be exacerbated by things like using ε-greedy behavior policies which immediately snaps to highest-valued actions, in contrast with things like Boltzmann policies over action-values or policy gradient methods, which are smooth with respect to changes in them. Do the authors have any insight as to how the trends might change should a smooth policy be used? <sep> While the focus is on deep RL, perhaps a simple, motivating example (e.g., a little gridworld or Markov chain) could make a stronger/clearer case as to what exactly is happening, and suggest what situations one might expect it to be a more prevalent issue. <sep> Beyond this, I have the following questions/concerns: <sep> Is there a reason for the choice of DDQN, over say, regular DQN? While one algorithm may perform a bit better, if the emphasis of the paper is to measure the extent of adversarial perturbations, it seems like it may paint a clearer/more convincing picture if a simpler algorithm is used, with fewer moving parts to attribute performance differences to. Along these lines, I think it would be more convincing to carry out the same experiments with another deep RL algorithm (perhaps a policy gradient one, to be representative of both value-based and policy-based methods) to see if comparable levels of and trends in transferrability are observed there. Can the authors comment on whether <sep> How were hyper-parameters chosen for the DDQN agent, and can the authors comment on whether such choices can have reasonably strong interplay with the transferability of perturbations? For example, would larger/smaller learning rates, or deeper vs wider networks be more or less resilient to such perturbations? I think in any case such details need to be included in the paper, to be clearer that the transferability quantified is in the scope of a particular instantiation of a specific algorithm. <sep> Overall, I'm erring toward acceptance in that it outlines an interesting framework to study adversarial attacks in deep RL, and provides some early empirical results and intuitions around them. I do think the paper falls a little short in that there wasn't a representative sample of deep RL methods, as well as not commenting on design choices made and how/whether they might interplay with the transferability measured. I'm willing to adjust my score should my concerns be addressed.","The work studies the transferability of perturbations/adversarial attacks on DRL agents. As a way to mitigate the cost of generating individual perturbation for each state in each episode, the authors proposed several variants to use same perturbation across different states across different episodes. While reviewers recognize the potential of the direction, they are not comfortable accepting the paper at its current state. The experiment results in its current form does not provide enough support to the claim. In particular, it is unclear how much the shared perturbation changes the original perception in comparison to the individual comparison, and how should the impact number differences be interpreted. Reviewers brought up concerns regarding all the experiments being evaluated on a DDQN agent, and not enough clarities has been provided on the different design choices. If perceptual similarity is not the indicator of environment transferability, do the authors have intuition on what does?"
"This paper concerns the problem of learning from single-label supervision, when this label is known not to be the truth. This is called complementary label learning. Some loss functions are proposed that are claimed to have the same theoretical minimizer as the one for standard labelling. <sep> The research agenda of the paper looks reasonable, even if it can be seen as a very specific instance of partial label learning (where one just considers the complement of the complementary label and tries to learn from it). A positioning with this latter approach therefore seems necessary. Also, after reading the paper, there are some unclarities left about the authors claim. Below are some more specific comments about that: <sep> Introduction: it is claimed that getting complementary label is easier than getting true labels, however complementary labels have to be certainly false, and while there are indeed theoretically more wrong labels than right ones, it is not entirely clear whether getting certainly false labels is easier than getting true ones in practice. Are there applications or empirical studies demonstrating that? Most mentioned papers do not appear to have actually applied the setting. <sep> Connection to partial label learning: the current framework can be seen as a peculiar case of partial label learning, as if I take a complementary label y¯, then its complement Y∖y¯ is a partial label certainly containing the truth. It would then be necessary to connect the current work to this trend, for instance to Cour et al. ""Learning from partial labels"" (JMLR 2011) or the more recent works of, e.g., X Wu, ML Zhang ""Towards Enabling Binary Decomposition for Partial Label Learning."". <sep> Definition 2: I do not really follow definition 2. First, are \\theta^* and \\theta arbitrary parameters values? Why call it \\theta^* (suggesting some kind of optimality)? If   \\theta^* is a minimizer of one of the two losses, then either the premise or the conclusion is a tautology (making the definition kind of meaningless). <sep> Proof of Theorem 1: I have some trouble with this definition. First, Equation (13) seems trivial if \\theta^* is the finite sample optimal model (also, why not identifying the search space with the space of parameters?). I also do not really follow the next line, as it is unclear how realistic it is to modify the parameters for just one instance? It is also unclear what is to be proven here, as \\inf \\sum \\geq \\sum \\inf, thus allowing for instance-specific parameters would always give something better than a global minimizer. In summary, I am not really convinced by this proof. <sep> In the experiment, I wold expect a comparison with other approaches (complementary but also partial label learning), but more importantly with the optimal models obtained on learning from the initial true labels, if only to demonstrate that the proposed theorems are valid. The asymptotic accuracies displayed are also very far from state-of-art standards (less than half of it) for CIFAR 10, which seems to contradict the fact that complementary labels have the same minimizer (hence comparable performances) as the one obtained with true labels? <sep> Finally, the paper contains an important numbers of typos or questionable grammatical structures. For instance in the first two pages only: <sep> ""supper"" --> super <sep> ""A complementary-label is only specific that the pattern"" <sep> ""in some questions refer to private."" <sep> ""the best hyper-parameter by empirical risk since"" (by empirical risk minimisation) <sep> ""can be summary as""","the paper undoubtedly tackles an interesting problem in the mainstream of learning with partial / unknown / weak / noisy / complementary labels. The authors have had a set of constructive suggestions and questions from the reviewers (and external comments), some positive, some negative. I find it a bit unsettling that to several major questions, the main feedback from the authors was a citation in the paper with no further action; (a) ablation tests of R2 end up in citing papers from a public comment, (b) R4 raised a key point in comment 2, with the links to partial labels learning. The authors' answer is not satisfying as one would have hoped at least of a partial justification of the author's approach in this context. The authors would have had time to develop at least elements of a formal comparison. Just citing the work is not enough;"
"###################################### <sep> Summary: <sep> In many real world applications for RL such as medicine, there are limits on the number of policies from which we can simulate data. This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Experiments on a medical treatment environment and Atari show that the approach obtains similar performance to on-policy RL with fewer changes of the simulation policy. <sep> ###################################### <sep> Pros: <sep> The proposed approach is straightforward, adaptive, and achieves results comparable to the classical on-policy setting with fewer policy switches on all environments shown. It is also applicable to both model-based and model-free RL. <sep> The paper is organized well, and the algorithms are clearly explained. <sep> Cons: <sep> I did not find the theoretical justification for the proposed approach to be very convincing for RL, since it is based on a construction in a simplified linear regression case. However, I think this is ok since the paper is application-focused. <sep> Based on the results on GYMIC, the proposed approach seems to have much greater variance than the other algorithms, especially at the early stages of training. This is often detrimental for the applications considered, such as medicine, in which robustness is also desirable. <sep> ###################################### <sep> Overall: <sep> I would lean toward accepting this paper. I am not completely familiar with the literature on RL with low switching cost, but the proposed approach appears to be novel. The experiments show that when combined with Rainbow DQN,  it effectively reduces the switching cost on a range of environments and is based on the training path, requiring less environment-specific hand-tuning than fixed or adaptive interval switching. <sep> ###################################### <sep> Further comments and questions: <sep> How were the six Atari games chosen? How do the different approaches compare in other games? <sep> There are a number of typos, e.g. ""deno"" in the first paragraph of section 3.1. <sep> ###################################### <sep> Update after reading other reviews and author response: <sep> I have decided to lower my score from 6 to 5, as I agree with Reviewer 3 that more experimental analysis of the method is needed (ablations, sensitivities, etc.) given that the theoretical backing is not convincing. The authors also did not directly answer our questions.","This paper studies RL with low switching cost under the deep RL setting. It provides new heuristics for doing so. The reviewers are worrying about whether the problem is important in practice, whether the policies obtained can be used in practice, and the theories might not be strong enough. The paper can be strengthened if better theory and more experiments are provided."
"This paper studies a new theoretical framework to understand the ability of <sep> ConvNets to deal with pattern recognition tasks. The authors suggest a new property of convents where filters have large dot products with patterns occurring in images classified as 1 and small dot products with patterns appearing with images classified as 0. <sep> It is assumed that there are two unique patterns whose occurrence in an image determines if the image is classified by 1 or 0. <sep> I like the classification problem studied in the paper and the attempt to understand how ConvNets work from first principles. <sep> My issue with this paper is that it uses rather cumbersome measures (PSI and detection ratios). The setting is very specific (one type of architecture, realizability assumption, a single positive pattern and a single negative one). <sep> The combination of these factors casts doubts on whether these measures and analysis will be of any use elsewhere.  The authors claim: <sep> ""Empirically, we identify a novel property of the solutions found by SGD. We observe that the statistics of patterns in the training data govern the magnitude of the dot-product between learned pattern detectors and their detected patterns. Specifically, patterns that appear almost exclusively in one of the classes will have a large dot-product with the channels that detect them. On the other hand, patterns that appear roughly equally in both classes, will have a low dot-product with their detecting channels. We formally define this as the ""Pattern Statistics Inductive Bias"" condition (PSI) and provide empirical evidence that PSI holds across a large number of instances. We also prove that SGD indeed satisfies PSI in a simple setup of two points in the training set"" <sep> I find this somewhat unsatisfying. The novel property mentioned by the authors is one of the first that comes to mind when thinking about the success of ConvNets. It seems very unlikely that this has not been observed before (at least empirically). While obtaining rigorous proofs of properties of NN is hard, one would expect that for the simple setting studied by the authors there would be a simpler explanation for the so-called PSI. <sep> This paper makes numerous restrictions and assumptions. <sep> Some examples: <sep> ""a natural model in this context is a 3-layer network with a convolutional layer, followed by ReLU, max pooling and a fully-connected layer."" <sep> Why is this a natural architecture? Has it been studied before or used before? <sep> Also in the classification task why is it assumed that n<d? <sep> There are many more such examples. <sep> The proof of Theorem 5.1 uses the Sherman-Morrison formula without giving the formula and showing how it is used. <sep> The survey of related work is very short, not mentioning many relevant works looking into similar pattern recognition problems. <sep> Some examples: <sep> Learnable and Nonlearnable Visual Concepts (Shvayster, 1990)On learning visual concepts and DNF formulae (Kushilevitz, Roth, 1996)","This paper considers a new model of input data specific for image classification problems. In particular, the high level idea is that each image contains certain patterns, and which patterns it contain decides its label. In this framework, under some stronger assumptions (e.g., patterns are orthogonal, one positive pattern and one negative pattern, PSI assumption, etc.) the authors showed that SGD on a 3-layer overparametrized convolutional network will be able to have a small sample complexity, while the VC dimension would be at least exponential in d. The paper also provided some empirical evidence on a modified MNIST dataset. While the idea seems to be an interesting first step, the reviewers find that the current version of theory still relies on fairly strong assumptions."
"This paper proposed AdaMa, which can automatically use adaptive learning rates for each agent in cooperative Multi-Agent Reinforcement Learning (MARL). AdaMa calculated the learning rate of each actor and critic according to their contributions of locally increasing value functions.  Simple experiments using toy examples show that the proposed AdaMa method can improve fixed learning rate method and other heuristics. <sep> Pros: <sep> The topic and idea of using adaptive learning rates to avoid hand-tuning are quite interesting and important. I think the related topics are worth investigating. <sep> The proposed AdaMa method looks reasonable to me, at least from an intuitive perspective. <sep> Experimental results also look promising. <sep> Cons: <sep> The experiments look simple, and the improvements seem to be incremental rather than significant (please correct me if I misunderstood or missed something). I think more experiments on larger scale tasks are needed to make the effectiveness of the proposed method convincing. <sep> Using first- and second-order Taylor expansion to obtain the best possible learning rates seems to be a reasonable idea. However, I think some more rigorous theoretical understanding is worth pursuing to show the benefits of the proposed method in a more convincing way, e.g., the speed of convergence for fixed and adaptive learning rate methods. <sep> Overall, I found the problem and the idea important and interesting. The proposed method is intuitively reasonable and verified by small scale experiments. However, the proposed method is not convincing in terms of the lack of larger-scale experiments or theoretical results.","This paper investigates how to deploy adaptive learning rates in multi-agent RL (MARL). In particular, the learning rates are adaptively chosen based on which directions maximally affect the Q-function, and take into account the interplay and balance between the actors and the critics. The topic is certainly of great interest when designing fast-convergent MARL algorithms. However, the reviewers point out the inadequacy and insufficiency of empirical gains in the reported experiments. Also, larger-scale experimental settings are needed in order to provide more convincing evidence about the practical benefits of the proposed scheme."
"PAPER SUMMARY <sep> This paper introduces a new approach to probabilistic federated learning, which builds on the previous PVI work of (Bui, 2018). <sep> The proposed approach follows the same recipe in PVI where local agents learn their own model posteriors from private data, and communicate their posterior representations to a server, which aggregate local posterior representations into a universal representation. Local agents then download the aggregated posterior and offset it with their current posterior. The offsetted posterior is in turn used as the new local prior to re-run the corresponding local posterior approximation (via a generalized form of variational inference). New local posterior estimates are subsequently communicated to the server and so on. <sep> However, unlike PVI, the proposed method aims to replace the parametric representation of posterior with a non-parametric particle representation developed by the prior SVGD work of (Liu & Wang, 2016). This necessitate the development of a distributed particle aggregation algorithm in Section 4, which is the key contribution of this work. This development is also motivated by two practical desiderata of federated learning: (a) a good trade-off between communication load (per iteration) and no. of communication iterations; and (b) well-calibrated predictions that are more trustworthy. <sep> Following the above summary, I will give my opinions regarding several aspects of this paper below. <sep> NOVELTY & SIGNIFICANCE <sep> On the high level of idea, this paper presents an interesting perspective on a practical federated learning system: communication trade-off & trustworthy prediction. These are definitely important problems in the direction of making federated learning more efficient and robust. This is the novel angle that I like about this paper. <sep> Its technical development, on the other hand, is leaning a bit more on the incremental side as the entire system is pretty much the same as that of PVI with the exception that a new particle representation is considered instead of PVI's parametric representation (in the statistical form of an exponential distribution). <sep> A common pattern here is that both representations allow universal posterior information to factorize additively across local devices (in the respective forms of local posterior representation). In both cases, this leads to a variant of a distributed sum problem where each local party has some running estimate of some piece of local information & the goal is to communicate asynchronously so that each can refine its local estimate and eventually, recover the correct sum of information. <sep> In the case of SVGD, however, the exact local update would require buffering all previous particle representations (i.e., past estimates) to date so that the downloaded posterior can be accurately offsetted to act as a prior for the local model (i.e. independent of local data). This necessitates the development of a distillation scheme in Section 4.2 which is, to me, the key technical contribution here. In addition, the theoretical analysis on the U-DVSGD's per-iteration decrease for the KL divergence is also an interesting contribution. <sep> On this note, it seems the authors have deferred the demonstration of how well the KDE distillation approximate the original particle representation to various places in the appendix. Perhaps putting some of those back into the main text would be better (if space permits). <sep> On the practical aspect of this paper (i.e. communication load & trustworthy prediction), while the demonstration is sufficient against point-estimate method such as FedAvg and DSGLD, there is no comparison against other non-parametric probabilistic methods such as PVI (Bui, 2018) and/or PNFM (Yurochkin, 2019). Given that the difference between PVI and DSVGD is a matter of posterior representation, comparison against PVI is probably necessary to showcase that the particle representation yields better calibrated predictions. <sep> Also, the probabilistic non-parametric federated learning work of (Yurochkin, 2019) also allows multiple rounds of communication (although it can also be used as a one-shot model fusion of pre-trained local models) so it would be good to also compare both the communication load & the prediction caliberation against this work. <sep> TECHNICAL SOUNDNESS <sep> I have made high-level check of the derivations and have not found any technical issues. <sep> CLARITY <sep> The paper is very well-written, especially the part that summarizes the background on SVGD and PVI. <sep> REVIEW SUMMARY <sep> In short, this paper presents an interesting perspective on non-parametric probabilistic federated learning via particle representation of posterior. The technical development is sufficiently novel with demonstrated practical advantages against FedAvg and DSGLD. These practical advantages however were not demonstrated against existing probabilistic non-parametric federated learning works such as PVI & PNFM -- this is perhaps strange given that DVSGD builds on PVI and is mostly different only in terms of posterior representation. <sep> --- post-rebuttal feedback --- <sep> The authors have addressed most of my concerns. My rating for this paper therefore remains on the positive side.","This work presents a distributed SVGD (DSVGD) algorithm as a new non-parametric Bayesian framework for federated learning. The reviewers concerned with the practical advantages of the proposed method, including the communication cost and the constraint of updating one agent per time. The authors rebuttal helped addressing some of the concerns, including proposing a new Parallel-DSVGD algorithm. This is very much appreciated. However, given the significant modification needed over the original version, we think it is better for the authors to further improve the work and submit to the next conference."
"Summary: <sep> This paper proposes an opponent modelling technique for imperfect information games.  During training, a VAE is trained to encode the agent's observed trajectory to a latent space, and then decode it back to the full trajectory (including opponent observations and actions).  The encoder can be used at runtime using only information the agent observes, and if the VAE latent means are included as an input to the agent's neural net (e.g., with A2C) then the agent's performance resulting performance is enhanced.  Empirical results are presented that support the technique's effectiveness. <sep> Positives: <sep> The paper is well motivated and clearly written.  I found it easy to read. <sep> The problem is significant and the approach is a natural fit.  Of course opponent observations should not be used at execution time, and training a VAE to recover the missing information seems like a good approach.  The empirical results support the technique and I found them convincing. <sep> Negatives: <sep> The paper is sometimes inconsistent in its description of techniques that need opponent observations at training time versus execution time.  The technique is often presented as ""not needing opponent observations"" while competing techniques ""need opponent observations"" (e.g., the end of Related Work:Learning Opponent Models, and the conclusion), whereas they mean (and often, but not always, clarify) that their technique does needs opponent observations during training, but does not during execution, and competing techniques need them during execution.  However, the authors appear unfamiliar with other opponent modeling work (I'm particularly familiar with the computer poker domain, although they cite two Ganzfried and Sandholm papers from that area) where there is a rich literature of opponent modelling being successfully used under these conditions (opponent observations needed only at training time, and never at execution).  The ongoing challenge in that community (with success against human professionals!) is to go further and perform opponent modelling without needing opponent obervations at any time, by using only the agent's observations at training time as well as execution time.  In general, the authors should be more careful about specifying ""at execution time"" whenever they claim their technique does not need opponent observations, and should probably be aware of more related work in this setting. <sep> ""Opponent"" modelling.  In the intro the authors describe using 'opponent' as a neutral term to mean 'other agents in the environment', and I'm sympathetic to that as I've encountered this awkward phrasing in my own work.  However, of the 3 environments described in this work, 2 are purely cooperative (Speaker-Listener, Double Speaker-Listener) and the third is a mixed setting involving cooperation and greedy behavior.  None of these three environments are a purely adversarial (i.e., zero-sum) setting where we would properly call the other player an 'opponent', or even a positive-sum or constant-sum setting that does not involve cooperation.  This is more important than just the choice of terminology, however!  In the experiments, I can imagine the LIOM technique could possibly be inaccurate but still perform well overall, since the other player is aligned with LIOM's goals and not rewarded for exploiting any mistakes.  In an actual adversarial setting, even if the opponent's policy was static (i.e., not updated in response to LIOM's policy, as in the cited Ganzfried papers), the opponent could still exploit any mistakes LIOM might make through its static policy, possibly driving its performance below that of NOM.  I liked the paper and I'm half joking when I say this, but: a better empirical analysis of an opponent modelling technique would involve at least one experiment that includes an opponent in the strict sense, to demonstrate that the technique is effective and robust in environments where true opponents exist. <sep> Empirical analysis.  I found this to be very good for supporting the main claims of the paper.  However, I would really have liked to see even a small investigation of robustness, even with static opponents (e.g., not with adapting opponents or worst-case opponents as in the Ganzfried and Sandholm papers cited in the conclusion, although I agree with the authors that that would be a great next step for future work).  For example, in Figure 2, if we were to generate additional holdout opponents for each game that were not part of the training set, and evaluate against them, how does the performance curve for LIOM and FIOM respond?  Do they still perform pretty well (e.g., they've usefully mapped the holdout opponent to something nearby in \\pi_-1)?  Does it revert back to NOM?  Does it do worse than NOM, by making painfully incorrect assumptions about the opponent?  The answers here are pretty relevant to this work, since while we might train against a population of presumed opponents, our real opponents out in the world are unlikely to share their observations with us while we re-train!  If the technique remains a bit robust if we move outside it's training, then that's much more exciting than a technique that only works with its specific training set and fails dreadfully outside of it.  Further along these lines, I'd have loved to see a graph plotting 'Size of \\pi_-1' on the x-axis and 'Average return after X steps' on the y-axis.  In such a graph, I'd really love to see: when '|\\pi_-1| == 1' does NOM perform better than in Fig 2?  How quickly does LIOM (and FIOM?) drop off as we increase the opponent population size?  Is there something special about the population sizes used here, or can the approach scale well outside of those values? <sep> Related to the above two points: while the paper describes NOM as a lower-bound for LIOM, and it makes sense narratively to ""bound"" LIOM between FIOM and NOM, it really is not a lower bound in a mathematical sense.  FIOM might do worse than NOM in cases where 1) the other player is adversarial, or 2) the other player encountered at execution time has not been trained against, or 3) the other player was trained against, but insufficiently to create an accurate model, and overfitting to a bad model can be far worse than having no model.  In other opponent modelling work, I've seen opponent-aware agents do far worse than a baseline agent in those settings. <sep> Recommendation and Justification: <sep> I think the paper should be accepted.  The topic is relevant, the proposed solution seems like a natural fit, and the empirical results are pretty extensive (measuring reward and prediction accuracy) and convincing in their support of the technique. <sep> I have a few issues in the Issues and Suggestions section - some minor clarity points on the environments, a larger (but easily fixable) point about consistently describing when opponent observations are and aren't needed, suggestions for experiments I would have loved to see to verify basic robustness, and some notes about missed related work that also occupies this setting (use opponent info for training, never at execution).  Only one point (a line in the conclusion about this being the first successful use of opponent modelling under these conditions) is a strict concern; so long as that is addressed (or convincingly rebutted, if the authors disagree with the prior works I've cited), then I feel the paper is over the bar for acceptance. <sep> I don't have specific questions for the authors, but if they'd like to accept, address, or rebut anything I've listed in Negatives or Issues and Suggestions, I'd certainly welcome their response. <sep> Issues and Suggestions <sep> Experiments section.  The description of the three games left me with quite a few questions (what exactly is observed by the players?  Are Speaker-Listener and Double-Speaker-Listener gridworlds, or something else? etc), and Appendix B provided helpful pictures but didn't clarify the fine details.  Since the Appendix is outside your page limit, can you be more explicit about exactly what the agents observe (pixels? If so, the whole map or a small region near them? If not pixels, how is the input formatted?  In Level-based Foraging, do agents observe their own level? The other player's level?) and what actions they take.  As a further example, for Speaker-Listener - the text says ""each agent and landmark is randomly assigned a color"".  Is the Speaker assigned a color?  It sounds like it from the text, but it wouldn't be useful for reward, so I'm guessing not? <sep> More on the game descriptions.  Speaker-Listener is the ""Cooperative Communication"" game from Lowe 2017, right?  If so, it seems appropriate to cite its source!  If not, best to note the differences.  Are the other games original to this work, or should they be cited also? <sep> Level Based Foraging.  It's not abbreviated as LBF while describing the game, but that abbreviation is used later, and that threw me for a bit.  I suggest abbreviating it in the description. <sep> Speaker-Listener and Double Speaker-Listener.  This is pretty minor, but I'm a bit concerned about a specific detail of these environments (especially S-L) and A2C that might give LIOM an advantage over NOM by leaking information, without having to model the opponent well at all (although from Fig3, Fig5, and Tab1, it appears to indeed model the opponent well).  Specifically, the VAE encoder is trained with the agent's (Obs, Reward, Act) stream, so it could encode something about Reward.  Does the agent's A2C component take in only Obs as an input, or does it also include Reward and Last Action?  The original A2C paper describes only taking Obs as input, but in the practical A2C implementations in the codebase I use, the last timestep's reward and last timestep's action are often used as inputs (concatenated on after the Obs convolutional layers, before the LSTM) as these ""observations"" often aid the agent's learning and asymptotic performance.  So here's the catch.  If the VAE encodes something about the last timestep reward, and the A2C component does not (i.e., NOM doesn't see it), then LIOM might have access to a valuable feature that NOM does not, unrelated to opponent modelling altogether.  And in S-L, I can imagine a Listener successfully solving the game even with a mute Speaker, just by taking steps and seeing what direction it has to go for its reward to go up.  Could an LSTM learn that?  Possibly!  I know Speaker-Listener is a standard game, but I'd be curious to see results with either 1) an A2C implementation that observes last_reward and last_timestep to see if NOM (or LIOM (Act,Rew)) improves, or 2) a sharp version of the environment where reward is granted on reaching the goal, just to confirm that LIOM isn't winning because it's being leaked more information than NOM can observe. <sep> In the conclusion, the statement ""To the best of our knowledge, this is the first study showing that effective opponent modelling can be achieved without requiring access to opponent observations."" is untrue (well, the ""this is the first"" part, not the ""to the best of our knowledge part""  :^D  ).  The authors mean (and should clarify) ""...at execution time"", but even then, there is a rich literature of prior work.  I'm most familiar with the work in the computer poker domain, where opponent information is only used (if at all) at training time and never at execution time, some approaches do not need opponent information at either training or execution time, and the techniques have been successfully deployed in real poker games to defeat top professional human adversaries.  For example, in addition to the two Ganzfried and Sandholm papers cited in this work, consider: <sep> Computing Robust Counter-Strategies, NeurIPS 2007, Johanson et al.  Uses opponent observations at training time to compute robust counter-strategies, uses agent observations at execution time to choose which counter-strategy to play against the current opponent (who may not be one of the opponents used at training time). The UCB method described in this paper was used to compete against human professional poker players in Heads-up Limit Texas Hold'em in the 2007 Man-vs-Machine Poker Championship, which the humans narrowly won.  In the paper's experiments against artificial opponents, opponent observations were used at training time but not execution time, and some experiments involved a holdout opponent for which no opponent observations were provided at any time.  In the competition against humans, no human data was used at training time or execution time, and only agent observations were used to decide how to adapt to the opponent. <sep> Strategy Evaluation in Extensive Games with Importance Sampling, ICML 2008, Bowling et al.  The same conditions were used as above (use opponent observations for training robust counter-strategies, use only agent observations at runtime), but adds an unbiased, low-variance method for considering all possible states the opponent could be in given what the agent observed, and selecting a counter-strategy to use in response.  This method was used to defeat top human professionals for the first time in Heads-up Limit Texas Hold'em in the 2008 Man-vs-Machine Poker Championship.  In that event, no opponent observations were used at either training time or execution time: an even purer application than is described here, although I definitely agree that the strictest constraint must be to disallow opponent observations at runtime. <sep> Online Implicit Agent Modelling, AAMAS 2013, Bard et al.  This technique is similar to the above, in that it does not use opponent observations at runtime at all, and can be used without any opponent observations at training time either.  This technique focuses on the creation of a portfolio of useful counter-strategies for use against a broad set of opponents, such that at least one counter-strategy will be effective against any particular (and previously unknown, not trained against) opponent. The ""Implicit Opponent Model"" approach is to model opponents by how we can respond to them, instead of trying to capture (unobserved!) information about how they act at individual decision points, and is the cleanest refinement of the approach used in the previous two papers.  Again, this is rhetorically purer than the approach described in this paper, in that it never needs access to opponent information at training or execution time. <sep> These are just some of the works that I'm most familiar with; the computer poker community has been studying this topic for a while, and under the conditions described in this paper (opponent observations available during training, but never at execution time).  I'm not listing these citations as a suggestion to cite them (they are related to this work, and the topic is much broader and longer than the papers cited by the authors, but this paper is fine without the particular citations I've listed above).  I only list them as evidence to disprove the conclusion's assertion that ""this paper is the first study showing that effective opponent modelling can be achieved without access to opponent observations (at execution time)"", as that is definitely untrue. <sep> I've read the other reviews, the author's responses, and the discussion - thank you everyone.  I'm still in favour of accepting the paper.","The submitted paper is well written and easy to follow and also the idea of using VAEs for making inferences about the opponents on which a policy can be conditioned on is sensible. Also the reported performance in comparison to two baselines is good (although I have concerns about the selection of the baselines—see below). Acceptance of the paper was suggested by 3 of the reviewers and rejection by one of the reviewers. While I don't share all concerns of the negative reviewer, I also suggest to reject the paper. <sep> My suggestion to reject the paper is mainly based on seeing concerns of the positive reviewers more critical as these reviewers themselves and some concerns I have on my own. In particular, I don't think that all MARL approaches can simply be discarded for comparison—no matter whether the opponents are learning or not. Regarding the evaluation, I think that an environment with real opponents must be considered and that robustness is a key property that should be studied (otherwise an approach with a fixed set of best response policies and inference about the opponent might perform as well). In that regard I also find the selection of baselines insufficient—the minimum I would expect is to consider a NOM baseline using an RNN (which as far as I can tell is not the case) such that it could make inferences about the opponent. <sep> I want to acknowledge that the paper improved quite a lot during the rebuttal period in which the authors extended their discussion of related work on opponent modeling. <sep> In summary, the paper could be improved substantially by an extended empirically study (more environments + baselines + ""mismatch"" settings). If the currently observed performance gains also hold in these settings, this can become a good paper but in its current form I think the paper is not demonstrating that the proposed approach performs favorably over natural baselines and works well against real opponents."
"This paper proposed a meta RL algorithm built upon an assumption that a shared policy with a task-specific final linear layer can maximize expected return for each task.  The proposed method trains the policy using soft actor-critic on training tasks and trains an adapter network to predict task-specific final linear layer from a single timestep transition (s, a, r, s'). The proposed method showed a competitive performance with the previous approaches and showed a faster adaptation speed <sep> Strengths <sep> This paper showed that a simple approach can achieve a competitive performance on meta reinforcement benchmarks <sep> The presented method showed faster adaptation compared to previous approaches <sep> Weaknesses <sep> Motivation for the proposed method is not clear. The main assumption about the existence of an optimal policy with a task-specific linear layer is not justified well. <sep> This paper claims that the proposed linear representation meta RL results in a better generalization to out-of-distribution tasks. However, it is not clear why the proposed method is better for generalization. I couldn't find any discussion about this observation or empirical study for clarifying why the proposed method is good for generalization. <sep> Faster runtime is one of the biggest strengths of the proposed method, but there is no discussion on why faster runtime matters for meta RL from the text. <sep> Experiments do not contain ablation studies for the proposed method. For example, I would curious about ""predicting final linear layer vs predicting embedding"" or ""predicting from a set of (s, a, r, s') transitions vs predicting from a single (s, a, r, s') transitions"". <sep> Comments / Questions to authors <sep> I see a strong connection between this approach and PEARL, which predicts a low dimensional embedding from a set of transition data policy. What is the main benefit of predicting the last linear layer compared to predicting the low dimensional embedding? <sep> Does the adapter network change the linear layer parameter in every time step during meta-testing? What's the effect of changing parameters in every time steps vs predicting a single parameter from a good data point? <sep> In Figure 1, the proposed method showed about -200 reward in Cheetah-Vel (hard) tasks. Is this a reasonably high reward to claim ""generalization"" to this task? <sep> Recommendation <sep> I recommend rejecting this paper because it is not clear why ""linear representation meta RL"" is better in general. I believe this is because the paper lacks a thorough discussion or empirical studies to clarify this question. Even though the proposed method showed slightly better performance on a few meta-RL benchmarks, it is not clear where the benefits come from and it is not clear whether we can expect a similar gain in more challenging problems. <sep> After the author feedback <sep> I appreciate the authors for more control experiments and additional discussion in the manuscript. In the current manuscript, it is clearer that the ""adapter network"" predicting the ""final linear layer"" of the network is a unique component in this paper. I found that using an adapter network trained to predict linear layer parameters during training and used to infer parameters during testing is a setting that was not explored. However, I still see strong connections to previous works and I'm still not sure how to place this paper among such related works. <sep> I see the approach in this paper looks similar to RL^2 approach e.g. meta-training an LSTM based policy and meta-testing on unseen tasks. In this case, inferring the hidden state of LSTM looks similar to what the adapter network does in this paper.  Two differences of this work from LSTM based RL^2 are 1) neural network architecture, and 2) training objectives. <sep> One can construct a neural network architecture that is identical to an adapter network during testing time. One can train this network end-to-end during meta-training and use the exact same inference as this paper during meta-testing. This approach could be called RL^2 with a special neural network architecture that predicts the last layer of a neural network. If predicting the last layer of a neural network is an important component, it should be studied as an instance among variants of RL^2 with slightly different architecture. One practical concern about the architecture studied in this paper that this network may not scale to a case where the action space is large and the policy uses a large penultimate hidden state. In this case, predicting the parameter of a linear layer becomes very expensive. Because of the existence of this special case, I am not fully convinced about the claims that this method may work well in general. <sep> This paper trains the adapter network to predict parameters of a neural network instead of training the adapter network end-to-end during meta-training. Because of this difference, the method in this paper cannot be called RL^2 and it could be claimed that this paper explores a method that is not explored previously. If the training method is an important component, there should be at least one ablation for this detail. <sep> I still have a concern that it is not clear what's the main finding in this paper. Specifically, whether architecture is important or objective is important. I see that the paper already compared with RL^2, so adding an ablation study on using linear parameter prediction for RL^2 and discussing the relation to RL^2 would further improve the paper.","Summary of discussions: R1 was positive on the paper in their initial evaluation, and although dissatisfied with the author's feedback, continued to support the paper. I agree with R1's assessment that other reviewers' call for more theory is somewhat unfair, considering the fact that very similar papers don't usually include theoretical justification beyond intuitive motivation. <sep> By contrast, R3 is the most negative on the paper, leaning towards rejection. The main concern is that open questions remain as to whether the reported performance can be attributed to the architecture, or the loss function proposed. This is an important point to clarify, and further ablation studies would make the paper stronger. <sep> After considering the strengths and weaknesses of this work, the final decision was to reject. Authors are encouraged to improve this promising work and resubmit to a future venue."
"Summary: <sep> The authors advocate for the use of Robustness curves, plotting the adversarial accuracy as a function of the size of the neighbourhood region of allowed perturbation. The problem that they identify is that if you only evaluate adversarial accuracy at some numbers of threshold, you might conclude that some models (and the method that was used to train them) are more robust than others while it would be incorrect at other thresholds. <sep> General comment: <sep> The paper clearly describes the problem it deals with and reads easily. On the other hand, I am not entirely convinced by the importance of the problem. If you have a particular specification that you care about  (""Robustness against l_inf attack for eps=0.1""), you can just verify that. On the other hand, if your goal is ""I want my network to be robust"", then it's not properly defined, so of course it's hard to evaluate. Robustness curves will help there but there is still the problem that you might want to be robust to L_infinity, L_1, L_2, brightness difference, Wasserstein difference, changes of small patches... and then the robustness curves will not help you (unless of course you compute one for each difference). At the same time, they are quite a bit more costly to compute that simple point measures. While I agree that you are going to get more information if you compute a full robustness curve than if you sample it at a bunch of points, I'm not convinced that it is worth the effort. <sep> One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs. plotting on a robustness curve the pointwise measures and interpolating through them. This would be a much cheaper solution (and is essentially what reporting experimental results for a few chosen eps achieves). For example, the results the authors give in their Table 1. based on point wise measures wessentially achieves what the authors want to show: no defense strictly dominate the others. <sep> Specific Comments: <sep> The toy dataset example presented in Figure 1. is great and provides a great explanation of the problem that the authors identify. The constructive proof in Appendix A. is also quite interesting and really drives the point that the authors want to make. <sep> The authors argue that robustness curves allow to compare ""global robustness properties and their dependence on a given classifier, distribution and distance function"". In practice, does it really give insights global robustness property? If I look at the L_infinity robustness curves, it does not tell me much about the robustness to L_2 perturbations. <sep> I don't understand how the robustness curves are generated for the L-infinity case? If PGD is used to find adversarial examples, it's not likely to be the ""closest"" adversarial examples that is going to be found, in all likelihood it's going to be one that matches the epsilon given as input to the PGD attack (due to the projection)? There is nothing that is even encouraging the sample to be close to the input beyond the constraints used for projection. <sep> Even for the L2 distance, there is still the usual problem that, although the CW attack encourages to find the closest sample, there is no guarantee that it will, and the effectiveness it will have at doing so might depend from model to model. As a result, it's hard to decouple the robustness curve from the attack that it used internally. <sep> Minor Notes & Typos: <sep> To improve the look of the paper, it should be possible to include manual linebreaks in the title so that it's not broken in every line. <sep> The authors talk in the introduction about ""recently proposed robustness curves"" and cite a paper from 2020 for them, but it seems like those curves were already in use before that. ""On the effectiveness of interval bound propagation for training verifiably robust models"", Gowal et al. had some in 2018.; ""Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope"" had some (transposed) in 2017. <sep> At the end of the introduction, the author say: ""It is our belief that the continued use of single perturbation thresholds in the adversarial robustness literature is due to a lack of awareness of the shortcomings of these measures"".  This seems overtly harsh. You could make the same point about training algorithms and say that authors only reporting on only a few datasets due it just out of lack of awareness of the fact that the relative performance of different algorithms will vary depending on the dataset. Given that computing robustness curves needs computing the closest adversary to a point, this is much more expensive so maybe computational cost might be the differentiating factor rather than ""lack of awareness""?","The authors study ""robustness curves"" which are plots of the robust error versus the radius used in the corresponding l_p-ball threat model. <sep> Pro: I completely agree with the authors that the current evaluation purely based on evaluation for a single radius is insufficient <sep> and one should report the complete curve. <sep> Con: The authors are overclaiming that they have come up with robustness curves. Very early papers e.g. even in the adversarial <sep> training paper of Madry there are plots of robust accuracy versus chosen threshold. Moreover, I agree with one of the reviewers that using PGD for the purpose of a robustness curve is inaccurate and in particular inefficient as several attacks for different radii have to be done. There have been several attacks developed which aim to find the adversarial sample with minimum norm and thus compute the robustness curve in one run. <sep> The additional insights e.g. intersection of robustness curves are partially to be expected and I don't find them sufficient to move the paper over the bar for *CONF*. As these insights are additionally only shown for relatively small models which seem far away from the state of the art, it is unclear if they generalize. However, I encourage to follow some of the reviewer's suggestions to improve the paper."
"Summary: <sep> Contrastive learning is applied in a semi-supervised setting with few training examples to provide features for a linear classifier. A theoretical analysis is provided to show that contrastive embeddings allow to predict using a linear transformation. The model allows to recover the topic structure of a corpus that is generated from a generative topic model. <sep> Evaluation: <sep> Overall, the paper provides a thorough theoretical analysis to prove that the approach allows to recover topic posterior information. However, I am not convinced of the impact of this analysis, and the experiments are not entirely convincing. Especially the chosen LDA baseline is weak and some parameter settings are not discussed. If the corpus is generated from an LDA generative model, LDA should be able to recover the topic information. A stronger Gibbs sampling variant would be more suitable in my opinion whereas other baselines like the BOW are not very informative. <sep> To sum up, the objective of the paper is not entirely clear. While the theoretical analysis is interesting, I believe the presentation, clarity and writing of the paper should be improved. I suggest to provide a stronger motivation for the theoretical analysis with an outlook for future work where this analysis could be useful. <sep> Strong points: <sep> Theoretical analysis <sep> Detailed proofs <sep> Weak points: <sep> The analysis rests on the assumption that the corpus is generated from a certain generative process. This is a strong assumption. In topic modeling, this assumption is usually justified in that the resulting topics turn out to be interpretable. However, the approach in this paper does not recover any topics specifically. It is applied for classification where generative approaches are often not the preferred choice. So it seems to me like the advantage of the generative model, the interpretability, is stripped away, and the model is repurposed for classification. <sep> The experimental baselines are weak. Word2vec is not developed for the purpose of document classification, online VB does not have classification performance as a strong point and bare BOW is surely not suitable as a realistic baseline. <sep> Hard to follow at times, structure could be improved <sep> Detailed comments: <sep> Eq. on p. 2: l is not explained <sep> Setup: A k-dimensional vector lies in the (k-1)-dimensional simplex, not the k-dimensional simplex <sep> The notation is slightly confusing. I would suggest to use the commonly used θ instead of w for the topic distribution <sep> ""By Bayes' theorem we have that g^{\\star}:= \\exp(f^{\\star}) satisfies the following:"" Please give some more hints here about what you mean. Write that the Bayesian theorem is applied in the last equality and that the label y=0 makes the document parts independent. Also, why is y ommited in the last expression? <sep> independent conditioned -> independently conditioned all of these methods all of these methods -> all of these methods <sep> I find it confusing that the parameter α is divided by K the number of topics. In FIgure 1 left, if this is the Dirichlet parameter α then this means you vary it between 1/20 and 10/20 or is this α/K <sep> Experiment with LDA: you write that you use online VB, but you do not report the parameters such as batch size. Online VB is extremely sensitive to the batch size. <sep> If you compare classification performance I would suggest to rather compare to the Gibbs sampling version of LDA which usually will give better performance with enough samples. Scalability should not be an issue with the size of data sets that are used here. <sep> The two variants Direct NCE and Landmark NCE should be more explicitly introduced. At the moment, the Direct NCE introduction is hidden and hard to find. In the beginning you say your approach is to use landmarks and the Direct variant is rather suddenly introduced in a later part without discussing the differences and implications in much detail. <sep> Update: <sep> The authors have not fixed some of the errors I pointed out in my review. For example, they still refer to the k-dimensional probability simplex and did not address some of the other corrections. Since it is otherwise a promising paper, I suggest to carefully revise and resubmit. I will not however recommend a paper with errors for acceptance.","This promising work proves that the proposed contrastive learning approach to representation learning can recover the underlying topic posterior information given standard topic modelling assumptions. The work provides detailed proof and detailed experiments. The analysis is interesting and yields interesting insights. However, the experimental results are somewhat weak by lacking comparison with more recent document representation work. <sep> Pros: <sep> Good detailed proofs and experiments. <sep> Interesting idea of using topic modelling to understand representation learning. <sep> Cons: <sep> The description of DirectNCE is somewhat hidden and could be better introduced in the paper. <sep> Experimental baselines are weak lacking a comparison to recent document representation work such as Arora et al. 2019. <sep> Stronger classification baselines could be incorporated."
"Premise and Contributions: <sep> This work presents a machine learning scenario where the training data is biased due to collection artifacts. The training data does not represent the true real world distribution. In order to overcome this obstacle, the work presents the following contributions: <sep> A semi-supervised method that uses unlabeled data (that follows the real distribution) in order to improve predictions. <sep> This method contains an adversarial network that forces the regression output distribution to be similar to the assumed true label distribution. <sep> This method contains an adversarial autoencoder that propagates the information from the assumed true distribution to the latent vector of the regression model. <sep> Strengths: <sep> The scenario seems plausible. <sep> The proposed method seems sound. Although I have a concern with the AAE regularization. <sep> The experiments are convincing in my opinion, but there are some issues that I talk about below. <sep> The paper is pretty clear. Although, I believe the scenario is not explained in a way that is simple enough to understand instantly. But of course after reading the paper it becomes clear. Maybe there could be some improvements in the abstract and the introduction. Also I noticed that the introduction repeats the abstract, there could be some more precise explanations instead of this. For example (Kim et al. 2020)? <sep> Issues: <sep> Section 3.2: ""To be a useful feature for Rpost, latent vectors should be arranged in a similar way to p(y), which possesses information about how the labeled dataset is skewed."" - p(y) is a distribution of labels right? How can the latent vectors have same distribution as a distribution of labels. I do not understand this. <sep> Is there generality with respect to different types of bias for this method? Not just biasing the dataset with labels above θ <sep> How is the assumed true label distribution selected? What happens when it is very different from the true label distribution? Very interesting experiments could be undertaken here. <sep> In Section 3.1: ""The model is concurrently trained on Dl for regression, and on Du to force the regression outputs to have similar distribution to p(y)"" - How is the network ""trained"" on Du is there are no labels? Or is it just running a forward pass? <sep> A real world application on a dataset that actually presents the issues of this specific scenario would make the value of the contributions much more convincing, instead of only testing on synthetic datasets. But I do not know about the availability of such datasets. <sep> Experiments: For the ablation study, how many samples are used for the ""Only AAE regularization"" and ""Only forcing output distribution""? <sep> My biggest concern, and a question to other reviewers and the authors: Are there any other comparable methods to the proposed method, or other methods that solve this task in comparable scenarios? Should any other method be included in the experiments section for comparison purposes? <sep> Current decision: 5 - until I read other reviews and understand some of the moving parts better. <sep> UPDATE: <sep> After reading the author's updated paper and comments I have decided to improve my rating to a 6 since some of my concerns have been eased. The most important concern being eased was the type of bias was too constrained at first and now there are experiments with a more unconstrained version of bias that is more convincing. <sep> Overall, I would say that future versions of the paper could look into a task and dataset that are close to their domain of applicability and where they can contribute an increase in performance. That would strengthen the case of this paper. I think a rating of 6 is fair for this version of the paper and I thank the authors for their efforts in updating the work and addressing my concerns directly and efficiently.","This paper addresses the real-world problem of semi-supervised learning where the distribution from which the labeled examples are drawn is different from the distribution from which the unlabeled examples are drawn. The task is motivated by structure-activity prediction for drug design (quantitative structure activity prediction, or QSAR). Examples represent molecules, and we wish to predict a real-valued measure of binding affinity. Exactly the general problem of data skew arose with exactly this task for example in one of the KDD Cup 2001 tasks. While the authors here mention that labeled data may be focused more on active molecules (those with a high continuous-valued response), in the KDD Cup 200`1 data the reverse was true, and the unlabeled test data were skewed to higher activity level. I say all this to agree with the authors about the real-world nature of the problem they address. Also, some reviewers felt more empirical evaluation was needed, so that may be an additional data set for the authors to consider using. <sep> Reviewer concerns including that the approach was simplistic, the empirical results were insufficient, and the claims were oversold. The author replies and revisions, and the discussion, moved the reviews to be more favorable but still not strong enough to justify acceptance yet. Nevertheless, the consensus is that the paper addresses an important problem and the revisions are headed in the right direction to make a strong future paper, and that the authors should be encouraged to continue this work."
"The authors propose a method for simultaneously learning the graph structure (or a graph generative model) and the parameters of a GNN for node classification. This is a topic of recent interest and highly relevant to the *CONF* community. <sep> The authors propose two different ways for the graph generator. First, a fully parameterized method that outputs a continuous (weighted) adjacency matrix. The second computes a spare k-NN graph based on the input similarities. Using the word ""graph generator"" or ""generative model"" is a bit of a misnomer here because both methods are not probabilistic but deterministic. The adjacency matrices output by these ""generators"" are then made into symmetric and positive matrices (i.e., adjacency matrices for positively weighted undirected graphs). Finally, in addition to the typical supervised loss, the authors also propose an unsupervised loss based on a reconstruction loss. <sep> First, I like the idea of adding an unsupervised loss and also appreciate the experiments and the results as robust. <sep> I have a concern about the approach though and perhaps this could be clarified in a conversation here. When you don't use the MLP-kNN ""generator"" then the adjacency matrix generated is not sparse at all. But what that means is that you are using (for each node) a fully connected layer. Do you think that the advantage of your method then is in the ""adjacency processor step""? When you do use the MLP-kNN, how do you compute the gradients? Is it that you only compute gradients for the edges you selected? In this case you would have lots of edges without ""supervision"" as you call it. Or are you also obtaining gradients for the edges not selected? In this case, creating the kNN graph is a discrete operation and it is not clear to me how to differentiate through such an operation. Of course, recent proposals have been made for differentiating through kNN but I don't see any reference or mention of what you are doing here. Could you please elaborate on that? <sep> Regarding the statements about LDS. First, you write that if two nodes v_i and v_j are not directly connected to any labeled nodes, then the RV between them (the edge) does not receive supervision. The statement (and the text following it) is somewhat misleading for two reasons. <sep> First, both of the nodes have to be not directly connected. But then in the next sentence you wrote that 80 and 89% of nodes are not directly connected to a labeled node for the standard benchmark graphs. The more appropriate analysis, however, would be to count the number of pairs of nodes where both nodes are not connected to a labelled node. This should also include the validation nodes used for the outer objective in LDS. Also note that LDS doesn't always use the standard benchmark graphs (it either constructs a k-NN graph where k is a hyperparameter of the method or initializes with a subgraph of the given graph). <sep> Second, your statement is true for one sampled graph. Remember that LDS samples a set of graphs in each iteration. It can happen that, even if there is a pair of nodes where both nodes are not directly connected to a labelled node in one sampled graph, one of these nodes might be connected in a different sample. Indeed, if a sampled edge connecting either v_i or v_j directly to a labelled node leads to a reduction of the loss, the next time said edge is probably more likely to be sampled. This is not to say that self-supervision is not a good idea (I do like the idea) but I'm not sure if your statements about LDS are quite accurate. <sep> Overall, I think that this paper strength is the proposed self-supervised loss and the experimental evaluation. It is rather weak on the methodology, its presentation, and related  discussion. There are several questions I need to hear your response to. Once these are clarified I'm open to adjust my score accordingly.","The paper received 5 reviews, one of which had positive feedback. Although there are merits associated with the paper, several concerns raised in the reviews and the discussion period that prevents the paper to be accepted. It appears that experiments on noisy graphs are not properly done and competitive baselines are not used for validations. The quality of the learned graph structure is not adequately analyzed. and the experimental setup was not clearly explained. All these indicate that there is a need for a major revision before the paper can be considered for acceptance."
"Summary: <sep> This work propose Graph Optimized Neural Architecture Learning, that uses a differentiable surrogate model to directly optimize the graph structures. More specifically, the surrogate model takes a graph structure as the neural architecture embedding and predicts a relative ranking, then applies gradient descent on the input graph structure to optimize the neural architecture. GOAL demonstrates superior performance compared to SoTAs. <sep> Weakness: <sep> -First, the method is quite similar to NAO, where an encoder and decoder approach the maps neural architectures into a continuous space and builds a predictor based on the latent representation. The only difference is that NAO is use a decoder to decode the optimized latent representation back to architecture representation while here GOAL applies gradient descent on a graph neural network. Also, how to decode the parameters back to the neural architecture discrete representation is not clearly explained in the paper. <sep> -Second,  this method can work well for small models and small search spaces, but can be hardly applied to larger models. Training the surrogate function for a larger search space or larger models can take more samples and more training time (e.g. large models takes much longer time to train, thus even a proxy accuracy should take more time to evaluate). A parameter sharing scheme can be very inaccurate in the beginning therefore results in suboptimal architecture selection. The inaccuracy is compounded when using the same model for both a surrogate function and neural architecture search. The evaluation on only NAS-bench partially verifies the reviewers concerns. <sep> Detailed feedback: <sep> The reviewer would like to suggest several fixes to this paper: <sep> -First, try to compare to better baselines (e.g. NAO, more recent differentiable search work) on not only NASBench, but on real CIFAR10 or ImageNet workloads. <sep> -Second, compare a non graph neural network based approach with GOAL and show the necessity of using a graph neural network. <sep> -Third, be more clear about decoding back the graph neural network parameterizations to the neural architecture representation. Include more details on the number of samples used to train the surrogate function and hyperparameteers used in algorithm 1. <sep> [1] ""Neural Architecture Optimization"", https://arxiv.org/abs/1808.07233","This work was deemed interesting by the reviewers, but they highlighted the following weaknesses in this version of the paper: <sep> Lack of comparison to other methods. <sep> Lack of novelty compared to previous work. <sep> Fundamental problem with training only on one dataset (MNIST), issue with possible overfitting."
"Final recommendation <sep> I support accepting this paper. While I am a bit disappointed the authors did not add in the paper the results they discussed during the rebuttal, I think the paper is interesting and clearer than at submission. <sep> Summary stochastic subset selection (SSS) is a method to learn to compress a set D by selecting a subset Ds such that the loss of a task performed on Ds is as close as possible to the loss if the task had been performed on the original D. The paper show how this general method can be applied to several tasks. For example, D can contain the pixels of an image and the task can be to reconstruct the original image from the subset. This corresponds to learning to compress. As another example, with the same D but a label prediction task, SSS learns to dynamically select sparse features for a classification task. The other tasks considered are dataset distillation (compressing a data set) and selecting prototypes for few-shot classification. Ds is constructed in two steps. A candidate set is first constructed by considering elements of D independently and in a second step elements of Dc are included in Ds iteratively by considering other elements in Dc and in Ds. Experiments are performed for one example of each task previously discussed and SSS is shown to outperform baselines. SSS is motivated by the need to reduce bandwidth, computation and memory footprint of deep learning in edge devices. <sep> Strong points <sep> Code is provided (I did not try to run it). <sep> The proposed approach is applied to four different tasks. This convinced me of its applicability. <sep> Empirical experiments seem well conducted (with some caveat, see weak point 3) and results are good. <sep> The problems tackled are interesting. <sep> Weak points <sep> The paper is very dense, which makes it hard to follow. Related to that, I think that some parts of the paper are too concise to understand them well. The supplementary material partly compensates that. <sep> While the experiments show that the proposed approach works well, I think that some experiments should compare full data pipelines. As an example, one experiment shows that using SSS to dynamically select pixels of an image and using this sparse representation as input leads to good classification accuracy. The paper implicitely suggests that this reduces inference cost if classification is performed on the device and bandwidth if inference is performed on another device. However, the cost of the pixel selection by SSS is never analyzed. I think comparing the memory footprint, the number of operations, bandwidth used and accuracy for all steps of the following pipelines would give a much clearer picture of the interest of the approach for deep learning on mobile devices: <sep> classification is done directly on the device, using an optimized network. <sep> pixels are randomly selected and classification is performed on the device. <sep> pixels are randomly selected, transmitted and classification is performed on another device. <sep> pixels are selected by SSS, which involves computing multiple embeddings, and classification is performed on the device. <sep> pixels are selected by SSS, transmitted and classification is performed on another device. <sep> Recommendation <sep> Overall, I vote for accepting. I am on the fence with this paper. The current results are good and I think the proposed approach has some potential and will be interesting for the community. On the other hand the paper is hard to read and the experiments, while good to show the strength of the approach, do not cover usefullness for the mobile use case. I think the former can be adressed in time and that the latter, while important, does not prevent the paper to be published. <sep> Details <sep> I like that each task is first formally described and then evaluated in a specific case with the experiments. However I only understood most tasks when reading the experiment sections. I would suggest to either provide a short illustration of an actual instance of the task when describing the task formally, to directly provide the experimental result after describing the task or to direct the reader to the relevant experimental section(s) for illustration. <sep> The paper states that model descriptions are available in appendix A, but I could not find them. <sep> For the Set Classification/Prediction task, I would suggest clarifying that for this task a set typically contains the features of a single example, so D=x. I only understood this when reading the experimental section. <sep> The Dataset Distillation: Instance Selection task was not clear to me. Here are a few points that puzzled me. <sep> The text suggest the proposed approach is applied to each Di, but figure 3(c) has no Di and i seems to be used as an index of x rather than D. Furthermore, in figure 3(c) a single D_s is constructed whereas I was expecting one per Di. <sep> The paper states that It is important to note that c is computed using only Di for every element in D. However c also depends on a query α which is computed using di, which, if I understood correctly, is the element of D and can be outside Di. So it seems that c is not computed using only Di. <sep> For the Dataset Distillation: Classification task, in figure 3d, the labels are not observed. How can the model learn? <sep> For this task, if the label is observed, how is this different from the first task? <sep> For the Image Reconstruction experiment, why is x 2 dimensional? Are the inputs black and white images? <sep> I think that the image reconstruction experiment should include a comparison to compression algorithms. I agree with the paper that the proposed approach is applicable to many other types of data than images, but it would be very interesting to see how the approach compares to expert defined methods. Furthermore, implementation in relation to mobile devices is the motivation of this work and as far as I know classical compression algorithms are still used on mobile devices. <sep> Questions <sep> Questions 1 and 2 in details above. <sep> I would also be interested to hear authors' opinions on weak point 2 and a comparison to compression. <sep> For classification, pixels not selected are set to 0. Wouldn't the inference cost be the same when using convolutional networks? <sep> Minor details <sep> For the set reconstruction task, would it also be interesting to learn a model on Ds and use it to predict Y from X? <sep> typos foro","The paper proposed a two-stage method to select instances from a set, involving candidate selection (learning a function to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability. The experiments show the performance of the proposed method on several use-cases, including reconstruction of an image from a subset of its pixels, selecting sparse features for a classification task, and dataset distillation for few-shot classification. I read the paper and I agree with the reviewers that in its current format the paper is hard to follow. I strongly encourage the authors to add more discussion and intuition on the proposed method and extend the experiments with more baseline comparison and ablation studies in the revised version."
"Summary <sep> The paper studies a certain notion of spectral sparsification of directed graphs. It claims the existence of nearly linear sized sparsifiers under this notion, and suggests empirical methods to produce such sparsifiers in nearly linear time. <sep> Comments <sep> Section 4: I am not sure what is the downstream justification for the notion of spectral approximation studied here (relative condition number) in the context of directed graphs, and it would help if the authors could elaborate on this. For example, the notion defined in Cohen et al. (2017) enables fast approximate solution of linear equations, which is a primitive in various algorithmic tasks on directed graphs. The notion in this paper does not rigorously lead to such consequences (or at least none are presented), which leaves the question why one should be interested in an existence result like Theorem 4.3. (The appendix contains some experimental results for linear system solving, but no rigorous claims.) <sep> About the proof Theorem 4.3: Could you please clarify how does the approximating matrix LSu=BTWo1/2TWo1/2B (defined in the end of section 8.2) give rise to a directed subgraph? How does one construct S (or in other words, why can LSu be written as LSLST for LS of the form in eq (18)) <sep> Section 5: I am generally not able to follow the derivations in section 5.2, and it may be needed to be written more clearly. Specific comments/questions: <sep> How come λn>0 if the matrix LSu+LGu  has non-full rank? <sep> You define v_i once as the eigenvectors of LSu+LGu in the first sentence sentence as then as scaled eigenvalues of LSu between eq (5) and (6), which one is it? Or are they supposed to be denoted differently? <sep> Could you elaborate on how you arrive at eq (5)? <sep> Section 6: The experiments in the main text concentrate on measuring the relative condition number of the sparsifier w.r.t the original graph as per the sparsification notion studied in this paper, but as written above it is not clear what do we actually get out of the sparsifier. In general I have doubts about fit to the venue; while *CONF* scope is broad and inclusive and spectral sparsification has certain potential connections to ML, this paper does not highlight any of them, and it is not entirely clear what it is attempting to achieve. <sep> Conclusion: <sep> Pros: Spectral sparsification of directed graphs is a relatively new field, so far with initial results that invite further research and improvements. The paper implements an algorithm which seems to improve over baselines under certain metrics. <sep> Cons: It is not clear what ML task the paper is trying to solve or improve upon, and what improvement is achieved. Viewed as a theoretical submission, the analysis is unclear to me in many parts, and at present I am unable to confirm its soundness. The algorithm implemented in the experimental section seems rather detached from the preceding theoretical definitions, whose purpose remains somewhat unclear. <sep> Post-discussion update: I thank the authors for their participation in the discussion. Unfortunately I find it mostly has not cleared the numerous question marks I have regarding the paper. I recommend putting more effort into clarifying the mathematical derivations and into positioning the paper correctly w.r.t. prior work on the topic.","The paper proposes a fast, nearly-linear time, algorithm for finding a sparsifier for general directed and undirected graphs that approximately preserves the spectral properties of the original graph. The reviewers appreciated the main contribution of the paper, but they were concerned about the correctness and clarity of the paper, as well as the relevance of the contribution to machine learning. Following the discussion with the authors, the reviewers still felt that these concerns had not been fully addressed by the authors' responses and the subsequent revision of the paper. After taking these concerns into account as well as evaluating the paper relative to other *CONF* submissions, I recommend reject."
"This paper proposes a training method for classification, with the goal of training with less data. The proposal is to train an auxiliary classifier at the same time. The auxiliary classifier and the main classifier share the early layers. The auxiliary classifier is a binary classifier that discriminates training data versus background/noise data. The proposed method is evaluated on image and speech classification tasks. <sep> On the positive side, the experimental results show consistent benefit across tasks. The facial recognition comparisons should be taken with a grain of salt, because of how the background data were collected (see page 7 and appendix). Other than that, the results seem quite consistent. <sep> However, I have substantial concerns. The experiments are not sufficient to support the claims: there is little results on training with less data; certain critical experiments are missing; some comparisons are not fair. The theoretical content is detached from the proposed method and hence not important. I'll elaborate in the following. <sep> There are three angles to look at the proposed method: <sep> Data augmentation. Part of the proposed method is building an extra class of training data that are background or noise. One example is given in Figure 5. It seems feasible for image and speech tasks. <sep> A critical point, that is missing from this paper, is how would this data augmentation work by itself without the auxiliary classifier. In other words, what if we just train a classifier with n+1 labels, where the extra label's training data are the background or noise data points? At inference time, one simply uses the first n logits. How much of the reported benefits would this simplified method achieve? This would show how important is the data augmentation versus how important is the auxiliary classifier. <sep> The appendix stated that the background/noise data include animal faces collected from open source images. Therefore, the discussion around Figure 4 and Table 2, which claims robustness advantage over ArcFace, is invalid. It's not a fair comparison when the training data is expanded with a particular emphasis. <sep> Semi-supervised learning with less labeled data. It seems that the most natural and meaningful task is where the auxiliary classifier is trained on all in-domain data, both labeled and unlabeled, while the main classifier is trained on a small number of labeled data. It's important to compare with existing methods in this setup. <sep> Unfortunately this paper does not include such experiments. Table 8 includes reduced-training-set experiments on CIFAR, however 3/5 of the training set is still way too big, and the remaining 2/5 are not utilized as unlabeled in-domain data. <sep> Regularization of the early layers. This is the stated rationale for the proposed method. The auxiliary classifier is designed to encourage the early layers to learn more meaningful features, and Section 3 supports this relation. <sep> Section 3 is very difficult to read. The conclusion is that the cross entropy loss of the auxiliary classifier is positively correlated with the quality of the estimation of unknown parameters of the data distribution, and therefore training with the joint loss of equation (2) is implicitly related to a good estimation of parameters, which in turn is related to better feature representations in the early layers (an implicit assumption here is that good features for the auxiliary binary classification are also good features for the multi-class classification, which could be a question by itself). <sep> This relation, however, is qualitative rather than quantitative. In other words, neither training nor inference of the proposed method utilizes the content of Section 3. Section 3 mostly serves as a motivational argument. The auxiliary classifier idea is intuitive by itself, and it seems that Section 3 can be removed without affecting the core content of this paper. <sep> The stated goal of regularization is to learn with less data. However the only results on this subject seems in Table 8 where 3/5 of CIFAR data are used. Table 8 is not the best setup, as mentioned earlier in the second point. Even not considering semi-supervised learning, 3/5 of the CIFAR training data are too many for a training-with-less-data experiment. <sep> Minor points: <sep> -- Figure 3 is not a fair comparison. With the joint training, the roles of ResNet blocks are likely shuffled. Some information that gets processed in later layers during normal training may get moved to early layers in joint training, and vice versa. Hence comparing at a fixed layer is not meaningful.","In this paper the authors propose an approach to improving the accuracy of the classification problem based on deep neural networks by detecting the in-domain data from background/noise. The strategy is designed in such a way that the detector and the classifier share the bottom layers of the network. Theoretical proof is given and experiments are conducted on a variety of datasets. The novelty of the work is to come up with a better estimate the pdf of the data and use it to help the classification based on the deep neural networks. There are concerns raised by the reviewers regarding the related work, the exposition and the experimental design. After the rebuttal from the authors, which is meticulous, some of the issues unfortunately still stand. The paper needs to make a stronger case in order to be accepted, especially, for instance, the theoretical and empirical comparison with the existing techniques sharing the similar idea."
"########################################################################## <sep> Summary: <sep> The paper describes a technique based on the modified generalized gradient descent for finding multiple high-quality local optima of deep neural networks. The search method does not require re-initialization of the model parameters and can be carried out in a single training session. Identified local optima are then used to build model ensembles which appear to outperform several other ensembling approaches. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, I vote for accepting. While I was not entirely satisfied with how the paper is written and how the approach is introduced and explained, I find the results to be quite interesting. While I do not know the literature sufficiently well, I think this method is original and well-founded. <sep> ########################################################################## <sep> Pros: <sep> While being simple and intuitive, the proposed method appears to succesfully and efficiently identify multiple high-quality local optima of a model. <sep> Possibly even more interestingly, ensembles containing corresponding models appear to outperform other alternative approaches. While inference with ensembles of models can be quite costly (growing with the number of models in the ensemble) and a similar or better accuracy could potentially be achieved with larger simple models sharing the same computational cost, this result is nevertheless very promising. <sep> The experimental methodology appears to be sound and some illustrative examples (like those shown in Figure 3) are interesting and insightful. <sep> ########################################################################## <sep> Cons: <sep> Certain parts of the publication are not entirely well written and some sentences are a bit confusing. Also, the text contains quite a few misprints. Some more serious mistakes can be found, for example, in Table 2 (DenseNet results) and central equations (5) and (6), which seem to use a wrong sign for the gradient (current sign seems to correspond to gradient ascent and not descent thus maximizing the loss and not minimizing it). Also, as a very minor comment, I believe that, strictly speaking, the gradient (with respect to Δ) of the loss in these equations should be computed at ϕω0(ti−1) because otherwise the right-hand side of these equations would be dependent on Δ(ti). <sep> In my opinion, the discussion in Section 3 could be clarified and simplified. Furthermore, I believe that the method could be explained and analyzed a bit better. For example, it would appear that the proposed difference equation (6) can be written as a gradient descent on the modified loss function with the added quadratic term ∼(ρ1/ρ2)Δ2. If correct, I find this simple perspective much more natural and insightful. This quadratic component can essentially flip the Hessian in the vicinity of the starting local minimum thus causing the trajectory to be repelled from it. This simple view also appears to have implications for what kind of local minima of the original loss (their Hessians) could finally attract such training trajectories, potential shifts due to finite ρ1, and the role that the decay of ρ1/ρ2 could play in the process of convergence. <sep> The related work section contains just a few ensemble papers and none after 2018. It would appear that this section could be expanded and include some more recent papers at least for reference. <sep> ########################################################################## <sep> Questions during rebuttal period: <sep> Please address and clarify the cons above. (I will update my score based on the authors reply.) <sep> Unfortunately, I am not an expert in this field, but two papers I came across doing a very quick search appear to be somewhat relevant: ""Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks"" and ""MEAL: Multi-Model Ensemble via Adversarial Learning"". I am not sure these particular papers need to be included in the prior work section, but I do think that this publication would benefit from a more in-depth literature overview. <sep> ########################################################################## <sep> Post-rebuttal. <sep> Thanks for a detailed response that clarified some of my questions. I think the overall quality of the paper increased and I am happy to see additional information (like additional literature and an ablation study in Section 5.4) and a somewhat improved explanation of the core idea. However, I am still hesitant to give this paper a higher rating, in part because I find Section 5.4 to be poorly written and somewhat confusing (it lacks any sort of conclusion or insight and I cannot read Figure 4 at all, the text is too small) and partly because the paper does not clearly put its results in the context of the recent model ensemble progress (and thus makes me doubt the impact of this result on the field; although it does seem to be promising compared to the mentioned baselines).","This work proposes a method to discover neighboring local optima around an existing one. Reviewers all found the idea interesting but argued that the paper needed more work. In particular, some of the claims are too informal or not sufficiently supported and the reviewers found the key section were difficult to follow. The paper should be resubmitted after improving the presentation of the results."
"Quality: <sep> The quality of the paper is below average. The loss function that integrates out-of-class samples is counter intuitive and seems to be chosen based on improved empirical evidence. <sep> Clarity: <sep> The paper reads well. <sep> Originality/Significance: <sep> The originality of the approach is limited. Main ideas are borrowed from SimCLR and applied to SSL. <sep> Detailed Comments: <sep> The paper uses contrastive learning idea proposed in SimCLR (Chen et al. 2020) to detect out-of-class samples and treat them in a different way than in-class unlabeled samples during semi-supervised learning. It also explores the idea of auxiliary batch normalization (from Xie et al. 2020) in the open-set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out-of-class samples through using the projection header function introduced in SimCLR paper. Although results across multiple benchmark datasets report significant improvement over other SSL techniques these improvements could be artificial as other techniques have no way of handling out-of-class samples. Integrating these other out-of-class detection techniques in ReMixMatch and comparing results with the proposed technique would offer a more compelling argument. Overall, the paper proposes a nice practical idea but for publication in *CONF* one would like to see more theoretical insight along with empirical evidence. <sep> Class conditional likelihoods has been shown to be not very useful in detecting out-of-distribution samples in cross-entropy loss learning. What aspect of contrastive learning makes it more useful for open-set classification? <sep> Cross-entropy term in equation 8 does not make much sense. It is simply computing the loss with respect to an incorrect label. If a sample belongs to an unknown class it is not clear why this would help SSL. If the purpose here is to capture shared characteristics of the samples then SimCLR trained with both labeled and unlabeled data already takes care of it. The authors try to justify this by considering that some seen classes would be similar to the unseen one but for a fine-grained classification task such samples may still hurt predictive performance. If there are no similar classes among labeled classes the authors argue that this loss will have a uniform affect for all classes. Again, this is not a compelling argument. No matter how dissimilar the unseen sample to labeled classes are it would be more similar to some of the classes than some others. Due to the normalization affect the weight distributions will significantly deviate from a uniform distribution. <sep> Baseline techniques are all SSL techniques. These are guaranteed to perform worse than the proposed technique because they have no way of handling out-of-class samples. What about other baselines? From Table 3 one can see that the main contribution comes from the detection of out-of-class samples. The authors compare the detection performance of their technique against other standard outlier detection techniques and show that the proposed detection outperforms all of them to achieve the highest AUC in the ""isolated"" detection task. However, it is still not clear why the projection header g achieves something that softmax probabilities cannot do. In other words why do class-conditional probabilities obtained from g are useful for out-of-class detection but the probabilities that one would obtain from the softmax layer of an architecture (such resnet) trained by cross-entropy loss is not much useful for the same task. Interesting  that not much insight has been provided in the SimCLR paper either. Along the same lines, as a contrastive loss function, triplet loss also seems to do well in open-world settings. <sep> Minor: <sep> Please correct the following: <sep> page1 Compared to prior approaches approaches have that bypassed ...","This paper proposes OpenCos for semi-supervised learning that can leverage unsupervised information in open-set scenarios where samples can be out-of-class. They first pre-train by learning an unsupervised representation using SimCLR on both the labeled and unlabeled data. Then, they detect out-of-class samples in the unlabeled set based on similarity measures on the representation learned in the previous step. The unlabeled data can now be split into in-class and out-of-class. OpenCos optimizes (8) which combines a semi-supervised loss for in-class unlabeled data and an auxiliary cross-entropy loss with soft-labels for the out-of-class samples. Finally, they perform an auxiliary batch normalization. <sep> The paper is easy to read and clearly structured. It also places the work well with respect to related work. <sep> The proposed approach makes sense; however, as pointed out by the reviewers, the novelty is marginal. The technical innovation seems to be an extension of SimCLR and the auxiliary batch norm of Xie et al."
"Summary: <sep> This paper proves a regret bound for an optimistic variant of a policy optimization algorithm in an advsersarial reward setting. The paper extends prior work in this setting by considering function classes with bounded Eluder dimension instead of linear functions. This yields guarantees for a kernel-based variant of the algorithm (which also apply to neural kernels like the NTK). <sep> Strengths: <sep> More general than prior work. The main strength of this paper is to extend the work of Cai et al. (2019) beyond the Linear MDP setting to one with a kernelized transition matrix. This essentially requires introducing some results based on the Eluder dimension from Russo and Van Roy (2014). The paper helps to fill out this line of research by showing that the algorithm from Cai et al. (2019) does indeed work when the linear MDP is replaced by a kernelized one. <sep> The proofs seem to be correct. <sep> Weaknesses: <sep> Setting seems unrealistic. The paper assumes that the entire reward function is revealed after each episode rather than only seeing the rewards at the visited states. This is integral to the algorithm since this reward function is then queried at new state, action pairs. I understand that this allows for adversarial reward functions and is the same assumption from Cai et al. (2019), but it seems unrealistic and unmotivated. I cannot think of a problem where the entire reward function would be revealed at the end of each episode, and without this assumption the entire algorithm and argument seem to break down. <sep> Calling this algorithm policy optimization seems misleading. The algorithm requires using the kernel-based approximator to learn not just a model, but an entire confidence set over models. In the usual RL nomenclature this would seem to be a model-based algorithm. While the policy is not simply attained by planning in the learned model, having to learn these models seems to make the algorithm indeed model-based. This would not be so important except a major motivation for the paper in the abstract and intro is to provide guarantees for model-free policy optimization algorithms like the policy gradient algorithms that are often used in practice. This motivation does not align with the algorithm the paper is actually analyzing. <sep> The main assumption is not clearly explained. The main assumption in the paper is that the transition kernel is in some RKHS, but the paper never explains what this assumption means and which sorts of MDPs it may apply to. It is more general than the Linear MDP, but how much more general? Moreover, the introduction makes it sound like the function approximation assumption is merely for the policy class, but in fact the approximator must be able to represent the entire transition kernel, which is potentially much more complicated. <sep> Novelty. To me the paper is only an incremental improvement over the results of Cai et al. (2019). Specifically, extending results from a linear setting to a kernel setting provides a technical challenge, but it is not clear how much insight is gained. The main result follows quickly from combining the results on Eluder dimension from Russo and Van Roy (2014) with thos of Cai et al. (2019). <sep> Algorithm seems computationally infeasible. The algorithm requires maximizing an integral over the entire state space against over a set of measures defined by a confidence set over models in the RKHS at every step. The paper argues that the algorithm can be implemented with supervised learning oracles, but having to solve several supervised learning problems at each step of training over all historical data seems quite inefficient. In short, this is not an algorithm that a practitioner would attempt to implement. <sep> Recommendation: <sep> I recommend to reject this paper, and gave it a score of 4. While the paper is a logical extension of some prior work, I am not convinced of the usefulness of the setting or algorithm proposed. <sep> If the authors can provide better motivation for the setting, novelty, and practical value of the contribution, I would consider raising my score.","While the reviewers appreciated the aim of the work, they found the technical contribution to be too incremental to be of sufficient interest and the exploration of the problem and its significance to be incomplete in the paper's current state."
"This paper studies decentralized gradient methods for training deep networks. It focuses on the so-called ""critical consensus distance"" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny-ImageNet. <sep> This is a nice contribution to the growing literature on decentralized training for deep neural networks. The connection between consensus distance and performance has previously been studied to a limited extent in various settings, so the contribution of this work is somewhat incremental. However, this paper makes the connection somewhat more rigorous through the theoretical developments in Section 3, and it provides a more detailed empirical investigation than previous work. I expect the results to be useful to those working on decentralized training and am supportive of accepting it. <sep> I have a few suggestions and comments, about which I look forward to hearing from the authors. <sep> You mention that consensus distance has previously been investigated to some extent (e.g., Fig 2 in Assran et al. 2019). Are there connections between consensus distance and other quantities that have been considered in the literature to relate training to performance (e.g., gradient diversity as in Yin et al. 2017, or the closely related gain ratio in Johnson et al., 2020). Similarly, is there a connection to stochastic weight averaging (Izmailov et al. 2018) and it's parallel version (Gupta et al., 2020)? <sep> It is not clear if there are specific aspects of the tasks considered that are important for the findings to hold. CIFAR-10 and ImageNet-32 are both relatively small datasets. Is it possible that in the centralized setting, ResNet-20 is overfitting, and the error from decentralized SGD has a regularizing effect, leading to better generalization? It would be interesting to perform further experiments to explore if this is the case. <sep> It would also be interesting to know if the results similarly carry over to the standard (higher-resolution) ImageNet training and models (e.g., ResNet-50), to know if the phenomena observed are relevant to large-scale training. While I appreciate that CIFAR and ImageNet-32 experiments are useful for quick experimentation, and running experiments on the standard ImageNet task are much more computationally expensive, CIFAR and ImageNet-32 are not very reflective of tasks where one would normally use distributed or decentralized training, since one can easily train a model on them using a single GPU in a reasonable amount of time (~1 hour). <sep> Regarding the experiments in Table 5 (longer training), why focus on prolonging training in phase 1? I would expect that extending later phases would potentially allow to overcome issues due to large consensus distances in phase 1. Did you explore this? <sep> The analysis focuses on symmetric (push-pull) mixing. Do you expect the same trends to carry over to push-only methods such as those considered in Assran et al., 2019? <sep> Nowadays, the half-cosine learning rate schedule is also commonly used for CV tasks (He et al. 2018). How do you expect this to affect CCD and the analysis leading to Remark 2? <sep> How does using more gossip iterations impact the practical utility of these methods? In particular, standard implementations of all_reduce only require that each node communicate 2 copies of the parameters per iteration. Now that we need to potentially perform multiple rounds of gossip between each optimizer update, are decentralized methods still attractive for reducing overall training time? On a related note, Tsianos and Rabbat (2014) also proposed to use multiple rounds of gossip to essentially reduce the CCD for convex problems, and show that it can lead to overall less communication overhead to reach a desired level of accuracy. Is it possible to show something similar in this setting? <sep> Additional references mentioned: <sep> Gupta, Serrano, DeCoste, ""Stochastic weight averaging in parallel: Large-batch training that generalizes well,"" *CONF* 2020 and arxiv:2001.02312 <sep> He, Zhang, Zhang, Zhang, Xie, and Li, ""Bag of tricks for image classification with convolutional neural networks,"" CVPR 2019 and arxiv:1812.01187 <sep> Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson, ""Averaging weights leads to wider optima and better generalization,"" arxiv:1803.05407 <sep> Johnson, Agrawal, Gu, and Guestrin, ""AdaScale SGD: A user-friendly algorithm for distributed training"" ICML 2020 and arxiv:2007.05105 <sep> Tsianos and Rabbat, ""Efficient distributed online prediction and stochastic optimization with approximate distributed averaging"" IEEE Trans Signal and Information Procesing over Networks 2016 and arxiv:1403.0603 <sep> Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and Bartlett, ""Gradient diversity: A key ingredient for scalable distributed learning,"" AISTATS 2018 and arxiv: 1706.05699","The authors study the problem of (insufficient) generalization in gossip-type decentralized deep learning. Specifically, they establish an upper bound on the square of the consensus parameter distance, which the authors identify as a key quantity that influences both optimization and generalization. This upper bound (called the critical consensus distance) can be monitored and controlled during the training process via (e.g.) learning rate scheduling and tweaking the amount of gossip. A series of empirical results on decentralized image classification and neural machine translation are presented in support of this observation. <sep> Initial reviews were mixed. While all reviewers liked the approach, concerns were raised about the novelty of the results, the lack of theoretical depth, and the mismatch between theory and experiments. Overall, the idea of tracking consensus distance to control generalization seems to be a practically useful concept. During the discussion phase the authors were been able to (convincingly, in the area chair's view) respond to a subset of the criticisms. <sep> Unfortunately, concerns remained regarding the mismatch between the theoretical and empirical results, and in the end the paper fell just short of making the cut. <sep> The authors are encouraged to carefully consider the reviewers' concerns while preparing a future revision."
"This paper proposed a self-supervised learning method of 3D shape descriptors for 3D recognition through multi-view 2D image representation learning. To represent the 3D shape, the authors first project the object to a group of 2D project images, which helps apply deep learning due to the image's matrix data format.  The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as ""self-supervised"" learning. The key idea of transformation equivariant representations is directly borrowed from existing works [1][2]. The method designed is almost the same as [1] except for the encoding network. <sep> [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET) <sep> [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT) <sep> Besides, there's no proof to verify transformation equivariant representations learning. The authors need to prove the properties for Transformation Equivariant directly for 3D objects. The current presentation of the paper is based on the 2D project image representation for 3D objects. If the authors wish to use Transformation Equivariant  in [1][2] above, the authors might want to consider adding additional 2D to the 3D reconstruction process, and  then consider the order of doing transformation and doing reconstruction etc.) <sep> May some visualization results can better convince the audience. For example, the authors would like to add some visualization results for a 3D shape descriptor for objects from the same category and different categories and show how robust the proposed shape descriptor is. <sep> The proposed approach had experimentally verified its effectiveness in 3D recognition.  However, for a paper being accepted to *CONF*, I would like to see more technical novelties/merits beyond directly extending the existing image representation learning approach to 2D projection images of the 3D image. For instance, the authors could propose a method that can be developed based on the ""3D Transformation Equivariant"" to 3D objects directly instead of its 2D projections. <sep> ---Additional comments after rebuttal-- <sep> I have carefully reviewed the authors' feedback regarding their comments on how their proposed method differentiates the existing (AVT and AET). Unfortunately, it did not address my concerns about the novel technical contributions in the proposed paper. Obviously, the authors applied the ""Transformation Equivariant Representations by Autoencoding Variational Transformations"" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). I am not sure why in the rebuttal, the authors claimed, ""Their proposed method distinguishes from AET significantly in two aspects"". I would urge the authors to check both papers below, and it clearly defines the Transformation Equivariant Representations learning by Autoencoding Variational Transformations, which could be applied for various types of data. [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT) <sep> Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the ""3D Transformation Equivariant"" to 3D objects directly instead of its 2D projections. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. I am not sure why the authors answered that ""3D objects are unavailable at the testing stage."" The proof of Autoencoding Variational Transformations for 3D data directly should not depend on the availability of 3D data.","The authors present a method for self-supervised learning of representations of 2D projections of 3D objects. By performing known 3D transformations of an object of interest, a encoder/decoder network is trained to estimate the applied transformation from a series of 2D projections. The proposed method is used as a regularizer and experiments are performed on supervised 3D object classification and retrieval. <sep> After seeing each others' reviews, one of the main concerns from the reviewers was the relationship between the proposed method and Zhang et al., CVPR 2019 (i.e. AET). The two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different. <sep> In their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition. However, there were still other concerns that the reviewers had e.g. R2 wanted to know why the model could not be applied directly to 3D shapes instead of 2D projections. <sep> Given the above concerns (specifically the relationship to AET), there is currently not enough support for accepting the paper in its current form. The authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future."
"Natural language grounding is an interesting research direction and has attracted many researchers in recent years. Previous work mainly considered grounding the text to image objects. This paper considers collaboratively to learn ""entity"" representations and natural language explanations with a reinforcement learning framework. Specifically, a multi-modal attention network is proposed to model the interaction between the entity representation and the text descriptions. The entire framework is trained over multiple games in a multi-task manner. Experiments are conducted on a newly designed benchmark. The proposed RL framework achieves reasonable performance in domain games (training & test are from the same games) and also has a strong zero-shot generalization to unseen games and ""entities"" (thanks to the parameter sharing and multi-task learning). Besides, the newly released dataset may facilitate future research in natural language grounding. <sep> I am not familiar with the natural language grounding literature, so this might be an educated guess, which is listed as follows, <sep> The writing is relatively clear, although some specific illustrations are sometimes hard to follow. For example, the term ""entity"" is very ambiguous. In the NLP field, an entity might relate to a phrase that has specialized semantic meaning. I read several recent papers in the natural language grounding field but still puzzled about the term (for example, the semantic meaning of ""entity"" in  [2] is different from this paper). <sep> After reading this paper, I am not sure which part is the main novelty. The proposed reinforcement learning framework is relatively standard (no specialized designed state, policy, and reward). The interaction module is designed in a slightly straightforward way. Utilizing parameter sharing to encourage stronger generalization ability also seems reasonable. The novelty might be further clarified. <sep> The proposed approach seems to be similar to [1] (cited). The general idea is to learn a parameterized policy model that inputs the pair (entity [this paper] or visual representation [1] and text) and outputs the action of the agent in the game (correct me if I am wrong). Depending on different scenarios, the encoding network can be slightly different. It might be beneficial to have a detailed discussion of the difference between the methods proposed in the two papers. <sep> In Figure 5, the reward of different approaches seems to have small differences. However, the final winning rate of games varies a lot. It might be beneficial to have some discussions on this part. <sep> If my understanding is correct, the contribution of the paper is mainly focused on the strong generalization ability of the proposed approach across different games. However, it may need to clarify which part of the proposed approach contributes to the most significant influence. <sep> Other minor points: <sep> I am interested in the O-Map baseline, which seems to be a very good upper bound of the proposed approach, I am surprised to see that the EMMA approach outperforms the O-Map in the training settings. Some intuitive explanations might be useful. The second question is that, though utilizing the upper bound (O-Map), the winning rate still has some room to improve. It is suggested to have some further analysis. <sep> I do not quite understand why the overall training process contains two rounds of stages. Is S1 stage training a warm-up training? <sep> I would like to hear the authors' responses to make my decision. <sep> [1] Zhong et al. RTFM: Generalising to New Environment Dynamics via Reading. *CONF* 2020 <sep> [2] Lai et al. Contextual Grounding of Natural Language Entities in Images. NeurIPS 2019 workshop","Like the reviewers, I find this paper extremely borderline. On the one hand, it is clearly written, about a topic I find fascinating, and generally well motivated if not shockingly novel (i.e. removing some of the simplifying assumptions from Zhong et al. 2020, e.g. requiring grounding to be learned, use of real language rather than synthetically generated). On the other hand, I agree with the leitmotiv present amongst the reviews that the problem at the centre of the experimental setting is very, very simple (3 objects, 3 descriptions). I am mindful of the fact that access to computational resources is unevenly distributed, and am not expecting a paper like this to immediately scale their experiments to highly complex settings with photorealism, etc, but I can't help but feel that a more challenging task, with a deeper analysis of the problems presented by both grounding and the use of non-synthetic language, would both have been highly desirable to make this paper uncontroversially worth accepting. <sep> As a result, the decision is to not accept the paper in its present form. Work on this topic should definitely be presented at *CONF*, but it's a shame this paper did not make a stronger case for itself."
"Summary <sep> This paper attempts to infer a network's accuracy at initialization without training it, which can speed up neural architecture search and greatly reduce the search cost. Specifically, they propose a metric based on the Jacobian of the loss with respect to a minibatch of input data. The authors show that with this metric, they can find architectures with reasonable accuracy on CIFAR-10/CIFAR-100 in the NAS-Bench-201, while using much less search cost compared to previous NAS methods. <sep> Strong points <sep> This paper is exploring a novel and interesting direction. Estimating the network's performance at initialization can greatly reduce the search speed. <sep> It might be difficult for the training-free metric to outperform conventional metrics (e.g., validation accuracy after training). This paper finds a good use case of the training-free metric in practice. The authors propose to use the training-free metric to select the initial population of evolutionary algorithms and empirically demonstrate its usefulness on CIFAR-10 and CIFAR-100. <sep> Weak points <sep> It will be helpful to make the motivation of the proposed metric more clear. I found the second paragraph in Section 3 hard to understand without digging into the cited literature. <sep> How to interpret the value in the correlation matrix \\Sigma? Empirically, Figure 1 shows that we want to find an architecture whose values in matrix \\Sigma are mostly around zero with a small positive skew. But how does this relate to the two motivations mentioned in Section 3: (1) being flexible and (2) being invariant/robust to small perturbations. Does small value mean less or more flexible/robust? This is important for people to understand why the metric works. <sep> In the experiments, all the images in the minibatch are data augmentations of the same image. This seems to be an important design choice. More insights on this part would be very helpful. How is this design choice (using data augmentations of the SAME image) related to the two motivations (flexible & invariant)? <sep> As mentioned by the authors, the two motivations are actually antagonistic. How does the proposed method balance them? <sep> Why choose cutout as the data augmentation strategy? Is the method sensitive or not to other perturbations, e.g., adding small noise? <sep> I notice that the correlation (tau) value in Figure 3 is not high. What is the correlation between the typical criteria (e.g., validation accuracy after training a small number of epochs) and the final test accuracy? <sep> The tau in Figure 3 is actually undefined. Is that Kendall tau? <sep> Why evaluate the correlation between validation accuracy, not the final test accuracy, as provided in NAS-Bench-201? <sep> Section 5 mentions that ""ori-test@12"" is used as the metric during search. Is that the accuracy on the test set after 12 epoch training (with learning rate decay to 0)? But the common practice is to use the validation accuracy during search and report the final test accuracy. This seems to be unfair. <sep> For NASWOT (N=100), the accuracy on CIFAR-10 is reasonable and actually pretty good considering the small search cost. However, the accuracy in ImageNet-16-120 seems to be very low. Why does this happen? Is it possible that the proposed method overfit to CIFAR-10? <sep> If I understand correctly, the architecture is searched on CIFAR-10, and then evaluated on all three datasets. What if we search on ImageNet-16-120 with this metric and can we match the performance of other baselines on ImageNet-16-120? This might be an unfair comparison, but will be important for people to know whether the proposed method/metric can generalize to different datasets. <sep> Table 2 only reports N=10 and N=100. What if we significantly increase the value of N and will the performance be similar or better than other methods like RS and REINFORCE? As the proposed method uses very small cost, even using a large N, the cost would still be reasonable. It will be great to see we can achieve much better performance when increasing N. <sep> The writing and organization in Section 4 need to be improved. Adding subtitles might make it easier to read. Also, Section 4 mentions REAL at the beginning but REAL is not explained in detail until Section 5. The rows ""Optimal (N=10/100)"" don't seem to help validate the proposed method and are a bit confusing. <sep> It will also greatly strengthen this work if the authors can show the effectiveness of the proposed metric on a more realistic search space, e.g., the DARTS search, and evaluate the found architecture on a larger dataset, e.g., ImageNet. <sep> Justification of rating <sep> I like the idea of estimating a network's performance without training it. But this paper needs more refinement before being accepted. As mentioned above, the motivation, and the relationship between the method and motivation, need to be explained more clearly. More analysis is needed to understand and justify the proposed method. The writing also needs improvement. <sep> After rebuttal <sep> I would like to thank the authors for the hard work during the rebuttal. The ablation study of the data augmentation strategy and other added results are very helpful. Regarding the explanation of the flexibility and invariance, although I could get some intuition, I am still not fully convinced. So I keep my original rating. One possible way to make this work stronger and meet the acceptance criteria is to provide some empirical (or even better, theoretical) analysis of the influence of Σ on the flexibility and invariance of a network.","This paper proposes an interesting new direction for low-cost NAS. However, the paper is not quite ready for acceptance in its current form. The main area of improvement is around the generalizability of the score presented, both empirically and (ideally) theoretically. The two main directions of generalizability that would be worth investigating are 1) different image datasets (see comments around Imagenet-16) 2) different/larger search spaces. Even simple search spaces consisting of a few architectural modification starting from standard architectures (e.g. resnets) would go a long way in convincing the community that the proposed method generalizes past NasBench."
"==================== Post rebuttal ==================== <sep> I thank the authors for their response and additional experiments. Please see my comments below. <sep> Relation to [1] The first version of [1] appeared on arxiv in February. I am not sure if it has been published since then or not, but regardless it is sufficiently ahead of the *CONF* submission deadline to consider it a prior work. [1] provides a Mapper optimization algorithm in Figure 4. There are some minor differences with your algorithm, but I don't see how they make your algorithm more communication efficient. Generalization analysis in [1] is for the mixing parameter learned from data (i.e. adaptive), that is why there is no dependency on it. Your analysis is for a fixed mixing parameter, but since it is not possible to know it in advance, learning it from data seems to be more reasonable. So I am not sure what is the advantage of a theorem with explicit dependence on it. I agree that the convergence analysis is new, but [1] also has two more algorithms. <sep> Experiments Despite that your paper has more experiments, EMNIST experiment in [1] is better suited for studying personalization in my opinion. EMNIST results in Appendix B in the submission are for digits only (and also use fewer clients, but that is less important). One of the reasons that a centrally trained model on EMNIST performs worse than personalized models is the shift in the distribution of characters and digits across clients. This is not the case for MNIST/CIFAR10: if I train a neural net using all of MNIST train data it will easily achieve 99+ average test accuracy (without any personalization). How the test data is split across clients does not matter because accuracy on each digit is roughly the same. That is what I meant by a ""single global model with good performance (i.e. trained on the full dataset)"". <sep> Having worked on FL with personalization myself, I've noticed that it is quite hard to achieve a meaningful improvement over the vanilla FedAvg + fine-tuning. This is also evident from Table 2 of [1], where none of their algorithms offer a convincing improvement. This paper claims that an algorithm very similar to Mapper [1] indeed outperforms FedAvg + fine-tuning. I'd be happy if it is so, but I do not find the provided experimental evidence sufficient. I recommend reproducing the EMNIST experiment of [1] (please don't discard characters, use the same number of clients per communication, etc.). If your algorithm can achieve 91+ accuracy, I'd consider it an improvement. In that case, a more detailed discussion of the differences with Mapper [1] that enabled the improvement would also be great. <sep> Regarding the number of parameters, based on eq. (1), personalized model is a convex combination of two models. So if I understood correctly, the number of parameters is increased both during training and testing. <sep> ==================================================== <sep> This paper considers the problem of personalization in federated learning. The proposed approach consists of allowing each client to have a local model for personalization and taking a convex combination of global and local models as personalized preditors. Authors also provide a theoretical analysis of the generalization and convergence guarantees and empirical evaluation. <sep> The proposed method is equivalent to the Mapper algorithm from [1]. Generalization analysis is also very similar. <sep> The empirical study is not conducted carefully: <sep> Allowing for a separate local model for personalization increases the number of parameters which is not accounted for <sep> Reported performance of the global model trained with FedAvg on MNIST and CIFAR10 is overly pessimistic. Prior works (e.g. [2]) report much better performance for the non-personalized model in similar heterogeneous data splitting scenarios. <sep> Splitting MNIST and CIFAR10 datasets by classes is not a realistic way of simulating heterogeneous distributions in my opinion (see [3,4] for more recent strategies). It also does not seem appropriate for studying personalization as we know that there is a single global model with good performance (i.e. trained on the full dataset) and the key challenge is the optimization over heterogeneous datasets rather than the need for personalization. I recommend datasets such as FEMNIST [5] for studying personalization. <sep> [1] Three Approaches for Personalization with Applications to Federated Learning <sep> [2] Communication-Efficient Learning of Deep Networks from Decentralized Data <sep> [3] Bayesian Nonparametric Federated Learning of Neural Networks <sep> [4] Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification <sep> [5] https://leaf.cmu.edu/","The paper studies personalized federated learning, mixing a global model with locally trained models. Reviewers agreed on the relevance of the problem and that the work contains valuable contributions, such as the generalization bounds. <sep> After discussion, unfortunately consensus remained that the paper remains narrowly below the bar in the current form. <sep> Concerns remained on novelty over the Mapper optimization algorithm which also has adaptivity to the local/global combination of models, the dependence of the generalization bound on the mixing parameter as it converges to the global model, <sep> as well as on the strength of the experimental findings compared to well-known FedAvg and related method in a realistic benchmark environment (such as e.g. Leaf), since the dataset choice (and even more its partition among clients) is a crucial aspect for measuring personalization in a fair way. We hope the feedback helps to strengthen the paper for a future occasion."
"SUMMARY <sep> This work proposes a novel neuro-algorithmic policy architecture for solving discrete planning tasks. It takes a high-dimensional image input and processes it through modified ResNet encoders to obtain a graph cost map and a start/goal heatmap. This is fed into a differentiable Dijkstra algorithm to obtain the shortest trajectory prediction which is trained using an expert-annotated trajectory via a Hamming distance loss. This module is evaluated in two dynamic game environments demonstrating generalization to unseen scenes. <sep> STRENGTHS <sep> The general idea of integrating BlackBox combinatoric shortest path algorithms in a differentiable planning module is interesting and has a lot of potential to be useful. <sep> WEAKNESSES <sep> The novelty compared to Vlastelica et al. (2020) is not clear. <sep> The design choices are not clearly justified and the considered use-cases for this particular architecture are limited. <sep> Important information is missing to make this work reproducible (see reproducibility section) <sep> The evaluation considers only shortest-path planning scenarios that are amenable to the proposed architecture (see evaluation section). The authors should either provide a clear motivation of the considered scenario types or evaluation on scenarios learning more complex representations. <sep> CLARITY <sep> The general idea of the work is clearly written although important information for reproducibility is missing (see below). I also felt that the authors did not make particularly good use of space. For example, Sec. 3 and Sec 4.1 could be condensed into a joint background section, leaving more space for more detailed experiments. The information in Sec 4.3 seems to be a better fit for either the related work or the conclusion section. <sep> Smaller clarification questions: <sep> What is the y axis in fig 5 (a). <sep> In the conclusion: What exactly is meant by knowing the topological structure of the latent planning graph a priori? How is this incorporated as an inductive bias into the neural network? <sep> REPRODUCIBILITY <sep> The results in this work are not reproducible. Relevant information on training (e.g. which optimizer was used? what were the learning rates? ...), hyperparameters (which parameters were tuned? which range was considered? how were they tuned?), baseline method training (e.g., how long was PPO trained, how exactly were rewards defined, ...) and environment generation settings are missing. <sep> EVALUATION <sep> The evaluation seems one of the weak points of this work. The problems here are threefold: <sep> the number of considered tasks is very limited and their type is very limited to scenarios where one image input provides enough information to generate a full cost map. This does not hold in most planning tasks. <sep> Because the architecture itself is a claimed contribution, this work would require a much more thorough evaluation of architecture design decisions such as which underlying CNN is used, what is the <sep> Simply using the PPO baseline is insufficient. First, there is no discussion on how and why this algorithm was chosen as a baseline. Second, more recent or closer related baselines are missing. Some of those are mentioned in related work and they seem to be more fair/useful comparison methods. <sep> NOVELTY / IMPACT <sep> The work is not sufficiently motivated. While planning, in general, is an important problem and differentiable planners are an important research topic, the motivation of this work is not clear. The authors should not only name the use-cases where an intermediate planning module might be beneficial but also discuss what the main insights of this work are. As the authors write themselves, a differentiable implementation of TDSP in a neural network can simply be achieved by applying theory from Vlastelica et al. (2020). In fact, that paper already demonstrates the use of Dijkstra in a neural network computation graph. This opens up the question of what is the key idea and value of the present work? <sep> Furthermore, the authors did not sufficiently consider/discuss existing related work. The related work does not sufficiently cover state-of-the-art. Combining planning modules with deep learning pipelines (although in slightly different ways) has been considered in several works. More importantly, the related work should discuss why the proposed approach is beneficial compared to approaches containing differentiable planners. The related work is also missing numerous relevant papers. Some potential examples involve: <sep> Kuo et al., Deep sequential models for sampling-based planning, IROS 2018 <sep> Kumar et al., LEGO: Leveraging Experience in Roadmap Generation for Sampling-Based Planning, IROS 2019 <sep> Gupta et al., Cognitive mapping and planning for visual navigation, CVPR 2017 <sep> Savinov et al., Semi-Parametric Topological Memory for Navigation, *CONF* 2018 <sep> Tamar et al., Value Iteration Networks, NeurIPS 2016 <sep> Finally, the related work section should not only provide a broader discussion of related works but also more clearly emphasize the relationship between these works and the present work (e.g. how and in what context is the present work better more useful than related works? Which ideas are used from related work and what is new to this work? ...). <sep> REVIEW SUMMARY <sep> Overall, I recommend rejection of this work because the issues raised in this review cannot be resolved without a significant amount of work. While this may be discouraging, I believe that the authors are considering an interesting topic that has a lot of potential and, after resolving the issues raised in this review, may result in a successful submission. <sep> POST-DISCUSSION SUMMARY <sep> I want to thank the authors for answering my questions and correcting misunderstandings and updating the paper. I still recommend rejection of this work given that the novelty is not yet fully clear. While the updated list of contributions is indeed a better match for this work, claiming that correct inductive biases improve generalization does not seem to be a new insight. As mentioned in my initial review, I still believe that the general line of work has a lot of potential and want to encourage the authors to fully address the general concerns raised in the reviews and resubmit the work.","This paper is about training a discrete policy that maps an image representation through a differentiable time-dependent path planning module. The method is based on [1] and the reviewers are concerned about lack of novelty with respect to this work, and also with [2], however the latter only appeared a few weeks before the *CONF* deadline, so I am not factoring it in my recommendation. Unfortunately, in light of [1], 3/4 reviewers do not recommend acceptance, and I agree with them. <sep> [1] Vlastelica, Paulus, Musil, Martius, Rolinek. Differentiation of Blackbox Combinatorial Solvers (2020). <sep> [2] Yonetani, Taniai, Barekatain, Nishimura, Kanezaki. Path Planning using Neural A* Search (2020)."
"Summary <sep> The paper proposes a procedure for identifying a subgraph GK of a given size K (measured by the number of edges) whose output through the GNN function f is as close as possible to that of the full graph G. The proposed method is a greedy approach which starts from an empty graph and gradually adds the next edge by minimizing the difference between the outputs using mutual information. Furthermore, to reduce the computational complexity, a node clustering is done on the graph and the attribution is applied first on the edges between C identified clusters and then transferred to all edges. <sep> Quality <sep> The experiments are relatively thorough and the writing is fine. <sep> Clarity <sep> The description of the method starts clearly at the high level but then lacks important information when the actual approach is presented in detail towards the end. In addition, more  discussions, motivations, and intuitions are needed for the design choices of the actual algorithm. Also, it is unclear if the main results are obtained using the cluster-based approach or not. Details described below. <sep> Originality <sep> While similar methods have been used in the image domain, to the best of the reviewer's knowledge, the work is original in the context of graph networks. The reviewer believes the approach is better suited for graph networks, due to the feasibility arising from the structuredness of graphs. <sep> Significance <sep> The work conducts experiments covering different kinds of datasets as well as graph networks and considers multiple relevant baselines for GNN explanation. The results are, in general, significantly better than the baseline approaches on 3 different criteria. The paper is significant in that the approach is fundamentally different from the baselines and that the results are qualitatively and quantitatively different or better. <sep> Major technical comments <sep> Experiments the results are obtained on three different datasets based on three different state-of-the-art graph networks and using an average of 5 independent runs. <sep> Various graph explanation baselines are considered and the results show clear general improvement over the baselines in the three criteria used by the paper. <sep> While the approach can generally be expensive, the time complexity of the cluster-based approach seems to be comparable to GNNexplainer and CXplain. <sep> The difference in faithfulness (accuracy) compared to prior works on explaining graph predictions is clear and significant. However, the mere improvement over the baselines is not surprising since the proposed explanation method directly optimizes an objective to mimic the full graph's function. <sep> How is X obtained in practice? What clustering algorithm has been used for the method? <sep> Does table 1 contain the results of the standard causal screening or the cluster-based one? The paragraph on the ""influence of cluster numbers"" seems to suggest the results in Table 1 belongs to the standard causal screening. If so, what are the results of the cluster-based method for w.r.t. contrastivity and sanity check? What is the computational complexity of the standard vs. cluster-based variant? What number of clusters is used for table 1, if it is cluster-based. <sep> Theory <sep> The greedy approach selects only one possible subgraph that explains the prediction. The distinction between ""irrelevant edges"" and ""redundant edges"" should be important here if the goal is to find the ""causal"" subgraph. Since the paper motivates the approach completely based on causal reasoning, an appropriate discussion is required. <sep> How are the super-edges of the supernodes constructed? Does any edge between any nodes of two supernodes induce a single superedge between the two supernodes? Or there can be multiple super-edges connecting the two supernodes? <sep> When doing the attribution on the new ""super"" graphs, are the intra-supernode edges always present when finding the attribution of superedges? How does this choice affect the goal of finding causal edges? Especially this choice virtually implying that intra-supernode edges are ""attributed"" when deriving the attribution of super-edges. <sep> When constructing Z, why is the original X that contains soft clustering used while the H is obtained using the hard clustering assignments? <sep> What is an interpretation of a z representation of a node? Why should the dot product of z vectors be a good proxy for their attribution? Separate arguments are probably needed for intra-supernode and inter-supernode edges. <sep> Minor technical comments the qualitative figures are interesting in that it shows the potential ways that the proposed approach can better find the key connections compared to the baselines. <sep> It's better to start the ""Task Description"" section by something similar to ""we define a graph of interest …"".  Otherwise it causes confusion since the definition, only based on edges, is in contrast to the previous paragraph and is only resolved at the end of the current paragraph. <sep> The statement ""Directly optimizing Equation 1 is generally NP-hard"" requires citation and/or explanation. <sep> in eq.3, shouldn't it read: |Gk|=k? <sep> Overall <sep> The paper proposes a different approach compared to the currently-existing explanations for graph networks. This type of ""causal"" explanation is considerably more feasible for and better fits graph networks thanks to their sparse and modularized structure. Although the results on accuracy are not surprising, they are still useful for applications that are interested in finding substructures responsible for a certain prediction. This is in fact quite commonly useful in many applications dealing with graph networks such chemistry, biology, image understanding, social networks, etc. The paper can greatly be improved on its clarity, especially when it comes to the design choices for the cluster-based variant. That being said, the results are quite convincing such that it empirically validates the approach. <sep> All in all, if the paper is improved on clarity during the revision period, I would lean towards acceptance. <sep> Post-Rebuttal <sep> My main concern was regarding the clarity of the paper. I believe it is improved in the revised version, so I increase my rating.","I found the main algorithmic contributions to be interesting and of potential value to practitioners, as highlighted by Reviewer 3. Like several reviewers, I found the causal framing to be confusing, or at least not really to be framed in a framework like Pearl's: the word ""confounding"" is thrown a few times in the manuscript, but there is no formal sense by which it is linked to what we commonly understand as confounding. We are still talking about what happens inside a predictive model (a deterministic function), not what happens in the real world (the authors are not alone as targets of my observation: my point applies to a lot of the papers in the references, where the causal interpretation is hardly illuminating for those coming from a causal inference background, for instance). The reply to Reviewer 2, for instance, cites [1], which is about Granger causality and has little to do with Pearl's framework. Despite its name, Granger ""causality"" is a probabilistic concept (or, at best, an idea for identifying non-causality) with a very minimal causal basis besides the use of time ordering. A much more rigorous explanation of confounding in this paper's context needs to be provided. <sep> That been said: as helpfully highlighted by Reviewer 3 (and summarized without any need to resort to a causal framing), there are several positive contributions added here, which might be of interest to the *CONF* audience. The causal framing unfortunately gets in the way without adding clarity. <sep> In its present state, the paper is not yet ready for publication. We hope that the reviewer comments prove helpful for preparing a strong future submission."
"The paper proposes an RL algorithm for multi-task learning. Under certain assumptions, the paper proves a sample complexity result for this setting. The paper presents a new algorithm based on this approach. The empirical results on the task of sequential portfolio optimization shows that this approach performs better than the policy of constantly rebalanced portfolio. <sep> I dont understand the aim of this paper and unfortunately, the paper did not help me either. It seems that the paper focuses on multi-task learning using RL. The main assumption the paper makes is permutation invariance (PI). However, the way paper defines it is not clear to me. <sep> Def 1: ""A policy network is PI if it satisfies pi(sigma(a), sigma(x)) = sigma(pi(a,x))"" for any permutation sigma. <sep> So sigma is a permutation of items in a set? <sep> In this case it seems the set is the output of policy network. What is the output of policy network: action? But the action is not defined as a set of items. It seems that one must guess that the actions are division of 1 resource among t entities. Lets say t = 2 and resulting action is [0.2, 0.8] <sep> If this is true the the two permutation of a are [0.2, 0.8] and [0.8, 0.2]. But then the definition says the left-hand side should be equal to both these permutations. So [0.2, 0.8] == [0.8, 0.2]? <sep> I dont understand this. <sep> By the defintion do you mean: pi(sigma(a), sigma(x)) = pi(a,x). This would mean that the rearrangement of state or actions do not change the output of the policy network. I can understand this but not the def in the paper. <sep> Finally, what does this PI means in real-life? Since the paper talks about resource allocation, does the PI mean that it does not matter which entity the resource is being allocated to as long as the share of resource does not change? For what kind of problems does such an assumption/property hold? What is one entity is better at utilizing resources than others? <sep> After going about the definition/assumption for quite some time, the main theorem of the paper does not even use the definition/assumption. So why was it introduced? <sep> From related work ""Lazaric et.al. provide a performance bound that bears similarity to ours, which one can consider an extension for our particular PI setting"". So which approach is more general, the one in Lazaric et. al or yours. Seems yours since Lazaric et.al. is an extension to your PI setting. But then the same sentence says yours is a particular setting? <sep> Corollary 2: "" the gain in sample efficiency can be exponential in m"". What is the meaning of 'can be'. Is it exponential or not? If yes, then under what condition? <sep> How is portfolio optimization a multi-task RL problem? It can be formulated as a resource allocation problem but apart from maximizing long-term returns what are the other task the agent must perform for this problem. <sep> Sorry to say that I tried quite hard but I could not make sense of either the text or mathematics of the paper.  It seems the paper presents an interesting attempt at resource allocation using RL but I found the writing highly confusing.","This paper tackles a problem of resource allocation using reinforcement learning. An important invariant - permutation invariant - is identified as an important characteristic of this problem. Then it is shown that taking advantage of such an invariant should dramatically improve the sample efficiency. <sep> On behalf of the reviewers, I would like to thank the authors for addressing many concerns raised in the initial reviews. Unfortunately, a further examination revealed several other potential issues that require further clarification: <sep> It seems that real-data experiments do not really demonstrate whether the benefits of the approach come from multi-task learning or from permutation invariance. It would make sense to run an ablation study. In particular, if the benefit is really coming from multi-task learning, then the theory part of the paper becomes less relevant. <sep> The metric used for finance application appear to be in-adequate. It is typical in finance academic literature to look some form of risk-adjusted returns. Is the MTL strategy just taking more risk? How statistically significant are the results? <sep> Given these concerns the paper can not be accepted in its current form but we encourage authors to address these and resubmit."
"Second Review <sep> I thank authors for taking time and answering my queries. However current manuscript fails to point out key difference between Akrout 19 kolen-pollack method and DKP (proposed method). Combining FA with DKP does not add sufficient novelty. As pointed out by other reviewers, paper should highlight key differences and reasoning for such combination. I am happy to see additional results with DRTP, however it is also important to test your approach based on there methodology.   It is difficult to gauge the significance of your approach, since training protocol varies a lot. I would request authors to add more baselines and training protocols  (future submission) to show that your method is robust and can also train deeper CNNs models. Current submission missed out on many key aspects, despite having promising direction. I hope our reviews help you in strengthening this promising work. <sep> Summary <sep> This work proposes an approach to update feedback weights in DFA using modification of kolen-pollack method, which helps in training deep CNN network. <sep> First Review <sep> Citation missing for key work on assessing the scalability of bio-inspired approaches and highlighting key limitations [Bartunov 18], variants of DFA [ Moskovitz 18, Frenkel 19]  and recently an approach similar to DFA with target projection known as LRA (also has similarity with Direct Kolen-Pollack) showing promising performance on deep CNNs [ Ororbia & Mali 2020]. <sep> The update rule used in recursive-LRA is similar to what proposed in this paper <sep> Delta_b(update for feedback weights)  = learning rate *( teaching signal(delta_k) * post-activation from layer below (a_(l-1)) <sep> For LRA Delta_b(update for error weights)  = learning rate *( teaching signal(error_k) * post-activation from layer below (a_(l-1)) <sep> For weight mirroring [ Akrout 19] Delta_b(updates for feedback weights) = = learning rate *( teaching signal(delta_(l) * delta(updates) from layer above (delta_(l+1)) <sep> One can see we can derive chain rule formulation with certain assumption in which feedback matrix or error matrix acts like transpose of forward weights (rotated 180 degree). <sep> As shown in Feedback alignment(lillicrap 16) the updates for FA and LRA lie with 90-degree w.r.t BP. One can provide such plots to show how far way are your updates w.r.t. BP and other bio-inspired approach. <sep> ""We also found that the optimal hyperparameters and optimizers for the backward weight matrices in DKP seem to vary greatly from one network to the next"" <sep> Can you provide more detail about your experimental setup? What are the range of hyper-parameters and how does DKP perform w.r.t BP and other variants? It is well known that DFA in its vanilla form suffer whenever tested with deep networks on challenging benchmarks such as imagenet (akrout 19, Bartunov 18). As shown by Moskovitz 18 and Crafton 19, integrating BP or making feedback weights close to forward weights helps in learning for complex benchmarks. So, what different does DKP offer? is it robust, speeds up the convergence, always stays consistent (robust against bad initialization). Current manuscript fails to highlight these points which could make current work stronger. <sep> Do you constraint your feedback weights, if so how? If not, then how does model ensure that feedback weights are respecting forward neural activities and helping it to converge? Won't feedback weights grow making discrepancy between forward and backward activities, thus slowing the convergence of the network? <sep> Comparison against other variants of DFA <sep> We would like to see detailed comparison w.r.t various variants or family of FA(Moskovitz 18, Frenkel 19] and LRA (since update rules are similar). <sep> [Bartunov 18] Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G.E. and Lillicrap, T., 2018. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Advances in Neural Information Processing Systems (pp. 9368-9378). <sep> [Akrout 19] Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T. and Tweed, D.B., 2019. Deep learning without weight transport. In Advances in neural information processing systems (pp. 976-984). <sep> [Moskovitz 18] Moskovitz, T.H., Litwin-Kumar, A. and Abbott, L.F., 2018. Feedback alignment in deep convolutional networks. arXiv preprint arXiv:1812.06488. <sep> [Frenkel 19] Frenkel, C., Lefebvre, M. and Bol, D., 2019. Learning without feedback: Direct random target projection as a feedback-alignment algorithm with layerwise feedforward training. arXiv preprint arXiv:1909.01311. <sep> [Ororbia and Mali 20] Ororbia, A., Mali, A., Kifer, D. and Giles, C.L., 2020. Reducing the Computational Burden of Deep Learning with Recursive Local Representation Alignment. arXiv preprint arXiv:2002.03911.","This paper investigates an improvement to the direct feedback alignment (DFA) algorithm where the ""backward weights"" are learned instead of being fixed random matrices. The proposed approach essentially applies the technique of DFA to Kolen-Pollack learning. While reviewers found the paper reasonably clear and thought the experiments were acceptable, there were significant concerns about the novelty of the approach and the fact that the proposed approach was a straightforward combination of existing ideas. Further, the paper could have done a better job situating (and applying) the proposed method to DFA variants that have been proposed since the original DFA paper came out."
"This paper studies the notion of disentanglement in a group representation theoretic setting. Disentangling is sometimes conceptualized as mapping distinct factors (e.g. position / orientation) to distinct subspaces. It is shown theoretically that such a naive notion of disentangling is impossible for topological reasons, and this is confirmed empirically. An alternative definition of disentanglement is given, where instead of confining the effect of each transformation to a subspace, an operator is used that acts on the whole latent space (this operator is chosen as a shift operator, which works for cyclic groups). It is shown empirically that an autoencoder with a shift operator in latent space is better able to learn rotations and translations. <sep> The paper does a good job explaining why the naive notion of disentangling leads to topological problems, and convincingly backs this up with experiments as well. The insight is not new to me personally, but I can't find a reference that explains it and I think it is not widely understood, so I consider this an important contribution to the (very muddled) discourse on disentangling. <sep> Definition 1 provides a new definition of disentangling. However, the statement is not very precise, and I am not convinced that it can reasonably be considered as a definition of disentanglement. The definition is: <sep> ""A representation is said to be disentangled with respect to a particular decomposition of a symmetry group into subgroups, if there is a family of known operators acting on this representation, potentially distributed across the full latent, where each operator is equivariant to the action of a single subgroup."" <sep> Based on the rest of the paper, I think this means that we have for each subgroup G_i an operator phi_i(g) acting on the latent space. The definition does not make it clear that we wish the encoder to be equivariant wrt this operator and some operator acting on the input space, but I will assume that is what is meant (otherwise, having an operator acting on the latent space is a rather vacuous requirement on the encoder/representation). The definition does speak of the operator being equivariant, which I will take to mean that it is a group representation, i.e. phi(gg') = phi(g)phi(g'). The operator being distributed I will take to mean that phi(g) can be any linear map, not necessarily acting trivially on a subspace or being (block-) diagonal / reduced. <sep> The definition mentions that each subgroup should have its own operator, but since all of them act on the whole subspace this seems to a trivial constraint. Indeed if we have a representation of the whole group acting on the latent space, simply restricting it to each subgroup gives us a representation of the subgroups. I would further note that what is done in practice in the paper is different from this definition, because we have one latent space per operator, not multiple operators acting on the same space. <sep> Under this interpretation, I don't see how the definition is saying anything else than that the network should be equivariant wrt some representation of the group acting on the input and output space. Although equivariance is a good property for various reasons, it does not seem to me to be reasonable definition of disentangling by itself. Indeed, the identity map satisfies this constraint trivially. <sep> It may be that I have misunderstood definition 1, but this strengthens the case for making it mathematically precise. <sep> Even if one can question whether Def 1 is a good formalization of disentangling, the paper does show empirically that it is easier to learn an equivariant encoder/decoder when the latent operator is a shift operator or a diagonalized complex version of it, rather than a disentangled operator (with one 2x2 rotation matrix block and an identity block; fig 3b). Although I don't know if these two approaches have been compared before, several older papers consider similar models to the shift operator model. <sep> For instance, in a sequence of papers Memisevic & Hinton considered factorized RBMs that do something similar. Cohen & Welling described a representation-theoretic version of this model which is very similar what is presented in this paper (at least the linear AE), and also gave a definition of disentangling (under this definition, the complex diagonal shift operator is disentangled while the original shift operator is not). Models with a stack of multiple operators were considered by Sohl-Dickstein et al. <sep> If one wishes to define a notion of disentangling based on subgroups and representations, it may be worth investigating subgroup adapted / Gelfand-Tsetlin bases. <sep> In summary, I think this paper contains several interesting observations and results, and I think the general direction is very interesting and deserves further study. However, I'm not convinced that this paper provides a good definition of disentangling, the experiments although convincing and well executed are restricted to simplified domains, and some of the insights / methods presented in the paper are already present in earlier work. Nevertheless I hope the authors will not be discouraged, and continue to work on this important and fundamental problem using the tools of representation theory. <sep> References <sep> Memisevic & Hinton, Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines, 2010 <sep> Sohl-Dickstein, Wang, Olshausen, An unsupervised algorithm for learning Lie group transformations, 2010 <sep> Cohen & Welling, Learning the Irreducible Representations of Commutative Lie Groups, 2014 <sep> Wakin, Donoho, Choi, Baraniuk, The multiscale structure of non-differentiable image manifolds, 2005 <sep> Post-discussion update: <sep> Having read the other reviews, author response and updated paper, I still think this paper is borderline. The insight that disentangling transformations as naively defined is impossible for topological reasons is valid and interesting, but seems to have been already observed by others, e.g. Falorsi et al. Nevertheless the paper does a good job explaining this so it could be useful, as some authors seem to not know about this issue. The definition of disentangling still seems a bit vague to me, and I'm not convinced of practical applicability of the proposed method.","This is a borderline case (quite comparable to the other borderline case in my batch). The paper has received careful reviews and based on my weighting of the different arguments I arrive at an average score between 5.75 and 6.. The authors present some worthwhile ideas related to disentanglement that deserves more attention and that could spark more research in this direction. At the same time, the level of novelty and significance of this work remains a bit limited. Taken together the paper is likely not compelling enough to be among the top papers to be selected for publication at *CONF*."
"The paper suggests a novel auto-encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo-inverse of the decoder.  It is noted that for a linear architecture, this gives PCA, therefore, this can be seen as a nonlinear PCA approach. <sep> In theorem 1, the authors claim that for the encoder-decoder solution to have the desired properties, certain equalities have to be satisfied by the local differential matrices of the encoder and decoder.  This gives rise to a loss function that is combined of 3 parts:  A reconstruction loss (as usual with autoencoders) plus a combination of a loss penalizing non isometric decoders, plus a loss penalizing an encoder that is not a pseudo-inverse of the decoder.  This loss function is claimed to be the main technical novelty of the paper. <sep> In the experimental part, the authors compare the merits of this approach on synthetically generated low dimensional manifolds in high dimensional ambient spaces, against other standard manifold learning algorithms, and show that the paper's method outperforms other method using a measure of distortion of triangle edges on a grid.  They also experiment with ""real data"" (e.g. MNIST), show the merits of the proposed algorithm when visualizing the 2 dimensional bottleneck of the autoencoder.  The comparison here is against other algorithms for high dimensional data visualization. <sep> The overall idea and theory seem interesting.  The experiments are a bit disappointing.  For the synthetic data, I am not sure I understand why they did not chose something of high dimension?  Maybe I am missing something, but would it be impossible to generate, say, a 50 dimensional manifold in 100 dimensions?  Maybe the triangulation part will be challenging, but that is not the only way to compare between the various algorithms.  As for the real data section (e.g. MNIST), I am not sure I see why you compare your algorithm against algorithms that are intended for 2-d visualization (e.g. t-SNE).  Your algorithm does manifold learning.  Why not,for instance, take all the images corresponding to some fixed digit (e.g. ""3""), which is presumably close to a low (but definitely more than 2....)  dimensional manifold, and see how well your manifold learning algorithm reconstructs them? <sep> The editorial level of the paper is not very high, due to grammatical English mistakes.  Here are examples (the list is not complete): <sep> p. 1 ""Autoencoder (AE) can also be seen"" => ""Autoencoders can also be seen"" or ""An  autoencoder can also be seen..."" <sep> ""AE is trying to reconstruct X...""  - The present progressive tense is not suitable here.  Maybe ""AE's try to reconstruct""?  Or ""AE's are designed to reconstruct..."" or ""An AE reconstructs..."" <sep> p. 2 <sep> Manifold learning generalizeS <sep> p. 4 <sep> ""As-usual "" => As usual p. 5 <sep> ""Does our suggested Loss... drives"" -> ""drive"" <sep> p. 6 <sep> Why is ""Denoising"" capitalized? <sep> ""In addition, we compared versus..."" => ""...compared against...""","The paper introduces a new formulation for learning low-dimensional manifold representations via autoencoder mappings that are (locally) isometric by design. The key technical ingredient is the use of a particular (theoretically motivated) weight-tied architecture coupled with isometry-promoting loss terms that can be approximated via Monte Carlo sampling. Representative results on simple manifold learning experiments are shown in support of the proposed formulation. <sep> The paper was generally well-received; all reviewers appreciated the theoretical elements as well as the presentation of the ideas. <sep> However, there were a few criticisms. First, the fact that the approach requires Monte Carlo sampling in very high dimensions automatically limits its scope. Second, the experiments seemed somewhat limited to simple (by *CONF* standards) datasets. Third and most crucially, the approach lacks a compelling-enough use case. It is not entirely clear what local isometry enables, beyond nice qualitative visualizations (and moreover, what the isometric autoencoder provides over other isometry-preserving manifold learning procedures such as ISOMAP). Some rudimentary results are shown on k-NN classification and linear SVMs, but the gains seem to be in the margins. <sep> The authors are encouraged to consider the above concerns (and in particular, identifying a unique use case for isometric autoencoders) while preparing a future revision."
"This paper provides an interesting research direction for the cross-domain of federating learning and backdoor attacks. This direction has very limited work until the recent 2 years. The work being proposed in this manuscript is simple and straightforward to implement. The pipeline has been clearly demonstrated. The experiments have multiple aspects presented and show promising results in various metrics. <sep> pros: <sep> Novel problem, may attract massive attention <sep> Simple and intuitive pipeline, conceptually easy to implement <sep> Results are versatile, many comparison tables are provided cons: <sep> Whole article is not self-contained, feel the connections between modules are very loose <sep> The design of the pipeline is very ad-hoc, so many engineering aspects can be tweaked and the performance could be dramatically altered. <sep> The experiments could use a few popular trojan attack methods, the baselines are not comprehensive. <sep> concerns: <sep> My major concern comes from the design of the pipeline and the experiments. The author(s) have created many splendid terms to describe the modules used in this work, however, their implementation uses both clustering and median, which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced (just like the non-iid data sets among clients). This kind of uncertainty due to the ad-hoc nature of the pipeline causes me to wonder: how bad this framework can be if any of the carefully cherry-picked modules fails its purpose? The mathematical motivation of this paper is missing and this causes the impression of untrustedness on the model design. It would be better if the author(s) can 1. provide some mathematical proofs or derivations to support your design. 2. provide a lower bound or upper bound for performance guarantee. <sep> Please compare it with a few Trojan attack methods in recent years. I believe no matter what kind of backdoor and Trojan attack, can be easily applied to FL by applying them individually on each client without too much trouble. <sep> An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks. KDD 2020 <sep> For federated learning, one important experimental factor is the number of clients (K= 5, 10, 50, 100, etc), the portion of data on each client (1%, 5%, 10%, etc), and data distribution assumption (iid, non-iid with feature shift or label shift, etc). Please evaluate the results by changing these important hyperparameters. <sep> The holomorphic encryption is a pretty standard concept in FL, I don't understand why the author(s) have listed this as the major contribution for the work. Especially, only one paper from 1986 is mentioned and nothing especially has been proposed in this work. It is just an unusual way to list your contribution. <sep> Code is not provided, I can not see the reproducibility of this work. <sep> minor: <sep> please attach your main context pdf in the submission and submit the appendix in the supplementary material","The paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks. <sep> The method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks. <sep> Both steps are a bit ad hoc in nature, and do not come with provable guarantees. <sep> Moreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and FL round. <sep> The authors further point in their response that "" no existing defense against backdoor attacks preserves the privacy of the clients' data."" This is in fact not true, as the differential privacy defense presented by the ""Can you really backdoor FL"" paper is in fact fully respective of user privacy. <sep> At the same time, the work on backdoor attacks and defenses is reminiscent of the ""cat and mouse"" work in adversarial examples: an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on. This is similar in the context of backdoor attacks. <sep> In fact, a recent work [1] proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against. (it is fine that the authors do not reference this work as it was published just recently) <sep> As the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited. <sep> [1] Wang et al. Attack of the tails: Yes, you really can backdoor federated learning, neurips 2020 <sep> https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf"
"General statements <sep> This paper introduces an interesting parallel between SDEs and GANs, and pushes the analogy to its practical implications as a way to learn neural SDEs. <sep> Globally, I found the paper a very good read, although it sometimes lack the details that could be useful for a non-specialist. This could and should easily be corrected by a few sentences here and there. <sep> Although I see that everything is included in the supplementary material to actually reproduce all experiments. Still, I believe that there could be some improvements to do. In particular: <sep> I think that the main text / the supplementary could be augmented with a short mention regarding the network structures. even when reading the code, it is not clear how time is handled (since the nets input not only tensors like X_t or H_t, but also time). Should I understand that the raw time stamp is simply concatenated to the other input ? <sep> you didn't clearly mention all the tricks and experiments you tried out. It is not clear to me to what extent the performance you report depends on the network structures you picked. <sep> All in all, I recommend acceptance. <sep> EDIT: <sep> after seeing all reviews, and most importantly thinking about it and pondering the answers given by the authors, I am sorry that I must lower my score. <sep> I still like the paper, that could be accepted in my opinion, but I think it oversells some contributions that are unfortunately not exploited (the brownian interval thing in particular, or am I wrong ?) <sep> Introduction <sep> Her are some comments along the way: <sep> I could regret that no general background is given for the curious reader that is not already a specialist in SDE or even ODE <sep> SDEs as GANs please explain the ""Initial condition"" statement better: why is it important that there be an additional source of noise here ? <sep> You are mentioning X and H as the (strong) solutions to your SDEs (1) and (2). Are they guaranteed to exist ? I guess the Lipschitz condition you assumed is enough for this. Is that the case ? <sep> In the ""training data"" item, H_0 is a function of Y_0. i/ is this Y_0 defined as above in ""initial condition"" ? ii/ Is there a reason H_0 is not a function of z_0 ? iii/ It makes the decision D done on training data actually to depend on \\theta, and not only on \\phi (through Y_0=l_\\theta(\\zeta_\\theta(V)). is that ok ? The item ""initial condition and hidden state"" does not make that point clearer to me. <sep> Could you briefly describe gradient penalty, instead of only refering to (Gulrajani 2017) ? That would make the paper more self-contained <sep> Efficient computation <sep> Section 3.1 (rough adjoint equation) is harder for me. I'm ok with the adjoint equation. Then, forgive me but I'm more uncomfortable with the (W, \\mathbb{W}) couple. What is meant exactly by ""sampling"" them ? It means drawing (s,t) and computing the related (W, \\mathbb{W}) ? For each, you compute the solution to the SDE ? <sep> now, assuming you get your a_t process. How do you actually use it to perform optimization ? Are you computing the gradient of the parameters wrt a_t and then averaging over time ? Basically, I need some more information on the general scheme to understand 3.1, assuming the adjoint equation is understood. <sep> Experiments <sep> The ""weights"" dataset is not super clear. Is the data actually a: 10xPx100 tensor, where P is the number of parameters ? (what is the value of P ?) Just to make sure: the same net is trained for all weights (what I assume), or is it a different net per weight ? <sep> Considerations in the ""lipschitz regularisation"" of your 2.3 section, you mention using gradient penalty, requiring adjoint, etc. But here, I understand that you actually didn't use these sophistications that were introduced in section 3.1 ? I think you should rephrase a bit here and there to actually better reflect these findings. <sep> References <sep> References are inconsistent. Sometimes abreviations, sometimes full names. <sep> Françios-Xavier -> François-Xavier","The reviewers agree that this paper has some interesting ideas. However, they believe it needs more work before it is ready for publication, especially so with regards to presentation (SDEs as GANs) and the experiments (backpropagating through the solver rather than using the adjoint dynamics). These would significantly strengthen the paper, but would probably require another round of reviews."
"SUMMARY: <sep> This paper presents a hierarchical nonnegative CP tensor decomposition method. It also proposes a training method that leverages forward and backward propagation. The method is tested on both synthetic and real datasets. These experiments illustrate how the method can be used to discover topics and how they vary over time. The hierarchical nature of the method makes it possible to group the topics into supertopics in multiple steps. <sep> I thinks the paper is well-written for the most part. The topic is interesting and should be of interest to the *CONF* community. It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. While this may be somewhat incremental, it seems like the authors are doing something that hasn't been done before. My main issue with the paper is that there are a few portions that are currently difficult to follow. In particular, in the discussion on approximation in Section 2.1, it is not clear if this idea is used in the implementation, which might make it difficult to replicate the results. Fixing this would help improve the quality of the paper. I provide more details on which parts are unclear below. <sep> ADVANTAGES: <sep> Introduction provides a clear overview of what has been done before. It is easy to see where the present paper fits in with the existing literature. It also does a good job of connecting to applications. <sep> The experiments in Sections 3.2 and 3.3 are convincing and nicely explained, especially the illustrations of how topics vary over time. The last sentence in Section 3.3 help explain the benefit of a hierarchical method. <sep> Well-written for the most part. <sep> Should be of interest to *CONF* community. <sep> CONCERNS/QUESTIONS: <sep> In the last sentence on page 2, it is not clear which loss function is used for Neural NMF when doing the backpropagation. Is it the expression in the Frobenius norm in the paragraph titled ""Hierarchical NMF (HNMF)"" above? <sep> In Section 2.1, I follow the discussion up to Equation (6). But the rest of the first paragraph is difficult to follow. For example, it's not clear to me how the columns of the hierarchical NMF factors are used to form NCPDs of ranks r(0), r(1), ..., r(L−2). The subsequent discussion about the dependencies and indices is also unclear. Perhaps this discussion could be made more clear with the help of an example? <sep> In Section 2.1, I think the second paragraph is clear until Equation (7), but the rest of the paragraph is difficult to follow. The discussion about approximating the relationship between the columns of Ai(0) and Ak(0) is unclear. What do you mean by ""relationship""? In what sense is (Wi)p1,p2 approximating it? It is not clear why the definition in Equation (9) is chosen. Is the idea that [[A~1(0),…,A~k(0)]]≈[[X~1,…,X~k ]]? In the experiments, do you use this approximation, or do you use the NCPD combined with HNMF for each factor matrix as discussed in the beginning of Section 2.1? <sep> In the last sentence of Section 2.1, you mention that ""Neural NCPD allows factor matrices for all other modes to influence the factorization of a given mode."" It makes sense that this is the case when all parameters, including the initial NCPD, are trained together via backpropagation. But does this still remain true when the initial NCPD is computed independently at first and then kept fixed throughout the fitting of the HNMFs, like in Algorithm 2? As far as I can tell, the approach in Algorithm 2 of keeping the initial NCPD fixed through the training is also what you do in the experiments. <sep> In the experiments, do you use a combination of the loss functions in Equations (11) and (13) (e.g., C+E), or just one of them? This is not clear from the discussion. Also, since the NCPD is computed by itself at the start of the algorithm and then kept fixed, isn't the term ||X−[[X1,…,Xk]]||F in (13) a constant that could be ignored? <sep> For the discussion in Section 2.2 about the derivatives, it would be a good idea to let the reader know that there is a more in-depth explanation available in the appendix. <sep> It is not clear how the Standard NCPD in Section 3 is computed. Could you perhaps add an explanation in the appendix or point to a relevant reference? <sep> Below Figure 2, you say that g∼N(z;0,σ2). What does z here mean? <sep> For the experiment in Section 3.2, did you try keeping each frame as a matrix in the tensor? In other words, did you try reshaping the tensor into a 4-dimensional tensor of size 37 × 3 × (number of x pixels) × (number of y pixels)? One motivation for using tensors is to avoid having to vectorize things like images, so it would be interesting to know if this also worked well. I don't think you need to change the current example, I'm just asking out of curiosity. <sep> MINOR CONCERNS/QUESTIONS: <sep> In Figure 1, the colors are a nice addition, but they're quite muted which makes it hard to see them. Could they be made brighter? Also, should the red line along the columns of the S2(0) matrix instead be along the rows of the A2(0) matrix? <sep> In Figure 1, in the caption, the tensor X is not using the bold tensor notation. <sep> Below Equation (7), in the definition of αj1,j2,…,jk, should the sum go to r instead of r(0)? It looks like it should based on the derivation in the appendix. <sep> Equation (15) should end with a comma instead of a period since the sentence keeps going below. <sep> In the second sentence of Section 3.1, the word ""size"" appears twice in a row. <sep> At the bottom of page 7, should the tensor size be 8 x 100 x 12721 instead of 8 x 10 x 12721 since the number of tweets are capped at 100? <sep> In the text to the left of Figure 7, the words ""the"" appears twice in a row in two places. <sep> In the appendix, in the section ""HNCPD expansion"", in the 2nd sentence (starting with ""We have that by definition...""), I think the square bracket ""]"" should be removed on the right hand side of the equation? <sep> ####################### <sep> Update: <sep> I thank the reviewers for their responses. I appreciate the effort they put into clarifying the paper. However, I still think Section 2.1 in particular is difficult to follow. I will therefore keep my original rating.","The paper presents a hierarchical version of NMF for the CP decomposition of tensors. <sep> The idea is similar to Chinocki etal 2007 and extends Gao etal 2019, and in Chinocki was presented for the standard linear formulation with regularisation terms. The extension here doesn't use the standard ALS algorithm but rather presents a neural network analogue, though the functions are still linear, its just that back-prop etc. are used for the computation. The authors point out their formulation is a more flexible representation and optimisation (in response to AnonReviewer5), and thus represents an improvement. While this is an interesting implementation, in NNs, the model is still fairly simple. <sep> Moreover the experimental results are restricted to a few data sets. There are literally hundreds of NMF variants in publication and many different evaluations are done. The experimental work here, while showcasing the work, is not extensive. For instance, more empirical comparisons should have been made against prior hierarchical NMF on a battery of data. <sep> So this is good, publishable work, and the authors have repaired many of the issues raised by the reviewers. The work, however, is borderline in empirical work and the contribution is not strong."
"This paper considers a certain generalization of convolutional neural networks and equivariant linear networks to the infinite dimensional case, while covering also the discrete case, and offers a universality result. In more detail, the paper first characterizes  equivariant maps as the unique  extensions of ""generator"", namely regular maps that provide target functions (or vector) defined over a basic domain. In other words, any map that takes functions (or vectors) into functions (or vectors) defined over the representatives from the symmetry's equivalent classes can be extended uniquely to an equivariant map by enlarging its target domain according to the equivariance rule. Second, infinite dimensional fully connected networks (FNNs) and general (equivariant) convolution neural networks (CNNs) are described. The main result of the paper is the Conversion Theorem (Theorem 11), and its consequences. The theorem specify the conditions under which an FNN can be approximated by a CNN. Since FNNs are known to be universal this implies universality of CNNs. <sep> I think the general CNN formulation and the Conversion theorem are of merit but i think the paper should undergo a rather serious revision before ready for publication. The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. I think attending these will provide a much more accessible and useful paper for the community. <sep> DETAILS. <sep> Relations to previous work. The paper discuss a rather broad generalization of equivariant network (equations 4 and 5). I think exploring the relations to existing generalization to convolution neural networks and equivariant networks is in order. For example, when taking S=T=[n] ([n]=1,2,...,n) and G=Sn (the permutations of [n]) equation (4) seems to boil  down to  Deepsets of Zaheer et al. mapping Rn→Rn equivariantly. Does theorem 1 implies that Deepsets (as equivariant model) is universal (as was proved in several previous works)? In this case, Theorem 3 states that any function f1(x) can be extended to be equivariant via fj(x)=f1(g⋅x), where g is a permutation such that g(j)=1. If this is all required for universality proof this would be a great simplification over previous proofs. This example could help the reader grasp this extension and also relate to previous works on equivariant learning.  What can be said about the equivariant tensor models and graph neural networks using the universality result in this paper? What can be said about the negative results of universality of second order (or higher order) tensor graph networks? Does the method in this paper imply group convolution networks are universal?  Are there any instantiations of the CNN discussed in this paper that are useful although not discussed in previous work? In summary, how the results in this paper relates to known and unknown results of CNN generalizations and what universality proof does it generalize? <sep> Theorem 11. First, looking at the proof, I feel there is a condition of the FNN ϕ that is missing from the theorem's formulation. That is, that the different layers of ϕ should be mapping to and from functions on base domains of G, C(B). Without it I can't see how the extension to equivariant maps work. How is it guaranteed that ϕ can be extended if internal states are defined on non-base domains? If this is true then how the FNN found from the universality result of FNN (e.g., Theorem 12 and 14) are guaranteed to satisfy this condition? Related to that, I couldn't exactly understand the claim of the second to last layers in the proof of Theorem 11: the first layer outputs a function in C(G/HT×B), but the second layer of the FNN maps functions from some different domain C(B2). If I understand correctly, the idea of the proof is basically using the extension mechanism of Theorem 3 to extend mapping to functions over base domains to equivariant mappings. This seems to produce a sort of lift in the domain of the target function at every step. Is that correct? Can you provide the proof for a simple example of equivariant networks such as Deepsets? Minor: there are some wrongs equation references in the proof of Theorem 11. <sep> The condition (C1) in Theorem is not clear to this reviewer. I think some examples and explanation should be provided. I could not really figure out, despite the following paragraphs, in what situations can we verify this condition and in what sense it limits the scope of the theorem. Is there a simple intuition or a way to check such subgroups exists? Are these normal subgroups?  Should HT be a strict subgroup of FS? Is this condition holds in standard cases such as [n] and G=Sn, or images and 90 degrees rotations? Does it mean S has to have a group structure? <sep> In Theorem 16: How  do we make sure the first layer can be seen as a generator? Can we given [N] the required base structure? Is [N] a base domain of G necessarily? This also relates to the question I asked above about the condition in Theorem 11. Anyway, this proof should be provided in the paper or supplamentary. <sep> The authors mention that the invariant case cannot be handled. However, invariant functions can be made equivariant by considering the trivial representation, e.g., in the discrete case let f(x) be invariant to Sn, then fi(x)=f(x) for all i∈[n] is equivariant I believe. Can we approximate this equivariant function?","This work studies the question of universal approximation with neural networks under general symmetries. For this purpose, the authors first leverage existing universal approximation results with shallow fully connected networks defined on infinite-dimensional input spaces, that are then upgraded to provide Universal Approximation of group-equivariant functions using group equivariant convolutional networks. <sep> Reviewers were all appreciative of the scope of this paper, aimed at unifying different UAT results under the same underlying 'master theorem', bringing a much more general perspective on the problem of learning under symmetry. However, reviewers also expressed concerns about the accessibility and readibility of the current manuscript, pointing at the lack of examples and connections with existing models/results. Authors did a commendable job at adding these examples and incorporating reviewers feedback into a much improved revision. <sep> After taking all the feedback into account, this AC has the uncomfortable job of recommending rejection at this time. Ultimately, the reason is that this AC is convinced that this paper can be made even better by doing an extra revision that helps the reader navigate through the levels of generality. As it turns out, this paper was reviewed by three top senior experts at the interface of ML and groups/invariances, who themselves found that the treatment could be made more accessible --- thus hinting a difficult read for non-experts. In particular, the main result of this work (theorem 9) is based on a rather intuitive idea (that one can leverage UAT for generic neural nets on the generator of an equivariant function), that requires some technical 'care' in order to be fleshed out. The essence of the proof can be conveyed in simple terms, after which following through the proof is much easier. Similarly, the paper quickly adopts an abstract (yet precise) formalism in terms of infinite-dimensional domains, which again clouds the essential ideas in technical details. While the paper now contains several examples, this AC believes the authors can go to the extra mile of connecting them together, and further discussing the shortcomings of the result --- in particular, the remarks on tensor representations and the invariant case are of great importance in practice, and should be discussed more prominently. Finally, while this work is only concerned with universal approximation, an important aspect that is not mentioned here is the quantitative counterpart, ie what are the approximation rates for symmetric functions under the considered models."
"Summary <sep> In this paper the authors introduce the notion of stable weight decay. The stable weight decay property can be defined in dimension 1 as follow: the effective learning rate represents an amount of time ellapsed between two iteration. The weight decay factor normalized (in log space) by the time ellasped should be constant across iterations. <sep> From their framework, the authors also propose SGDS, a minor modification to SGD where the amount of L2 regularization is increased with momentum, in order to balance for the larger step sizes that the momentum will yield. <sep> When applied to Adam, the authors derive AdamS, supposed to work better than the previous AdamW, which already improved how weight decay and Adam interact. AdamS has the stable weight decay property, unlike Adam or AdamW. AdamS weight decay amount is scaled by the denominator of Adam, taking the average over all dimensions to make it isotropic. <sep> The authors test their methods on vision tasks, where Adam is known to underperform compared to SGD, and their method AdamS achieves significant gains compared to AdamW. <sep> Review <sep> The reformulation introduced in equation (3) is an interesting alterntive view to look at weight decay, but I find it is not really used by the authors. The definition of the weight decay rate can be done from equation (2). <sep> The authors introduce the notion of stable weight decay and seem to right away assume it is a desirable property. In particular, there is no theoretical justification that this is the case. <sep> The benefit from SGDS is limited, except for hyper parameter tuning (as only the first few iterations of SGD are ""unstable""), but it serves as a nice illustration of the new concept. <sep> The part on adaptive methods is more interesting but the authors deviate significantly from their theoretical framework. Verifying the stable weight decay property is actually not optimal, because it is not isotropic. The authors trick is to average the value of the moving average of the squared gradients along all dimensions before using it to rescale the weight decay. <sep> The experimental section of the paper focuses only on vision. It would have been interesting to see the effect of AdamS on other type of tasks. <sep> Overall I think the idea introduced by the author is interesting, although the theory is not completely coherant. The experiments shows significant improvement but could have been on a more diverse set of tasks. Still,  I recommend acceptance. <sep> Remarks <sep> In the Introduction, talking about adaptive methods: ""are a class of dominated methods to accelerate"", I'm not sure what dominated means here. <sep> what is the point of having β3? after equation (4), the authors say ""SGD with momentum is β3=0"", that doesn't seem right, if β3=0, then the gradient is completely ignored. <sep> ============= <sep> Update avec rebutal and discussion with AC and reviewers. <sep> After discussing with the others reviewers and AC, I have come to share their concerns with the overall fragility of the paper. We agreed that the methods is sound and likely to work better than AdamW, the proofs are not sufficient. In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. The authors do not systematically compare across learning rates which make it hard to interpret the results as being conclusive. In fact only CIFAR-10 is evaluated with multiple learning rates. <sep> The remark by Elya Loshchilov should also be adressed. Note that you cannot use stochastic noise as a justification, because for heavily overparameterized neural network, the amount of stochastic noise at the optimum is zero (i.e. perfect fitting of the training set). But there is another explanation: Intuitively for a fixed β2, vt goes to zero as the current gradient goes to zero, and the ratio of the gradient by vt will converge to some constant, which prevents convergence. vt goes to zero at the same speed as the gradient but with a delay of 1/(1−β2). If there is no convergence, then the gradients won't actually go to zero. <sep> The only way to prevent vt from going to zero is to have β2→1 (i.e. the previously mentioned delay going to infinity), but in that case v¯t won't go to zero neither. This is only an idea of a possible justification and I would encourage the authors to think carefully about this stability issues in the next revisions. <sep> Finally I would encourage the authors the remove from the theoretical analysis parts that are not actually used (which I and other reviewers have noted).","The paper proposes a novel way to have weight decay-like update rule. Empirically, the authors claim that it improves generalization when applied to momentum-based optimizers and optimizers with coordinate-wise learning rates. <sep> This paper has been thoroughly discussed, both in public and private mode. <sep> The strength of this paper lies in the possible gain in generalization performance due the proposed change. <sep> The weaknesses are: <sep> the very confusing and not scientific motivation of the proposed change <sep> the experiments are not fully convincing <sep> More in details, we all found the discussion on ""stable"" and ""unstable"" weight decay extremely confusing. The claim of the paper is that ""stable"" weight decay should be preferred over ""unstable"" one. However, to validate a scientific claim it is necessary to carry out an empirical or theoretical evaluation. The theoretical one is simply missing: a number of proposition and corollaries are stated with some simple mathematical facts completely disconnected from the optimization or generalization issues. As it is, removing these arguments would actually make the paper better. <sep> On the empirical side, there is no experiment that supports the simple claim that ""the unstable weight decay problem may undesirably damage performance"". Instead, what we see are experiments in which the modified update rule seem to perform better, but they don't actually show that ""stability"" or ""instability"" are the specific issues at play here. Indeed, any other explanation is equally valid and the experiments do not support any specific one, but rather they can only support the claim that the proposed algorithm might be better than some other optimization algorithms. The specific reason why this is happening is not clear. <sep> Turning to the empirical evaluation, the discussion elicited the fact that, a part for CIFAR10, the experiments are carried out without tuning of the learning rates. Hence, it is difficult for us to even validate the claim of superiority of the method. I don't subscribe to the idea that a deep learning paper requires experiments on ImageNet to be valid. Yet, given that there is no supporting theory in this paper, the empirical evaluation should be solid and thorough. <sep> For the above reasons, the paper cannot be published at *CONF*."
"Summary: <sep> The paper goes beyond the task of link prediction between vertices, but focuses on predicting formation of higher-order structures, which involves multiple vertices simultaneously. For instance, given that authors A, B and C have a paper together, task of inferring authors A, B, C having another paper with a fourth author D is an example of higher-order relationship. <sep> More specifically, authors propose a kernel estimator which predicts the evolution of the graph through simplices and how they evolve with respect to some local neighborhoods. Their estimator considers how the graph, subsequently the simplices, have evolved within some window of history. The inference process is somewhat guided by a scoring mechanism that takes into account the past interactions between vertices. <sep> The paper proves consistency of the estimator, meaning that the estimator converges to the true function in probability. Additionally, they prove asymptotic normality (in the sense of Central Limit Theorem) of the estimator. <sep> The authors present numerical performance of their method against various other methods based on 4 dynamic graph datasets. <sep> Strengths: <sep> For the problem of predicting higher-order (incremental) structures in an evolving graph, authors propose a method which predicts formation of higher order simplices from existing ones, while capturing substructures between vertices of the simplices. They back their intuitions up by proving theoretical guarantees for the consistency and asymptotic normality of their estimator, which implies favorable statistical properties. <sep> Authors explain the differences and similarities between existing inference strategies clearly, enabling the reader to understand for which task the proposed method is specialized. <sep> Experimentally, authors argue that their method shows superior performance against various different methods, such as hyperedge-based higher-order prediction algorithms as well as heuristic, random walk and deep learning-based methods. For the task of predicting d+1-simplices from existing d-simplices, the method shows superior performance in most of the settings. <sep> Weaknesses: <sep> Section 4, which is a critical part of the paper as it presents the theoretical guarantees with respect to the proposed estimator, is rather complicated and lacks explanations of theorems, definitions and assumptions. In my opinion, this section requires further elaborations as to give some intuition. For instance, what the implications of consistency of the estimator are in practice and why α-mixing property is necessary could be explained further. <sep> It is important to distinguish between the settings for which the proposed method and the existing work are designed for. None of the existing methods were designed to predict higher-order simplices from existing lower-order simplices. Hyperedge-based methods predict interaction of a group of nodes, while Benson et. al. predict closed simplices from open simplices. Link prediction methods focus on single edge evolutions. These tasks are not aligned with each other and prior work is not designed for the experimental setup proposed in the paper. I believe the experiment does not reflect the whole potential of other methods and is biased towards the current method, but conveys the message that proposed algorithm has superior performance for the particular task in question. <sep> Using history of the evolving graph requires additional storage load, which is not discussed in details. It is argued that the proposed method has comparable/improved run-time performance, but space complexity is also very crucial, especially when graphs have growing number of edges and many vertices. In my opinion, this is an important ground of comparison which should be presented with more details. <sep> Additional Comments/Questions: <sep> Why would you give higher score when two vertices co-occur in a higher dimensional simplex in the past? <sep> How would you compare storage requirements of your approach to existing higher-order prediction algorithms as in Benson et. al. 2018 or hypergraph-based approaches? <sep> Definition of sub-complex includes Bt,k but the value of k is not specified. Do you pick a single, fixed k across all definition? This is not properly defined and creates a confusion. <sep> Section ""4.1 Consistency"" has a confusing presentation of definitions and concepts. Definition of SC is not clearly presented and I believe bias/variance terms for the estimator may need further intuitions and explanations. <sep> I couldn't see any insight for how information is encoded by the proposed method. Specifically, how are simplices (closed simplices) inferred from the dataset? How are open and closed simplices are distinguished from the raw data? <sep> Score: <sep> I vote for accepting the paper, but my decision is borderline. I haven't checked the proofs in details but I appreciate the analytical effort that was put into this paper. Having consistency and asymptotic normality results strengthens the claims for the proposed estimator. However, I am not totally sure in which real-world tasks this method has significant advantages and would outperform existing methods because the experimental setting is designed for the task of inferring (d+1)-simplices from d-simplices. I am open for further discussion with the authors regarding my concerns and their future comments/responses.","This paper proposes a method for predicting higher-order structure in time-varying graphs. The paper was reviewed by three expert reviewers, and while they expressed appreciation for the sensible solution, they have remaining concerns about the novel contributions and comparisons (analytical and empirical) with previous approaches. Also, the paper would be clearer if examples are used to illustrate the important points of the paper. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers."
"After reading author replies: <sep> I would like to thank the authors to respond to my doubts on some of the results. But I decide to keep the review and the score, because Theorem 1 and Claim 1 are still not well explained. In particular, the explanation like ""if the 2nd inequality in Eq. 11 is violated, the network can not capture the amount of information measured by the entanglement entropy "" still looks like a conjecture or intuition rather than a mathematical statement. <sep> Originality: High. <sep> The proposed tensor network (TN) based text classification model looks new and interesting. <sep> It consists of two parts: a word-level generative TN model, used to find concise representation of each word; and a sentence-level discriminative TN model, used to classify the sentence based on the outputs of the word-level TNs. <sep> By using the word-level TN for input representation, the dimension explosion problem may be effectively avoided. <sep> Clarity of model description: OK. <sep> Clarity of training method: Low. <sep> The training method of the overall model is not presented. Since there are two TNs, how to train the overall model may not be a trivial problem. <sep> Clarity of analysis: Low. <sep> The major problem of this paper is the analysis presented in Sec 3.2 and Sec 3.3. <sep> I understand that the authors may want to find some theoretical justification on how to choose the bond dimension in the sentence-level TN, as a function of the bond dimension of the word-level TN and the so called entanglement entropy. <sep> However, the result in Theorem 1 is incomprehensible. Only two inequalities are presented, without stating any condition or implication of the inequalities. Since both m and the bond dimension are hyper parameters, what does the 1st inequality mean? Is it a necessary condition on how to choose their values? What happens if the inequality is violated? Even the proof of this theorem does not answer these questions. Same doubts are on the 2nd inequality as well. Additionally, how does one even know what the entangle entropy of a model is before training the model? <sep> The statement of Claim 1 is even more problematic. Not able to understand what it says. <sep> Clarity of expeiremtal results: OK, but not clear enough. <sep> The authors claim that when combined with BERT for word embedding, the proposed model can outperform the SOTA methods. <sep> However, there are several things that are not clear. <sep> It would be more fair to compared the combined model with BERT with some similar models that also use BERT for word embedding. <sep> It is claimed that the proposed method is better than word-GTN. But word-GTN is a unsupervised learning model. Why is it meaningful to make such a comparison? <sep> In the introduction, another TN based method ""TSLM"" is mentioned. Would it be more fair to compare the proposed method with TSLM, as both of them use TN for modeling? <sep> Overall, the proposed model looks interesting and shows some potential improvements over SOTA. However, the quality of the paper is degraded by the unclear claims and some questionable experimental results.","While the submission has promising components, the reviewers were not able to reach a consensus to recommend acceptance. The main concerns is that (1) theorem statements and assumptions are not clearly explained, and (2) the novelty of the approach is not made clear, and (3) there remain concerns on whether the experimental results are due to hyperparameter search or improvements due to the model."
"Edit after seeing others reviews -- I think I gave this paper a MUCH higher score than the other reviewers, simply because it is very novel with Fon language. I agree with all of your points about what is lacking, but in my mind, the novelty was enough to still give a 7. Now I definitely think that is too high. I think this paper can reasonably be rejected, but I'd like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community. With such low resources, we cannot expect the same type of work as we would for other languages. <sep> Overview: This paper discusses the problems of common tokenization strategies for low resource african languages, and proposes a new tokenization method to overcome these problems. They train low resource NMTs using 4 different tokenization strategies, to show that their proposed tokenization method leads to the best NMT results by several metrics. <sep> Contribution: The authors contribute a new tokenization method, code, and a dataset. <sep> The good: Very interesting and important work! Many people will be excited to use this data. Paper is mostly clearly written, and easy to read. The paper flows well. Someone with this paper could reproduce the work, more or less. <sep> The bad: <sep> Figure 1 is difficult to read and messy. First, by ""Input"" you actually mean ""Source"". The input would be the source sentence with its appropriate tokenization, no? Also, I think putting the english translation in a different font or color would be greatly helpful to our eyes. I really think this must be fixed! Figure 1 is presently not pleasant to look at, even though it has interesting results`! <sep> Section 4 - I think you really need to re-state that the algorithm has a human-in-the-loop for clarity. Before describing your algorithm, humans are only mentioned once in the algorithm. Indeed, at first, the words ""The following algorithm"" confused me, because I thought it was more a ""methodology"", since Step 2 is where the humans are in the loop, unless you have a Fon POS tagger and I am misunderstanding? But then at the end, I saw you include Encode as step 4, so it is the machine...  The fact that I flustered a bit with my understanding here, was confused, and had to spend a few minutes thinking about it, means it needs a bit of tweaking. Maybe add a comment saying Step 2 is the human-in-the-loop step of the algorithm? <sep> Suggested additions: <sep> I think more specific linguistic details about Fon are missing. For example, if you could give us one or two sentences of Fon in the beginning of the paper, that demonstrate some of the difficulties of the language, I think this would greatly strengthen the motivation. You tell us that Fon is ""a language with special tokenization needs"" and that ""standard tokenization methods do not alwaysadequately deal with the grammatical, diacritical, and tonal properties of some African language"", and you cite the relevant papers. But I would still like to be shown. I think just including two sentences that have some of these features, and that gets the point accross of ""how would we tokenize this?"" would really help the motivation. Its not that I/readers dont believe you when we are told, but being shown makes it much more interesting and give people an appreciation for Fon tokenization challenges! <sep> Can we get any information about how the annotators were trained? I think this is standard for such papers. <sep> Other smaller suggested fixes: <sep> Section 5, near the end - Little grammatical mistake. ""... bunch of those errors has"" should be ""errors have"". <sep> Section 6.3 - Please change ""The results from Table 2 and Table 1"" to say ""Table 1 and Table 2"". It does not make sense to list them out of order. I also think it makes sense to switch Figure 1 and Figure 2 entirely. I.e., Figure 1 should be your results table, and figure 2 should be the examples for us to see. <sep> Section 6.3 - Slightly confusing wording. The second sentence is confusing to me, and I am a native English speaker. ""It is important to note that while BLEU of other methods reduced on the Fr→Fon task, WB improved on it.""  To me saying ""BLUE reduced for the other methods"" means that you have some other baseline you are comparing to. Am I missing something? Are you comparing against Fon--> Fr? <sep> Questions: <sep> Section 6.2 - Does it really take all 500 epochs to run, or do you have early stopping at some point when the loss flatlines? <sep> Because BPE is such a standard baseline, why do you not include it as a baseline? I know you cite the Abbott & Martinus, 2018 paper, stating that BPE is bad for analytical languages, but I still think it would prove a point to show BPE performing badly for your data. <sep> Overall: Very interesting work, and can't wait to see this data be used :-) I think the paper could be greatly strengthened by taking some time to include an example that demonstrates the linguistic and typological features of Fon that makes it difficult.","The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). Low-resource machine translation is a very important topic and it is great to see work on African languages - we need more of this! <sep> Unfortunately, the reviewers unanimously agree that this work might be better suited for a different conference, for example LREC, since the machine learning contributions are small. The AC encourages the authors to consider submitting this work to LREC or a similar conference."
"The paper describes a simple extension to the location-only monotonic GMM attention mechanism from Graves (2013), which takes the source/key context into account when computing attention weights.  The proposed method improves ASR performance over model using the baseline GMM attention which does not take source-content into account,  generalizes better to input sequences much longer than those seen during training, while also obtaining competitive performance to other streaming seq2seq ASR models on ""matched"" test sets. <sep> Pros: <sep> Incorporating source content is an obvious and useful extension to monotonic GMM attention, combining the strengths of content-based approaches such as additive or dot-product attention. <sep> It improves performance and generalization while being simpler than existing techniques in the literature (e.g. Mocha, CTC/transducer models which have more complex loss functions). <sep> Cons: <sep> The description of the proposed mechanism is inconsistent with existing literature and is very unclear and confusing in parts. <sep> Experiments are somewhat incomplete/missing important comparisons, e.g. comparing baseline GMM attention to the proposed ""source-aware"" variant in Tables 1,2,5, and comparing to other location-based attention mechanisms, even if non-monotinic, e.g. from http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf <sep> Overall the writing/language use could use improvement. <sep> Detailed comments <sep> At the high level, the idea of incorporating the source keys K into GMM attention is a good one.  The proposed method seems to work, and be  simpler to implement than alternative monotonic alignment mechanisms used in seq2seq ASR models.  However, given the current state of the text, with many confusing details, I feel that the paper is not yet ready for publication without significant revisions. <sep> Many details in the paper, especially Section 2, are unclear: <sep> Sec 2.2. and throughout the paper:  The described ""GMM"" and ""SA-GMM"" attention always use a single component, so don't really count as a Gaussian mixture.  Using multiple components would explicitly allow for multimodal attention weights for each output step.  Moreover, since the mixing weights are generally computed independently at each step, using multiple components makes it possible for the base GMM attention to ""discard the non-informative tokens"".   This mechanism, which would be more precisely called ""Gaussian attention"", is strictly less flexible than the base GMM attention mechanism that was originally described in Graves, 2013. <sep> This claim is repeated in paragraph 2 of Sec 3: ""uni-modal similar to conventional GMM attention"".  When using multiple mixture components, GMM attention is not unimodal. <sep> Eq 8: The notation here is unclear.  Why is there a softmax over ψih (a scalar AFAICT)?  Is the softmax computed over all attention heads?   Why is this necessary?  It seems to impair the training of some heads, at least for SAGMM-tr according to paragraph 4 of Sec. 4.2 <sep> Figure 1 is difficult to interpret.  The two plots have difference horizontal axes and therefore don't seem to be directly comparable...  It's not clear what the ""key width"" in Figure 1b is trying to convey since there is always going to be a single weight per (discrete) encoder step j. <sep> Sec 2.3.  There is no particular motivation given for the proposed method for integrating source keys.  Why not include Kj in the computation for the standard deviation Σi as well?  And why is the same weight δj used as a scaling factor (eq (12)) and the mean offset in eq (10)?  These design choices deserve more explanation, and possibly empirical justification. <sep> Sec 2.4: Is it possible to train SAGMM-tr from scratch?  Or does it need to be first trained using SAGMM and then fine-tuning with truncation enabled? <sep> Experiments: <sep> Are the different GMM attention variants used encoder self-attention layers as well?  Or does the encoder use conventional ""soft attention""? <sep> As above, it seems unfair not to include any experiments using multiple components when comparing different variants of GMM attention. <sep> Sec 4.2, Table 1:  Please clarify the differences between the three models labeled (Ours).  Is the difference only in the encoder-decoder attention layer? <sep> English usage.  Just a few examples of grammar errors and unclear text, as there are too many to list. <sep> page 1, ""attend subset of long sequences"" is missing a preposition, e.g., ""attend to a subset"".  It seems that ""long sequences"" is meant  to refer a single source sequence. <sep> page 1, ""mismatch between attention parameters from decoders and information distribution in encoder outputs"".  This sentence is difficult to parse.  What is the mismatch here?   Why would the source encoding and decoder query vector need to ""match"", especially in a purely location-based attention scheme? <sep> page 4: ""fixed length with hyperparameters""  What are they hyperparameters being referred to here? <sep> page 5, Sec. 4: ""enables early inference"".  What does ""early inference"" mean? <sep> page 5, Sec. 3: ""for the inference"" -> ""for inference"" <sep> page 5, Sec 4.1:  ""1 second speeches"" -> ""1 second long utterances"" <sep> page 5, Sec 4.1: ""from 30 vocabulary"" -> ""from a vocabulary of 30 words"" <sep> Other comments: <sep> Sec 2.2: Sutskever et al., 2014 did not use content-based attention.","The paper proposed a useful incremental extension to the monotonic GMM attention by incorporating source content. <sep> It has shown comparable performance for online and long-form speech recognition, but falls behind on the machine translation task. For online ASR, it would be more convincing to include latency comparisons across different streaming models besides WERs. <sep> The presentation of the paper can be further improved although it already got better based on reviewers' comments. <sep> As in the discussion, a more accurate description of the method would be ""multi-head Gaussian attention"" instead of GMM attention. <sep> The main factor for the decision is limited novelty and clarity can be further improved."
"Center-wise Local Image Mixture For Contrastive Representation Learning <sep> The paper introduces a new contrastive learning method for unsupervised representation learning. The main idea is to consider the semantic similarity between different images and incorporate it in the learning procedure, in contrast to the many contrastive learning methods which only used augmentations of the query image as positives. <sep> The main contribution is 2-fold: a) use nearest neighbors from the same cluster which are closer to the centroid than the anchor as positive samples; b) use more complex augmentations, i.e. [CutMix] and multi-resolution during training. The proposed method achieves state-of-the-art results for unsupervised learning on Imagenet and transfer learning tasks on Pascal VOC, COCO, and LVIS. <sep> Pros <sep> The paper is written well and easy to understand. <sep> The method is simple and yet powerful, achieving state-of-the-art results on several standard benchmarks. <sep> The method is easy to implement and reproduce. <sep> The paper shows the importance of better modeling of intra-class variance by means of sampling positives among the nearest neighbors. <sep> I appreciate quite detailed ablation studies (but not all of them). <sep> Cons <sep> The main ideas presented in the paper are not entirely new. <sep> The idea of using highly related images in the learned so far representation space as positives (vs single exemplar + its augmentations or just naive nearest neighbors) for unsupervised representation learning was already explored in [CliqueCNN]. <sep> Since the proposed positive sampling method is the cornerstone contribution of this paper, it would be nice to see how it compares with the sampling method proposed in [CliqueCNN], where only the nearest samples which form a clique (mutually very similar samples) are used as positives. <sep> [CutMix] augmentation is a previously published work. And the contribution of this paper is applying CutMix in the context of contrastive learning, which is yet another augmentation among a huge variety of possibilities. E.g., CutOut, MixUp, Attentive CutMix, etc. The paper gives no justification for why CutMix is especially better in this context. <sep> Similar Multi-resolution augmentation and the importance of using different image resolutions during raining and testing were explored in [FixRes]. <sep> p.4 ""Cluster-based method regards all samples that belong to the same center as positive pairs, which breaks the local similarity among samples especially when the anchor is around the boundary."" It is not very clear how the proposed method differs from the clustering-based methods in this sense. It seems like the proposed sampling method can also break local similarity around the cluster boundary because the anchor will be attracted to the cluster center increasing the distance to the samples from other clusters, which will result in very pronounced hubs in the representation space (as in Fig. 3). <sep> It is not clear from the experiments what is the main performance booster compared to the KNN baseline in Tab. 6. Is it (A) the sampling of positives from the top 40 nearest neighbors within the cluster (as explained in A.2) or (B) discarding the positive which are further from the cluster centroid than the anchor?  The experiment in Sec. 4.4 do not answer this question, because the K-means baseline in Tab.6 randomly selects positive samples from the entire cluster and not from the top 40 closest samples within the cluster. To prove that (B) is crucial one would need to make an extra ablation study (A), where the positives are sampled from the top 40 nearest neighbors within the cluster (w/o discarding those which are further from the centroid than the anchor). <sep> Minor. Would be nice to see and extra ablation experiment on the full training schedule (1200 epochs) where all paper contributions are enabled one by one, e.g.: (i)  baseline, (ii) baseline + proposed positive sampling, (iii) baseline + proposed positive sampling + CutMix, (iv) baseline + proposed positive sampling + CutMix + multi-res. <sep> I understand that it is computationally demanding, however, it would provide a better picture of the performance contribution of the final components (after tuning them using a shorter training schedule with 200 epochs), since improvements brought by some components on the shorter training schedule can become less significant when the network is trained longer. <sep> After rebuttal <sep> After reading the authors comments' and other reviews, I think that this is a borderline paper that could benefit from more rigorous experimental validation. <sep> [CutMix] CutMix: Regularization Strategy to Train Strong Classifierswith Localizable Features, ICCV 2019.[CliqueCNN] CliqueCnn: Deep unsupervised exemplar learning, Bautista et al., NeurIPS 2016.[CutOut] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout, 2017.[MixUp] Mixup: Beyond empirical risk minimization, Zhang et al., 2017.[Attentive CutMix]  Attentive CutMix: An Enhanced Data AugmentationApproach for Deep Learning Based ImageClassification, Walawalkar et al., 2020.[FixRes] Fixing the train-test resolution discrepancy, Touvron et al., NeurIPS 2019.","There are two main contributions in this paper. First, the use of NN from the same cluster as ""views"" of the data as understood in classical contrastive learning. Second, the use of additional augmentation techniques, namely cutMix and multi-resolution. The reviewers noted that the paper is written well and easy to understand, that the ablation study is conducted well and that the model shows good empirical performance on several tasks. <sep> At the same time, the somewhat limited novelty of the paper was also discussed. As noted by R4, all aspects of the present paper have been discussed in previous work. The difference with previously published clustering-based SSL methods was also not very clear. This was discussed in the rebuttal but without strong evidence supporting the claims. Moreover, the ablation study is conducted on models that are trained for 200 epochs. While this is understandable from a pragmatic point of view, the conclusions may be completely different when the model is fully optimised. <sep> Because of all the points raised in the discussions, this paper is a too close to borderline to be accepted. We recommend the authors improve the manuscript given the feedback provided in the reviews and discussion and resubmit to another venue."
"A brief summary of the paper. <sep> This paper proposed a novel architecture termed VEM-GCN to address the over-smoothing problem of GNNs in the node classification task. The main idea is to optimize the graph topology by removing the inter-class edges as well as adding the intra-class edges, and then the noise information would not pass between nodes with different categories. The framework learns with two alternating steps: E-step optimizes the topology while M-step improves the performance of GCN. The experimental results show that the proposed VEM-GCN achieves higher classification accuracy than baseline methods. <sep> Main contributions of the paper. <sep> This paper proposes a joint learning framework for GNN classification model and graph topology, which leverages variational EM as a learning framework. <sep> This paper presents a graph topology learning algorithm based on SBM and neural networks, which employs the node embedding and labels to optimize the topology. <sep> Extensive experiments to demonstrate the performance superior of the proposed method. <sep> Strengths <sep> The strengths of this paper are summarized below, <sep> The proposed method is clearly introduced. Concretely, the theoretical background and the algorithm details are both well defined and written in a clear way. <sep> The paper is well-organised and well-written. <sep> Extensive experiments are conducted and results analysis are given. The visualization result is intuitive to demonstrate the property of the proposed method. <sep> Weaknesses <sep> The weaknesses of this paper are summarized below, <sep> The main hesitation with this paper is the novelty of the proposed method. Actually, optimizing the graph topology is a hot research direction, and a variety of works are presented about this topic recently. In addition to the AdaEdge, LDS and TO-GCN mentioned in the paper, other works, e.g., ""Graph-Revised Convolutional Network"" (ECML-PKDD 2020, arxiv: 1911.07123), ""Deep Iterative and Adaptive Learning for Graph Neural Networks"" (arxiv: 1912.07832), and ""Graph Structure Learning for Robust Graph Neural Networks"" (arxiv: 2005.10203), also study the same problem. Among them, DIAL-GNN also leverages an iterative and alternating framework to learn GNN and topology, which is similar to this paper. The differences between LDS/AdaEdge and this work are also minor. GRCN and TO-GCN optimize topology and GNN simultaneously in an end-to-end way, which seems more efficient than this work. In summary, the novelty of the presented VEM-GNN is a little bit minor. <sep> A minor concern is about the motivation of this paper. The authors claim that ""over-smoothing is caused by ""indistinguishable features of nodes in different classes produced by the message passing along inter-class edges"", so ""adding intra-class edges and removing inter-class edges are helpful to suppress over-smoothing"". However, I have a different understanding about the over-smoothing problem. A more common definition (by Li et al., 2018) about over-smoothing is ""if a GCN is deep with many convolutional layers, the output features may be oversmoothed and vertices from different clusters may become indistinguishable."" In my opinion, the over-smoothing is caused by the depth (or receptive field) of GNN and the message passing manner, but irrelevant to the graph topology. Assuming that we remove all the inter-class edges and connect all the intra-class edges in the graph. In such a situation, when the GNN goes deep, the output embeddings of each node with the same class still become indistinguishable (the embeddings of nodes in the same class will converge to an identical embedding). Maybe we can obtain a perfect classification model by this way, but the embeddings are still failed to represent the property of each node, and they are useless to be applied to other tasks (e.g., anomaly detection). In summary, I agree that topology optimization is beneficial to enhance the node classification performance, but its effect on surpassing over-smoothing is suspicious. <sep> The impact of each module/design in the proposed framework is not clearly stated. Specifically, a probability matrix q¯ϕ is acquired by the learned adjacency matrix <qϕ>, and then the adjacency matrix for GCN is sampled by <q¯ϕ>. The question is: why don't we define p=1 directly to obtain a definite adjacency matrix? Such a design can be viewed as a ""DropEdge"", so is that the main contribution term for restraining over-smoothing? Authors should add more ablation study to demonstrate the impact of ""learned topology"" and ""probability matrix"" respectively. <sep> Correctness and Clarity <sep> The claims and method are well written without significant errors. The paper is well-organised and written in a logical way. <sep> Additional Comments <sep> Here are some additional comments for the authors, <sep> More baselines considering topology optimization should be included, such as GRCN, DIAL-GNN, Pro-GNN (the papers of these methods are mentioned in Weaknesses Section). <sep> It is better to demonstrate how much edges are added/removed since sparsity is an important factor affecting the efficiency of GCN.","The authors propose a new approach to topology optimization to address over-smoothing in GCNs. This is a borderline paper. Topology optimization is clearly important and relevant and the approach tries to optimize the topology (add/delete edges) by viewing the problem as a latent variable model and aiming to optimize the graph together with the GCN parameters to maximize the likelihood of observed node labels. A number of related joint topology optimization approaches exist, however, as discussed in the reviews and the responses. The proposed methodology is termed variational EM but is a bit heuristic in the sense that E and M steps do not follow a consistent criterion (the direction of KL is flipped between the steps). A number of comparisons are provided with consistent gains though the gains appear relatively small. No error bars are provided despite request to add them to better assess the significance of these results. It remains unclear whether the gains are worth the added complexity."
"This paper proposes to impose a particular sparsity structure (butterfly network) to replace dense connected layers in deep neural networks. It is motivated by the theoretical results involving the Fast Johnson Lindenstrauss Transform (FJLT). The work is well motivated and experimental results validate the theoretical findings. However, it is not clear the advantage for the case of image datasets as CIFAR10 and CIFAR100. I have the following comments and questions that should be clarified to evaluate the relevance of the results: <sep> When comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure? Most very well-known architectures are based on large concatenation of convolutional layers with a dense layer at the end. Is this last layer the one that is replaced? What is the compression attained by this replacement? <sep> In this architecture, the sparsity pattern is fixed, and it seems to work very well for the dataset used in the paper. It would be interesting to provide some insights on why this particularly structure works well on natural data. Is there any property on datasets that makes butterfly pattern optimal? <sep> For natural images, it is well known that sparsity structure imposed by convolutional layers is optimal because of local structure of natural images. I think a comparison replacing a convolutional layer by a butterfly layer could bring some useful insights. <sep> Theorem 1: What is Omega in the exponent? I couldn't find its definition. <sep> Figure 1(a): The small difference between normal and BF models seems not to be statistically significant. It would be good to show some variability of results (error bars). <sep> For the results of Figure 1(b), it would be also useful to report the training time reduction and model compression attained by the normal and BF models. <sep> Figure 1(b): The starting point for the four methods should be the same. Please include in the plot the Test Accuracy at the beginning (epoch 0). <sep> Figure 1(b): Please extend the plots beyond epoch 20 to see how the values are stabilized for the four methods. <sep> Quality of Figures, for example Fig 3, must be improved (too small fonts, blurred, etc.)","This paper shows that linear layers can be replaced by butterfly networks. Put simple, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters and also gives the theoretical and empirical analysis to validate this claim. In this regard, the paper would be appealing. But the theoretical results given in this paper are incremental."
"In this work, the authors introduce a method called LIME for imparting certain mathematical inductive biases into a model. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction. Each of these pretraining tasks is a sequence-to-sequence mapping involving 3 basic components of reasoning (Rule, Case, and Result), where two of these three components are provided as input and the third component is the target output. After pretraining on these tasks, the authors fine-tune on 3 different proof datasets, and find that the pretraining almost always improves performance, sometimes by a large margin. <sep> Strengths: <sep> This approach is creative and thought-provoking; pretraining is an important topic in ML nowadays, and this paper gives several interesting insights about how to structure pretraining. Therefore, publishing this paper at *CONF* could help inspire others to use and develop improved variations of pretraining. <sep> One aspect of the pretraining that I found particularly impressive was how the authors found such clear improvements from such small amounts of pretraining. This is in stark contrast to the usually massive pretraining datasets that are used, and stands as an especially strong piece of evidence for the model's usefulness. <sep> The experimental setup is well-motivated, drawing on a principled analysis of the problem domain. <sep> The paper is overall clearly written and clearly structured. <sep> There were some interesting discussion points and ablation studies analyzing the approach in more detail. I particularly liked the discussion about how loading the vocabulary weights had little effect, showing that the inductive biases that were imparted were abstract in nature. It was also useful to see that LIME was more useful than other pretraining tasks, ruling out the possibility that you could get similar improvements from just any pretraining task. <sep> Weaknesses: <sep> Part of the paper's motivation for imparting inductive bias through a dataset, rather than through an architecture, is that designing an architecture ""strongly requires human insight."" This is true, but LIME also seems to strongly rely on human insight, so this point is not a benefit for LIME over architectural approaches. This is not a huge problem, but it does not seem like a great motivation for LIME. <sep> Related to the previous point, it would be good to discuss the fact that the usefulness of LIME may be limited by the need to design the right pretraining task(s). As Table 4 shows, the nature of the pretraining task is very important; and although the authors were able to create some successful pretraining tasks for mathematical reasoning, it might be harder to create similarly useful tasks for larger-scale tasks in, e.g., language or vision. Again, this is not a huge problem, but I think it at least deserves some discussion. <sep> Though the goal of the approach (if I am understanding correctly) is to give inductive biases for induction, deduction, and abduction, the paper gives no direct evidence that it has done so: The authors create an approach intended to impart certain inductive biases, and this approach improves performance on 3 tasks that plausibly would benefit from those biases. But this result does not necessarily mean that the model has the inductive biases that were intended to be imparted; it's possible that LIME imparted some other inductive biases that are also useful for mathematical reasoning but that are not related to induction, deduction, and abduction. Thus, there is a bit of a gap between the motivation and the actual experiments. <sep> It's not entirely clear to me that the specific tasks (Deduct, Induct, Abduct) will necessarily enforce the types of reasoning that they are intended to enforce. For instance, consider the following input/output example: {A : a, B: b, C: d+e} <s> A A + B = C -> a a + b = d + e. Such an example is intended to show deduction, but it could instead be viewed as induction (where A A + B = C is the Result, a a + b = d + e is the Rule, and the Case dictionary should be read in reverse, treating the values as keys and the keys as values). Thus, related to the previous point, I think there is some concern that the LIME tasks may not necessarily encode the intended primitives. The results show that the LIME tasks clearly encode something useful, but it's not clear exactly what useful things they encode. <sep> Recommended citations: (you definitely don't need to include all of these or even any of these, but I'm pointing to them just in case they're useful): <sep> You already cite the GPT-3 paper (Brown et al.), But it might make sense to cite it in a second place as well, for the sentence where you say ""However, there is another potential advantage of pre-training--it may distill inductive biases into the model that are helpful for training on downstream tasks."" Another paper you can cite for this point is this one: Can neural networks acquire a structural bias from raw linguistic data? https://arxiv.org/pdf/2007.06761.pdf <sep> Like your approach, the following paper also uses carefully-constructed synthetic datasets as a way to impart targeted inductive biases into a model. (However, they use these tasks for meta-training, not pre-training): Universal LInguistic Inductive Biases via Meta-Learning. https://arxiv.org/pdf/2006.16324.pdf. This paper might also be useful as an example of how you can address the last two points I listed under weaknesses, as this paper gives examples of how to test whether a model has some specific inductive biases; the paper I linked to in the previous bullet (Warstadt and Bowman) also does this. (However, adding such analyses might be more work than would be doable for a camera-ready). <sep> It might be good to cite Peirce when first mentioned in the intro; right now, the citation to Peirce is buried deep in the paper, after he has already been discussed at length. <sep> Some more potentially-relevant examples of architecturally encoding inductive biases for math: https://arxiv.org/pdf/1910.02339.pdf, https://arxiv.org/pdf/1910.06611.pdf <sep> Other comments (these are not things that have affected my assessment. Instead, they are just comments that I think might be helpful in revising): <sep> Note that there is another approach in ML called LIME, which could potentially cause confusion. It's completely up to you, but I would consider renaming to avoid confusion. Here is the other LIME by Ribeiro, Singh, and Guestrin: https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?casa_token=VrGSeKoqOnkAAAAA:tmzXq2uCWkUVyPdd9ytCNK4LSdRfIwsIeX4hd8EMkjnjevZ4d-rCeIIM7acIRWGtQlQemUqDlAJx-Q <sep> Abstract: ""neural architecture"" should be ""neural architectures"" <sep> Abstract: ""on three large very different mathematical reasoning benchmarks"" should be ""on three very different large mathematical reasoning benchmarks"" <sep> Abstract: I did not understand what ""dominating the computation"" meant until I read the rest of the paper. <sep> The intro says ""It is commonly believed that the benefit of pre-training is that the model can learn world knowledge by memorizing the contents of the natural language corpus."" This statement seems strong - I am more inclined to think that much of the benefit comes from learning linguistic structure, not world knowledge. So it might be safer to reword as saying ""One plausible explanation for the benefit of pretraining is…"" <sep> Page 3 says ""the BERT pretraining objective,"" which suggests that BERT is the objective. But BERT is a model, not an objective; the objectives are masked language modeling and next-sentence prediction. <sep> Table 1: The formatting of the table makes it look like the first two rows are numbers copied from Li et al. But from the prose of your paper, and from looking at Li et al, I'm pretty sure that these numbers are from your own re-implementation. Is that correct? If so, it might be best to format the table different - using the citation within the body of the table gives a strong suggestion that the numbers come from Li et al., in my opinion. <sep> Table 4 and Table 5: In the caption, say what task these results are for, so that the table can be understood on its own. <sep> Please double check the references: Several of them seem to only list authors, title, and year when there is at least an arXiv version that could be listed as well. E.g., ""Mathematical reasoning via self-supervised skip-tree training"", ""Enhancing sat solvers with glue variable predictions"", ""transformers generalize to the semantics of logics"". Also, where possible, cite a paper's actual publication venue instead of arXiv - e.g., the Raffel et al. T5 paper appeared in JMLR, not just arXiv. <sep> Summary: Overall, I am rating this an 8 because I find the strengths compelling but think that the weaknesses in framing hold the paper back from an even higher score. I would consider increasing the score if those weaknesses were addressed, though those weaknesses are deep enough that it would be hard to properly address them in time.","The authors propose a pretraining strategy learning inductive biases in transformers for deduction, induction, and abduction. Further, the claims and results seem to indicate that such pretraining is more successful in transformers which provide a more malleable architecture for learning inductive (structural) biases. There are open questions that remain, specifically surrounding disentangling high performance from structural bias learning (i.e. is pretraining doing what we think it is) and whether datasets are the ""correct"" mechanism for imparting such biases/knowledge."
"Summary of paper <sep> This paper draws links between three common regularisation methods for continual learning: EWC, MAS, and SI. It shows that MAS and SI approximate the Absolute Fisher matrix. The authors provide many experiments to test their claims and assumptions. Finally, the authors also propose a cheaper way to run EWC. <sep> Review summary <sep> I really like the majority of this paper. Unifying these regularisation methods is great, and not obvious (particularly in the case of SI). The accompanying experiments are crucial and well-conducted. The paper is also written well, with an emphasis on good research practices. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC (""Batch-EF""), as I detail later. If it were not for this, this paper would be a clear accept for me. I hope to resolve this issue with the authors during the discussion period, depending on which I can raise (or lower) my score. <sep> Pros of paper (mostly already written in the ""Review summary"") <sep> The paper is written well, with good detail and very good experiments. <sep> The work is of significance for continual learning, with interesting conclusions. <sep> Cons of paper <sep> I am not convinced that the minibatching that the authors suggest (both for SI as an approximation to the AF and for EWC in the last paragraph of Section 5) is correct (""Batch-EF""). It appears to me that by minibatching instead of squaring each gradient element, one should obtain much worse approximations to the empirical Fisher information matrix (""EF""). <sep> Intuition: By calculating the gradient over minibatches and then squaring, one is reducing the noise that is being squared. Intuitively, this must affect the EF calculation in a bad way. For example, consider a full-batch calculation. In this case, Batch-EF will just be g2. At the end of training, when we have converged to a low loss, this will be very small. In Appendix D.2, the authors argue that when gradient noise >> gradient, then Batch-EF ≈ EF, and derive that Batch-EF has larger values than EF. However, I expect Batch-EF to have smaller values than EF because of the reduced noise on average. <sep> Additionally, I have myself experimented in the past with Batch-EF. I did not find that Batch-EF gave the same results as EF for EWC on similar benchmarks. I do not know why, in this paper, the authors found that the two gave same results (Table 1); perhaps it only works for specific hyperparameters. <sep> Finally, a small note that may be of interest to the authors: the HAT codebase (github.com/joansj/hat) implements EWC as a baseline, however, my collaborators and I found that they implement EWC differently/incorrectly. One of the ways they are different is to do Batch-EF (along with other differences). <sep> I am also not convinced that OnAF (""Online Absolute Fisher"") and AF are / should be the same. After training, individual gradients should be relatively small (as we have converged to a solution), meaning that I would expect the AF to have small values in general. However, during training (especially near the beginning), gradients can be large, meaning that OnAF can end up having large values. Empirically, the Pearson correlations in Figure 2 (mid) show differences between the OnAF and AF versions. <sep> Additional suggestions to authors <sep> The authors could consider adding a reference for the Absolute Fisher (Section 4.1). Can they say anything about the links between the Absolute Fisher and the empirical Fisher? <sep> Typo Section 4.2 second para: ""Max-likeilhood"" <sep> I felt that Section 5 got complicated, with many algorithms that need to be compared. I strongly recommend splitting the experiments part (Section 5.3) into experiments relevant for the two preceding sections (5.1 and 5.2) to reduce the complexity of writing. <sep> Update to review <sep> I am increasing my score from 5 to 6. I believe this paper is a good paper. However, an extremely extensive discussion with other reviewers has left some questions / concerns. Although I disagree with some of these, I agree with others: <sep> Some claims are overstated in the paper. The authors already changed these claims somewhat in the updated paper. Some reviewers are arguing for further changes. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC). <sep> One of the biggest reason I find this paper is interesting is not mentioned (enough) by the authors. In my opinion, this is a big reason why the work is significant, and if I were writing the paper, I would put it as one of the biggest motivations: <sep> There have been works recently looking at the Generalised Gauss-Newton approximation (= EF for classification), and trying to view optimisation algorithms as approximating the Hessian matrix. For example, see Khan et al., 2018 (""vAdam""), Kessler et al., 2020 (""BAdam""), Zhang et al., 2018 (""Noisy Adam""), Osawa et al., 2019 (""VOGN""). Such works provide evidence that different approximations of the EF can work well. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. Should we try and approximate the Fisher matrix in more ways in CL? <sep> Finally, it is my personal opinion (although others disagree) that the current paper is significant enough / provides enough insight already to be a good paper. However, performing a further state-of-the-art experiment or similar would undoubtedly improve the quality significantly. <sep> I very much look forward to an updated version of this paper.","The paper proposes a unification of three popular baseline regularizers in continual learning. The unification is realized through a claim that they all regularize (surprisingly) related objectives. <sep> The key strengths of the paper highlighted by the reviewers were: <sep> The established connection is valuable and interesting, even if weaker than suggested originally <sep> Good motivation (unifying different regularization methods is useful for the community) <sep> Clear writing <sep> The key weakness of the paper is a weak empirical validation of the claim that these three regularizers work because they regularize the norm of the gradient (as mentioned in the discussion by R3). Rather, the key claims are correlational. The authors correctly say that (1) the three regularizers all regularize related objects (namely different norms of the gradient) and (2) they reduce forgetting. However, it is not sufficiently well demonstrated that (1) => (2). Relatedly, given that the paper does not have a very clear theoretical contribution, it would be really helpful to demonstrate utility. It would be useful to extend experiments that apply these insights to developing novel regularizers or improving/simplifying hyperparameter tuning. <sep> Additionally, in the review process, the link was discovered to be weaker than originally suggested. The paper casts the relation in terms of the Fisher Information Matrix, which suggests it is theoretical and sound. After the discussion, it seems that viewing this relationship in terms of the Fisher Information Matrix is somewhat misleading. The three different regularization methods all regularize different norms of the gradient (L1 or L2), which are empirically, and under some assumption theoretically, related. More precisely, EWC regularizes the trace of the Empirical Fisher, which is equivalent to the L2 norm of the gradient of the loss function. SI regularizes a term similar to the L1 norm of the gradient. These effects were seen by the reviewers to be somewhat loosely related to the Fisher Information Matrix. <sep> Based on the above, I have to recommend rejection. I would like to thank the Authors for submitting the work for consideration to *CONF*. I hope the feedback will be useful for improving the work."
"In this paper, two unsupervised agents are utilized at cross-model by using the dual nature of the unsupervised machine translation model, in which forward translation of agent_1 is combined with the backward translation of agent_2, more synthetic translation pairs are obtained to train a new supervised machine translation model. The result is improved on multiple unsupervised machine translation, and this paper claims that more diversity is brought to the synthetic data, so a better translation model can be trained. This paper uses a reconstruction BLEU or BT BLEU [1] metric to compare the effect of the inside-model with that of cross-model, and finds that cross model translation has a lower back-translation effect, which shows that the diversity is enhanced. Furthermore, CBD is compared with the ensemble method and achieves better performance. The proposed method is quite simple yet effective, but it is also a kind of data enhancement. <sep> In addition to these contributions, the paper also has some shortcomings <sep> The evidence in this paper can not support the claim that the current performance bottleneck of UMT is due to the lack of diversity: the performance upper limit of UMT is still due to the lack of clear supervision signal, which limits the further performance growth. Because the training of CBD is divided into two stages, the diversity of the second stage only brings more training data to enhance the supervised machine translation model, rather than unsupervised machine translation effect. <sep> Source of promotion: the second stage of CBD method adopts (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) synthetic translation pairs, it is not clear how much performance growth comes from increased data and how much growth comes from the new model implementation (ott et al., 2018). It is not appropriate to attribute all contributions to the diversity brought by CBD. I suggest that the author should use (y_s, x_t) data to train based on the (ott et al., 2018) model, and report the effect comparison (In my experiments, the second stage model implemented with fairseq trained only on (y_s, x_t) surpass both agents trained with XLM due to more efficient implementation in fairseq). <sep> Unfair comparison with the enable distillation: authors need to compare CBD with the model trained on the synthetic data (y_s, x_t) of the ensemble of agent_1 and agent_ 2. In the training data (x_s, y_t), (z_s, y_t), (y_s, x_t), (y_s, z_t) for the second stage of CDB,  x_t, golden language sequences as translation target is stronger than synthetic language sequences (silver) as target. Therefore, It is necessary to report the real result of ensembled distillation. The current results are very unreliable. In addition, it is necessary to compare the training time of the CBD method and ensemble distillation training (including the decoding process after the first stage of training) to show the efficiency of CBD. <sep> The non-golden language sequence as a translation target is called pseudo-NMT (PNMT). The author adopts a variety of model structures, which is slightly redundant. They can directly add the synthetic data decoded by cross model to continually train the original XLM model with a supervised translation objective (which is naturally supported in XLM from my experience), and report the effect comparison between them. <sep> The essence of the CDB approach is a process of self-supervised training, so it is necessary to compare self-training/tri-training introduced in [2]. <sep> In general, the CBD method in this paper is a simple and effective data enhancement method to improve the performance of the model. However, due to the lack of many important details of the implementation, despite the promotion, the source of promotion is unknown. In addition, the unreasonable comparison of the baseline models deepens my concern about the real promotion of this CBD method. <sep> [1] Li, Zuchao, et al. ""Reference Language based Unsupervised Neural Machine Translation."" arXiv preprint arXiv:2004.02127 (2020). <sep> [2] Sun, Haipeng, et al. ""Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios."" arXiv preprint arXiv:2004.04507 (2020).","This paper proposed an additional training objective for unsupervised neural machine translation (UNMT). They first train two UNMT models and use these models to generate pseudo parallel corpora. These parallel corpora are used to optimize the UNMT training objective. The experiments are conducted on several language pairs and they also compared with several alternative works. <sep> All the reviewers admit that the proposed method is straightforward and effective. The authors claim that the new training objective is used to enhance the ""data diversification"". This point has been questioned by the reviewers. Some reviewers are convinced by the response and some still have different opinions. From my point of view, the proposed method can also be considered as a kind of combination of (pseudo) supervised NMT and unsupervised NMT. <sep> The presentation and description of its key contributions seem unclear. However, we encourage the authors to modify their paper and we believe this proposed method can inspire the MT community for further research. At the moment, the paper is seen as not yet ready for publication at this time."
"Post-response update: I thank the authors for their response. I updated my score, but still think the paper needs improvement to be of interest to the *CONF* community. <sep> The submission analyzes the behavior of gradient descent and adaptive variants for scale-invariant models, including batch-normalization, by looking at the trajectory of the iterates when projected on the unit sphere. It contributes a formula for the equivalent learning rate if the gradient step was to be taken on the unit sphere for SGD and Adam and shows an approximate equivalence between gradient steps and normalized gradient steps taken on the unit sphere. <sep> The spherical perspective is an interesting and insufficiently explored aspect of modern machine learning models. As the magnitude of the weights is essentially an irrelevant free parameter, understanding its behavior is likely to yield insights on how we train those overparametrized models. <sep> The submission, however, does not provide a strong motivation for the presented results. The stated contribution that ""in the presence of BN layers, standard SGD behaves like Adam without momentum"" is also slightly over-selling the results of section 3. The main issue is that the significance of the explicit learning rates derived in section 2 or the equivalence between gradient descent and a normalized variant in section 3 is unclear. The manuscript does not provide sufficient context to understand what the derived formulas change about our theoretical understanding of those methods or what they imply for applications. <sep> As it stands, I am worried that the submission would have little impact. I believe the manuscript would benefit from major revisions to be of interest to the community and my initial recommendation is a rejection. <sep> The major issues that need to be adressed are: <sep> The technical definitions of the terms introduced in the work, like ""equivalence of order 2"", and the significance of the results, need to be stated in the main text. That two methods produce similar updates in terms of the overall model is interesting. But the significance of the result depends on what information it gives for the study or the application of the method, and this is currently unclear from the manuscript. For example; <sep> why does the memory vk in the AdamG* update increase over time (with β>1) if ηλ>2? Or is it a setting that is not supported by the assumptions in Thm. 2? <sep> What is the effect of L2 regularization if the complexity of the model is independent of the magnitude of the weight vector? <sep> Why is β=1 if there is no L2 regularization but β<1 if ηλ<2? <sep> Does it tell us anything about the difference between an Adagrad-style complete sum and Adam-style exponential moving average? <sep> The claimed contribution that SGD behaves like Adam without momentum is over-selling the results of section 3. Those results show that a gradient step is approximately a normalized gradient step projected on the unit ball. While this version might have similarities with Adam, this is not what the stated contribution would imply to most readers. <sep> The message of the paper is obscured by excessive formality. It might of course useful for a subset of readers to state the result in terms of topological equivalence to quotient manifolds, isomorphisms or to relate the results to the canonical metric on the sphere. But it makes the paper harder than necessary for the average reader, especially when technical terms, like the canonical metric, do not appear after their introduction. I strongly advise the authors to keep the main message of the paper accessible.","Three reviewers recommend rejecting or weak reject. The studied problem is interesting, but as one reviewer pointed out, it is not that clear how this work changes our theoretical understanding of those methods or what they imply for applications. Overall, I feel this work is on the borderline (probably it deserves higher score than the current score), but probably below the acceptance bar at the current form."
"This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. This is an important observation, which I have not seen in published work (though I believe is generally understood by many practitioners with the robustness community), and is an important contribution. <sep> The paper then proceeds to use a theoretical example (adapted from the model in Tsipras et al 2018) where adversarial training increases the accuracy difference across classes. More on this later. The paper proposes an algorithm, Fair Robust Learning (FRL), to address this issue. The starting point is a standard Lagrangian-based approach to approximately ensure constraints that the performance for each class should be close to the overall performance. The FRL algorithm then proposes 2 modifications. First, rather than applying the constraint on the robust errors directly, they decompose the robust error into clean error and a robustness term (similar to TRADES) and approximately maintain constraints on both terms. Second, for difficult classes, they propose increasing the adversarial radius, instead of/in addition to only increasing the corresponding Lagrange multiplier. The experiments demonstrate that their FRL approach improves the worst-class clean and robust errors, relative to both standard adversarial training and the baseline Lagrangian approach, without losing much in average-class errors. <sep> Overall, I believe the paper is fairly well-executed and investigates an important topic. The motivating empirical observations are an important contribution. The proposed approach is natural and well-motivated, and the experiments show improvements in the worst-class errors, as should be expected conceptually. The empirical explorations of different variations on the core idea are also valuable. <sep> There were some points in the paper which I believe could be improved. <sep> For me, the theoretical example does not add much insight regarding why this effect occurs. <sep> Regarding Theorem 1, the comment says that ""when the term A is large, the model has close clean errors between the 2 classes, namely Rnat(f,−1) Rnat(f,+1). Assuming A^2 >> q(K), the terms simplify to: <sep> Rnat(f,−1)=P[N(0,1)≤A(1−K)] <sep> Rnat(f,+1)=P[N(0,1)≤A(1−K)] <sep> which means we have a K factor difference between the two errors. In this case, it seems that even before adversarial training, we see a difference in class errors between the classes. <sep> Additionally, in Theorem 2, using a similar approximation B^2 >> q(K), we'd have <sep> Rnat(fadv,−1)=P[N(0,1)≤B(1−K)] <sep> Rnat(fadv,+1)=P[N(0,1)≤B(1−K)] <sep> where the two terms differ by a K factor again, only the leading constant is different. <sep> The claim in Equation 12 that ""the ratio Ω is large (e.g. >1)"" seems to be the result the section builds to, but it seems it is more naturally explained by the fact that if there is already a difference in clean errors, since adversarial training will increase these errors (Tsipras et al 2018), then adversarial training will also scale up the difference in these class-wise errors. <sep> It's possible that this explanation is actually what's happening, and it doesn't seem clearly inconsistent with the empirical data, since in Figure 1 (left), there are already class-wise differences, which generally match the shape of the class-wise differences after adversarial training. But it does seem very different from the story the paper is trying to tell, and it would be good to figure out which one it is (or acknowledge both). <sep> Moving on to the experiments: <sep> The TRADES baseline has robust accuracy almost 10% lower than the standard TRADES model - can you comment on this? <sep> Section 4.3 comments ""we claim that only upweighting its cost (or Reweight) could not succeed to fulfill the cost-sensitive classification goal in adversarial setting"" - could you explain why this is? (after all, reweighting is the basic approach used here too) <sep> How does the Re-Margin approach compare to the Reweight approach, when the reweighting also involves sweeping over the training ϵ? <sep> It's nice to see the code. Hopefully this can also be released along with the paper if accepted. <sep> Other notes: <sep> It would be nice to connect this to the broader fairness literature. This is unfortunately not my area of expertise, but difference in performances between classes (or demographic groups) is a very common metric, and it would be nice to relate this work more closely to work there. ""Distributionally Robust Neural Networks for Group Shifts"" (Sagawa et al 2020) is a (somewhat arbitrary) example of work which seems related, but I'm sure there are many more. It would be great to connect to this, especially papers which address why/when difference in performance across classes is expected. <sep> I give the paper a 6 overall, though could adjust my rating in either direction depending on the author feedback. <sep> EDIT: Score changed from 6 to 5 during discussion, see comments below.","This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes. <sep> In discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. Please see reviews and public discussion for further details."
"=== Contributions === <sep> This paper proposes a new framework for A/B testing in the frame of randomized online experiments. This new framework enables testing whether qualitative treatment effects for some specific segment(s) of the tested population can be detected or not. <sep> The approach relies on a scalable algorithm with: <sep> adaptive randomization: in this setting, observations are assumed to be dependent on each other, since treatment can be adjusted by looking at previous rewards; <sep> a nonasymptotic upper bound on the type-I error for the online updating, <sep> a maximum number of data peeking times that is growing with the number of observations. <sep> Moreover, a bootstrap method is provided in order to determine the stopping boundary. This  circumvents the absence of any tractable analytical form for the limiting distribution of this new test statistic. <sep> Finally, the method is accompanied with experiments on the finite sample performance of the test procedure with simulated and real data from Yahoo!. <sep> === Strong points === <sep> The paper is well written and all results are well introduced with interpretation in words which makes it easy to follow. <sep> Direct application in practice of the approach can be personalization which is currently a challenge for tech companies. Hence this paper is of great interest for the ML community. <sep> === Weak points === <sep> Minor: <sep> Authors claim that Figure 3 reports experiment results regarding QTE. However, on this figure, we see only the results for ATE and HTE. Would it be possible to add them? I assume QTE results are better than ATE and HTE ones... <sep> === Recommendation === <sep> Overall, I vote for accepting this submission. My acceptance is supported by the strong points stated above. My grade can be further strengthened if the authors can address the points which for me need to be clarified, the biggest one being the correction of Figure 3 to fully support the efficiency of the approach. <sep> === Additional feedback === <sep> For Figure 2, what do S1 and S2 mean? I guess ""random"" refers to probability 0.5 to be assigned to one or the other treatment and Adaptive to the epsilon-greedy approach. <sep> How long does it take on average to compute each test for Yahoo data? Do you recommend applying the test for low dimensional data (number of features is 5 for Yahoo)? <sep> Minor details: <sep> -For readability, I would add in the title of Figure 1 that A is the treatment applied and Y the associated reward, since they have not been yet introduced at the time of the figure's reference. <sep> For reproducibility of the results, are the Yahoo data used for the experiment freely available? If yes, would it be possible to add a link to the repository? <sep> === Questions to help to clarify === <sep> How does it relate with Bandits tests that are also online? Would it make sense to add some experiments to show when it is better to use Bandits tests over BAT method for A/B testing? <sep> === After authors' feedback period === <sep> I read carefully authors' responses to all reviews. The author's addressed my concerns and I guess the ones of the other reviewers too. One limitation that can be raised now is that the method is better suited for low-dimensional data. <sep> Hence, I keep my accept score.","The paper proposes a new framework for online hypothesis testing aimed at detecting causal effects (of treatments on outcomes) within subgroups in online settings where treatments are randomized. Such settings occur in online advertising where different versions of the same website may be presented to a set of otherwise exchangeable users via A/B testing. <sep> Under the standard causal assumptions of SUTVA, and sequential ignorability, in addition to a set of regularity conditions, the authors derive a result (Theorem 1) leading to an online test (Theorem 2). Since the resulting test's limiting distribution does not have an exact analytic form, the authors instead propose a bootstrap approach to determine a set of parameters to properly control the error rate. <sep> The author validate their approach by a simulation study, as well as via a user click log data from Yahoo! <sep> The reviewer opinion was somewhat split on this paper, in particular some reviewers raised concern about some (conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions (the dichotomy between linear models and neural networks is surely a false one -- the semi-parametric literature obtains nice parametric style results, although perhaps not always for tests, without assuming parametric likelihoods all the time)."
"This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of ""incidental supervision signal"" for a downstream classification task. In particular, when labeled data is only available in noisy or partial form, or over a different domain than the target test domain, this data may still be used to improve a classifier, but it's unclear how to tell which forms of incidental supervision will be most useful. Having a measure which allows us to compare different types of such supervision enables us to make intelligent tradeoffs. <sep> PABI is proposed as a very general framework. The most general form of the measure, dealing with updates to the concept class prior, seems that it could capture any kind of incidental supervision. However, this means most of the work is in understanding how to apply and approximate it. This paper provides several such methods, particularly focusing on ""inductive"" learning (from constraints or partial/noisy gold labels) or ""transductive"" learning (from complete gold labels on different input domains). Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors). <sep> Computing PABI may be challenging in some cases. In the case of transductive learning, it seems that a model needs to be trained on the incidental signal, although this is better than the combinatorial explosion of jointly trained models that would be required to test relative improvements directly. However, it's not clear if efficient approximations for PABI will be feasible in all cases. This and other questions about the breadth of application of PABI are left for future work. <sep> Strengths <sep> I think this paper is very well-motivated, situates itself well with respect to previous work, and presents clear advantages. Having a unified framework for comparing the utility of different kinds of incidental supervision signals seems potentially very useful, especially these days when incidental supervision of various sorts is instrumental in state-of-the-art models. It is also extremely relevant for data annotation and task design, which often has to make tradeoffs between these factors (i.e., noise versus partial annotation or dataset size). <sep> There is a lot of content in this paper, including mathematical developments, algorithms, and experimental results. While I did not carefully check the proofs in the appendix, and I am not familiar with PAC-Bayesian theory or the associated literature, the paper seems technically sound to me. <sep> Weaknesses <sep> While the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better. <sep> As proposed, the PABI framework seems very general—which is good. But the paper only shows how to realize the framework in a couple specific cases, for ""inductive"" and ""transductive"" learning independently. This is still more general than previous work, but from the first few pages of the paper I was expecting something even more general. <sep> It seems to me that the combination of inductive and transductive learning may be possible using something close to the paper's proposed methods , but this isn't addressed by the paper except a glancing mention in Footnote 6. <sep> It also is not clear to me from the paper's text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. In particular, it seems that in this case the approximation method proposed for transductive learning would indeed have to reduce to training a combined model. Related issues were finally mentioned briefly in the last paragraph of the paper, and something along these lines appears in appendix A.3, but I think a more up-front clarification of the limitations is warranted. <sep> More broadly, the question in the back of my head when I began reading the paper was if this would help explain why and when language model pretraining (and other more flexible related-task pretraining) works well. The paper points to related work in this area, such as Gururangan et al 2020 (""Don't Stop Pretraining""), leading me to think this paper would shed light on the issue, but in the end the issue was not mentioned and seems perhaps out of scope. <sep> This is fine. All I would ask of the authors is to be more explicit about the limitations of PABI (or the proposed realizations of it) from the beginning, laying out the scope of this work and stating the limitations outright instead of only pointing to the appendix. It seems to me like PABI is more of a foundational framework which is ideal for future work to build into, rather than already being a general solution in itself. I think it would be best to pitch the paper this way. <sep> Recommendation <sep> Accept. Important problem, lots of solid content, clear benefits over previous work and directions for the future. Great work. <sep> More comments/questions <sep> I think the point of the formulation in Section 2.2 can be made a bit more explicit. It seems like the point is for applying PABI to partial labels. If that's true (or there's more to it) then might as well just say it there, or at least give this case as a motivating example. <sep> Regarding the cross-domain results: why are the incidental supervision sets so small? It seems that there is a ton more incidental supervision available for NER, and in both cases the incidental supervision data is even smaller than the test set. Why not use more? It seems to me that the use case here is when a large amount of incidental supervision is available anyway. It also seems like the low-data setting is not totally fair to the vocabulary overlap baseline. <sep> Typos, style, etc. <sep> When describing your experiments, I think it's worth mentioning that they are on English text. <sep> Figure 3: I don't understand which numbers correspond to which model in the caption. This would be much easier to read in a table. <sep> P. 7: something's wrong with ""twitter(Strauss et al., 2016)"" <sep> P. 7: The FitzGerald et al 2018 dataset is called ""QA-SRL Bank 2.0"". <sep> P. 7: servers -> serves <sep> P. 7: ""the lower bound for is""","This paper first makes the observation that incidental supervisory data can be used to define a new prior from which to calculate a PAC-Bayes generalization guarantee. This observation can be applied to any setting where there is unsupervised or semi-supervised pre-training followed by fine-tuning on labeled data. The PAC-Bayes bound is valid when applied to the fine-tuning. For example, one could use an L2 bound (derived from PAC-Bayes) on the difference between the fine-tuned parameters and pre-trained parameters. <sep> But the paper proposes evaluating the value of pre-training before looking at any labeled data. Let π0 be the prior before unsupervised or semi-supervised training and let π~ be the prior after pre-training. The paper proposes using the entropy ratio H(π0)/H(π~) as a measure of the value of the pre-training. As the reviewers note, this is not really related to PAC-Bayes bounds. Furthermore, it is clearly possible that the pre-training greatly focuses the prior but in a way that is detrimental to learning the task at hand. <sep> I have to side with the reviewers that feel that this is below threshold."
"This paper proposes a new objective to learn representations that allow for efficient and accurate search with the Multi-Bernoulli Search data structure. The motivation for such a scheme is strong and the preliminary empirical results demonstrate the utility of using the proposed representation learning objective and the data structure. <sep> However, I am currently leaning towards a reject because of two main reasons: The first being the clarity of the presentation in the paper that makes it hard to identify (i) novel contributions, (ii) key algorithmic and technical details -- the main paper is supposed to be somewhat self-sufficient; with the current version, it was significantly hard for me to follow through the presentation even when repeatedly referring to the supplement. The second reason is the lack of positioning of the proposed scheme/objective/data structure to a long line of research on the use of machine learning (with novel metrics/objectives and data structures) for search, including the learning of space partitioning trees [A, B, C], locality sensitive hashes [E] and representations [D] -- it is quite possible that the proposed scheme is solving a different and/or more general problem but I believe it would be useful to connect this ""Search Data Structure Learning"" to the existing work on ""Learning to Search"". <sep> Beyond the aforementioned high level comments, please find the following specific comments/questions that should be addressed: <sep> The set of relations R and the per-database RD could use further motivation as to how they relate to search and/or relate to the task of learning search data structures. <sep> It is not clear why the distinction between ""absolute"" and ""relative"" is necessary in the context of the problem being targeted in this paper. <sep> The SSWR definition is unclear to me and can use more exposition. It is not clear that δt,t=1,…,T is a sequence of sets. Also, it is not explained why the oracle costs are only incurred for the final δT and not all intermediate ones (unless δt−1⊆δt) or to the complete ∪i=1Tδt. <sep> At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator G that is aggregated over database-query pairs (D,q) -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)? Can this be clarified, and if it is deliberate, please motivate the need and advantage of learning across databases. <sep> The algorithm description is very hard to follow -- there are M back-end data structures and also the learned representation is used to generate M probable back-ends to insert in and T probable back-end to search from. It is not clear how the search involves T passes over the M back-ends. It would be good to clarify in the main paper that the ""outcomes"" are keys for each backend and all T keys are tried in all M back-ends. <sep> How are we leveraging multiple databases as shown in the previous loss function? <sep> The halting mechanism requires better presentation and clarification. It makes intuitive sense. But the presentation of the training algorithm (at least in the main paper) seems insufficient to provide enough context about the specifics of the halting mechanism. <sep> [A] Li, Z., Ning, H., Cao, L., Zhang, T., Gong, Y., & Huang, T. S. (2011). Learning to search efficiently in high dimensions. In Advances in Neural Information Processing Systems (pp. 1710-1718). <sep> [B] Cayton, L., & Dasgupta, S. (2008). A learning framework for nearest neighbor search. In Advances in Neural Information Processing Systems (pp. 233-240). <sep> [C] Dong, Yihe, et al. ""Learning Space Partitions for Nearest Neighbor Search."" *CONF* 2020. <sep> [D] Sablayrolles, A., Douze, M., Schmid, C., & Jegou, H. (2018, September). Spreading vectors for similarity search. In International Conference on Learning Representations. <sep> [E] Wang, J., Liu, W., Kumar, S., & Chang, S. F. (2015). Learning to hash for indexing big data -- A survey. Proceedings of the IEEE, 104(1), 34-57.",This paper addresses an interesting problem and all reviewers agree. Most reviewers found the paper difficult to understand and it was hard to see the novel contributions. The paper will need a significant revision before publication.
"In this paper, the authors introduce a region-based approach for unsupervised scene decomposition. It extends the previous MONet by introducing the region-based self-supervised training. Instead of purely generating foreground masks in an RNN, they simultaneously predict the bounding boxes and segmentation masks using a Faster-RCNN-based framework. The supervision comes from the object reconstruction loss and the self-supervised loss of classification and regression of anchors in the RPN module. The experiments and comparisons are only conducted on the synthetic CLEVR and Multi-dSprites dataset. <sep> [Paper strength] <sep> The paper is well motivated, and the proposed approach seems to be reasonable. <sep> The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection, which could ensure the consistency of object mask and bounding box. <sep> [Paper Weakness] <sep> The self-supervision between segmentation masks and detection bounding boxes is the main contribution. While incorporating the self-supervision into MONet is meaningful and interesting, the overall novelty does not look significant. <sep> Clarification of Methods: <sep> How to learn mk in a self-supervise way is unclear? MONet uses spatial attention to identify the most salient object one by one, which makes senses. But here you segment all the objects in one step. How could this be possible in an unsupervised way? From the example in Fig. 2, it looks R-MONet could pick out some small and far-away objects first, which is not intuitive. <sep> In Faster-RCNN, the positive/negative samples are selected by calculating the IoU threshold between the sampled bbox and the ground truth bbox. However, in this self-supervised approach, there is no ground truth bbox. Although the authors proposed to use the pseudo bbox from the segmentation mask mk, how could this be reliable since mk is likely of poor quality, especially at the initial stage. <sep> The selected K value is unclear. In the original MONet, the spatial attention network is an RNN-like structure, they decompose the scene step-by-step. Therefore, they define the K steps. However, in this Faster-RCNN-based framework, the objects are selected in one step, how to select the K-1 objects in all proposals? <sep> Results: <sep> Most of the results are with toy images. There is no result on real images. <sep> There is no result to really demonstrate the effectiveness of the self-supervised loss. The author should compare their R-MONet(UNet) with the baseline of R-MONet(UNet) w/o the self-supervised loss, i.e. removing the object detection branch. Another missing baseline is MONet(UNet). <sep> In Table 1, the MONet (ResNet18+FPN) is 10 percent lower than the original MONet. Does this means the network structure has a greater influence on the performance than the self-supervision component. <sep> In Table 1, the R-MONet(Lite) performs worse. Once again, I guess this poor performance comes from the network structure, as the input image is 64*64, the Resnet downsamples the image to a very low resolution which losses the spatial information. <sep> The visual results cannot demonstrate the advantage of the proposed approach. For example, in Figure 3, the visual performance of MONet and R-MONet(UNet) are quite similar. <sep> Update: <sep> In general, I am happy with the authors' responses. They did show the advantage of the introduced self-supervised loss. Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. I am willing to increase my rating.","The paper has good contributions to a challenging problem, leveraging a Faster-RCNN framework with a novel self-supervised learning loss. However reviewer 4 and other chairs (in calibration) considered that the paper does not meet the bar for acceptance. The other reviewers did not champion the paper either, hence i am proposing rejection. <sep> Pros: <sep> R1 and R3 agree that the proposed model improves over related models such as MONET. <sep> The value of the proposed self-supervised loss connecting bounding boxes and segmentations is well validated in experiments. <sep> Cons: <sep> R4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., e.g. ""stick breaking, spatial broadcast decoder, multi-otsu thresholding"" so it becomes more self-contained. R4 also suggests improving the writing more generally. <sep> R4 still finds the proposed ""method quite complex yet derivative"" after the rebuttal. <sep> All reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix. These could be part of the main paper in a future version."
"This paper studies how temperature scaling affects training dynamics of neural networks (with softmax layer and cross-entropy loss). The theoretical analysis shows that neural networks trained with smaller inverse temperatures (beta) exit the linear regime faster, which implies better performance. Experiments on image classification and sentiment analysis confirm that tuning temperature improves neural network generalization, even for state-of-the-art models. <sep> Strengths: <sep> The topic is novel and interesting. <sep> Empirical results confirm the importance of tuning temperature. <sep> Weaknesses: <sep> Theoretical contribution seems unclear. <sep> Experiments are limited to two tasks. <sep> I am leaning towards rejecting the paper. My biggest concern is about the theoretical contribution. The main conclusion of the theory seems to be that smaller beta is better, because it helps neural networks exit linear regime faster. But this doesn't explain the experiment results: optimal beta varies a lot across models, and it is sometimes quite large (beta >= 1). The paper empirically shows that small beta causes more instability as an explanation, but there is no theoretical explanation. Therefore, it is unclear how to use the current theory. <sep> Ideally, I hope the theory can be extended to explain why small beta causes instability (the conclusion section mentions this as future work), and/or how neural network architecture affects optimal beta, but these extensions do not seem obvious. <sep> The experiments can also be expanded. Currently, there are only two tasks in the experiments. While the results are impressive, I would be more convinced if there are more tasks/models. For example, I wonder what the optimal beta is for state-of-the-art BERT-based models.  What about structured prediction tasks? Does the size of training set affects optimal beta? <sep> On the positive side, I think the topic is novel and interesting, and the current empirical results are solid. If the theory can be extended to explain the tradeoff between small/large beta and the role of architecture, I would recommend this paper. Alternatively, the paper can also be improved by expanding experiments. <sep> Other suggestion and question: <sep> Section 2 (theory) may be easier to read if there is a short summary of main claims/results. <sep> For IMDb experiment, is there a reason for choosing GRU instead of more recent BERT-based models? <sep> Some of the figure fonts are too small. <sep> Feedbacks after author response: <sep> I am maintaining my rating after reading the author response. It is a close decision. I like the topic, but I think the draft still has room for improvement to become a great paper. The updated draft is much clearer and answers some of my questions. Most importantly, the updated theory section explains why small beta can be bad: it slows training. While I appreciate the clarification, I think this argument still doesn't fully align with the experiment results. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain. To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. For example, I wonder if this is somehow connected to the slow training argument; perhaps the failed runs indeed suffer from slow training.","Three reviewers are mildly positive, while one is negative. The substantive comments of the reviewers are consistent with each other; it is merely their evaluations that differ. <sep> One contribution of the paper is that it shows how using temperature tuning can yield similar accuracy to using batch normalization; this is useful because batch normalization is not always possible. The revised paper shows improvements, and we appreciate the engagement of the authors with the reviewer comments. However, there are remaining weaknesses such as a weak argument based on the empirical.results. <sep> This paper can be improved based on the comments made by the reviewers. We encourage the authors to resubmit to a future venue."
"Summary <sep> This work claims to address the problem of learning the structure of interactions between multiple point processes. This can, e.g., be used to predict co-occurrences between different types of events. The authors develop a method that (a) is theoretically principled and (b) scales to large datasets. <sep> Reasons for score <sep> The overall problem is clearly relevant, and some aspects of the contribution appear to be promising (connection to GAMs, approach to inference). However, in its current form, the paper is excessively unclear, to the point where I am not sure that I understand the precise problem the authors are trying to address (despite my familiarity with the area). <sep> Pros if the problem is that of understanding correlations between multiple point processes, then that is an important problem that is clearly relevant to the ML community. <sep> The approach builds on generalized additive models, which gives their approach a solid theoretical foundation <sep> The inference algorithm based on minimizing KL-divergence appears to be interesting. <sep> Cons the paper lacks clarity throughout, making it difficult to understand the problem, let alone the contribution itself (examples below). <sep> the experiments are not explained in sufficient details (NYC taxi dataset for example), and as such they are not supporting the contribution. <sep> Comments <sep> Referring to ""stochastic processes"" is too general—maybe talking about ""point processes"" would be better. For example, a Gaussian process is a stochastic process, but doesn't have anything to do with the processes considered in this work. <sep> Section 2, ""Formulation"" is confusing. Usually, a multi-dimensional / spatial poisson process is a single point process defined on RD, and a realization of that process is a collection of points in D-dimensional space. This is what, e.g., Flaxman et al. (2017) consider. <sep> In this paper, the authors seem to assume that every dimension is a different process, and somehow events in each dimension are grouped together based on their arrival order (""Each ti is the time of occurrence for the i-th event across D processes and T is the observation duration""). I am not even sure this is always possible, since it seems to assume that every process / dimension has exactly N events during the interval [0,T]. Thus, I fail to understand even the basic foundation on which their work builds. <sep> It seems to me that the authors are really considering a multivariate Poisson process, where the intensity function of each process are dependent. In that case, the likelihood would be different from (1). Can the authors comment on this? <sep> Experiments: <sep> ""the sample size is randomly chosen by the mixture of Gaussians"" - what does this mean? a GMM does not define a distribution over number of samples. <sep> where is the notion of timestamps in the synthetic GMM experiments? <sep> Figure 3 does not seem to be referenced in the text. How can the intensity function be 1D for if the experiments are with 2D Gaussians? <sep> NYC taxi dataset: how is space taken into account? Defining a ""common pick-up"" as one that happened within at 1 min interval but at opposite ends of the city seems pointless. <sep> Other comments / questions: <sep> How does your work compare / contrast to multivariate Hawkes processes? <sep> In what way is Flaxman et al. 2017 non-convex? <sep> "" ...where the different events are marked so that a particular subset of events can be used to generate the intensity function"" what does this mean? <sep> POST-REBUTTAL <sep> Thank you for rebuttal which has helped me, to some extent, to understand your model. I still believe the formulation needs to be clarified before the contributions can be assessed objectively. <sep> For example, on page 2: <sep> Given a realization of timestamps t1,t2,…,tN with ti∈[0,T]D from an inhomogeneous (multi-dimensional) Poisson process with the intensity λ. Each ti is the time of occurrence for the i-th event across D processes and T is the observation duration. <sep> This is confusing: only a single one of the dimensions will likely represent ""time"". Other dimensions will be space, etc. Furthermore, why would every dimension need to be within [0,T]? Different dimensions might use different units and be in different ranges. <sep> I am upgrading my score slightly, because I sense that your contributions might be interesting once properly explained, but in the present state I believe the paper is not ready for publication.","This paper proposes a method for modeling higher-order interactions in Poisson processes. Unfortunately, the reviewers do not feel that the paper, in its current state, meets the bar for *CONF*. In particular, reviewers found the descriptions unclear and the justifications lacking. While the responses did aid the reviewers understanding, the paper would benefit from rewriting and more careful thought given to the experimental design."
"Edit: I was unaware that papers could be submitted to arXiv simultaneously, I am sorry for that. Here is my (very late) review. <sep> I made it before reading other reviewers' reviews. <sep> An efficient method for fitting long-range style interactions in point clouds is presented. <sep> It makes use of NUFFT rather than convoluting a long-ranged (thus system-size and expensive) kernel directly in real space. <sep> Along with this architecture, a method for training it efficiently is presented. This 2 steps strategy consists in training the short-range part of the kernels well with a lot of (supposedly inexpensive) short range data, and fitting the long-range kernels with less data in a second time. <sep> Overall, the paper is clearly written and clearly exposes the methods used. <sep> The results are interesting for applications but do not seem ground-breaking (although I am not an expert of point clouds -specialized networks). <sep> In terms of experimental results, I think a couple of points would deserved to be answered (see below). <sep> In conclusion, I think the paper is marginally above acceptance level. <sep> In terms of results, the paper clearly shows that NUFFT scales essentially like O(N) (with N the points number) whereas naive direct space convolution scales as O(N^2). <sep> However, when I read the algorithm (page 3), my main curiosity is : how well does the algorithm deals with large systems (large domain \\Omega), and the competing parameters seems to be the resolution of the function g_\\tau (which plays the role of mollified dirac) versus the system size \\Omega or L. Concretely, I expect that for too large \\tau/L (presented in appendix, Equation 20), the precision of the long range kernels will be poor and error will be large; and in the opposite case of very small \\tau/L, precision will be good but compute time will increase (I guess it would increase as FFT does, in O(L/\\tau log(L/\\tau))). <sep> Probably there is a regime where direct convolution (which does not need the approximation introduced by g_\\tau, as far as I understood), is better then the NUFFT approach introduced here. I would guess that in the large system size (large \\Omega=L^d) and small particles number N limit (i.e. the low density limit), the direct approach does well ? <sep> I think such a discussion (and possibly a couple of experiments) would improve the paper a lot, showing the limits of the method and letting the reader understand the reasons for its strengths (which are very real, I do believe it !) <sep> A bit more of commenting on the experiments' results would be appreciated. For instance, it seems that the 2-scale training strategy is especially efficient (not needing many samples) when the LRIs are sufficiently strong (figure 3, right). This is probably an effect of the ""screening"" of LRI by short-range Interactions when alpha_1 is tooo strong compared to alpha_2, making it harder to learn about LRIs. <sep> Also, the fact that all curves essentially collapse beyond a given N_sample could receive a comment. <sep> A couple of remarks like that about the strengths and limits of the approach would be nice. <sep> Aside from these 2 main comments, I have minor remarks of presentation: <sep> figure 2 left and figure 3 both sides: poor choice of colors/linestyles. Curves come in pairs, and this should be suggested in the choice of display, using similar colors for pairs, and different linetyles in different pairs. It would improve readability (also, think of the color blind people!) <sep> fig2 , right: the O(N), O(N^2) scalings are un-readable. make them parallel to the plots you do and simple black dashed to improve clarity. <sep> In the tables, although here results are ""trivial"" (always the same line has the smallest error), you could use the convention of putting in bold the better numbers. <sep> Table 1, you say ""mu_2 can be arbitrary here"". What you mean is that it needs not be defined, because alpha_2=0 ? <sep> I found this sentence confusing (maybe it's just me).","This work proposes an efficient method for modelling long-range connections in point-cloud data. Reviewers found the paper to be generally well-written. On the less positive side, reviewers felt that the novelty of the work was marginal, and that the experimentation, limited to synthetic data in one domain, was too limited. These concerns remain after the discussion phase. In addition, the authors stated during the discussion that ""Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems"", which conflicts with the presentation of the work as motivated by more general point cloud modelling problems. Given these weaknesses, the final decision was to reject."
"Review of ""Factor normalization for DNNs"". <sep> The paper makes an observation that datasets used for training many deep neural nets exhibit a strong factor structure, i.e. have a small number of dominant principal components explaining most of the variance.  If we were to remove the dominant factors, the residuals would have much weaker correlation structure and allow faster DNN convergence with SGD.   The paper proposes to separate the dominant factors and the residuals,  train the original DNN on residuals with a much faster SGD learning rate,  and then recombine with a shallow small NN learned on the dominant factors trained using its own (slower) learning rate. <sep> First the positive aspects:  this is an interesting idea, and I haven't seen it commonly used in practice for DNNs.  While mathematical analysis is conducted for linear models with GD and is fairly straightforward, it nicely illustrates the main issues, and the hope is that the linear intuition continues to apply to deep NNs.  Experimental results suggest that indeed convergence rate can be improved on several datasets.   In terms of criticisms,  there is very limited scholarship of related ideas that have been used both for linear models and for DNNs, in particular (a) various factor-based models that already exist,  (b) preconditioning of linear systems,  (c) neural nets trained on some other sort of residuals -- e.g. laplacian pyramid DNNs .  Another criticism -- is that there is no discussion of how to do large-scale PCA / factor analysis for high-dimensional image data arising in say modern DNN image classification pipelines, and its computational cost,  as simple numpy.linalg.svd won't work.  The paper also claims that strong factor structure (with a small number of dominant components) is prevalent in modern ultra-large scale DNN applications -- I would like to see some references / supporting evidence beyond just computer vision. <sep> Overall in my opinion the paper needs to consider the context of related works, and has a few other correctable issues, but I would certainly encourage the authors to continue to improve it. <sep> Additional details: <sep> Prior work -- that should be cited / contrasted with your approach:(a) Since a substantial part of the paper analyzes linear models,  it's important to mention that factor structure has been long exploited in various ML / stats works.  For example factor models, and principal component regression (PCR) attempt to focus the modeling power on the principal components. In other applications (e.g. in financial modeling) one can assume that factors components are less predictable and focus the modeling effort on the residuals. Recent work by Alex Smola et al,  ""Deep factors for forecasting"" has looked at the deep-NN instantiation of this idea.  Basic autoencoders or bottlenecks used in DNN architectures also attempt to capture the low-dimensional structure.  These are all different from what you're doing, but hinge on the same basic concept -- so providing a discussion of your work in context of related work would be important. <sep> b)  There is a long history of using preconditioners for gradient-based and other iterative solvers of linear systems.  In particular there are some low-rank preconditioners for accelerating convergence of linear systems:  Nicholas Higham, et. al, ""A new preconditioner that exploits low-rank approximations to factorization error"".  There is also work on applying preconditioners specifically for SGD, e.g. see works by Michael Mahoney. <sep> c)  For some domains a low-pass filter, or some other low-resolution model can serve to replace PCA or factor models. For example Laplacian pyramids have been widely used in image processing to separate the dominant modes from details.  There are existing DNN approaches based on laplacian pyramids: <sep> e.g. ""Deep laplacian pyramid networks for fast an accurate super-resolution"". <sep> You mention that ""none of these optimization methods has considered the covariance structure"".. related to the Hessian.  There is a substantial effort to develop second-order methods for stochastic gradient descent, in particular at ICML 2020 there was a workshop on this topic.  ""Beyond first order methods in ML"".   http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=102387&copyownerid=156008 <sep> In section 2.1. -- it's worth mentioning explicitly that you're specifically analyzing linear regression under gradient descent.  as you look at Loss(y, theta' X).   It's also worth explicitly mentioning that the condition number of the Hessian plays a crucial role in convergence rate of GD.   You talk about the top eigenvalue, where the bottom one in your example is fixed at 1,  but it's worth mentioning the condition number. <sep> Sec. 3.1. How do you conduct 'standard principal component analysis' for ultrahigh dimensional features.  This is a computationally tricky problem, ""standard"" methods won't work.   How do you decide on the dimension -- i..e. number of factors to keep? <sep> Is the ""time consumption""  including the time to do PCA? <sep> Can you give some references claiming strong factor structure in several DNN applications in ultra-high dimensions?  I do not contest that this is the case -- but it would be useful to have supporting evidence.  What number/fraction of factors is typically required in these applications to capture a nontrivial fraction of variance? <sep> Sec. 2.1. Using lower-case kappa for a matrix is strange,  I initially assumed it's a scalar.  Maybe use another capital letter. <sep> While the paper is mostly pretty readable, there are various small issues with english language (from stylistic to grammar) use e.g. ""In fact ample amounts of empirical evidence"" --> ""ample empirical evidence"", e.t.c. There are typos in references, e.g.  Zeiler,   ""Computer ence, 2012"". What is that?","The authors proposed to pre-process the original input features into a low dimensional term and its corresponding residual term via SVD. The paper empirically demonstrated the neural networks trained on such factorized exhibit faster convergence in training. Several issues of clarity were addressed during the rebuttal period by the authors. <sep> However, the reviewers still felt that there were some remaining fundamental issues with the paper, <sep> The motivation is not echoed in the experiments, namely most of the experiments on CIFAR and CatDog dataset using a low dimensional factorization of d=1 which is trivial and often part of the whitening preprocessing. <sep> The proposed factorization via SVD will be difficult to scale up to high dimensional features, large training sets and higher d >> 1. <sep> The empirical experiments show a marginal improvement in the training speed, especially in the image recognition tasks, yet there seems an early plateau in test performance when compared to the baselines. <sep> The theoretical analysis in Section 2 studied linear models. Yet, the rest of the paper focuses on non-linear neural networks. It is difficult to see the connection between the analysis and the rest of the paper. <sep> Thus, I recommend rejection of the paper at this time as the current version of the paper needs further development, and non-trivial modifications, to be broadly applicable."
"########################################################################## <sep> Summary: <sep> This paper studied the effect of pretraining from a miniaturization perspective. The proposed, dataset-internal contrastive self-supervised learning benefits in zero- or/and few-shot learning settings. <sep> ########################################################################## <sep> Reasons for score: <sep> Overall, my score is marginally below the acceptance threshold. <sep> The paper provides a very in-depth survey of recent work on pretraining research and a clear scoping of the problem to be addressed in this work against the other work. I also like the framing of dataset-internal and dataset-external to distinguish between self-supervision and task-supervision, respectively. I also like the list of limitations of the prior work in the Related Work section. <sep> The description of the proposed method in Section 3 was very detailed and easy to follow. <sep> Overall messages and findings from the experiment were clear and convincing to me. <sep> Cons: <sep> Although the clarity of methodology description and clear motivation compared to prior work, my main concern of this work is the lack of generalizability of the proposed method to other low-resource, long-tail problems. The predicting pseudo, noisy labels itself is not a novel idea. The main novelty of the method comes from predicting new labels from word token space directly, which seems to be very limited to the task itself; free-formed tag labels for text. As described in the conclusion section, the method seems to be only applicable when input text and label tags are in the same place. <sep> Another concern is the unclear description of how two binary labels are optimized in a contrastive way. Usually, there should be an additional loss to maximize the difference between two similar-looking examples of different labels in contrastive learning like SIMCLR. I only find that (5) in Figure 1 is a simple aggregation of binary classifiers, rather than contrastive optimization. When the batch set of positive and negative (pseudo) labels are prepared in L_i, does the ordering of the labels matter? Also, again do you pair positive and negative labels for pair-wise optimization? <sep> I like to suggest changing the framing of this work from the general findings of pretraining schemes to task-specific findings. That is, Section 4 should be introduced earlier than Section 3, and describe how difficult the task is and how existing pretraining methods fail to achieve good performance. Until section 3, I had a hard time understanding whether the proposed method in Figure 1 is generalizable to any downstream tasks or specific to certain tasks. The partial examples in Figure 1, like ""measuring an interaction"", ""interaction"", ""p-value"" are mentioned without even describing what they are. Otherwise, it would be really nice to add more applications in a long-tail, low-data regime, proving that the proposed framework is general enough. <sep> ########################################################################## <sep> Questions during the rebuttal period: <sep> Please address and clarify the cons above. <sep> Here are some questions: <sep> In section 3, what do you mean by ""Sampling pseudo-labels provides a straight-forward contrastive, partial autoencoding mechanism…""? <sep> In Section 6.2, authors controlled the hyper-parameter for a better-controlled experiment. However, I wonder this is a fair game. Each training technique may have different levels of optimality. So, I guess given a fixed set of parameter ranges, providing the optimal setting might be a more fair setting. Or, at least providing them in the Appendix might be more convincing. <sep> ######################################################################### <sep> Some typos and suggestions for presentation: <sep> (1) Too many styles in the text (e.g., bold, italic, the numbers) make it difficult to read sometimes. I know they are used to help follow the main points but they are quite distracting.(2) Table Tab. 2 -> Tab. 2","The paper presents a self-supervised model based on a contrastive autoencoder that can make use of a small training set for upstream multi-label/class tasks. <sep> Reviewers have several concerns, including the lack of comparisons and justification for the setting, as well as the potentially narrow setting. Overall, I found the paper to be borderline, the cons slightly greater than the pros, so I recommend to reject it."
"SUMMARY: <sep> Sketching is a popular technique in numerical linear algebra for achieving various desirable properties (e.g., lower complexity, one pass methods). The present paper considers a particular kind of sketch for which the sketch matrix is learned from data. It shows how such learned sketches can be used in two types of problems: Hessian sketching (Sec. 3) and Hessian regression (Sec. 4). The authors give both algorithms and provide theoretical guarantees. They also apply these techniques to a number of both synthetic and real datasets in the experiments. For the most part, the experiments indicate that the proposed methods give a consistent, but not necessarily very large, improvement. <sep> I think the idea of using learned sketches is interesting, and it seems like it has not been applied to the problems considered in this paper before. I think the paper strikes a good balance between algorithms, theory and experiments. I like the paper, but there are a few issues that must be addressed before it can be published. Most importantly, there seems to be errors in both Lemma 3.1 and Theorem 3.2. These should hopefully be easy to fix though. <sep> In the current state, I vote to reject the paper. But if my concerns, especially the issues with Lemma 3.1 and Theorem 3.2, can be addressed, I'll be happy to increase the rating. <sep> ADVANTAGES: <sep> Learned sketching is a fairly new and interesting idea. <sep> It seems like learned sketching has not been used for Hessian sketching and regression before. <sep> Good balance of algorithms, theory and experiments. <sep> CONCERNS/QUESTIONS: <sep> In the list that describes performance improvements under ""Our Contributions"", it is not clear what the quantities x∗ and X∗ represent. In Sections 5 and 6, these quantities are used to represent the optimal solution to the unsketched problems, but here they seem to represent the solution for the sketched problem using one of the random sketches. If it's the latter, are x∗ and X∗ the best performing solutions produced by the competing methods? It would be helpful if you clarified this. <sep> In Algorithm 1, how is α chosen? Is α updated adaptively, or is it just a fixed small number? Is there any rule of thumb for how to do this? <sep> Lemma 3.1: Is seems like the bound on Z^2 stated in the lemma is wrong. Based on the upper and lower bounds on Z2(S) towards the end of Section A, it seems like this bound should read <sep> Z2(S)(1+η)2−3η≤Z^2≤Z2(S)(1−η)2+3η. <sep> Lemma 3.1: Since you use that max(η,η2)=η when applying the result of Vershynin (2012) in the proof, you should add the condition that η≤1 in the lemma statement. <sep> Proof of Lemma 3.1: In Section A, 2nd sentence, you say ""Since T is a subspace embedding of the column space of A..."". I think you should add that this is true with probability at least 0.99, to clarify where the 0.99 probability of success in the lemma statement comes from. <sep> Proof of Lemma 3.1: In Section A, you use the inverse of R in multiple places. However, it seems like R won't be invertible unless A is of full rank. Can the proof be adapted for a case when A is rank deficient? If not, you should add that A is assumed to be full rank in the lemma statement. <sep> Proof of Lemma 3.1: This is related to the previous point. In the second to last equation on page 10, it seems like the equation minx∈Sd−1||SUx||=miny≠0||SUWy||||Wy|| <sep> only will hold if W is full rank, which requires A to be full rank (since AR−1=UW and U has d columns and is full rank). Can the proof be adapted for a case when A is rank deficient? If not, you should add that A is assumed to be full rank in the lemma statement. <sep> Theorem 3.2: Given the error in Lemma 3.1, the bound in Theorem 3.2 needs to be updated accordingly. Also, in the proof in Section B, in the first inequality you use <sep> 1Z^1≥111+ηZ1(S). <sep> There's a sign error here; it should read <sep> 1Z^1≥111−ηZ1(S). <sep> Proof of Theorem 3.2: In Section B, 2nd sentence, it would be helpful for the reader if you said ""holds with probability 0.99"" instead of ""holds with high probability"", and then also clarify that you do a union bound with the 0.99 probability from Lemma 3.1 to get a success probability of at least 0.98 in Theorem 3.2. This may be obvious to readers familiar with the area, but being clear with these things would make the paper accessible to a wider audience. <sep> Solving (4) deterministically would cost O(nd2). For Alg. 3 to be worthwhile, it therefore seems like the number min(σ1/σ1′,σ2/σ2′) in Theorem 4.1 must be smaller than d. Could you say something about why we expect this min to be small? Is it the case that the min may be large with some small probability? <sep> In the experiments, m is chosen as m=kd for some integer k. This is different from the m=O(d2) required for CountSketch to be a subspace embedding. Have you found that this choice m=O(d) always works well in practice, or have you encountered datasets where such a choice has proven to be too small? <sep> For the experiments in Section 5.2, is there any reason why you only use learned sketches in the first round for the Gaussian and Swarm behavior datasets, rather than use them all rounds as for the other datasets and experiments? <sep> In Section 6, is there a reason why you choose η=1 and η=0.2 rather than using the rule for setting η in Alg. 3? <sep> In Section C, you refer to ""standard bounds for gradient descent"". Can you please provide a reference for those that are unfamiliar with the literature? <sep> The paper ends abruptly with no conclusion. <sep> MINOR CONCERNS/QUESTIONS: <sep> On page 2, in the 2nd paragraph, 3rd sentence, you say ""If A is tall-and-skinny, then S is wide-and-fat..."". Should it be ""short-and-fat"" rather than ""wide-and-fat""? <sep> The usage of R is a bit confusing. It is used to mean the R matrix in a QR factorization in Alg. 2, the inverse of the R matrix from a QR factorization in Alg. 3, and the upper bound on the nuclear norm in Section 5.3. It would be less confusing if a the inverses in Alg. 3 and the nuclear norm bounds were called something other than R. <sep> In Section 5.2, for the Swarm behavior and Gisette datasets, you say that Bi is of size 2430×30 and 5030×30, respectively. Should this be 2400×30 and 5000×30 since n is 2400 and 5000 respectively for the two datasets? <sep> ####################### <sep> Update: <sep> The authors have addressed my questions adequately. In particular, my main concerns with the theoretical results and proofs have been fixed. I have updated the score from 3 to 6 for now.","This paper considers convex optimization problems whose solutions involve the solution of linear systems defined in terms of the Hessian. It presents algorithms that reduce the runtime of standard iterative approaches to solving these problems by iteratively sketching the Hessian; the novelty lies in the fact that the authors use the idea of learned sketches which have been used prior for problems in data mining. In particular, the authors use the approach to learning sketches of Liu et al., 2020 to learn the entries in sparse sketching matrices for the Hessian, and propose using the Iterative Hessian Sketch algorithms of Pilanci and Wainright, 2016 to iteratively solve the concerned optimization problem. The advantages of learned sketches are that they may allow using smaller sketch sizes while making progress on the problem, as they are learned to work well on the distribution of Hessians from which the problem instance is drawn. <sep> The consensus of the reviews is that the idea of using learned sketches for convex optimization seems to be novel, and this paper is an interesting attempt, but falls short of the level of contribution required for publication in *CONF*. The main concern is that the theory provided for the use of the learned sketches is incremental: the analysis does not reflect the fact that the sketches are learned; instead, the algorithm builds in a safeguard by using both a random sketch and a learned sketch, and the analysis uses the properties of the random sketch to proceed. The empirical results are suggestive, but the convergence rates of the learned and random sketches do not vary much, so the benefits seem marginal for most of the problems considered (with the exception of a standard least squares problem, for which we know learned sketching performs well). <sep> The paper is recommended to be rejected, as the theory is weak, and the empirical results are borderline."
"This paper defines a new task for clinical data by combing multi-modal and multi-task settings into one task. It collects a dataset called M3 as the benchmark for the multi-modal and multi-task benchmark in the clinical domain. The dataset has 6 prediction tasks, i.e., in-hospital mortality, decompensation, length of stay, phenotyping, readmission, and long-term mortality, and it has 4 modalities, i.e., physiological time series, clinical notes, tabular data, and waveforms. Specifically, this paper also provides a multi-modal multi-task model where the time series data are encoded by LSTM, clinical notes are encoded by text CNN and tabular data are also encoded by existing methods.  In experiments, the authors conduct an ablation study and compare the proposed method with the method of Harutyunyan et al. and Khadanga et al. <sep> Quality: The overall quality of the paper is marginally below the acceptance threshold. The problem definition, data collection, and model design are reasonable. However, I am concerned about the paper presentation and the experiment design. The authors need to compare the proposed multi-modal multi-task model with state-of-the-art single-modality models. <sep> Clarity: The presentation of the paper is easy to follow, but the structure of the presentation may need to be improved. <sep> Originality: The collected dataset is also new because of the new setting. But in terms of model design, the feature embeddings are learned by existing methods. <sep> Significance of This Work: The direction of this work is significant and worth being paid attention to. Considering multi-modal multi-task settings in the clinical domain is useful for the development in this area. <sep> Pros: <sep> The multi-modal multi-task setting is interesting and important for future related research. This paper provides a new direction for moving machine learning forward in the clinical domain. <sep> Collecting data from MIMIC-III is a reasonable choice for creating a multi-modal multi-task dataset. It provides a solution on how to create a new dataset for clinical prediction task with new settings. <sep> The model design is reasonable. I think the authors choose the right frameworks for dealing with different modalities. <sep> Despite this, I am concerned about the presentation and experimental design of the paper, which are summarized as the cons as follows. <sep> Cons: <sep> The paper structure could be improved by incorporating Section 2 ""background"" and Section 6 ""related work"" together. The mentioned work in Section 6 is related to ""machine learning in the clinical domain"" in Section 2. <sep> There are other forms of multi-modal learning in the clinical domain, and the authors should take them into consideration and discuss them in the background. For example, Moradi et al. and Nguyen et al. have some work about the text and image multi-modal learning in the clinical domain. <sep> As for the experiment, Table 3 is useful in showing the model design, but I think the result presentation and experiment design in Table 4 can be improved. The authors could provide how the choices of encoders can influence task performance if there are only two baselines. This can help the readers see what's the potential of doing research on this dataset and directions.  For example, the method of Harutyunyan et al. only uses the time series modality in Table 4, but it can be used as the encoder to process time series in the proposed model design, and maybe the authors should consider this setting as a baseline. To sum up, I think the comparison experiment in Table 4 is not compelling enough to illustrate the model design. <sep> There are many existing works using single modality data towards the six tasks. The authors should compare those models on a single modality to demonstrate improvement when incorporating a multi-modal multi-task dataset. <sep> Some typos: In the conclusion section, ""he first benchmark"" should be ""the first benchmark"" <sep> References <sep> Moradi, M., Madani, A., Gur, Y., Guo, Y., & Syeda-Mahmood, T. (2018, September). Bimodal network architectures for automatic generation of image annotation from text. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 449-456). Springer, Cham. <sep> Nguyen, B. D., Do, T. T., Nguyen, B. X., Do, T., Tjiputra, E., & Tran, Q. D. (2019, October). Overcoming data limitation in medical visual question answering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 522-530). Springer, Cham.","The paper has two contributions. A novel benchmark for clinical multi-modal multi-task learning based on the already released MIMIC III and a multi-modal multi-task machine learning model. While the paper does show value in providing a curated benchmark and combining/unifying existing approaches to a timely problem, the reviewers agree that the paper provides insufficient novelty to warrant publication."
"To improve the practical performance of meta-learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta-learning, namely (1) the optimal (linear) predictors cover the embedding space evenly, and (2) the norms of the optimal predictors remain bounded as the number of tasks grow. Numerical experiments show that the proposed regularization terms help achieve better performance of meta-learning in some tasks. <sep> This work serves as a nice attempt to instruct the practice of meta-learning with theoretical insights. Below are some of my concerns. <sep> In some experimental results, the improvement due to the proposed regularization seems to be at the same level of the standard deviation, as well as the difference between the reproduced results of existing meta-learning algorithms and those reported in earlier papers. This casts doubt on the true efficacy of the proposed methods. <sep> For the loss function in Eq. (4), it is more reasonable and natural to introduce two weighting parameters (as tunable hyperparameters) for the proposed regularization terms. <sep> The authors often talk about ""enforcing/ensuring the assumptions"". However, from my understanding, whether the assumptions (on the optimal linear predictors, or ""ground-truth"" predictors) hold or not depends on the learning problem itself, NOT on the algorithms. Therefore, there is no way we can enforce/ensure these assumptions. I would prefer using the phrase ""respecting the assumptions"" (used by the authors on Page 8); this seems more accurate and reasonable. <sep> Following the previous point, I'm curious about one question: if the learning problem actually doesn't satisfy the two assumptions, then is it still helpful to add the proposed regularization terms to the loss function? (I'm not sure, but my guess is no; indeed, it might even hurt.) To solve puzzles like this, I would encourage the authors to conduct some synthetic experiments, where they can design the data generating process (e.g. they can control whether the true linear predictors satisfy the assumptions or not). Since this work is a connection between theory and practice, I believe that experiments with synthetic data can help explain things more clearly and make the claims more convincing.","This paper is a systematic study of how assumptions that are present recent theoretical meta-learning bounds are satisfied in practical methods, and whether promoting these assumptions (by adding appropriate regularization terms) can improve performance of existing methods. The authors review common themes in theoretical frameworks for a meta learning setting that involves a feature learning step, based on which linear predictors for a variety of tasks are trained. Statistical guarantees for such a framework (that is, statistical guarantees for the performance of trained on an additional target task) are based on the assumption that the set of weight vectors of the linear predictors span the space (ie exhibit variety) and that the training tasks all enjoy a similar margin separability (that is, that the representation is not significantly better suited for some of the tasks than others). <sep> The current submission, cleanly reviews the existing literature, distills out these two properties and then proposes a regularization framework (that could be added to various meta-learning algorithms) to promote these properties in the learned feature representation. <sep> Finally, the authors experimentally evaluate to what degree the properties are already observed by some meta learning methods, and whether the proposed additions will improve performance. It is established that adding the regularization terms improves performance on most tasks. The authors thus argue that incorporating insights obtained form recent theoretical frameworks of analysis, can lead to improved performance in practice. Naturally, the purpose of the presented results is not to establish a new state of the art on a set of benchmark tasks, but to systematically study and compare the effect of adding regularization terms that will promote the properties that are desirable for a feature representation based on statistical bounds. <sep> I would argue that the research community should support this type of studies. The work is well presented and conducted. Most importantly, the study has a clear and general message, that will be valuable for researchers and practitioners working in on meta-learning. <sep> However, the reviewers did not recommend publishing this type of study for *CONF*. The authors are encouraged to resubmit their work to a different venue."
"This paper proposes two fully-connected layers based neural graph pooling methods for graph neural networks, named Neural Pooling Method 1 and Neural Pooling Method 2. The first method uses a first FC to reduce the feature dimension and then FC2 to compute the weights to do weighted-average over features for different nodes. The second method uses two FC to reduce the dimension and then compute second-order statistics by Flatten(H^{\\top}H). Experimental results on four datasets (PTC, PROTEINS, IMDB-BINARY, IMDB-MULTI) of two tasks (bioinformatics, social networks) show that the proposed graph pooling method can improve the performance by 0.5%-1.2% accuracy while decreasing the std. <sep> Strengths: <sep> The proposed method is simple and motivated by several limitations of current graph pooling methods such as average and summation, DIFFPOOL, SORTPOOL, TOPKPOOL, SAGPOOL, and EIGENPOOL. <sep> The proposed approach is simple and the experimental results can deliver improvements on several tasks and datasets. <sep> Weaknesses: <sep> My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) <sep> Based on the author's writing, it is unclear what is the second-order statistics for graph pooling, why it is important to have second-order pooling, and how the proposed method can capture the second-order statistics. <sep> The proposed graph pooling method is only experimented with 1 underlying particular choice of GNN (Xu et al., 2019), so it is unclear how well the method can perform on other GNN architectures. <sep> The four datasets only have 2 or 3 classes and upto 620 nodes. So it is clear how well the method can generalize to large-scale graph classification problems. <sep> The improvement of the proposed methods compared with SOPpool is marginal. For example, On PROTEINS, the accuracy is improved by 0.5% with the same std. On other datasets, the improvements are only at most 1.2%. To show the proposed approach is better, more datasets or tasks should be used. For example, there are five bioinformatics datasets (MUTAG, PTC, PROTEINS, NCI1, DD) and five social network datasets (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K). <sep> There is not enough discussion and analysis of the results. Especially, there should be some analysis to compare the method 1 and method 2: For different datasets, when one method is better than the other? Some examples would be helpful, too. <sep> While the author explains the proposed method has lower complexity, there is still no formal analysis or quantitative measures of running time from experiments. <sep> The writing can be improved, In the abstract and introduction, the author should describe the approach briefly and explain its characteristics including why it can handle variable number of nodes, invariant to isomorphic graph structures, capture information of all nodes, and especially why it can collect second-order statistics. <sep> Furthermore, there is a lot of repetition of problem statements. The problem and notation is introduced formally in section 3.1, but is repeated again and again at the beginning of section 3.2 and section 3.3 <sep> Questions: <sep> Do both of your method 1 and method 2 capture second-order statistics? My understanding is that only method 2 captures second-order statistics by computing Flatten(H^{\\top}H). Is this correct? <sep> How do you compare your method with SOPpool (Ji and Wang, 2020)? <sep> Have you tried other datasets or other tasks? <sep> Have you tried your graph pooling approaches on other underlying GNN models? <sep> Is your standard deviation in Table 2 based on 1 run of 10 folds or multiple runs of 10-fold cross-validation? <sep> Minor: <sep> Please give better names for your approaches and give a better title. ""Neural Pooling Method"" is too general and thus not particular enough to summarize your method.","All four knowledgeable referees have indicated reject due to many concerns. In particular, reviewers pointed out that the novelty of this paper is not clear because the difference from related work is very limited (i.e., the difference from Z. Wang and S. Ji is not clear, other than using one additional layer), and they were concerned that the results of the experiment are not convincing (For example, the results reported in this paper are significantly inferior to those reported in other papers, the GNN architecture used is limited, and the performance difference especially in the additional experiments in the revision, is very marginal). No reviewers were convinced by the authors' claims even through the author's rebuttal and revision. <sep> One important note: Reviewers have stated that they did not explicitly check the identity of the author and did not pose a problem on this, but if we follow the link specified in the original submission, we can see the identity of the author, which may be considered as a violation of the double-blind policy. This is a small and regrettable mistake, but it can be a serious problem in the review process. In this review process, reviewers unanimously suggested rejection even ignoring this issue, but it seems that you need to pay attention in your future submissions."
"Summary. The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same (memorized) policy in every scenario. They begin by introducing a simple bandits environment wherein they derive the optimal policy and identify regimes in which it involves memorization vs. learning. Then they train an RL^2 agent and verify that it behaves as expected in these regimes. Next, they expand their approach to a slightly more complicated gridworld environment which does not have an analytic solution to the question. The agent behaves as expected in the gridworld environment. <sep> Strong points. This paper tackles a novel question which is fundamental to the field of metalearning. By carefully analyzing the two regimes of learning and memorization in the context of metalearning, this paper will increase awareness about the fact that the two regimes exist. The paper is clearly written and does an excellent job of putting experiments in the context of past ML research. The experimental setup is simple but goes straight to the heart of the issue. Figures and text do a good job of analyzing results and communicating them to the reader. Overall, this paper was an interesting read. <sep> Weak points. The idea that ""sometimes memorization is best and other times learning is best"" does border on the obvious. Indeed, as soon as the authors derive their analytical solution, it becomes clear that we can expect the RL^2 agent to learn the same behavior. For me, there were no surprises in the experimental sections. To the authors: was there anything that was surprising or not obvious to you? What additional information can the experiments tell us, apart from confirming theoretical predictions? <sep> Having said that, I also believe that very simple, well-executed research ideas sometimes make the best papers. This paper appears to be one of those cases. And even though the ideas are simple, they are significant and they are not a major part of the dialogue in the meta-learning community yet. So even if the ideas seem obvious, I think there is value in communicating them well. <sep> I have one concern about the bandit task setup: the authors adjust σl, the width of the Gaussian from which they are sampling the reward, as a proxy for aleatoric uncertainty and hence task complexity. In doing so, they essentially equate ""stochasticity of the environment"" with ""task complexity."" And yet, there are many other ways in which a task can be complex. Sometimes, all the information needed to perform a task is present and yet the task is difficult to solve because one needs to interpret/integrate the information in a particular way. This is why, for example, puzzles are considered difficult tasks. It is also why simulating the 3-body problem is a complex task. To the authors: can you clarify what you mean by ""task complexity""? <sep> In the closing paragraph of the paper, the authors claim that their approach ""allows us to study the emergence of inductive biases in biological systems"" but this claim is not supported by the rest of the paper, which makes almost no connections to biological systems. There are certainly ways in which these results are relevant to learning in biological systems, but the authors did not explore them in this paper, and so this claim is not well supported. In the same paragraph, they bring in contrasting notions of Darwinian and Lamarkian inheritance. Since they do this in one sentence -- the last sentence -- it is hard to understand what their claim is. And it was not clear that this was one of the main takeaways of the paper, as these concepts do not appear anywhere else in the paper. If the authors want to draw these conclusions, then they should add additional discussion on these topics. Otherwise, they risk misleading readers. <sep> One additional minor suggestion would be to invert the color scale of Figure 6, as ""white -> red"" signifies values of increasing size in all preceding plots, but in Figure 6 it currently signifies values of decreasing size. <sep> Minor grammatical suggestions <sep> -- ""the question which aspects of behavior"" -> ""the question of which aspects of behavior"" <sep> -- When typing quotes in LaTex, use `` and '' instead of """" so as to make them open & close correctly <sep> -- ""interplay of the agent's lifetime,"" -> ""interplay between the agent's lifetime,"" <sep> -- ""We numerically show"" -> ""We show numerically"" <sep> -- ""as well as explicit models of memory"" -> ""and explicit models of memory"" (same issue occurs later) <sep> -- ""the agents does not have"" -> ""the agent does not have"" <sep> Recommendation. 6 : Marginally above acceptance threshold <sep> Reasoning. This paper is well written and the experimental setup is simple, well-executed, and produces results that are relevant to the main question of the paper. The main question of the paper -- when does it make more sense to learn vs. memorize a behavior -- is significant to *CONF* and to the field of machine learning. There are a number of relatively minor weaknesses (as described above) but this is overall a nice paper and would be a good contribution to *CONF* 2021.","There was fairly detailed discussion among three of the four reviewers. The fundamental concern of the reviewers is regarding the contribution of the paper. During the rebuttal, the authors clarified the following: <sep> while the effects of varying uncertainty / horizon lengths is well-understood for Bayes-optimal policies, it is not understood for existing meta-RL approaches, which is the topic of this paper <sep> That is, the contribution of the paper is to understand the effects of varying uncertainty/horizon lengths for meta-RL approaches. However, it is known in prior work that meta-RL algorithms such as RL^2 can implement Bayes-optimal policies in principle. As a result, it's not clear whether this contribution is significant relative to prior knowledge, and this paper does not seem to bring any new insights. <sep> An alternative framing of the paper would be to consider the question of how meta-RL solutions compare to Bayes-adaptive optimal policies. While this framing would be interesting and novel, the current version of the paper does not sufficiently answer this question, since the only experiments include RL^2 (and such a study would require experimenting with more sophisticated meta-RL algorithms beyond RL^2). <sep> As such, this paper isn't suitable for publication at *CONF* in its current form."
"Paper Summary <sep> In this paper, the authors proposed to train high quality classifiers from datasets with some mislabels. <sep> For this purpose, the authors considered adjusting the softmax prediction using an additional term α as follows: <sep> PC(i|x;θ)=exp⁡(mi(x;θ)+αi(x))∑jexp⁡(mj(x;θ)+αj(x)) <sep> where m(⋅;θ) is a model and θ is its parameter. <sep> The authors claimed that, by adjusting α through training, the trained model m(⋅;θ^) with an optimal parameter θ^ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without α performs well on clean test data <sep> P(i|x;θ)=exp⁡(mi(x;θ))∑jexp⁡(mj(x;θ)) <sep> In the proposed method, for the training set D={xn,yn}, we first train the model by minimizing the following loss function: <sep> θ^,α^=arg⁡minθ∈Θ,α∈RN×K−1N∑n=1N∑i=1K1[yn=i]log⁡exp⁡(mi(xn;θ)+αni)∑jexp⁡(mj(xn;θ)+αnj) <sep> where K is the number of classes. We then classify the new instance x by y^=arg⁡maximi(x;θ^). <sep> In Theorem 1, the authors claimed that the above estimator θ^ converges to the true parameter θ∗. <sep> Pros & Cons <sep> [Pros] <sep> The experimental results indicate that the proposed method is effective on several datasets. <sep> [Cons] <sep> The paper contains a serious flaw in its problem formulation and the subsequent theorems. The proposed problem formulation has a trivial solution which is completely useless. The effectiveness reported in the experiments seems to be just an artifact caused by the tunings of hyperparameters. See my comments in ""Quality"" below for the detail. <sep> Quality <sep> The paper contains a serious flaw in its problem formulation. <sep> Recall the training problem: <sep> θ^,α^=arg⁡minθ∈Θ,α∈RN×K−1N∑n=1N∑i=1K1[yn=i]log⁡exp⁡(mi(xn;θ)+αni)∑jexp⁡(mj(xn;θ)+αnj) <sep> This problem has a trivial solution that αnyn→+∞ for ∀n, which leads to exp⁡(mi(xn;θ)+αni)∑jexp⁡(mj(xn;θ)+αnj)→δ(yn=i) <sep> where δ(yn=i)=1 if yn=i and 0 otherwise. <sep> Note that this trivial solution does not depend on the model m(⋅;θ). Thus, any parameter θ can be an optimal solution θ^ as long as m(⋅;θ) is finite. <sep> The above observation indicates that the proposed method do not work as expected if the training problem is solved appropriately. Thus, I conjecture that the good performances reported in the experiments are the artifact caused by the tuning of hyperparameters, e.g., the training converged to local optima that occasionally performed well. <sep> Note that the above observation on the training problem also suggests that the claim of Theorem 1 (the estimator θ^ converges to the true parameter θ∗) is not correct. <sep> In the proof, the authors considered the following objective function: <sep> LC(θ,α)=−E∑i=1K[∑k=1Kπ(i|k,x)P(k|x,θ∗)log⁡PC(i|x,θ)] <sep> Let U(i|x,θ∗)=∑k=1Kπ(i|k,x)P(k|x,θ∗). We then have <sep> LC(θ,α)=−E∑i=1KU(i|x,θ∗)log⁡exp⁡(mi(x;θ)+αi(x))∑jexp⁡(mj(x;θ)+αj(x)) <sep> By taking the derivative with respect to ω∈{θ,α}, we have <sep> ∂LC(θ,α)∂ω=−E∑i=1KU(i|x,θ∗)(∂(mi(x;θ)+αi(x))∂ω−∑k=1Kexp⁡(mk(x;θ)+αk(x))∑jexp⁡(mj(x;θ)+αj(x))∂(mk(x;θ)+αk(x))∂ω) =−E∑i=1K(U(i|x,θ∗)−exp⁡(mi(x;θ)+αi(x))∑jexp⁡(mj(x;θ)+αj(x)))∂(mi(x;θ)+αi(x))∂ω <sep> Thus, any θ,α that satisfy U(i|x,θ∗)=exp⁡(mi(x;θ)+αi(x))∑jexp⁡(mj(x;θ)+αj(x)) are optimal. <sep> In the proof of Theorem 1, the authors only considered a specific α, and overlooked the existence of other α that are equally optimal, which led to the wrong claim that θ^ converges to θ∗. <sep> Clarity <sep> Apart from the serious flaw above, I think the paper is clearly written and the main claim of the paper is easy to follow. <sep> Originality <sep> The use of the adjustable parameters for fitting noisy data is studied in the literature of robust learning. I would like to suggest the authors to see [Ref1] and references therein. In [Ref1], an additional penalty is imposed on the adjustable parameter to avoid the trivial solution I raised above. <sep> [Ref1] Consistent Robust Regression, NeurIPS17. <sep> Significance <sep> Because of the flaw I raised above, I think the contribution of this paper is not significant. <sep> Feedback after discussion <sep> I would like to clarify my thought here. Recall that using the weight decay is equivalent to adding the L2 regularization to the training objective. An important observation here is that the addition of the L2 regularization to the proposed objective will make the global optima non-trivial (apart from the trivial ones I raised), and there might be a hope that the new global optima has some useful properties. What this observation indicates is that the use of the L2 regularization (or weight decay) is an essential factor for the proposed method to output something meaningful. This fact also implies that the analysis of the objective function alone (without the regularization, in Section3) is no longer meaningful. Moreover, because the L2 regularization (or weight decay) is an essential factor, the tuning of its weight should have a major impact to the resulting model. I therefore think it will be important to investigate the effect of such a weight in the experiments, instead of just using a standard weight.","The paper addresses learning with noisy labels, by detecting and correcting samples with noisy labels. Reviewers had concerns about the empirical evaluations, specifically about comparing to additional methods, about hyperparameter tuning, and about the improvements being vey small. There was also a concern that the analysis of the objective does not take into account explicitly the L2 regularization induced by weight decay. Based on these concerns the paper is not ready yet for publication."
"3342 Identifying Treatment Effects Under Unobserved Confounding <sep> Summary <sep> The authors propose a representation learning method for estimating causal effects in the presence of unobserved confounding when covariates that act as proxies for a latent confounder are available. The authors connect the problem to some recent results on the partial identifiability of VAE and non-linear ICA models. The authors lay out some conditions under which the causal effect may be identifiable and propose a VAE-based model, CFVAE, that enforces some of these conditions on the estimated model. They compare CFVAE to a previously proposed VAE algorithm, CEVAE, and other methods that are designed to work under unconfoundedness. <sep> Feedback <sep> Identification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. However, this means that the bar for making contributions in this area needs to be high. Because the conditions for identification can't be falsified in a particular application, making unclear statements about when the effects of interest are and are not identifiable is very important. Readers who misunderstand will only experience silent failures and make poor decisions as a result. <sep> Unfortunately, I don't believe this paper meets this bar of clarity. I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. In particular, the observation that one might estimate a decoder up to a different affine transformation in the treated and control arms seems like a useful insight. But the results in the paper are incomplete, disorganized, and in some cases wrong. I'll list out a few general points here, then discuss some substantive issues with the paper's specific argument. <sep> General points about identifiability arguments <sep> Identifiability is not a property of the method <sep> Identifiability is a property of the data generating process and not a property of the model being used to do estimation. If the causal effect of interest is not identifiable in the process that generated the data, then using an ""identifiable model"" to estimate the causal effect will not solve the problem. <sep> The exposition in the paper seems to argue that using the right model will make the causal effect identifiable. This may just be a matter of unclear writing, but this is a broader point of confusion in the ML community, so it's important that the authors be clear on this point. <sep> Here, I think it would make sense for the authors to state what their assumptions are about the data generating process, separately from the parameterization of their model. Reading the paper, the distinction between these two layers of assumptions was unclear. <sep> Relaxing identifying assumptions is not an option <sep> In the same vein, if there are assumptions that the authors need to make to eliminate identification failure modes, then showing that the model ""works"" when those assumptions are relaxed does not inspire confidence. Unlike standard model misspecification which can be detected and debugged based on observables, making the wrong identifying assumptions in a model can result in silent failure, where the model can fit the observed data perfectly, but return a causal effect estimate that converges to the wrong place, or doesn't converge at all. <sep> If one is able to relax the identifying assumptions and still see success in experiments, this means either (a) the assumptions were unnecessary, or (b) the experiments did not probe the method well enough. <sep> Assumptions needs to be stated clearly, with implications clearly highlighted <sep> When making identification arguments, assumptions play the role of eliminating equally plausible causal explanations of the observed data, until only the true one can remain (if the assumptions are true). These are not the kinds of assumptions that eliminate exotic corner cases; instead, they eliminate cases like the most obvious explanation for the observed data like the absence of unobserved confounding. Bounding arguments like the Manski and Kallus et al papers that are cited in the introduction construct the full set of causal explanations that identifying assumptions must narrow down to a point. <sep> All of this is to say that clearly stating assumptions, and the cases they eliminate, is essential for any identification argument. In the paper as it is written now, many assumptions are made implicitly or in passing, and it is unclear which assumptions are made for illustration (e.g., the noise on the outcome going to zero) and which assumptions are essential for the argument in general. The assumptions are always framed as ""mild"" and do not highlight situations that the assumptions eliminate (i.e., in which cases they would fail to hold). <sep> Specific Concerns <sep> Balancing covariate implies that the naive estimator ""just works"" <sep> The primary identifiability result of the paper involves the assumption that the observed covariates are ""balancing covariates"", satisfying t \\indep z | x. The authors argue that this is a weaker condition than requiring that x satisfy unconfoundedness. This may be true, but the gap between the two assumptions is, at most, a set of knife-edge violations of faithfulness. In terms of estimating causal effects, simply doing the standard covariate with x and ignoring z would give the right answer. <sep> This can be shown in two ways. First, graphically, t \\indep z | x implies that there is no backdoor path through z from t to y when you condition on z. So z doesn't induce any non-causal association between y and t. Secondly, using the standard adjustment formula: <sep> \\mu_t(x) = E[ E[Y | X = x, T = t, Z = z] ] <sep> = \\int_z E[Y | X = x, T = t, Z = z] p(z | x) dz <sep> = \\int_z E[Y | X = x, T = t, Z = z] p(z | x, t) dz  (using the balancing covariate property) <sep> = E[Y | X = x, T = t] <sep> In particular, the naive regression function E[Y |  X = x, T = t] only fails in cases where p(z | x, t) \\neq p(z | x); i.e., when the distribution of the latent variable is different in the two observed treatment arms even after conditioning on x. The balancing covariate property eliminates this possibility. <sep> This also means that the VAE model specified can only generate data where the latent variable z does not introduce confounding. <sep> Other Concerns <sep> The adjustment formula in equation (2) is wrong. The last integral should be with respect to p(z | x), not p(z | x, t) (see the argument above). <sep> There is a substantive difference between estimating individual level causal effects given the observed outcome (a counterfactual query) versus estimating the CATE. The authors do divide this into ""pre-treatment"" and ""post-treatment prediction"", but the counterfactual query presents additional identification questions. In particular, whether the reported expectation is correct depends on Cov(y(1), y(0) | z, t), which is never observable. None of the identifying assumptions in the paper make any arguments about this quantity, so the models in the paper are making implicit strong assumptions here. <sep> The f^{-1}(y) notation in the paper is very unclear in the case that there is actually outcome noise for y. The function f relates the latent z to the expectation of y, not y itself. When y includes independent noise, the distribution of f^{-1}(y) does not yield the marginal distribution of z; you need to deconvolve the independent noise in y, which is non-trivial.","The reviewers noted that this is an important, interesting but difficult topic. They appreciated that the authors clarified their assumptions in the theorem statements. Nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted. They still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper."
"Non-autoregressive decoder (NAT) greatly improves translation efficiency, but often relies on iterative refinement (IR) to retain the translation performance. Unfortunately, this strategy makes the decoding efficiency of IR-based NAT much more sensitive to batch size and computing device: with larger batch size and CPU, IR-based NAT runs even slower than the standard auto-regressive decoder (AT). <sep> This paper targets at alleviating this IR bottleneck by proposing hybrid-regressive translation (HRT), which combines AT and NAT in two stages. AT in the first stage aims at offering an initial coarse target hypothesis, implemented by predicting target words at every k position, to serve as a better target context for NAT such that NAT can produce high-quality translation with only one iteration in the second stage. The authors introduce several tricks to optimize HRT, including joint training, mixed distillation and curriculum learning etc. Experiments on two WMT translation tasks (four translation directions) demonstrate the effectiveness of HRT, which consistently accelerates decoding by >50% compared to the AT baseline and yields comparable or even better translation quality. <sep> Overall, this IR issue is under-studied in the literature, which deserves more attention. But I am not fully convinced by the current experiments, especially the comparisons. <sep> My major concerns: <sep> = Firstly, only comparing with MP10 is not enough. HRT consists of two parts. The first part AT acts very similar to the semi-autoregressive model (Wang et al, EMNLP 2018), which should be treated as a baseline. The second part, NAT or MP used in experiments, involves different variants when different iterations are used. For example, Figure 2 in Mask-Predict paper (Ghazvininejad et al, EMNLP 2019) discovers the trade-off between speedup and translation quality. Could you please show the Pareto frontier to prove that HRT reaches a better trade-off compared to MP? Besides, one thing should also be noticed: the speedup yielded by HRT seems less charming compared to recent NAT models, such as Levenshetain Transformer 3x-4x (Gu et al, NeurIPS 2019), and JM-NAT (k = 10) 5.73x (Guo et al, ACL 2020) where both models achieve similar translation quality to the AR baseline. <sep> = If I understand correctly, the advantage of HRT in handling different batch sizes and computing devices mainly comes from its usage of one-pass NAT. In Figure 3, we observe that HRT1-1 produces clearly larger speedups as batch size increases from the CPU graphs. However, we observe a reversed trend with MP1 in Figure 1. Could you give readers some explanation? <sep> = Although this paper mainly focuses on improving NAT, efforts on accelerating decoding is not limited to NAT. Relevant studies, which should be at least discussed in related work, include model quantization [1] and simplified decoder [2, 3], to name a few, are all missed in the paper. In particular, some models like AAN [2] and Deep encoder+Shallow decoder[3] can already produce similar speedups (might be smaller) with comparable translation performance. These models do not rely on knowledge distillation and those complex optimization tricks used for HRT. Could you please explain the advantage of HRT over these models? <sep> = The claim ""Chunk is superior to Rand, which indicates that the balanced distribution of deterministic tokens is necessary"" (above Section 4) is strong. From Figure 2, what I see is that Chunk performs slightly better than Random. It's hard to conclude that the balanced distribution is necessary. <sep> My minor concerns: <sep> = How did you handle the positional encoding in AT of HRT? Did you consider the interval k? <sep> = The proposed optimization tricks are somehow orthogonal to HRT itself. What if you apply them to the baselines, like MP and semi-autoregressive model? <sep> = How many runs did you perform when reporting the speed numbers in Figure 3? <sep> = I cann't find the number 34.08 (Table 3) in Table 2 for En->Ro translation. Is there something wrong? <sep> = The citation format should be adjusted accordingly. The authors always use the format of ""author (year)"". <sep> = It would be better to see more results on the translation of distant language pairs, such as WMT English-Chinese. <sep> [1] Bhandare et al.  Efficient 8-bit quantization of transformer neural machine language translation model. 2019 <sep> [2] Zhang et al. Accelerating neural transformer via an average attention network. ACL 2018 <sep> [3] Kasai et al. Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation. 2020","This paper proposes a new method to combine non-autoregressive (NAT) and autoregressive (AT) NMT. Compared with the original iterative refinement for non-autoregressive NMT, their method first generates a translation candidate using AT and then fill in the gap using NAT. <sep> All of the reviewers think the idea is interesting and this research topic is not well-studied. However, the empirical part did not convince all the reviewers. The revised version and response is good; however, it still does not solve some major concerns of reviewers."
"############################################################################### <sep> Summary <sep> The paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. The paper claims this new method provides a significant improvement over state-of-the-art branching strategies, either based on expert-designed rules or on imitation learning of strong branching. <sep> ############################################################################### <sep> Pros and cons <sep> Pros: <sep> the idea of learning branching strategies using reinforcement learning instead of imitation learning makes a lot of sense and seems like a promising direction to follow the proposed representation of B&B trees is original the presented results seem promising <sep> Cons: <sep> the experimental setup followed in the paper is questionable the presented experimental results are inconsistent with the literature, which I find suspicious the overall method description comprises several blind spots and fallacious arguments <sep> ############################################################################### <sep> Recommendation <sep> While I like the idea pursued in the paper, and I believe the proposed method might be promising, the paper is mostly experimental, and is not sound enough on that side. I therefore recommend rejection of the paper. <sep> First, I found the reported experimental results suspicious. The paper is mostly based on the work from Gasse et al., reuses the same code, (almost) the same problem benchmarks, and some portions of text which are identical. This is fine, but then why did the authors conduct experiments only on 3 out of the 4 available benchmarks ? And why are the results inconsistent with those reported in the original paper ? In the presented experiment the SVM model consistently performs better than the GCN model in terms of number of nodes. Why is it so, if literally the same code has been reused, as mentioned in the appendix ? This alone raises serious doubt in my mind about the validity of the reported numbers. At the very least the authors must provide an explanation for that. Also, Figure 4 indicates that a pre-trained GCN model results in a tree size of 350 on independent set problems, which does not coincide with the number 418 reported in Table 1. <sep> Second, the experimental setup itself is questionable. The authors consider two setups for the SCIP solver, ""clean"", with depth-first-search node selection and all other functions disabled (whatever that means), and ""default"". No explanation is given as to why those two setups are considered. Some experiments are conducted under the ""clean"" setup (Table 1, Figure 2 and 3), other are made under the ""default"" setup (Table 2), and for some experiments this information is missing (Figure 4). What is the purpose of the ""clean"" setup ? It is known that node selection and branching strategies do interact with each other, and all the evaluated branching methods were proposed in the context of a default, state-of-the-art solver. Why then comparing such methods in the ""clean"" setting ? What is the justification here ? Also, the paper lacks a proper ablation study. How much of the reported improvements come from the policy model ? From the RL training ? From the proposed novelty measure ? <sep> Third, I found several arguments to be fallacious. The authors claim their method results in non-myopic policies, without defining what they mean by that. Assuming they mean the policies have access to non-local information, they later on contradict this claim, by acknowledging that the proposed model only processes the local node's LP-based information. The authors also argue that imitating strong branching (SB) is not a good idea, as it results in small trees not because of its branching decisions, but because of side-effects. To back-up their claim, the authors present numbers showing that, once those side effects are removed, the SB tree size is much higher. But in the same table, the GCN model trained to imitate SB results in trees much smaller than the expert (418 vs 1304 on independent set), which again is suspicious, and most importantly contradicts the original claim of the authors. Given those numbers, imitating SB looks like a good idea. <sep> Finally, the proposed method comprises blind spots. The authors present their model as a primal-dual iteration policy, for what looks like a simplified GCN architecture for the one from Gasse et al. The same structure (bipartite graph) and the same features are used. The authors present a Lagrangian dual LP formulation in Equation 4, however I do not see the connection to the proposed model. How is the proposed model a primal-dual iteration policy ? The authors then present what is I believe the most original contribution of the paper, a distance metric for branch-and-bound trees. However, again I found the description of the method rather sloppy. Counting the number of integral points inside a polytope is a hard problem in itself. Is the counting performed over box relaxations only ? If so, how can the proposed representation process MILPs with unbounded variables ? This seems to be a strong limitation to me, which at least deserves a discussion. <sep> ############################################################################### <sep> Questions to authors <sep> I would appreciate if the authors could clarify why they conduct some of their experiments in the ""clean"" setting, why they don't report results on the complete benchmark from Gasse et al., and also comment on the performance of SVM vs GCN, which contradicts what is reported in Gasse et al. <sep> ############################################################################### <sep> Final recommendation <sep> I thank the authors for their detailed response. The authors have clarified some technical details and blind spots of their methods, have fixed their evaluation metric which was wrongfully comparing tree sizes on solved and unsolved instances, and have presented an additional experiment in the appendix on the original benchmark from Gasse et al. These changes are going in the right direction. However, I remain concerned about the experimental setup in the paper, and therefore my final recommendation is still rejection. <sep> First, I still find suspicious that the main experiment in the paper is conducted on only 3 (modified) benchmarks out of the 4 proposed in Gasse et al. The authors claim they did not run experiments on the 4th benchmark due limited computational resources, but at the same time they present complete results on the 4 original benchmarks in the appendix. Therefore I believe the explanation given by the authors is fallacious. Results on the 4th (modified) benchmark, and even better, on other additional benchmarks, would be much more convincing and alleviate any doubt about cherry-picking. <sep> Second, I am not convinced by the argument the authors present to justify their two solver settings: clean and default. I do agree that challenging problems should be solved under the default setting, however I do not see why decision quality should be measured using the clean setting. Decision quality matters in the default setting as well, and one could argue it matters in the default setting only, if the final goal is to improve the solving time of the solver on challenging problems. Moreover, if the authors want to use the tree size as a mean to measure branching decision quality, they must also provide the optimal solution value to the solver at the beginning of the solving process, in order to deactivate side-effects from pruning. See G. Gamrath and C. Schubert, 2018, ""Measuring the Impact of Branching Rules for Mixed-Integer Programming"". <sep> Last, the method proposed by the authors seems to be effective only in the specific benchmark they propose. In the additional experiments they present in the appendix (Table 4), their method does not convincingly improve over the original method from Gasse et al., as the performance gains on the Easy training instances degrade rapidly as one moves away towards the more challenging Hard instances. The very name of the paper, ""Improving Learning to Branch via Reinforcement Learning"", seems to claim that reinforcement learning improves existing learning to branch methods. However, the improvement observed by the authors is very specific to the ""backbone"" setting they propose, and does not seem to translate to the original benchmarks of the methods they compare to. The authors justify their choice of benchmarks saying ""we focus on a more realistic industrial setting"", but I am unsure whether the hypothesis of a ""backbone graph"" is particularly realistic in industrial applications. I do not think the original benchmark from Gasse et al. is particularly realistic either, however I would not give more or less value to either one of the two. As such, I believe it is crucial that the authors report experimental results on both benchmarks in the main body, and provide a discussion as to why RL seems to bring improvement in the restricted ""backbone graph"" benchmarks, but not in the original ones which have more variability. This, in my opinion, would have a much higher scientific value than simply presenting both a new method and a new benchmark, while disregarding how the method performs on previous benchmarks from the literature. <sep> In light of the changes made by the authors I am willing to raise my rating, however I still recommend rejection for *CONF*. <sep> ############################################################################### <sep> Additional feedback p.2 §2: a linear programming -> a linear program p.2 §2: efficiency-effectiveness trade-off -> What do you mean by efficient and effective ? What is the difference ? Do you mean the decision quality / computation time trade-off ? <sep> p.2 §3: ignores the redundant information and makes high-quality decisions on the fly -> This is quite vague. How does your policy achieve that ? What redundant information are you talking about ? What is the efficiency-effectiveness trade-off ? <sep> p.2 §3: For exploration, we introduce a new representation of the B&B solving process -> How is this new representation related to exploration ? I am missing the argument here. <sep> p.2 §5: set covering, maximum independent set, capacitated facility location -> In Gasse et al. 2019, to which you compare as a baseline, there is a fourth benchmark, combinatorial auctions. I must say that not reporting experiments on this complete benchmark is suspicious, as I do not see a reason for not doing it. <sep> p.2 §4: primal-dual policy -> Where does this name come from ? Why is your policy primal-dual ? <sep> p.2 §5: the long-term overestimation of strong branching -> What does that mean ? <sep> p.2 §5: based on primal-dual iteration over reduced LP relaxation -> What does that mean ? <sep> p.3 §2: in line 2 -> line 3 I believe p.3 §2: visulization -> visualization p.3 Equation 3: I am not sure I understand this formula. I suppose that ϕt=(θt,σt) ? Then σt is never updated, but only θt is ? What is ϵi here ? I suggest that you clarify the meaning of this equation, which I could understand only after reading Wierstra et al. 2014, Natural Evolution Strategies. Making it explicit what it corresponds to (expected gradient δR(θ)δθ over the population) would greatly help the reader. At the very minimum, all terms should be defined properly. <sep> p.3 §4: if strong branching finds [...] -> I hardly understand that sentence. Do you mean that strong branching will induce additional domain propagation (Achterberg 2007, Constraint Integer Programming, §2.3) ? <sep> p.4 §2: we set the reward rt=−1 with discount γ=1 -> Maximizing such a reward is equivalent to minimizing the B&B tree size. And as a result it aligns with your evaluation metric (nodes). This should be mentioned, again, for clarity. <sep> p.4 Section 4.2: Primal Dual POlicy Net -> I don't really understand this primal-dual thing, given that you use the same features as in Gasse et al. (Table 3) and just replace their GCN model by a simpler, underparameterized version (A.2.2). <sep> p.4 Equation 4: I do not see the point of introducing a Lagrangian relaxation here. Neither Equation (4) nor λ are ever used or referred to in the text. <sep> p.4 §4: fC,fC -> fC,fV <sep> p.4 §4: one layer -> one hidden layer ? <sep> p.4 §4: For efficiency, we do not include problem set S, which makes it a partial observable MDP -> Then your model only has access to local information, and as such results in myopic policies, which contradict one of your initial claims. Myopic by definition means your vision is limited, which is the case here if your model can not observe the full state of the solver but only the local LP.  Unless you mean something else with ""myopic"", which does not have a formal definition in the paper. I think you confuse myopic policies with greedy policies. <sep> p.5 §1: as the collection of those leaf subproblems -> Which leaf subproblems ? The leaves of the complete B&B tree ? The partial tree ? <sep> p.5 §2: w(Ri):=… -> I understand Q is the original MILP, while Ri is a box. Q restricted to Ri is therefore a regular MILP as well (the local MILP). Counting the number of feasible solutions for a MILP is I believe an NP-hard problem. How do the authors afford to do that ? Do you mean that you look for integral solutions within Ri ? A second comment here: What if some variables in Q are unbounded ? Your weight function does not seem to handle that case... <sep> p.5 §2: the feasible solution -> a feasible solution ? <sep> p.5 §3: Notice that a polytope in the set representation is invariant with the generating order [...] -> This sentence is ambiguous, and is simply not true. Branching on x1 then x2, depending on whether it is the left child or the right child which is subsequently branched on, does not yield the same collection of polytopes. <sep> p.5 §3: pruning behavior -> What is meant here by pruning behavior ? Node selection ? A common procedure to compare branching strategies, is to removes potential side-effects from node selection by providing the algorithm with the optimal objective value from the start. See, e.g., Gamrath and Schubert, 2017, Measuring the impact of branching rules for mixed-integer programming. In this scenario where the goal of B&B is just to close the dual gap, the branching strategy remains a major component of B&B.  This seems to contradict your point here, that your novelty measure is both relevant for branching, while it is mostly driven by the pruning behavior. <sep> p.6 §3: we have two settings -> Why having those two settings ? What is the point of the ""clean"" setting ? Also, I think you mean depth-first-search, not deep-first-search. <sep> p.6 §7: under clean setting -> Why under that setting ? Would the ""default"" setting be more representative for evaluating the performance of branching strategies ? It is known that branching interacts a lot with node selection, which you arbitrarily changed here to depth-first-search. Is there a reason for that ? <sep> p.7 Table 1: I suggest that you group FSB and RPB together, since they can not be compared to the other methods in terms of number of nodes (unfair node counting). Also, the RPB Navg value should not be bolted, since RPB can not be compared to here in terms of nodes. <sep> p.7 §1: under the default setting -> Why do you suddenly switch to the ""default"" setting ? <sep> p.7 Table 2: Those numbers are very doubtful. Why does the GCN model result here in larger trees than the SVM model, while the opposite is observed in Gasse et al. ? This should, at the very least, be discussed. <sep> p.7 §3: founded -> found p.8 §2: Then, we check dual value c^ -> If you want to assess the capacity of each strategy for closing the dual gap, why not simply reporting the evolution of c^ over time ? <sep> p.8 §2: our RL agent successfully employs a non-myopic policy to maximize c^ in the long term -> I do not see that from the curves... I only see that the tree size is smaller, and distributed differently that with the other methods. <sep> p.8 Section 5.5: This ablation study is missing a key ingredient: what is the performance of the PD model, trained via imitation learning ? Which part of your improvements comes from your PD model ? Which part comes from RL ? <sep> p.8 Figure 4: Why don't those numbers align with those in Table 1 ? Is the ablation study conducted in the ""clean"" setting ? Why do all curves in Figure 4 start at 350 nodes, when the optimal GCN model in Table 1 is at 418 nodes ?","The paper describes an RL technique to learn how to branch in discrete optimization. This advances the state of the art in comparison to previous imitation learning techniques. However, the reviewers and a public reader raised concerns about the validity of the experiments due to several inconsistencies and differences with previous work that might suggest some cherry picking. This is too bad since the reviewers really liked the work, but it is important to make sure that the experimental evaluation is done fairly. I read the paper and I share the concerns regarding the experimental methodology. Hence the experimental evaluation needs to be revised before publication."
"This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. <sep> The idea of using teacher model's classifier to directly reshape the student model's feature representation is somewhat novel. It considers the situation of single teacher model and multiple teacher models. The teacher ensemble is achieved by concatenating features from each of the teacher model and then conducting dimension reduction using PCA. The methodology illustration is simple yet clear. There are multiple experiments on major face recognition datasets and demonstrate superior performance against baselines such as L2KD-s. <sep> Regarding the concerns, I am listing them into bullets. <sep> why the experiments make the setting of templates using teacher model to extract feature, while the query using student model to extract feature? <sep> Would the comparison of using student model for extracting both template and query feature be possible? It can provide a direct comparison to other methods, i.e. ArcFace trained using ResNet18 compared to the student model with ResNet18. <sep> Current experiments lack the comparison to the state-of-the-art methods, i.e. ArcFace and CosFace. <sep> the ablation is emphasizing on the ProxylessKD combining with different losses. It does not consider the knowledge distillation itself. For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD? Meanwhile, in many KD papers, there are also intermediate layer feature distillation, would it harm the overall performance under this paper's setting? It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation. <sep> In ablation, how would the number of teachers influence the student performance? Meanwhile, how would the network architecture influence the student performance? i.e., fixing the teachers to be the same, while varying student architecture with multiple hypothesis, i.e., ResNet, AttentionNet, DenseNet? It is good to know what specific architecture is favored under the authors' proposed framework.","This paper presents a knowledge distillation method for face recognition, by inheriting the teacher's classifier as the student's classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated ""Ok but not good enough - rejection"", 1 rated ""Marginally below"" and 1 rated ""Marginally above"". The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers' comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state."
"SUMMARY OF CONTRIBUTION: <sep> This work presents a pair of algorithms, PeerRL and PeerBC, which address certain forms of noise in reinforcement learning and behavioral cloning respectively.  For PeerRL, an arbitrary RL algorithm trains on an augmented reward signal, which is generated by subtracting a past reward signal (sampled randomly from the replay buffer) from the current reward.  PeerBC uses an augmented maximum-likelihood behavioral cloning loss that penalizes a policy for log-probability of a randomly sampled action for a randomly sampled state.  Empirical results show that Deuling DQN with the PeerRL reward outperforms the base DDQN algorithm in a noisy-reward version of the cart-pole task.  They also show that PeerBC outperforms standard behavioral cloning in learning from synthetic demonstrations on several Atari games, as well as the cart-pole and Acrobot tasks, all with noisy versions of the base reward signals.  The also evaluate PeerBC in in combination with reinforcement learning in a co-training setup, demonstrating a significant advantage over co-training with standard BC. <sep> AREAS OF CONCERN: <sep> The main concern with this work is that the gains in performance observed with the PeerRL and PeerBC algorithms may not actually reflect the ability of these methods to correct for a specific type of noise.  For PeerRL, the modified reward is fact that PeerRL is able to outperform the version of DDQN with access to the true reward (Figure 2) suggests that at least part of advantage of PeerRL is better scaling of the reward signal, rather than noise reduction.  It would be helpful to compare PeerRL against noiseless scales and shifts of the true reward.  It also seems likely that simply adding a constant baseline value to the noisy reward would help reduce the variance of the return (PeerRL subtracts a noisy baseline value).  Comparing against this simpler variance reduction method would help us understand the importance of the specific form of the augmented PeerRL reward in improving performance. <sep> For PeerBC, the concern is that the modified loss simply biases against high-entropy policies.  When the true policy is itself stochastic, the learned policy found by PeerRL may end up being the mode of this policy.  The advantage of PeerBC observed in Figure 3 may result from the fact that less noisy policies perform better in these tasks.  The fact that PeerBC outperforms the expert policy in Enduro (Figure 3c) would seem to support this hypothesis (the expert's true policy is more stochastic than the learned policy).  It might be helpful to compare PeerBC to a simpler approach which learns a stochastic policy (under the standard maximum likelihood objective), but always takes the most probable action under this policy during evaluation. <sep> While the authors claim to address the case where the rewards or demonstrated actions are perturbed by some arbitrary confusion matrix.  The theoretical and empirical analysis of PeerRL however are limited to the case of binary rewards. Similarly, the theoretical analysis of PeerBC appears to limited to the case of binary actions, though empirical results consider larger action spaces. <sep> More generally, it is not clear that the noise models (for RL and BC) capture the challenges typical of RL and imitation learning (discussed in the first paragraph of the introduction).  In RL, the issue is often not that the reward is noisy in a fixed state, but that it is sparse within the state space, such that the return under a random policy is noisy.  For behavioral cloning, errors in human-generated data often reflects the fact that the human demonstrator has actually selected a suboptimal strategy, rather than the noisy execution of an optimal strategy.  Distributional shift in behavioral cloning does not reflect noise in the training data itself, but instead reflects the compounding error that occurs when we execute an imperfect policy for many timesteps. <sep> These issues with the noise model could be addressed by making it clear earlier in the document exactly what noise models are being considered, and providing examples in the introduction of settings in which such noise would be expected in the rewards or demonstration data. <sep> The authors attempt to describe their approaches to handling noise for RL and BC as instances of a more general peer evaluation algorithm.  The relationship between the RL and BC solutions appears to be superficial, however, as PeerRL and PeerBC seem to address noisy supervision in very different ways.  Furthermore, there are no theoretical results presented that apply to the general algorithm.  The derivation of the RL solution from the common framework (Equation 2) is incomplete, as the loss Eva^RL is never made precise.  It would likely be more clear to the reader if PeerRL and PeerBC were presented as separate, but analogous algorithms for their respective learning problems. <sep> There are also some apparent technical issues with the theoretical results: <sep> At no point is it specified that the error rates must be less than 50% for binary rewards or actions. Theorems 1 and 2 are clearly wrong if this condition does not hold.  While and attentive reader should be able to infer this constraint on the noise model, it should be explicitly stated. <sep> The r_peer term (the true peer reward) in Lemma 1 is never defined. <sep> It is unclear what the variable theta in tau_theta refers to in section 4.2. <sep> There seems to be an error in the proof of Lemma 1. The decomposition of the noisy reward in lines (7) and (8) seems to assume that, even in the absence of noise, the expected reward signal will be zero, regardless of the policy being followed, or the current state.  This decomposition should incorporate the probability of true reward given the current state (or averaged over states given the current policy).  This may mean that Lemma 1, and potentially Theorem 1, are incorrect (though it is likely that this can be corrected without substantially changing the contribution of the paper). <sep> CONCLUSION: <sep> Even though the work considers a relatively constrained class of noise models, there are still practical cases where such noise in rewards (and particularly in demonstrated actions) is an issue, and effective methods for dealing with such noise are potentially valuable.  The issue with this work is that it is not yet clear that the proposed methods are actually effective in mitigating these types of noise in RL or BC.  It seems likely that at least part of the improvement in performance stems from other factors, either the preference for more deterministic policies in PeerBC, or the rescaling of the reward signal in PeerRL.  Without resolving these issue, it is impossible to know whether the proposed methods will actually be useful in settings where we have noisy rewards or demonstration data.  There are also technical issues which raise doubts about the validity of the theoretical results presented in this work.","The reviewers had some initial concerns about this submission. While the authors' rebuttal does a good job to address these concerns, the reviewers still have some doubts about the contribution of this paper and potential impact. In particular, it is not clear whether the performance improvements observed with the proposed algorithms is due to the ability to correct for noisy rewards or whether there are multiple other explanations for the improvement in performance. This makes it hard to predict whether the proposed algorithms will actually be useful in settings where noisy rewards or demonstration data are present."
"The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. The proposed dataset consists of three sets of video sequences, procedurally generated, which are either generated from slight variations of existing works (Sprites-MOT) or on the basis of existing datasets (dSpirites, Video Object Room). For evaluation, authors propose to use a slight variation of the protocol of the MOT challenge for evaluation (with the addition of a Mostly Detected measure which does not penalize ID switches). As part of the  paper, they also evaluate and discuss the performances of four object-centric representation models, one of them (Video MONet) being an extension of an existing approach, proposed as part of this paper, and the remaining being state of the art approaches for the task. <sep> Paper Strengths <sep> Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. Potentially, this can be useful for the community and can help to create a common ground for evaluation. <sep> The benchmark is comprehensive, in that it contains both data and evaluation measures. The adoption of MOT metrics also appears to be a reasonable choice. <sep> The comparison between different models is interesting, and authors have also added a custom designed novel method (Video MONet). They investigate a set of challenging scenarios and carry out out-of-distribution tests. <sep> The paper is a pleasure to read, and authors have also made a good effort in writing a clear and comprehensive supplemental. <sep> Paper Weaknesses <sep> My main concern about the paper is the lack of novelty. While I realize that the objective of the paper is to create a common evaluation background, I fail to see a sufficient level of quality and of novelty. In particular: <sep> the dataset associated with the benchmark are mostly based on existing works - they might be appropriate for evaluating this task, but the level of contribution is a bit limited; <sep> on the metrics, the only contribution is to suggest using MOT metrics, which are again pre-existing; <sep> the experimental and the insights it gives, again, can foster the community towards better model, but it's not a sufficient contribution for *CONF* in my view. <sep> Overall, I think the paper could be a nice contribution to the literature on unsupervised learning of object-centric representation, but it lacks sufficient contribution for *CONF*, in my view. I would therefore suggest to reject the paper.","This paper received 4 reviews with mixed initial ratings: 7, 5, 4, 5. The main concerns of R1, R2 and R4, who gave unfavorable scores, included: lack of methodological novelty (analysis-only paper), absence of experiments on real data (3 synthetic-only benchmarks), missing baselines and an overall inconclusive discussion. At the same time R5 notes that the offered fair comparison between SOTA methods was indeed ""much needed"", and the paper can ""serve an important role"" in guiding future developments in the community. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. R1, R2 and R4 did not participate in the discussion, and R5 stayed with the positive rating. <sep> AC agrees with R5 that the provided analysis is insightful, and the effort put into organizing the research community around a single set of benchmarks and metrics is indeed valuable. However, given a simplistic nature of the proposed datasets and lack of other methodological contributions, the submission is not meeting the acceptance bar for *CONF*. After discussion with PCs, the final recommendation is to reject."
"Strengths <sep> The authors target an important problem in Federated Learning: how to personalize the model to mitigate the nonIIDness. <sep> Weakness <sep> The proposed method is not novel. The third step which fine-tunes in the local and global models using a gate network is essentially fusing the global and local models. It is surprising to me that this method works better than fine-tuned after FedAvg. Most importantly, such an empirical method lacks analysis or convincing experimental results. <sep> Hyper-parameters are not well-discussed. The author mention that all experiments use the same learning rate 0.0001. This is definitely misleading. We have to adequately tune the hyper-parameters for each baseline and then make a fair comparison. Using the same learning rate for all baselines are wrong experimental settings. I believe fine-tuning after FedAvg can even get comparable performance if fully tune the hyper-parameters (learning rate, decay, batch size, epochs, rounds, etc). <sep> The authors claim that ""client and global models are not constrained to be the same model and could be implemented any two differentiable models."" However, the authors do not provide the experimental result for this argument. I guess when the model architecture is different, the difficulty of hyper-parameter optimization will increase, which weaken the application of the proposed method. <sep> The proposed method has a severe efficiency problem. It requires holding three DNNs at the edge. This is impractical in federated learning where the edge devices are mainly resource-constrained (low memory, low computational ability) <sep> The training time is not mentioned. <sep> The proposed method does not use a client sampling strategy, a common practice in cross-device FL, to mitigate the scalability issue. What the performance if we want to learn 10 thousand sensors? Please check the original FedAvg for details. <sep> The dataset CIFAR10 and CIFAR100 are not difficult enough to demonstrate the concept of the proposed algorithms. Does it still work in a high-resolution setting like ImageNet (224*224). I believe training three DNNs will lead to serious efficiency issues. <sep> The opt-in and opt-out strategy is totally empirical without any intuition about why it works. That the author connects this strategy with a privacy guarantee is somewhat misleading to readers. Please provide an analysis in revision and properly describe the benefit. <sep> In the Introduction section, the following argument is a lack of evidence. Please cite related works to make the argument more convincing. <sep> ""Extended phases of local training between communication rounds can similarly break training, indicating that the individual client models will over time diverge towards different local minima in the loss landscape. Similarly, different distributions between client datasets will also lead to divergence of client models"". <sep> The overall writing does not affect my understanding but can be improved. <sep> Related works are not fully discussed. In some knowledge distillation-based method, the personalization is also their benefit. For example, FedGKT [1] also has a personal client model and a global server model, which is just a single step training method without the need of multiple steps training. <sep> [1] Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. https://arxiv.org/abs/2007.14513 <sep> Overall Score <sep> Given the above concerns, I recommend reject this paper in the current stage. <sep> Questions <sep> May I have the comparison results between ""the naive fine-tuning after FedAvg"" and the proposed method? Please fully tune hyper-parameters for each baseline. <sep> Suggestions <sep> I encourage the authors to do a deeper analysis and a better experimental design. If all the above concerns are addressed, I am happy to increase the score.","The paper presents a personalized federated learning approach using a mixture of global and local models. Four reviewers evaluated this paper; one of the reviewers is luke-warm (6) while the rest of the reviewers pretty negative to this work (3, 3, 3). The reviewers pointed out many weaknesses, especially about novelty, motivation, contribution, presentation, etc. Most importantly, although the idea of a ""mixture of experts"" makes sense, it is not clear what the real technical contribution of this paper is in terms of federated learning. <sep> Considering all the comments by the reviewers, I believe that this paper is not ready yet for publication. The authors need to improve the novelty and technical soundness of the proposed direction to convince the readers including reviewers."
"This paper proposes a method for generating policies in cooperative games, using a neighbourhood-based factorisation of reward, and an iterative algorithm which independently updates policies based on neighbour policies and then propagates the policy to neighbours using function space embedding. <sep> The experimental results looked promising, so there seems to be an idea here worth communicating. <sep> The paper was very hard for me to follow. I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc.). Instead the main body of the paper felt like a collection of pieces that were used when developing the algorithm. I would suggest it might be easier to follow if written from the top down, instead: present a high-level overview of the idea, give a (detailed!) description of the algorithm, the experiments, and leave the derivation to the appendix. <sep> Despite being in the appendix, the algorithm is less than half a page, and doesn't explain the variables. eta and kappa might be described elsewhere, but it would be helpful to reference where. J is a loss: which one? <sep> One of the claimed contributions is this is principled method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statemen <sep> One of the claimed contributions is this is principled method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in? <sep> t: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in? <sep> Another claimed contribution is computational efficiency. How does the computational cost compare to the baselines in the experiments? <sep> Proposition 1: ""The optimal policy has the form ... 1/Z exp(...)"" <sep> I found the use of optimal slightly hard to follow throughout this. The usual definition of optimal policy would be a value maximising policy, which would be an argmax rather than a softmax. Following that definition, this proposition wouldn't be true, so it seems like it needs more explanation, or more careful wording. <sep> The cited PRL article (Levine 2018) seems to retain this standard use of optimal: it uses a distribution over trajectories with an equation similar to here (a softmax over accumulated trajectory rewards), and makes use of the property that trajectories corresponding to an optimal policy have maximum probability in that distribution. <sep> Can the authors clarify this use of optimal? <sep> Proposition 1: <sep> For clarity, explain the intention of psi. Is this the future accumulated reward given the current state and selected action? <sep> =-= comments after author discussion <sep> The authors were quite active in editing the submission, and addressing the concerns I had. I still find the paper a bit hard to follow, but none of my original concerns remain.","The paper describes a framework for multi-agent reinforcement learning that uses Markov Random Fields. Unfortunately, the paper is not clearly written and would benefit from significant revisions that improve its structure and make the model and approximations more explicit. <sep> In particular, the paper says a graph says which agents i,j communicate. This is typically called the ""coordination graph"" in this setting, see <sep> ""Collaborative Multiagent Reinforcement Learning by Payoff Propagation"", Kok and Vlassis, 2006. Note that within that paper they provide Q-function decomposition, which can only serve to approximate the optimal policy. <sep> The authors of this submission claim that an MRF is sufficient for optimal policies. I fail to see how this is true. In particular, Proposition 1 has to be checked more carefully. I tried to go through it, but it did not seem to make sense to me. Why is there an exp() term in the definitoin of the optimal trajectory probability? Why would minimising the KL divergence be enough to obtain an optimal policy? Perhaps it gives an optimal policy within the class of MRF policies, but that's not the same thing as the globally optimal policy. <sep> Overall, I find the lack of clarity and in depth discussion of early related work disturbing, particularly with respect to the theoretical claims in the paper."
"This paper proposed a set of methods for post-training quantization of dnns. The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. The authors presented promising experimental results on various neural networks to support the proposed methods. <sep> However, there are serious concerns about these claims as follows: <sep> AdaQuant <sep> It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. But this joint optimization would also increase the search space (at least) quadratically, resulting in significant computational cost. Note that the biggest merit of post-training quantization is its simplicity (cf., QAT incurs full-blown training epochs); thus increased cost for post-training quantization is not desirable. Since AdaQuant is the major claim, the authors should provide more discussion on how they dealt with this increased complexity. <sep> The authors claim that AdaQuant avoids overfitting, but the reason does not seem to be clear. There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there's infinite solution when B << N? how B>= Ck^2/(HW) is derived for the convolution case?) <sep> Also, it seems that the ""per-channel"" quantization method is utilized in this work, but the formulation in (2) seems to be for ""per-layer"" optimization. Why they are different? <sep> How much time does it take to solve this joint optimization? <sep> Integer Programming <sep> The authors proposed an Integer Programming formulation, but there seem to be missing information: what is the formulation of the penalty function, ""deltaL""? The authors described it simply as ""Loss"", but it is not clear what the exact method it is calculated. In fact, deltaL can be pretty complex functions, which might not be independent terms for each layer; thus the formulation like (3) might not be correct. Note that the impact of quantization in the earlier layers affect the quantization impact in the current layer. Without clear explanation and justification about it, the proposed IP formulation does not make sense. <sep> The authors mentioned that deltaP should be additive and sum up to the total benefit. How can one guarantee it? <sep> Also, it seems that the complexity of the IP optimization increases as the number of layers increases. How much computation time increases if the number of layers are large? <sep> Batch normalization tuning <sep> Unfortunately, there is a very similar idea proposed by [Sun et al., NeurIPS 19]. Cf. ""Sec.3 Trans-Precision Inference in FP8"". <sep> Also, there are several suggestions to improve understanding of readers. <sep> Currently, the ablation study looks very confusing. It is not clear which of the pipeline options (light, advanced?) include what kinds of techniques. Please do specify (maybe in a separate table) the list of techniques covered by different pipeline options. <sep> The proposed method is not much evaluated by various neural networks. It would be desirable to expand the coverage of neural nets as much as the prior work did. <sep> Currently, the proposed methods only utilized ""per-channel"" quantization. How much accuracy the proposed methods can maintain if they adopt ""per-layer"" quantization? <sep> What is the definition of ""compression ratio""? (typically compration RATIO is like 12:1, and compression rate is like 2X, 3X...)","This paper received mixed reviews, 3 positives (7, 6, 6) and 2 negatives (4, 4). Due to the divergence of the reviews, I carefully read the paper and made my best efforts to understand the paper and the review comments. This paper proposes to learn a quantization network using a small calibration set given a network trained with the full precision. The combination of AdaQuant, integer programming, and batch-norm tuning makes sense although they do not have substantial novelty. The three components are reasonably tightly-coupled and comprise a complete algorithm. However, the sequential-AdaQuant distracts the main claim of this work significantly. This is probably added during the review process but looks ad-hoc to me. Sequential AdaQuant seems to be effective to improve accuracy, but cannot be applied before the bit allocation was set, which makes it require integer programming no more. Because of this issue, the overall presentation becomes confusing and the argument sometimes sounds unfair (please refer to the last posting by R5.). <sep> In addition, the presentation of this paper could be improved, especially for the details of the integer programming formulation. It is not clear how to define some variables mathematically. The discussion about the size of the calibration set together with the overfitting issue is lacking, and rigorous discussion and analysis would make the paper much stronger. The reviewers are not convinced of the novelty of this paper, and they rather believe that this is an engineering-oriented work. Considering this fact, the evaluation of this paper is not very comprehensive. The ablation study with respect to the size of the calibration set should be conducted more intensively. The experiment fails to show the benefit of mixed precision quantization effectively and it is limited to presenting the compression ratio in Figure 3. The authors used a small calibration set taken from the training dataset, which looks weird because they claim that the post-training quantization requires only a small ""unlabeled"" calibration set at the beginning of the abstract; it is more desirable to use arbitrary examples in the same domain. <sep> Despite the interesting aspects, I believe that this paper needs a focus and substantial improvement for publication, and, consequently, recommend rejection."
"Strengths: <sep> To my knowledge, this is the first credible effort to apply Hamiltonian Monte Carlo to Q learning in order to avoid some degree of the random sampling required for its almost sure convergence. At least the algorithm novelty is apparent. <sep> The experiments clearly demonstrate the merits of the proposed importance sampling scheme for obtaining improved convergence of Q learning, and for yielding coverage of the space comparable to exhaustive sampling. <sep> The idea of using importance sampling during training is a key distinguishing feature from a majority of other works which use it for, upon the basis of a fixed prior policy, improving the current policy. <sep> Weaknesses: <sep> The preliminaries section is disjoint/fragmented. The limitations of Q learning (equation (1)) should directly motivate use of Hamiltonian Monte Carlo (HMC) in Section 2.2, but instead the manner in which HMC is presented is as additional preliminary material. This is not inherently disqualifying, but a number of exotic terms are introduced in the ``preliminaries"" Section 2.2 without explanation or otherwise connection to the RL problem, such as momentum variable, leapfrog integrator, Hamiltonian, etc. Why are these entities pertinent to MDPs and Q learning? This needs to be more carefully and conscientiously connected. A similar comment is true for Section 2.3 -- matrix completion or the need for addressing matrix estimation problems is nowhere to be found in 2.1 and 2.2, which leaves the reader confused as to why matrix completion is being discussed. This overall disjointedness then makes the conceptual innovation of this work more mysterious to understand, which is a concern. <sep> The reasoning in the paragraph before Section 3.1 seems incorrect. The fact that samples in a limited pool of IID samples concentrate around the region with high probability density is not evidence that this is a poor estimate for the expected value, but rather that the expected value is not representative of a uniform distribution across  the space. Thus, I think the authors might have intended to  be talking about higher-order moments of this conditional distribution, such as the skewness, kurtosis, etc. or otherwise risk measures such as CVaR. <sep> The main convergence theory (Theorems 1-2) do not exhibit any discernible complexity or sample efficiency gains over vanilla Q learning. Moreover, a coherent discussion of the technical innovations required to establish these theorems is absent from the manuscript. These limitations alone are very concerning. <sep> The matrix completion step (equations (9)-(10) is playing the role of a proximal operator on the Q learning update, or otherwise some projection of the Bellman error onto a low-dimensional subspace of features. Therefore, the steps conducted in Section 3.2 are very similar mathematically to entropic regularization, which may also be seen as a special case of a proximal operator on the space of Q functions. This connection is not made in the paper, as well as the more computational/statistical motivation for where the matrix competition step comes from. I strongly suggest the authors consider better explaining the links between step (4) of Algorithm 1 and proximal methods in any revision of this work. <sep> Proofs seem fairly routine in my reading. What is new or innovative here? Again, this is not explained anywhere. <sep> Minor Comments: <sep> References missing on distributional/representational aspects of Q learning: <sep> Dearden, R., Friedman, N., & Russell, S. (1998, July). Bayesian Q-learning. In AAAI (pp. 761-768). <sep> Koppel, A., Tolstaya, E., Stump, E., & Ribeiro, A. (2018). Nonparametric stochastic compositional gradient descent for q-learning in continuous markov decision problems. arXiv preprint arXiv:1804.07323. <sep> Jeong, H., Zhang, C., Pappas, G. J., & Lee, D. D. (2019, August). Assumed density filtering Q-learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (pp. 2607-2613). AAAI Press.","The paper considers exploiting low-rank structure in Q-function and the Hamiltonian Monte-Carlo (HMC) to approximate the expectation in Q-learning to reduce the stochastic approxiamtion error, and thus, achieves ""efficient RL"". The authors tested the algorithm empirically within some simple environments. <sep> As reviewers (R1, R3, R4) mentioned, the major bottleneck of this algorithm is the assumption that the dynamics is known up to a constant, which is extremly strong, and thus, limits the application of the algorithm. I suggest the authors to consider the common RL setting, without any knowledge about the transition models, and make fair empirical comparison with baselines in the same setting."
"Summary: <sep> This work proposes that many common issues with GAN methods are based on the weighting of the samples given to the generator's objective function. They focus on a study of the original GAN objective proposed in Goodfellow et al. where the generator's objective is the negative of the discriminators objective. The GAN community quickly observed that when the discriminator outperforms the generator with this objective, the saturating nature of the sigmoid function causes the gradients to vanish for the generator's objective. For this reason, a new objective (NS-GAN) was proposed which modifies the generator's objective to alleviate this gradient vanishing issue.  The authors argue that this modified objective is to blame for a number of common issues with GAN methods -- most notably the mode-dropping issue. The authors present theory that backs up these claims and they propose a new generator training objective which re-weights the gradients of the generator objective to have the same average magnitude as NS-GAN but have the same relative magnitudes of the original GAN objective. <sep> The authors demonstrate the impact of their new loss function in a series of quantitative and qualitative settings. <sep> Strong areas: <sep> I am a very big fan of work that questions standard assumptions that are taken almost as fact within our community. When GANs were originally proposed, most researchers saw the NS-GAN objective as a strict improvement over the MM-GAN objective and moved on. This work does a great job to demonstrate that NS-GAN is most definitely not ""superior"" to MM-GAN and may possibly be a worse choice of objective function. This change may seem insignificant on the surface and the authors here clearly show that it has an impact -- one large enough to consider if it should be used at all. To aid in their claims, the authors provide clear and concise explanations backed up by easy to understand theory. Particularly interesting to me was the argument that the popular Adam optimizer does not alleviate the saturation issues found with MM-GAN even though my own intuition on the optimizer tells us that it should help with issues like this. <sep> Weaknesses: <sep> The empirical results presented appear promising. The proposed approaches (MM-Unit and MM-NSAT) considerably outperform the NS-GAN objectives. My biggest issues are with the quality of the baselines. I am not an expert in the GAN field, but from inspecting the Conv-4 model with spectral normalization on CIFAR10, it appears like the best performing model achieves an FID of approx 42 and the worst model gets around 48 (figure 8, bottom right). While this difference is notable and the error-bars indicate it is statistically significant, I am concerned because these numbers seem to considerably underperform prior work. The original paper on spectral normalization for GANs reports an FID of 29.3 for standard CNNs. This model uses the NS-GAN objective (to my knowledge) so, I am confused as to why the authors did not simply replicate their setup -- especially since the standard CNN model proposed in the original spectral norm paper can be trained with reasonable compute and the authors have released code. If I am misunderstanding something about the experiments, please do let me know, but I am confused by this choice. <sep> I also took some issues with the D-JS-CD score proposed to score the class distribution of generated samples. There have been a number of proposed metrics like this such as IS and classifier score (https://arxiv.org/abs/1905.10887). I understand that this score (unlike classifier score) does not rely on a conditional model, but if a new score is to be proposed and it is used to convince the reader that a new method performs well, then it should be applied to some baseline models. Pre-trained models from prior GAN papers could be used to obtain these scores. <sep> As a researcher from a different field, my main concern about the experimental results is that the results are quite far from the current state-of-the-field. State of the art results are by no means required for publication in this venue, but the baseline models presented here perform much worse than they have been shown to in prior work. Since a near 20-point increase in FID can be achieved with a few tweaks to the NS-GAN objective (SN-GAN), I am left wondering if the presented improvement from the MM-NSAT objective will vanish once those improvements are applied or if it will still hold. Since this is not shown, then I am uncertain of the significance of the observations made in this work. <sep> Some more nit-picky issues: <sep> The text in the figures is too small and near impossible to read. I would present all of the results in Figure 8 in a table instead. There is no information gained by seeing loss curves. A table would save much more space and allow the readers to more easily compare this work to other works. In Figure 7, I find ""best"" and ""worst"" picks by qualitative methods to be somewhat unconvincing. You should show the highest and lowest FID or just show random samples. <sep> My recommendation: <sep> I am not an active member of the GAN community so I am more than willing to accept if my recommendation goes against more senior folks who work in that field. <sep> I found this to be an interesting work that provided a non-trivial insight, backed it up with clear and easy-to-follow theory and demonstrated their observations held on some medium-scale experiments. My biggest issues come from my reservations about the experimental results. The baseline NS-GAN with spectral normalization presented in this work greatly underperforms previously published methods that use the same objective. The difference between the presented baseline and the proposed method is smaller than the difference in performance between the presented baseline and previously published methods with the baseline objective. <sep> These discrepancies give me sufficient doubt where I am not certain that the insights of this work provide a sufficient improvement when combined with architectural improvements and proper parameter tuning. <sep> For these reasons, I am advocating against acceptance of this work but making it clear that I think this paper is borderline. <sep> If the experimental setup was more in line with prior work and the same trend in results held, then I would be more likely to recommend acceptance of this paper.","I think we did learn something new from this paper, and I think the reviewers all seem to agree with this. <sep> The observation you make about the objective seems correct and interesting (though reviewers and ACs do sometime miss errors), but I have the following complaints that keep me from recommending acceptance: <sep> The theory seems right, but in practice, all sorts of GANs with all sorts of objective functions experience ""mode collapse"", <sep> so it doesn't seem like the issue you point out w/ the NS-GAN objective can be the whole story. <sep> However, we generally don't ask of a paper that it tells the whole story all in one go... <sep> I do think the experiments are somewhat poorly done (compared to those for say the median paper about GANs that gets accepted to one of these conferences). Moreover, many people have made similar experimental claims to the ones that are in this paper that haven't held up on more complicated data sets, so I tend to apply more scrutiny to such claims when they're only evaluated on smaller tasks. <sep> There have, as R3 points out, been a huge number of papers proposing tricks for training GANs, and some of them work really well. <sep> What I'm missing from this paper is an exploration of the relationship between your observation and those (mostly ad-hoc) tricks. <sep> Does your observation explain why those tricks are necessary? <sep> Does it explain why some existing trick works as well as it does? <sep> If your observation is totally orthogonal to existing tricks, can you get much better performance on a challenging data set by using it? <sep> I don't feel like I got satisfactory answers to those questions. <sep> All this being said, the paper was borderline, and I think if you dealt with some of the complaints above you would have a pretty good shot of getting a revised version accepted at another major machine learning conference."
"Summary <sep> The work provided a nice new method with some performance gains by combining several existing techniques. The presentation was clear and organized, with the new method getting both better performance and some improvements in interpretability. It provides a variety of visual analyses that are typical of this area of research and present the contrasts between this work and prior efforts. <sep> However, I was unimpressed with the characterization of the RIDE method upon which this method was based.  The paper states: <sep> ""The novelty in RIDE is using the `2 distance between two different observations as a measure of qualitative change between the states. However, this introduces conceptual difficulties as the embedding space is not explicitly trained with any similarity measure. This is in contrast to <sep> ICM, which trains the state representation to minimize the forward prediction error. Because of this explicit training objective, there is no conceptual difficulty in viewing the intrinsic reward in ICM <sep> as a measure of its dynamics model uncertainty."" <sep> RIDE also trains using prediction errors based on 1) prediction the next state's embedding and 2) predicting the action taken using the two states' embeddings. This encourages the embedding space to store information which can predict actions as well as the effects actions have on the environment. While I admit there is still some uncertainty in this interpretation, the authors of this work do not comment on this explanation at all which should be critical when replacing the mechanism for training these embeddings with an alternative which contains different information. This is quite a big change in the algorithm, as the focus on the impact of actions for the embedding space appears to be the inspiration for calling RIDE ""Impact-Driven Exploration"". <sep> Given the replacement of action-focused embeddings with distance-focused ones (according to a policy), the author's algorithm might more accurately be called ""Depth-Driven"" rather than impact-driven. <sep> Before publication, I believe the authors should reduce the emphasis a bit on not being able to explain RIDE's l2 distance (e.g. ""This is in contrast to RIDE, in which the l`2 distance is not known to correspond to any similarity measure."") and should recognize the explanation that RIDE gives for its embedding space more clearly so as to be able to more effectively explain the difference (advantage! in these tasks) in their own approach. <sep> Quality <sep> The paper is presented according to a high standard of quality. They document and explain their methods clearly, including hyperparameters. <sep> Clarity <sep> The paper presented its results clearly, using the standard visuals for the field and with clearly written and easily readable text. Thank you! <sep> Originality <sep> The paper provides a novel recombination of methods that others have developed in a non-trivial way with original analysis into the distinctions between these methods and the effects of their combination. <sep> Significance <sep> The new method shows some improvements over existing methods in terms of sample efficiency on a number of tasks. <sep> The new method has some advantages in interpretability that were explored, though the practical value of this interpretability could be explored a bit further than just sample-efficiency. Are there any kinds of tasks your method might solve that would not be possible with the existing method? For example, you could imagine that a task with extremely long hallways would receive no intrinsic reward in RIDE but your method would have no trouble finding the motivation to keep running down the hallway. In contrast, are there any tasks RIDE would do better at than your approach, or does the ""distance""-focused metric work in a strictly superior way? I would have been more impressed by the paper if additional tasks such as this would have been included. <sep> I think a bit less emphasis could be given to the description of RIDE vs RIDE-SimCLR's similarity to temporal distance between states. RIDE is clearly not designed for that, RIDE-SimCLR is, so while it is nice to prove that RIDE-SimCLR does have this property it could be made a tiny bit more brief. <sep> The qualitative comparison with RIDE in terms of the heatmap especially could have been developed a bit more. The authors only say ""Why RIDE and RIDE-SimCLR assign high rewards to these actions is an interesting question left for future work"". For RIDE, the reward assignment feels fairly clear: taking the ""open door"" action creates a substantial change in RIDE's action-focused embedding space, so this is well-rewarded by RIDE. It would be nice for the author's to suggest a contrast with RIDE-SimCLR: it seems to me that it naturally does not provide a lot of intrinsic reward for actions but rather through movement away from where it has been, so it is natural that there is more reward for moving into new rooms rather than specifically on opening the door. RIDE-SimCLR also seems to avoid wandering the first room, which may be because RIDE is looking for objects it could interact with but RIDE-SimCLR just wants to get away from its current position as fast as possible. <sep> Furthermore, I would have appreciated seeing a bit more analysis on running this algorithm without any external reward. This was mentioned in the appendix briefly but the current method was only compared with a random policy and not with RIDE.","I thank the authors for their submission and very active participation in the author response period. I want to start by stating that I rank the paper higher as is currently reflected in the average score of the reviewers. The reasons for this are that a) R2 and R3, while responding to the author's rebuttal, do not seem to have updated their score or indicated that they want to keep their initial assessment of the paper -- in particular, R2 has acknowledged that additional experiments by the authors were useful and results on KeyCorridorS4/S5R3 are nice, and b) I disagree with R2's sentiment that MiniGrid is not a suitable testbed -- it is by now an established benchmark for evaluating RL exploration and representation learning methods (see list of publications on https://github.com/maximecb/gym-minigrid). However, despite my more positive stance on the paper, I fully agree with R1 and R2 that a comparison to EC is needed in order to shed light into which factors of EC-SimCLR actually led to improvements in comparison to RIDE. I therefore recommend rejection, but I strongly encourage the authors to take the feedback from the reviewers and work on a revised submission to the next venue."
"POST-REVISIONS <sep> Thanks for the revisions made to the theoretical results. I still find parts of the discussion in Appendix F to be unclear. <sep> Firstly, how do you derive eq. (5) from eq. (4)? In eq. (4), the denominators \\sum_i q_i are independent of ""\\Phi(x_i)"", but in eq. (5), they have a dependence on \\Phi through z_{i,b}. I think the change in normalization important to show the invariance principle holds (as the invariance principle requires a conditioning on each value \\Phi takes), but am unable to follow your derivation. <sep> Secondly, I'm not convinced that the maximizing partition for eq (5) assigns all examples with y=1 to one group, and those with y=0 to another group. Wouldn't the maximizing partition also depend on what \\hat{y} evaluates to for those examples? <sep> Overall, I'm able to see what the authors are trying to get at with this example, but unfortunately the revisions aren't sufficient to address all of my concerns regarding the theoretical results. <sep> The paper presents an approach for training models that generalize well to out-of-distribution (OOD) samples, particularly when the source of domain shift (e.g. spurious correlations or sensitive groups) is not known before hand. The paper combines ideas from two prior papers from the domain generalization and fairness literature: (i) invariant  risk minimization for OOD generalization (Arjovsky et al.) and (ii) adversarially reweighting for fairness without protected groups (Lahoti et al.). <sep> At a high level, the proposed approach seeks to minimizes the average classification loss across the worst-case partitioning of the dataset into two groups. Experimental results on datasets with synthetically generated spurious features show that the proposed approach is able to generalize better to OOD samples in the high noise regime, without having knowing aprior which features are spuriously correlated with the labels. <sep> Pros: <sep> The question tackled is practically important: how one can generalize to OOD samples without knowing the exact source of discrepancy between train and test data. <sep> The experimental results look encouraging <sep> Cons: <sep> The paper lacks a clear theoretical motivation for the specific optimization objective that the authors end up using (eq 3). In particular, do we know (at least in some a simple setting) that maximizing this objective over soft-group memberships ""u_i"" will identify the partitioning of the data that maximally violates the Invariant Constraint? I elaborate on this next. <sep> Relaxed training objective lacks strong theoretical backing: <sep> The authors directly adapt the training setup of Arjovsky et al., where the goal is to train a model which learns the same conditional label distribution for any given input ""x"" across a set of known partitioning of the training data, dubbed as the invariance constraint . Each of these partitions, referred to as 'environments', represent a different training distribution, and the goal is to train a model that performs equally well across all of them. Arjovsky et al. show that for the special case of linear invariant predictors, the training problem can be relaxed into an unconstrained objective with a regularization penalty. <sep> The present paper extends the setup of Arjovsky et al. to problems where the environments are not a prior known, and seeks to minimize the average classification loss over a partitioning of the data that maximally violates the invariance constraint. However, they do not explicitly solve this optimization problem, and instead simply minimize the worst-case value of the ""relaxed training objective"" of Arjovsky et al. over all (soft) partitioning of the data. <sep> Is the relaxation that Arjovsky et al. employ with known environments still relevant to your problem formulation, where you would like the invariance constraint to hold for all possible partitioning of the data? <sep> At the very least, this requires a discussion. Ideally, it would be nice to see a derivation of the relaxation for some simple special cases: e.g. like Arjovsky et al., can you show that for linear predictors, ""finding a partition that maximally violates the invariant constraint"" is equivalent to ""maximizing the relaxed unconstrained objective in eq. 3 over partitions""? <sep> Other comments: <sep> Eq 3:  I think ""w"" is a scalar here (otherwise evaluating the gradient at w = 1.0 doesn't make sense). Please make that explicit and also provide some intuition for why this regularization penalty with a scalar ""w"" makes sense for your problem set up. <sep> I am not entirely sold on the general theme of this paper of exchanging lessons between fairness and domain generalization. The authors are definitely correct in crediting a prior fairness paper for the idea of  the idea of adversarially re-weighting examples with a soft groups model, but as they themselves point out this idea has existed in different forms in the domain generalization literature (e.g. DRO). So my reading is that the paper seems to slightly over-emphasize the connection to the fairness literature, but this is a personal take. Having said this, the paper does provide (in Sec 2) a nice literature overview of similar problems tackled by the domain generalization and fairness communities. <sep> In the color MNIST experiments, you observe ""IRM(eEIIL) generalizes better than IRM(eHC) with sufficiently high label noise"". If I understand correctly, IRM(eHC) has access to the true environments, whereas IRM(eEIIL) uses environments inferred from data. Wouldn't we expect the former method to have an advantage over the latter? <sep> Additional baseline: Would it make sense to compare with (a form of) DRO for the color MNIST task (e.g. ones cited in Table 2)?  You do mention in another experiment that Lahoti et al. compare with DRO for their particular fairness application, but do those observations also apply yo the tasks you consider in this paper. <sep> Iterative training: I think a natural extension of your approach (which you've probably already thought about) is to solve (EIIL) using an iterative technique that alternates between maximizing over ""q"" and e.g. performing gradient descent updates on ""\\Phi"". Iteratively performing full optimizations over both sets of parameters may not in general have good convergence properties. <sep> Might be a relevant citation for the use of soft partition assignments for fairness: https://arxiv.org/pdf/2002.09343.pdf <sep> Fig 1: Would be nice if the plots were color blind friendly :) <sep> References: Might be good to mention the conference venue wherever available: e.g. Hashimoto et al. appeared in ICML 2018.","The paper analyzes connections between algorithmic fairness and domain generalization literatures. The reviewers found the paper interesting but they also raised some important concerns about it. <sep> The applicability of the method presented in the paper is not clear nor well-discussed in the paper. <sep> The papers and the revised version do not not cite important related work. <sep> The mathematical exposition in the paper is a bit hard to read. Even after revision, the reviewers find part of the paper(Appendix F) very hard to read. <sep> Overall, the paper in the current version is below the high acceptance bar of *CONF*."
"Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. These cases are referred to as corner-cases. The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion. The approach is tested on the PASCAL VOC database. <sep> The problem is important and relevant. I find the motivation clear, however, I disagree somewhat with the reasoning in parts of the introduction (see detailed comments). The approach is reasonable, and there is some evidence that performance is improved on these specific corner-cases by the addition of similarly identified corner-cases to the training set (Table 1). I am not sure how the approach would generalize, however. As I understand it the test-sets where performance is improved consists entirely of corner-cases identified in a similar manner, so whether the approach will generalize, depends entirely on how representable corner-cases identified in this manner are. It would have been much better to use a independent dataset of corner-cases identified manually. <sep> Detailed comments <sep> ""First, segmentation benchmarks require pixel-level dense annotation"", I do not believe this is necessarily true, and there is little need to state this. One could certainly think of useful benchmarks with hard examples only. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. <sep> ""Second, it is much harder for segmentation data to be class-balanced in the pixel level, making highly skewed class distributions notoriously common for this particular task"", ""Besides, the ""universal"" background class (often set to cover the distracting or uninteresting classes (Everingham et al., 2010)) adds additional complicacy to image segmentation (Mostajabi et al., 2015)."", while this may be true for training datasets, I do not see how this is a problem for benchmarks necessarily. <sep> I find the description of the construction of the test dataset used in the different iterations unclear. It is my understanding, but I am not actually sure so it would be good to have the approach clarified, that the test datasets (T(1), T(2), and T(3)) of iteration 1, 2, and 3 are hard examples, and are thus biased towards the methods involved. That is, they consist of examples that the proposed segmentation model disagrees with the ""competing"" models the most on. It is clear from the Table that these images are selected both based on mistakes of the competing models and mistakes of the target model. After one and two iterations we see that the target model now does much better on the next iteration of hard examples, but we really do not know how representative these hard examples are. If the methods tend to disagree on a limit number of typical cases, then these cases will be added to the training set and it is not so surprising that improvements in the target model is seen. To evaluate how this approach generalizes to the real world, an independent dataset would have to be used. <sep> ""This also provides direct evidence that existing segmentation models could be particularly weak at certain real-world generalization, which is not surprising because the 1,464 training images are deemed to be extremely sparsely distributed in the space of natural images."" I am not sure I agree with the evidence part. The dataset in question is selected to be hard (as far as I understand), so it is not surprising that the methods perform worse on it and does not say much about generalization. Essentially this is just evidence that the selection procedure is working as intended. <sep> Clarity <sep> While I believe I understand most of the manuscript to an acceptable level, it contains quite a lot of sentences that would benefit from some editting. Some examples below. <sep> Introduction is wordy, with long and difficult to understand sentences. Words that mean different things than the authors probably intended are also used, see examples ""boosting"", ""spot"", ""cover"" and wrong words are often used. <sep> ""While the performance of segmentation models, as measured by excessively reused test sets (Everingham et al., 2010; Lin et al., 2014), keeps boosting"", keeps boosting is a bit of a weird phrase here, ""keeps improving"" perhaps? <sep> ""suggesting their insufficiency to cover hard examples that may be encountered in the real world"", maybe replace ""insufficiency"" and ""cover"" with ""inability"" and ""handle"". <sep> ""such test sets may only spot an extremely small subset of possible mistakes that the model will make"", ""spot"" is likely the wrong word to use here, maybe contain? But even so, test sets do not contain mistakes, the methods possibly make mistakes on the test set. Consider rewording. <sep> ""The existence of natural adversarial examples (Hendrycks et al., 2019) also echos such hidden fragility of the classifiers to unseen examples"", while I could guess at what the sentence means, it does not really make sense. <sep> ""which possess inherent transferability to falsify different image classifiers with the same type of errors"", not sure what you mean by this. <sep> ""It is clear that images in S are visually much harder."", something is missing. <sep> ""weakly labelling method of filtering"", what do you mean by this? <sep> ""Specifically, given the target model ft, we let it compete with a group of state-of-the-art segmentation models {g_j}^m_{j=1} by maximizing the discrepancy (Wang et al., 2020) between f_t and g_j on D."" They are not really competing are they? The point, as I understand it, is not to select the best model, but to find the most ""controversial"" image. <sep> I would have prefered legends in each figure, as opposed to having to scroll up and down to find the relevant information. <sep> ""indicating that many images in T(1) are able to falsify both the target model ft..."", the images are not really falsifying the model. <sep> ""This suggests that the target model begins to introspect and learn from its counterexamples"", the word introspect appears to be wrongly used here (and elsewhere). <sep> ""Moreover, the top-1 model on T(0) does not necessarily perform the best on T(1) , conforming to the results in (Wang et al., 2020)."", what results are you talking about specifically? And I guess it should be ""confirming"". <sep> Originality <sep> I don't find the work very original and it is not clear to me if the work is very novel. A lot of literature is referenced under related work, but I find it to be mostly tangentially related. It would be good if the authors could describe how this specific problem has been addressed before in image analysis and particularly image segmentation. A google search brings a number of works on hard negative mining. But ""human-in-the-loop"" techniques such as interactive training [1, 2] also enable annotators to focus more time on harder examples. The methodology itself is not groundbreaking. Multiple trained models have been used in combination to assess prediction certainty previously and uncertainty has also been used in active learning setups to focus annotations on difficult regions [3]. I am sure there are even more relevant links, but this is what a couple of minutes googling brought up. <sep> Significance <sep> While the problem is relevant and the method possibly useful, because of previously mentioned concerns with respect to novelty and generalizability of the results, I do not think it will have a wide ranging significance. <sep> [1] Gonda, Felix, et al. ""Icon: An interactive approach to train deep neural networks for segmentation of neuronal structures."" 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). IEEE, 2017. <sep> [2] Berg, Stuart, et al. ""Ilastik: interactive machine learning for (bio) image analysis."" Nature Methods (2019): 1-7. <sep> [3] Casanova, Arantxa, et al. ""Reinforced active learning for image segmentation."" arXiv preprint arXiv:2002.06583 (2020).","This paper studies how to efficiently expose failures of ""top-performing"" segmentation models in the real world and how to leverage such counterexamples to rectify the models. The key idea is to discover most ""controversial"" samples from massive online unlabeled images. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness. <sep> However, there exists some limitations coming from R2 and R3, for example, 1) Segmentation benchmarks may not require pixel-level dense annotation. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. 2) It is much harder for segmentation data to be class-balanced in the pixel level, making highly skewed class distributions common for this particular task. 3) Citing the field of computer-assisted annotation as relevant work. <sep> In the end, I think that this paper may not be ready for publication at *CONF*, but the next version must be a strong paper if above limitations can be well addressed."
"Paper Summary <sep> This paper addresses the problem of estimating the distribution parameters of features extracted from a set of high dimensional observations, a problem that is common in the physical sciences. To solve this problem, the authors present a deep learning approach that utilises a combination of (i) deep ensemble training, (ii) post hoc model selection, and (iii) importance weighted parameter estimation. First, a deep ensemble is trained to solve a regression task (observation -> feature). During testing, this ensemble is frozen and used to generate feature samples from unseen observations. Using these feature samples, it is possible to estimate the distribution parameters using maximum likelihood estimation. The authors evaluate their method on X-ray polarimetry, and compare it with two other approaches, one of which is also a deep learning approach. On all tasks, the presented method outperforms both baseline approaches. <sep> Assessment Summary <sep> This paper presents a flexible, data agnostic, and easy to implement approach to parametric density estimation. The paper is clearly written, giving a good understanding of the different components of the approach. I would feel confident to implement this approach myself. <sep> However, I believe that the presented methodology, in particular steps (ii) and (iii) achieve the opposite of the authors intentions (see negatives below). Further, the use of an ensemble model is not properly motivated. The experimental section does not give justification to each of the model components. <sep> I therefor cannot support the acceptance of this paper. <sep> For a future manuscript, I recommend the authors to add an ablation study (see below). <sep> Positives <sep> Flexible approach: The approach is separated into two stages: (1) Training a deep ensemble on a regression task, and (2) maximum likelihood estimation of distribution parameters under the ensemble and an unseen test set. This two stage process makes the approach very flexible. The ensemble is trained once on a data set, and then exploited on multiple test sets - even with different likelihoods - for ML estimation. <sep> Data agnostic: The approach does not make any further assumptions about the data, apart from the fact that the ensemble can be trained in a regression task. <sep> Easy to implement: The presented approach chains a number of simple components together (deep ensembles, sample reweighting, ML estimation). Each of these components can be found or easily implemented in most common learning frameworks. Advancements in each of these components can have a trickle down effect on this approach. <sep> The paper is clearly written. <sep> Negatives <sep> Step (ii) of the approach is used to select a sub-set of models from the ensemble that was trained in step (i). Step 3 & 4 in algorithm 1 gives a hint of how this selection is implemented: On a new data set the features y are estimated from each neural network of the ensemble. The density parameters are then fit to approximate the distribution of features from each network under a given likelihood (I believe it should say maximize instead of minimized in algorithm 1: 3). The models for which the parameters can be best fit will be selected for step (iii). The authors claim that this removes the models with highest bias from the ensemble. However, at least in the way I understood it, this approach will in fact select the most biased models. Imagine a network that always output the same feature values. It will be easy to fit these values using the density parameters. <sep> Step (iii) will give highest importance to the most confident model prediction, disregarding whether that prediction is correct or not (we cannot know during test time). A confident but wrong predictor can therefore dominate the loss for a given sample, essentially eliminating the benefit of the ensemble. <sep> The experimental evaluation does not give any insights into which components of the approach actually help in performance. A proper ablation study should be carried out (see recommendations below) <sep> Minor Comments <sep> Figure 2 does not follow the formatting guidelines. Figure and table should be separated. <sep> Wrong citation command (citet <-> citep) <sep> Section 2.2, 2nd paragraph <sep> Section 2.2, 4th paragraph <sep> Section 2.3, 2nd paragraph <sep> Section 2.3, 4th paragraph <sep> Section 2.3, 5th paragraph <sep> Section 2.4, 2nd paragraph <sep> Section 3.1, 1st paragraph <sep> Section 3.1, 5th paragraph <sep> Section 3.4, 1st paragraph <sep> Section 4, 1st paragraph <sep> Recommmendations <sep> Ablation study: The authors present 3 components to their approach: (1) A deep ensemble, (2) model selection, and (3) importance weighting. Whether each of these components is necessary to achieve the performance as presented in the results section is not clear. To test that, the authors should carry out an ablation study by varying the size of the ensemble, varying the number of M top performing models, and by adding or removing the sample reweighting from step (iii). <sep> Review Update <sep> I thank the authors for their thorough response and the additional experiments. Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). I would have liked to score the paper higher, but at this stage I believe the paper is still not ready to be published. The authors acknowledged in their update that the review process helped them to understand their own work better. As a result, some aspects of their approach have been changed (e.g. removing step(ii), changes to step (iii)). I believe changes to the method go beyond the scope of the discussion phase and instead justify resubmission. This would give the authors some more time to get an in depth understanding of their approach as well. <sep> Author comments on step (ii) <sep> I thank the authors for clarifying. In their response, the authors claim that they will use a held-out data set with known density parameters. It is then possible to evaluate which models in the ensemble best estimate these density parameters. I have some issues with this claim: <sep> This is not made clear in the paper. <sep> The approach assumes that the density parameters are unknown. Adding this assumption will weaken the paper. <sep> The selected models will be biased toward the held-out set. <sep> Author comments on step (iii), now step (ii) <sep> In their update, the authors change the reweighting scheme. Instead of having a model-based weight, the reweighting is now done solely on a per-sample basis. I believe this looks like the right direction to take. <sep> Ablation study <sep> The ablation study is important. One possible addition would be to make a comparison for different ensemble sizes.","The reviewers are in consensus that the manuscript is not ready for publication in its current form: more comprehensive evaluation, and careful analysis (either theoretical or empirical) of the simple-but-effective methodology would improve the quality further. The discussion was constructive and helped the authors to reason about their work better. <sep> The AC recommends Reject and encourages the authors to take the constructive feedback into consideration ."
"SUMMARY <sep> The paper proposes a method to simultaneously learn effective representations and efficient exploration in a reward-free context. The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward derived from a k-NN entropy estimation of the state distribution. Then, authors empirically evaluate the method over a set of visual Mujoco tasks and Atari games. <sep> STRENGTHS <sep> the paper addresses a very relevant reward-free exploration objective as a preprocessing to RL <sep> the paper combines representation learning and state entropy maximization into a promising practical method <sep> WEAKNESSES <sep> the paper might be too incremental with respect to previous (albeit unpublished) work the paper somewhat fails to empirically illustrate and validate the reward-free phase <sep> EVALUATION <sep> Unfortunately, over some concerns regarding the novelty of the presented method and its experimental validation, which I find somewhat weak for an essentially empirical work, I would lean towards rejecting the paper. <sep> DETAILED COMMENTS <sep> C1) The exploration component of APT has striking similarities with the method in [1], which also seeks the optimization of a k-NN estimate of the state distribution entropy in a reward-free context. While it would be in general acceptable to overlook unpublished work, I think that the connections with [1] are too many to avoid a deeper discussion over distinctive contributions. The method in [1] does not seem to learn representations, but I am wondering if this contribution alone would be substantial enough. <sep> C2) The paper does not present an explicit empirical evaluation of the reward-free phase, thus I am not sure on how APT is performing in entropy maximization. Especially, what is the impact of the three sources of bias that are introduced in the entropy estimation (avoiding bias correction and constants, avoiding importance weighting, scaling distances with the standard deviation)? <sep> Can authors present state entropy plots, and possibly compare the performance of APT with other methods seeking a similar objective, such as MaxEnt (with representation learning), SMM [2] or MEPOL [1]. <sep> C3) I have some concerns on the theoretical and practical implications of considering a reward function that is actually depending on the current policy. It is sound to fit a value function for a reward of this kind? Maximizing an ever-changing reward might cause instability and prevent convergence? <sep> C4) The benefit of employing a non-parametric method to estimate the entropy of high-dimensional inputs is clear, since density modeling would be quite hard. Could density modeling over a reduced latent space be a viable option instead? <sep> C5) I would argue that the idea of simultaneously learning representations and exploration is the main selling point of the presented method, since maximizing the state entropy might help learning superior representations and viceversa. But from the ablation study in Section 4.3 this conclusion does not arise naturally, as learning representations alone seems almost as good as learning both. May I ask the authors to clarify this point? <sep> C6) The scores on Atari games are reported without confidence intervals, leaving some doubts over the statistical significance of the results. Moreover, it is not completely clear from the aggregate performance where and how APT is helping in these experiments. Can authors provide a deeper explanation on why the original performance of SimPLe and VISR is not reproducible? <sep> [1] Mutti and Restelli. A policy gradient method for task-agnostic exploration. Arxiv, 2020. <sep> [2] Lee et al. Efficient exploration via state marginal matching. Arxiv, 2019 <sep> QUESTIONS <sep> May the authors address the comments listed above in their response? <sep> ADDITIONAL FEEDBACK <sep> For the Atari experiments I would suggest to focus more on hard-exploration games, such as Montezuma or Pitfalls, instead of providing just an aggregate performance over full sets of games. It would be nice to include some visualizations and interpretations on the behavior that APT learns in the reward-free phase. <sep> I believe that the multi-environment pre-training setting is quite interesting and, to the best of my knowledge, completely novel. The results are promising, and I would suggest to include this setting, together with a deeper analysis, in the main paper. <sep> Dashing the lines in the reported plots would help visibility, especially without colors. <sep> typos and rephrasing: <sep> when referring to the environment I would use reward-free instead of task agnostic (e.g., page 2, paragraph 3) <sep> the 26 games subset instead of the 100k subset (page 7, last lines) <sep> I would rephrase ""The notable difference is that APT (representation) decouples the action space dimension from pre-trained models"" which is not crystal clear (Section 4.3). <sep> #################### <sep> AFTER RESPONSE <sep> I would like to thank authors for their detailed response and for their effort in improving the paper according to reviewers' suggestions. Unfortunately, after authors clarifications, I still have some doubts on the concerns raised with C1 and C2 (see below). Thus, I am keeping a slightly negative evaluation for this paper. <sep> Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. However, in Figure 6 MEPOL does not seem to suffer a particularly high-variance. To me, the most likely reason for the improved performance is that APT guarantees an action-level feedback as opposed to a trajectory feedback (see [1]). <sep> However, I am still skeptical about this action feedback: the reward-to-go becomes non-Markovian and Bellman equations does not hold anymore (see [2]). This casts some doubts on the actor-critic procedure APT employs to optimize the rewards. Authors may have a good point on the notion that the encoder is breaking the dependence between policy and rewards, but I think the topic warrant some additional discussion. <sep> I would suggest the authors to rephrase this work to give a more central role to the scalability to high-dimensional observations, which I believe is the main contribution of the paper, and to include a more thorough discussion of (Mutti et al., 2020) in the main text (beyond the related work section). <sep> [1] Efroni et al. Reinforcement learning with trajectory feedback. Arxiv, 2020. <sep> [2] Zhang et al. Variational policy gradient method for reinforcement learning with general utilities. NeurIPS 2020.","The authors propose a particle-based entropy estimate for intrinsic motivation for pre-training an RL agent to then perform in an environment with rewards. As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, ""A Policy Gradient Method for Task-Agnostic Exploration"", Mutti et al, 2020--MEPOL. What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments. The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin. Unfortunately this work does not meet the bar for acceptance relative to other submissions."
"Updates after discussion/revision period: <sep> It appears that the paper has improved. However, the changes appear to be so substantial that the paper is now essentially a different paper which would require a new review process. <sep> This paper describes a neural architecture that resembles the transformer but includes explicit representations of constituency and dependency structure. The model is trained for masked language modeling (MLM) and evaluated via MLM on held-out data and its ability to induce constituency and dependency trees. The results are better than trivial baselines for unsupervised constituency and dependency parsing but not as strong as related recent work. <sep> This paper has an interesting idea at its core: stemming from the success of models like ordered neurons, can we define a neural architecture that uses both constituency and dependency structure? The related notions of height and distance, combined with the syntactically-inspired attention masks, also seem interesting. <sep> Unfortunately, however, the current submission needs a lot of work before it will be ready for publication. There are three types of concerns I have about the paper in its current form: <sep> The description of the method is dense and difficult to understand, and has at least a few notational issues (details below). <sep> There are very few experimental details provided. Yes, space is limited in the main paper, but appendices are permitted as well and there were no appendices included in the submission. Basic questions about the experiments are not included, making it difficult to assess the paper. <sep> The experimental results are difficult to evaluate, as there are different datasets and assumptions made by different work, so there does not appear to be a baseline that the new model can be compared to fairly. it's unclear what the take-home message is. It would be beneficial to have some simpler baselines run by the authors on the same data they are using for training, in order to have a comparison that is fair. <sep> I'll discuss these three points below. <sep> Clarity of model description and notational issues. <sep> First, I had some confusion about Alg. 1. <sep> The BuildTree function call in line 8 does not match the function signature in line 1. Or is it supposed to be calling some other function that's not part of this algorithm? <sep> What is k on line 12? k only seems to be defined in line 13. <sep> I don't understand the notation in this part of line 14: ""head_i \\in {head_i}\\head_k"". I think it's confusing to use i in both head_i and {head_i}, because i is (I think) being used as an index which is iterated over. It would be more clear to use a different index (e.g., j) for one of these two instances of i. <sep> I also had some confusion about the StructFormer description. <sep> Eq. (5) contains p_j^{C(i)} but then right below it when the equation is being described, there is a p_{i,j}^{ct} -- are these two related in some way? It's not clear to me how the notation should line up here. <sep> Why does Eq. (9) have p(i \\leftarrow k) repeated twice? I think one of them should have a j. <sep> In addition to my confusion about the uncertainties above, I found it difficult to follow the description of the StructFormer due to its density. Sec 4 would benefit from a more leisurely exposition, along with figures and potentially an example. <sep> The experiments are difficult for me to assess because many details are missing. For example: <sep> (a) How is masking done during training with masked language modeling? That is, what is the masking rate? Are whole words masked or subword units? Are any words that were selected to be masked left unchanged or swapped to be other words as in BERT? <sep> (b) What optimizer was used and how were learning rates selected? Was any early stopping done? If so, what criterion was used? <sep> (c) What is the value of the convolution kernel parameter W? <sep> (d) How was the gap value of 0.1 selected? How sensitive is the performance to this value? <sep> (e) It's not clear to me whether the masked language modeling comparison between the transformer and structformer is fair. Is the number of parameters comparable between those two models? Were they trained in similar ways? <sep> (f) UUAS is never actually defined (though I was able to figure it out based on context). <sep> The empirical comparison leaves something to be desired. The StructFormer reaches higher parsing accuracies than trivial baselines, but not as high as related recent work. However, the paper notes that it does not consider that related work as perfectly comparable. Some results that I think would be more comparable are not included in the results tables. Page 1 has the following text: ""Previous works, that have trained from words alone, often requires additional information, like pre-trained word clustering (Spitkovsky et al., 2011), pre-trained word embedding (He et al., 2018),..."".  Pretrained clusters or embeddings are unsupervised and therefore would be reasonable to compare to. The results tables omit results from these papers, instead reporting results from papers that use gold POS tags and other such information. But I would argue that a system that uses unsupervised word clusters or unsupervised word embeddings is fair to compare to. Why aren't the results from Spitkovsky et al and He et al included in the results tables?  In addition to comparing to prior work, it would help to characterize the performance of the StructFormer to include baselines that are trained on the same dataset as that used by the authors, so that we can disentangle the impact of some of the choices made here. <sep> Below are typos and minor things: <sep> p. 5: ""higher then all"" --> ""higher than all"" <sep> p. 5: ""a unsupervised"" --> ""an unsupervised"" <sep> p. 6: ""to converges faster"" --> ""to converge faster"" <sep> p. 6: ""Alogrithm"" --> ""Algorithm"" <sep> p. 7: The sentence starting with ""Since POS tags"" is not a complete sentence. <sep> p. 7: ""infering"" --> ""inferring"" <sep> p. 7: ""effect"" --> ""affect"" <sep> p. 7: The sentence starting with ""Because it is"" is not a complete sentence. <sep> p. 7: ""only have"" --> ""only having"" <sep> p. 8: The caption of Fig 4 mentions s, but I think c was meant instead, as c is present in the figure while s is not. <sep> p. 8: ""premising"" --> ""promising""","This paper presents a novel approach to grammar induction. Like older work by Klein and Manning, the paper finds benefit in jointly inducing both constituency and dependency structure. However, unlike most approaches to grammar induction, the model is not generative -- rather, it is a transformer-based architecture that is trained to optimize a masked language modeling objective. The resulting parses appear to beat non-trivial baselines, but direct comparisons with several relevant state-of-the-art systems are not drawn. Reviewers overall found the approach interesting and novel. However, nearly all reviewers raised serious concerns about experimental comparisons with related work and brought up several missing state-of-the-art baselines that, like the proposed system, do not require gold POS. Reviewers also pointed out issues with clarity in several sections. In rebuttal, authors provide a substantial update to the original draft. So substantial that all reviewers mentioned in discussion that the new draft would effectively require an entirely new review. While I applaud authors for the substantial revisions, and while *CONF* guidelines do not explicitly limit the amount change to a draft allowed in rebuttal, in this case the revisions are sufficiently drastic that I agree with reviewers that a new review process is required. Thus, I recommend rejection but strongly encourage authors to resubmit."
"This paper discusses an approach to perform importance sampling by reviewing performance over past batches and suggesting future batches. <sep> First I will comment on the listed contributions: <sep> • To our best knowledge, we are the first to propose to directly learn a robust sampling schedule from the data themselves without any human prior or condition on the dataset. <sep> From this description I believe this paper has done it before: ""CASED: Curriculum Adaptive Sampling for Extreme Data Imbalance"". <sep> The paper seems to go beyond this and focuses on multi node training so this may be a way to refocus the paper in contrast to the existing work. <sep> • We propose the AutoSampling method to handle the optimization difficulty due to the high dimension of sampling schedules, and efficiently learn a robust sampling schedule through shortened reward collection cycle and online update of the sampling schedule. <sep> It is not clear to me what the intuition of P(x) is. If H* is the ""optimal sampling schedule"" and then P is defined to be the ratio of x in H* over all x in H* then it is not clear to me what you are converging to. It would be better for the reader to make this as easy to understand as possible. Also, I think this would be the place where this approach is different from the CASED approach. <sep> On the experiments to demonstrate that this method is better: The replicate runs seem to almost have no variance. It is the case that only the model init was varied between runs with fixed train/valid/test sets? If so this does not seem sufficient to confirm that this method works because the data you sample from is always the same. This should be randomized at least for CIFAR. <sep> Typo: ""Algorithm 2: Search based AuoSampling""","The work focuses on a new method for sampling hyper-parameter based on an ""Population-Based Training"" schedule that tend to sample more often configurations that gave good performances in the past. The authors have conducted experiments to verify the superior of their method, especially for the effectiveness and generalisability. <sep> Pros: <sep> simple method that can be implemented without much effort, <sep> good empirical performances on Imagenet, <sep> paper well organised and written. <sep> Cons: <sep> lack of explanation about the DensNet121 performance degradation [partially addressed in the rebuttal], <sep> additional simple experiments in Section 4.4 were recommended to evaluate the generality of the method [addressed in Table 5], <sep> empirical validation seems not sufficient [partially addressed in the rebuttal], <sep> similarity with respect to prior art, such as the focal loss [partially addressed in the rebuttal], <sep> clarification of the randomisation strategy in experiments [addressed in the rebuttal]. <sep> Despite most of the issues being addressed, the reviewers decided that this paper would benefit more work to be accepted for the conference this year."
"Summary <sep> The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. SN is a lightly parametrised module that gives freedom to the CNN to dynamically re-adjust the means and variances of the feature maps in the original instance normalization framework. The authors argue that SN then learns to emphasize important styles and suppresses less important ones, with the underlying assumption that the first and second-order statistics are often sufficient and necessary representations for style. CN is a twin invention that randomly swaps the mean and variance statistics of features of two channels. This leads to diversified virtual styles in the training data, effectively factoring out the model's dependence on style cues for recognition, according to the authors. <sep> Pros <sep> It is great news to the field that such simple recipes introduce gains in the performances. From the industrial point of view, the expansion of choices for model design means the likelihood of introducing uniform gains across the board in industrial applications (think about how much $ worth it will be if it cuts the error rate by 0.1 pp across all image classifier applications). From the research point of view, it is exciting to confirm yet again in 2020 that there is much room for improvement in those modular components, after the introduction of SE blocks and the likes. <sep> Cons <sep> The above excitement should be checked with a pinch of salt. I find it difficult to follow the rationale behind key assumptions (e.g. that shape = content and texture = style) and the experimental section is, I have to admit, a bit chaotic. And I suspect if the chaos is intended, for downplaying the discouraging side of the results. <sep> Shape=content and texture=style? No. <sep> First of all, we need a decent, if not mathematical, definition of the four terms above. To me, ""content"" is the causal cue that constitutes the GT label for the task at hand; I refer to all the rest cues as ""style"". ""Texture"" to me is a local pattern that can be captured by sliding windows of, say, size 10x10 pixels and ""shape"" is any pattern that is more global than texture. Under this terminology, I find it hard to agree that texture is not content. Texture does contribute, as the causal cue, to the final task at hand in many real-world computer vision applications - texture classification, fine-grained cat categorization, medical diagnosis with CT scans, and semantic parsing for detecting snowy and watery road conditions, to name a few. <sep> The oversimplistic view of the world that underlies this paper makes the justifications for the proposed methods all the more fragile. <sep> It is difficult to agree that SN and CN, which are argued to control style and content for the benefit of the recognition task at hand, are really working as speculated. It is also disputable whether the first and second-order statistics really encode the style and/or texture component. I find it a bit dangerous to let such a convenient and simplifying view of the matters be published and guide researchers in the field to adopt the same kind of viewpoint. In the revision, try to introduce more depths in your arguments and include more empirical analysis to make a point SN and CN work in the promised way. <sep> Introduce order in the experimental reports. <sep> So I made a table below to summarize the experimental setups in this paper (sorry for the dots everywhere - formatting tricks ;)). I'm having a hard time convincing myself that SN and CN really work empirically, given those highly specific choices of settings per task and analysis. For example, why is ImageNet performance for SN and CN not compared against augmentation baselines considered in the CIFAR experiments in Tab1? What are the individual performances of CN and SN for CIFAR on those 4 architectures in Tab1? What is the effect of location for SN in a CNN (equivalent analysis for CN is presented in Tab4)? There are many, many questions unanswered. I do observe a few improvements introduced by the two modules here and there, but I can't forgo the impression that these are only selected highlights that comply with the authors' arguments. Please introduce the much-needed order in the experimental section, and only then will I be able to assess if the new technology is truly innovative. <sep> .  Section  . <sep> .  Data  . <sep> .  Arch  . <sep> .  Evaluation  . <sep> .  Baselines  . <sep> .  Authors' methods  . <sep> .  Tab1  . <sep> .  CIFAR  . <sep> .  4 archs  . <sep> .  mCE,CleanAcc  . <sep> .  Cutout,Mixup,Cutmix,AA,Advtr,AugMix  . <sep> .  SNCN, SNCN+AugMix <sep> .  Fig2  . <sep> .  CIFAR  . <sep> .  28-2WideResNet  . <sep> .  mCE,CleanAcc  . <sep> .  WA,RA  . <sep> .  CN <sep> .  Fig4  . <sep> .  CIFAR  . <sep> .  40-2WideResNet  . <sep> .  mCE  . <sep> .  VanillaModel  . <sep> .  SN,CN <sep> .  Tab4  . <sep> .  CIFAR  . <sep> .  40-2WideResNet  . <sep> .  mCE  . <sep> .  VanillaModel  . <sep> . CN <sep> .  Tab5  . <sep> .  CIFAR  . <sep> .  40-2WideResNet  . <sep> .  mCE  . <sep> .  VanillaModel  . <sep> . SN,CN,SNCN,SNCN+Crop,SNCN+Crop+CR <sep> .  Tab2  . <sep> .  ImageNet  . <sep> .  ResNet50 . <sep> .  mCE,CleanAcc  . <sep> .  PU,AA,MaxBlur,SIN,AugMix  . <sep> . CN,SN,SNCN+AugMix <sep> Key reasons for the rating <sep> The simplicity and effectiveness of the technologies argued by the authors are eclipsed by the oversimplifying assumptions and inefficient experimental exposition. The rating reflects this disappointment. Please aim to improve the paper in the rebuttals and paper revisions.","This paper proposes two mechanisms, SelfNorm (used during training and inference) leveraging an attention-based recalibration of mean and standard deviation for instance normalization, and CrossNorm which performs cross-channel swapping of mean/stdev. Is is shown that the combination (often combined with AugMix) performs well across several datasets in terms of model robustness. Overall the paper has strength in the fact that the method is interesting, simple to implement, and modular. However, reviewers brought up a number of issues including the overstated motivation/writing, lack of clarity, and most importantly need for clear experimental results (comparing to uniform/standard baselines) and identification of the separate mechanisms. It is especially uncertain why it is necessary that they are used together (often with AuxMix as well) to obtain the strong performance. As a result, the score for this paper is borderline, tending towards a weak acceptance. <sep> It is appreciated that the authors provided a lengthy rebuttal, including new results in a different domain (NLP); however, the reviewers agreed that not all of the concerns were addressed. After a lengthy discussion, all of the reviewers agree that while the method is simple, modular, and effective when combined (hence the positive scores from some reviewers), the authors fail to describe the underlying reason for the method's gains, especially with respect to the individual parts (SelfNorm vs. CrossNorm) and why the results only come when these rather independently derived modules are used together. The exposition of the experimental results, with differing baselines/conditions that make it very hard to understand where the effect is coming from, exacerbates this issue. <sep> As a result of these concerns, I recommend rejection of this paper. However, the method is interesting and results promising, so I hope that the authors can clarify the writing and improve the presentation of the results (specifically separating out the effects of SelfNorm and CrossNorm, as well as analyzing how they interact together to improve results) and submit to a future venue."
"This paper presents a large benchmark of machine learning tasks for molecules represented by the 3D coordinates of their atoms. The benchmark is a combination of existing data sets and newly created ones, and covers a variety of applications and tasks, from small molecules to RNA or protein structures, and including classification, regression and ranking tasks. In addition, three deep-learning algorithms are implemented and evaluated on these benchmarks, and compared to state-of-the-art methods that do not use 3D information, and empirically demonstrate the benefit of incorporating 3D information in the networks. <sep> The vast majority of machine learning methods that have been developed for molecules use either 1D or 2D information. Therefore, this resource and the empirical demonstration that using 3D information can improve performance can be very valuable to the community. However, I have a number of concerns about this paper: <sep> I am assuming this has not been incorporated to the paper for anonymization reasons, but could the authors confirm that they are indeed planning to make both the data sets and the code used to produce the results (in particular, the three proposed neural networks architecture) available? This is obviously essential to the paper and I would feel more comfortable accepting a version of the paper that includes this information (possibly with URLs withdrawn if there is a concern about maintaining the review process blind). <sep> The paper does not discuss the nature of the 3D information further than ""By representing a molecule's atoms and their 3D positions"". However, molecules do not have a fixed 3D structure, but rather multiple conformations driven in particular by rotatable bonds. Determining the multiple possible conformations of drug-like molecules is still an ongoing research topic (see for example the review of Hawkins (2017)), not to mention the determination of the 3D structure of proteins, which is indeed the topic of one of the data sets provided. What information is provided in the different data sets (a single conformation? multiple conformations?) and how does this affect both algorithms (if several conformations are used) and prediction performance? <sep> I would really refrain from using ""atomistic learning"" to describe what the community has been referring to as ""learning from 3D molecular representations"" for decades. <sep> Actually, the abstract (and, more generally, the paper) reads as if neural networks were the only kind of machine learning algorithms that could be applied to molecules and that very little work has been done in the past to incorporate 3D information in chemoinformatics. While it is true that most current techniques rely mostly on 2D (for small molecules) or 1D (for large molecules) representations, it is not for lack of trying to incorporate 3D information, but because 1) this information was either lacking or incomplete (in the sense that a single conformation gives somewhat limited information; for example, you may have the crystal structure of a protein, but that doesn't inform you directly on the pose of its pocket when binding a specific small molecule) and 2) earlier attempts at making use of 3D information have often found that it did not improve performance (see Swamidass et al. (2005) or Azencott et al. (2007)), either because of the aforementioned incompleteness or because the methods were not up to par. The framing of the paper ignores decades of work in chemoinformatics, in particular (but not limited to) around kernel methods. I am listing a few examples of such papers below, not because I think they should all be included in this paper, but because in my opinion the paper would benefit from considering this body of work. <sep> In addition, although some authors have already used ""atomistic machine learning"" in the context of chemoinformatics (see Schütt et al. (2018)), the term ""atomistic learning"" is already often used in opposition to ""holistic learning"" in education. <sep> Finally, in Section 4, the paper could benefit from stating very explicitly what is novel and what is not novel in the three proposed 3D architectures. <sep> Axen, Seth D., et al. ""A simple representation of three-dimensional molecular structure."" Journal of medicinal chemistry 60.17 (2017): 7393-7409. <sep> Azencott, C.-A., et al. ""One-to four-dimensional kernels for virtual screening and the prediction of physical, chemical, and biological properties."" Journal of chemical information and modeling 47.3 (2007): 965-974. <sep> Gaüzere, B., Brun, L., and Villemin D,. ""Two new graphs kernels in chemoinformatics."" Pattern Recognition Letters 33.15 (2012): 2038-2047. <sep> Hawkins, Paul C. D. ""Conformation generation: the state of the art."" Journal of Chemical Information and Modeling 57.8 (2017): 1747-1756. <sep> Mahé, P., et al. ""Graph kernels for molecular structure− activity relationship analysis with support vector machines."" Journal of chemical information and modeling 45.4 (2005): 939-951. <sep> Mohr, J. A., Jain, B. J., and Obermayer, K. ""Molecule kernels: a descriptor-and alignment-free quantitative structure–activity relationship approach."" Journal of chemical information and modeling 48.9 (2008): 1868-1881. <sep> Nettles, J. H. et al. Bridging chemical and biological space: ""target fishing"" using 2D and 3D molecular descriptors. J. Medicinal Chem. 49, 6802–6810 (2006). <sep> Rhodes, N., Clark, D. E. & Willett, P. Similarity searching in databases of flexible 3d structures using autocorrelation vectors derived from smoothed bounded distance matrices. J. Chem. Info. Mod. 46, 615–619 (2006). <sep> Schütt, K. T. et al. SchNet - a deep learning architecture for molecules and materials. The Journal of Chemical Physics 148(24), 241722 (2018) <sep> Swamidass, S. J., et al. ""Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity."" Bioinformatics 21.suppl_1 (2005): i359-i368.","For many problems such as ligand-protein binding, quantitative structure activity prediction (QSAR), predicting protein function from structure, etc., the 3D geometry of the molecules is of great importance. One way to represent this is simply to assign locations to all atoms in 3-dimensional space. If using graph convolutional kernels or other relational representations such that aligning molecules is not necessary, these approaches with 3D geometry can be efficient and far more effective than 1D or 2D representations. The contribution of the paper is to make this point and to produce a resource with this kind of 3D data. Such a resource would be of high value. Nevertheless, reviewers feel provision of such a resource is perhaps not a major contribution to the *CONF* and ML communities. There is a sense that more innovative and substantial contribution would come from addressing also the challenge that 3D geometry can changes and that there may be multiple low-energy conformations of biomolecules that should be considered. The authors contend that unlike ligands which are small and may have many low-energy conformations, large biomolecules have a much more constrained conformational space. <sep> This meta-reviewer is sympathetic to the authors' point and appreciates the importance of the resource. Nevertheless, even large biomolecules often have some portions of flexible conformation and high 3D structure variation that should be considered. And indeed addressing the kind of multiple instance problem that arises by considering multiple conformations of large molecules or of ligands binding to large molecules would certainly require and likely yield bigger *CONF*/ML innovations. In the end the paper contributes a useful resource but does not excite the reviewers substantially enough, without those extensions or others, for a recommendation of acceptance at this time."
"This paper proposes effective gradient flow (EGF), which is a layer-wise normalized gradient flow. Compared to (unnormalized) gradient flow, the paper shows that the proposed EGF is (slightly) better correlated with metrics like test loss and test accuracy (see Table 1). Given that this claim is supported with experimental results, the paper would become much stronger if a larger number and a more diverse set of data-sets were used (in addition to CIFAR-10 and CIFAR-100, which are two very similar image-data-sets) as to show that the claim holds more generally. Apart from that, given that the correlations are (only) about 0.4 in Table 1, it seems that only some aspects are explained by EGF. <sep> This said, if we continue with the insight that EGF is helpful, how would EGF help to design better sparse models and optimization methods? Why is it easier to compute EGF instead of directly the relevant metrics (like test loss etc.) to determine what training methods and model architectures work better?  In fact, Figure 2, top row, exactly does this: test-accuracy is directly used to determine which training methods work best for sparse models. What is the additional insight/benefit of using EGF (like in the bottom row of Figure 2) instead of test-accuracy ? <sep> The paper also proposes the SC-SDC framework, see also results in Table 3. The key idea is to compare  sparse and dense networks that have the same number of (non-zero) weights. While this is a good start, I am not sure that this is really a fair comparison, though. I would expect the sparse network to have a larger capacity than a dense network with the same number of parameters. While there are many ways to see this, the simplest  might be that  a weight in a sparse model requires two parameters, its value and its index if we wanted to encode the model. Another way to see it might be that a dense model can be pruned to become sparse(with fewer weights), but without losing much prediction accuracy. As a consequence of potentially comparing sparse and dense networks of different capacities, the results in Table 3 might be biased in favor of sparse models. From this perspective, it is remarkable that sparse models do not clearly outperform dense models in Table 3, which might indicate that the training of sparse models with the analyzed approaches still suffers from the problems outlined earlier in the paper. <sep> Equipped with the proposed methods EGF and SC-SDF, the paper then analyzes several (standard) approaches like batch normalization etc as to determine which of these approaches are useful for training sparse models.  Again, given that only two very similar data-sets were used (CIFAR-10 and 100), it is unclear if the found results would generalize to other data sets. Moreover, it is not clear why EGF and SC-SDF are needed to determine which training-methods work well for sparse models. In fact, Figure 2 (top row) directly shows the test-accuracy for the various approaches--i.e., without using the proposed EGF and SC-SDF. EGF is shown in the bottom row in Figure 2, as to illustrate that EGF has a similar behavior as test accuracy in the first row in Figure 2. What is the additional insight obtained from EGF compared to using only test-accuracy as to determine that  batch-normalization works well ? <sep> Some minor points: <sep> How are ""test loss"" and ""test accuracy"" defined in this paper, and what is the difference? I did not immediately find the answer in the referenced paper (Jiang 2019). <sep> Are really all 6 digits statistically significant in Table 1 ? <sep> Figures 7 and 9 in the appendix are missing the actual image. <sep> The paper has several typos, like ""and."", or the first sentence in Appendix A <sep> +++ updates after authors' feedback +++ <sep> I  thank the reviewers for their detailed response. I still feel that more work (experimental and theoretical, as outlined in my review) is needed. <sep> I also would like to make sure that my point is not misunderstood when I said that the sparse model requires twice as many parameters to be stored (the value and the index),  compared to the dense model. Using compression algorithms, the number of bits to store the model can obviously be reduced (below the factor of two). Anyways, the deeper point that I wanted to make  was the connection between minimum description length  (number of bits to store the model) in information theory and the model complexity /capacity in statistics/machine learning: see BIC in https://en.wikipedia.org/wiki/Minimum_description_length <sep> Hence, the bits to store a model are directly related to the model-complexity/capacity in machine-learning/statistics.","This paper proposed a new measure of effective gradient flow (EGF), and also compared sparse vs. dense networks on CIFAR-10 and CIFAR-100. The notion of EGF would be interesting, but the paper did not present enough evidence to support this notion."
"Summary: <sep> In the paper, the author(s) propose 1) a method to dynamically adjust the sampling rate on radar data using 2D object detections (algo1) and previous image and radar data (algo2); 2) an end-to-end transformer-based 2D object detection model using both radar and image data. The author(s) show experimental results on Oxford Radar RobotCar dataset. <sep> Pros: <sep> The paper is easy to follow and well written. <sep> The author(s) empirically evaluate the proposed DETR model on NuScenes dataset and it beats the faster RCNN baseline. <sep> Cons: <sep> Magic numbers?: in paragraph 2 of section 4, the author(s) list the way to split the regions and sampling rates for different regions, but does not explain the reason to do so. In my view, this is what the author(s) claim as a novelty ""to dynamically allocate sampling rates on the different region"". It is somewhat hack-y to me and hurts the novelty of this paper. Thus I would expect the author(s) can show more solid motivations and reasons for using such a split and allocation method. <sep> The evaluation in table 1 looks too subjective to me. If I understand correctly, this evaluates the quality of different reconstruction methods qualitatively. Is this evaluation from one person? And I am not sure if the visual quality of reconstruction has a direct and strong correlation with detection quality. Btw it seems to me it is hard to tell the difference between the reconstruction visualizations in Figure 2. <sep> Need more analysis on the latency. In algorithm 1&2, the proposed method uses detection results about 0.2s  before to aid the CS. Assume a car moves 30 mph, in 0.2s it can move 2.68m, which is about 3/4 length of a car. Would this affect the performance? <sep> In the evaluation of DETR (table2), do you use the image + CS radar or just image + plain radar? I think it would be good if you can also test the detection performance using image + CS radar. <sep> Could you provide the motivation of using the transformer rather than faster RCNN on detection? There are numerous differences between these two, but it is unclear from the paper which one plays the main role in boosting the performance. <sep> Questions: <sep> Could you explain more about why the block size is 50x100? On page 3, ""The radar data is split into 8 equal regions, in azimuth and 37 equal regions in range."" Should the block size be 8 x 37? <sep> Also please address and clarify the cons above <sep> Post-rebuttal review: <sep> I carefully read through the rebuttal and other reviews and  I would stick to my current rating. <sep> Cons 1: The rebuttal does not provide sufficient explanation on how the magic numbers are chosen (though it is somewhat explained in the rebuttal, it looks more like a design and lacks experiments to back it up: why these numbers but not other numbers?) <sep> Cons 2: I would suggest the author(s) further improve the evaluation metrics to make it more objective and convincing. <sep> Cons 3: I believe the 2.68m difference in detecting cars should be regarded as a very large error (under IoU 0.5 metric, it would be counted as a misdetection) and should be handled properly. <sep> I think the approach presented in this paper is interesting, and I encourage the author(s) to do more analysis to make it more solid.","The main idea of the paper is to use image data to guide radar data acquisition by focusing on the blocks where the object has appeared. Four reviewers have relatively consistent rating: 3 of them rated ""Ok but not good enough - rejection"", while 1 rated ""clear rejection"". The main concerns include ad-hoc choices of algorithm design, lack of algorithm novelty, not adequate experiments in illustrating the performance, etc. During the rebuttal, the authors made efforts to response to all reviewers' comments. However, the major concerns remain, and the rating were not changed. While the motivation is clear and the work has merits, the ACs agree with the reviewers' concerns and this paper can not be accepted at its current state."
"This paper presents a method to unsupervisedly discover structure in unlabeled videos, by finding events at different temporal resolutions. <sep> Strengths: <sep> The paper focuses on the important problem of exploiting weakly labeled video data, by exploiting its structure, for example by recovering temporal structure in an autoencoder fashion. <sep> Use of multiple modalities to cross-supervise each other. <sep> Code is available. <sep> Weaknesses: <sep> The concept of hierarchy is not well defined or well motivated. While most hierarchical papers refer to hierarhies of concepts, the hierarchy considered in this paper is much weaker as a hierarchy, and it refers to subactions within longer actions (not actions that are specific instances of more abstract actions). While this way of understanding hierarchy can be valid, it is never explained or motivated in the paper, or even compared to the standard way of understanding it. <sep> The overall motivation for the method has a lot of gaps. For example: <sep> Regeneration of low level concepts from high level concepts: what are we expecting from a network that moves from a ""high in the hierarchy"" concept to a ""low in the hierarchy"" concept? Should we expect the network to randomly select one subconcept? The specific information is not there, the problem is ill-defined (for example, we can go from ""cat"" to ""animal"", but not from ""animal"" to ""cat""). How are we expecting any reconstruction? The paper does not provide any justification or intuition. <sep> Why are the authors using those specific pairs in the L_dyn term? (last line of page 4 -- I suggest adding equation numbers). Apart from no motivation, there are no ablations showing that those are the correct pairs. <sep> Why is the ""low"" case even necessary? Wouldn't it be possible to train only with the ""high"" one? This would probably imply rethinking some of the losses, but overall the method would look very similar. This is, the idea of hierarchy would disappear, but this idea is not used in the experiments anyway. <sep> What is the motivation for the two modalities? I can understand it can help, but it is not central to the method. This is not necessarily bad, but it requires some explanation. <sep> The explanation revolves around demonstration data. It is unclear why demonstration data is important for this method, and why it is not general for any human action. The introduction explaining demonstrations in a robotics scenario does not feel related to the content of the paper. For example, a lot of stress is given to ""agents"" interacting in ""environments"". <sep> Some terms introduced in the paper would benefit from a change. For example, a ""concept"" in the paper is actually an ""event"", not a ""concept"". This is more in line with the literature, for example the dataset they use labels that as ""event"". <sep> Results on chess are hard to believe. Do the authors think that the system has really learned (unsupervisedly) to identify interesting openings? It could instead be learning strong biases like length of openings in the dataset. <sep> Quantitative results are not convincing. How does FLAT and even the supervised method perform much worse than the two trivial baselines (random and equal division)? Does FLAT use text data? <sep> Conceptual assertions that I do not believe to be true: ""under a concept, the sequence of frames is nearly deterministic"". This is not true, there are nearly infinite ways of having a sequence of frames (video clip) depicting how to crack an egg, for example. Different background, different way of performing the action, different elements in the scene, speed of the action, point of view, etc. This is related to the ""regeneration"" point above. <sep> Additional comments and questions: <sep> Figure 2 is hard to understand. What is it exactly representing? What should we learn from it? <sep> Does the algorithm have any ""motivation"" to not predict always a single concept per sequence? <sep> What is the relationship between u and v? <sep> Have you tried smaller networks? 8 layers and 8 heads just for the Encoder seem like a very big model for such a relatively simple text. <sep> In the first paragraph of page 8 the paper mentions that there is only marginal improvement in low level concepts. How are these evaluated? As far as I could understand, there was only GT available for the high level ones. <sep> Final recommendation <sep> Overall, I believe the paper as it stands is not ready to be presented to *CONF* and I recommend a rejection.","Although the paper studies a relevant and important topic, which is about learning of hierarchy of concepts in an unsupervised manner, the reviewers raised several critical concerns. In particular, although the hierarchical structure of concepts is the key idea in this paper, the concept of hierarchy itself is not well explained. How to define the hierarchical level of concepts should be carefully and mathematically discussed. In addition, empirical evaluation is not thorough as reviewers pointed out. Although we acknowledge that the authors addressed concerns by the author response, newly added results are still confusing and more careful treatment is needed before publication. I will therefore reject the paper. <sep> This work reminds me the the topic called ""formal concept analysis"" (e.g. see [1]), which mathematically defines concepts as closed sets and constructs a hierarchy of concepts in an unsupervised manner. This method can be viewed as co-clustering and also has a close relationship to closed itemset mining. This approach is used in machine learning (e.g. [2]). I think it is beneficial for the authors to refer such existing and well-established approaches to elaborate this work further. <sep> [1] Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order, Cambridge Univ. Press (2002)[2] Yoneda, et al., Learning Graph Representation via Formal Concept Analysis, arXiv:1812.03395"
"Contributions: <sep> This paper analyzes the convergence of decentralized SGD with asynchronous updates, quantization and local updates, which is novel and challenging. <sep> The proposed algorithm requires significantly less communications to converge. <sep> The authors have done extensive analysis of the convergence under different settings with detailed proofs. <sep> The authors have done some large-scale experiments and show their algorithm performs great in practice. <sep> Strong points: <sep> The authors have done concrete non-trivial analysis. <sep> The algorithm is very general, several existing algorithms can be its special cases by different choice of parameters. <sep> The experiment section provides a large amount of empirical evidence. <sep> Weak points: <sep> Assumptions are too strong for Theorem 4.1 and 4.2: <sep> Assuming each node can sample from global data is too strong. Section I removes this assumption but without highlighting key steps. <sep> Step size requires the knowledge of the number of total steps. <sep> Number of total steps needs to be larger than n4. Even nodes don't communicate, the algorithm should still converge because the global sampling. <sep> The benefit of local steps is not clear. For example, if we optimize the convergence rate in Theorem 4.1 over H, the best choice is H=(λ22r2⋅f(μ0)−f∗L2M2)1/3. That is, the optimal H is smaller when r is larger. <sep> The H2 term in Theorem 4.1 and 4.2 may not be good enough. If set H→∞, then this bound should reduce to the single-machine SGD. However, the H2 term will go to ∞. <sep> Theorem 4.2 requires T∼O(∗). Does it work if T is greater? <sep> Definition of T is confusing. <sep> Arguments for acceleration is not convincing. The algorithm only have one pair of nodes communicate, it's not clear how to replace T with nT. <sep> Recommendation: <sep> Weak reject. As of the current version, the proofs need to be improved. However, I believe the authors can improve in the next version. <sep> Further questions: <sep> Is it possible to merge Section I with Theorem 4.1 or show the proof? I think there will be one term that depends on ρ2. When ρ2=0, Section I will reduce to Theorem 4.1. <sep> Lemma F.3 is confusing. I think Γt should decrease with t, or use diminishing step size ηt to control this term. Then there's no need to set η∼1T. <sep> Can you also show the run time plot for ResNet? <sep> Optional improvements: <sep> It may be better to remove some small terms to make rate more clearer. For example, <sep> For Theorem 4.1, use 1≤r2λ22 can get rid of the constant 1. <sep> For (14) and (19), use rλ2≤r2λ22 to get rid of the first order term. <sep> The 3rd equation in Section D, h~is also depends on g~i, which is not reflected. <sep> The 1st equation in Section E has an extra '-'. <sep> Is the coefficient $\\frac{n - 2}{n}# in Eq (18) missing? <sep> Update <sep> Thanks for the authors to address my questions. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept.","The reviews were a bit mixed: on one hand, by combining and adapting existing techniques the authors obtained some interesting new results that seem to complement existing ones; on the other hand, there is some concern on the novelty and on the interpretation of the obtained results. Upon independent reading, the AC agrees with the reviewers that this paper's presentation can use some polishing. (The revision that the authors prepared has addressed some concerns and improved a lot compared to the original submission.) Overall, the analysis is interesting but the significance and novelty of this work require further elaboration. In the end, the PCs and AC agreed that this work is not ready for publication at *CONF* yet. Please do not take this decision as an under-appreciation of your work. Rather, please use this opportunity to consider further polishing your draft according to the reviews. It is our belief that with proper revision this work can certainly be a useful addition to the field. <sep> Some of the critical reviews are recalled below to assist the authors' revision: <sep> (a) The result in Theorem 4.1 needs to be contrasted with a single machine setting: do we improve the convergence rate in terms of T here? do we improve the constants in terms of L and M here? What is the advantage one can read off from Theorem 4.1, compared to a single machine implementation? How should we interpret the dependence of (optimal) H on r and lambda_2? <sep> (b) The justification for T≥n4 is a bit weak and requires more thoughts: one applies distributed SGD because n is large. What happens if T does not satisfy this condition in practice, as in the experiments? <sep> (c) Extension 1 perhaps should be more detailed as its setting is much more realistic than Theorem 1. One could use Theorem 1 to motivate and explain some high level ideas but the focus should be on Extension 1-3. In extension 2, the final bound seems to be exactly the same as in Theorem 1, except a new condition on T. Any explanations? Why asynchronous updates only require a larger number of interactions but retain the same bound? These explanations would make the obtained theoretical results more accessible and easier to interpret."
"Summary <sep> The paper proposes a heuristic version of top-k negative sampling which is computationally effective. The main toolbox for this heuristic is locality sensitive hashing. The contribution is mainly algorithmic with an implementation. Experimental results support the effectiveness of the heuristic. <sep> Strengths <sep> The use of LSH in machine learning applications is very promising, and this paper takes this trend to yet another direction where it can make a difference. <sep> The approach is simple to describe and implement. <sep> Weaknesses <sep> There are several claims that are mostly marketing, namely ""adaptivity"" and ""distribution awareness"". Yes, the sampling depends on the updated weights. But exactly how and in what manner that relates to, say, approximating the true objective, are not treated. The best intuition we can get is by thinking of the new method as a heuristic to top-k negative sampling, leading to believe that maybe some of the latter's statistical/optimization properties are inherited. But this relationship is not made at all. <sep> The Theorems are mostly ornamental, they do not add anything new or relevant. <sep> It's true that the proposed algorithm is amenable to a CPU implementation. But the CPU on which the experiments are run is a behemoth (28 core, 224 thread). So it clearly still needs the added parallelism to be competitive. <sep> The significance of the result are weakened when compared against one of the simplest alternatives, the sample softmax (orange curves in Figure 4). It is evident that in terms of reaching the vicinity of the ultimate accuracy, the sample softmax takes the same (if not less) time than the new method [the per-iteration plots are not very relevant, they demonstrate interim accuracy but have no bearing on ultimate accuracy and speed up]. <sep> So claims like ""[these alternatives] fail to demonstrate any training time improvements"" (page 2) are clearly false. Granted, sample softmax would be of order O(log⁡N), but in practice this clearly is not a handicap. <sep> For an application such as this, the details of LSH's choices are critical, yet the paper only glosses over them. For example, the similarity metrics should differ between the ""embedding"" version and the ""label"" version, this is briefly alluded to in the experiments section without much explanation or references. [Edit: thanks for clarifying this.] <sep> Overall <sep> The motivation and approach of the paper are very strong. But the results are not as strong as they are hyped up to be, since a simple alternative achieves the same practical accuracy/time benefits and no guarantees are given for the accuracy of the new heuristic sampling, not even in terms of approximation of another heuristic such as top-k. This means any potential adoption would be based merely on experimental evidence. The fact that the algorithm can be implemented on a (highly parallel) CPU is not a good enough selling point, especially when not pitted against an equivalent optimized CPU implementation of the simple alternative. <sep> [Edit: The authors do not give any substantive feedback to my review, except for clarifying the hash choices. It is surprising that they object so vehemently to my intuitive description of their method as a heuristic to top-k, when they themselves write ""Our proposed negative sampling scheme is a proxy to topK-softmax. It selects the top-k classes via LSH [...]"". Also my reading of the sampled softmax is directly from their paper, showing a comparable accuracy-time tradeoff, but I was not refuted on this and instead was given other references claiming the inferiority of that method. I have updated my recommendation to reflect these shortcomings.]","All reviewers agree that this paper is interesting, but needs improvement in order to be suitable for a highly competitive venue such as *CONF*. Reviewer 3 is especially incisive and detailed, but other reviewers make similar points."
"Summary <sep> The paper suggests a novel framework (coined L2E) to learn a policy that is optimized to adapt quickly (and exploit) a wide range of unknown opponents. <sep> To do so it trains a base policy that is optimized so that it can maximize its expected reward against a variety of opponents using only a few updates of its parameters (i.e. a few gradient steps of a straightforward optimization problem). <sep> The various opponents that are used for the training of this base policy are generated in two steps. <sep> First, given a base policy, an ""hard-to-exploit"" opponent is generated adversarially (in a procedure coined hard-OSG), to minimize the the reward that the base policy would get by adapting to it (using the few updates that are allowed to it in its adaptation step). <sep> Then, given a base policy and a ""hard-to-exploit"" opponent, more diverse opponents are sequentially generated (in a procedure coined diverse-OSG) by optimizing their expected reward against the base policy while maximizing their ""diversity"" with the ""hard-to-exploit"" opponent and the already generated diverse opponents. <sep> More formally, this diversity between two policies is defined (and optimized) as the MMD between the distributions over the trajectories that they generate (when the policies are seen as MDPs ""playing"" against the given base policy). <sep> The base policy is then trained iteratively (as described above) against the diverse opponents, that are themselves generated (as described above) with the current iterate of the base policy. <sep> After exposing this training procedure, the authors evaluate L2E on 3 toy games, showing that the trained policies are indeed able to benefit from little adaptations to a variety of heuristic opponents and perform better than some baseline methods. <sep> They also empirically confirm that their ""diversity-regularized policy optimization"" indeed generates diverse policies. <sep> Last, the authors empirically show the effect of their hard-OSG and diverse-OSG modules on the performance of L2E. <sep> Pros <sep> This paper is tackling a very relevant, interesting and difficult problem. <sep> I find the general approach of optimizing a policy to be able to ""rapidly"" and exploit a broad range of opponents to be very exciting. <sep> To the best of my (admittedly limited) knowledge, the suggested approach is significantly novel. <sep> While maybe a bit ""roughly used"" the diversity-inducing regularization term, using the MMD over the distribution of trajectories induced by the policies is interesting and potentially has a broader applicability than only L2E. <sep> Cons <sep> After careful reading, several key points remain unclear to me. Most notably, after training of L2E and when facing opponents with unknown policies, how does the base policy adapts? Is it done using eq. 2? If yes, is the expectation over the trajectories approximated with the actual observations made during the observation? How many observations are being used? If my understanding is correct, clarifying those points would help put in perspective how fast it actually takes for L2E to adapt in practice. <sep> It looks (to me, because no comment is made about it in the manuscript) like L2E must scale terribly with the size of the action space. First it must be extremely computationally intensive. While this remains feasible for the toy games that were used in the experiments, I am having very high doubts that this would scale well with larger games (even BigLeduc poker is ridiculously tiny compared to actual poker). At least, some comments about the computational aspects of L2E, or empirical evidence that L2E can handle larger games would be nice. <sep> A point is made, several times in the manuscript, that the base policy becomes ""more robust and eliminates its weaknesses by learning to exploit hard opponents"". First, it is not really clear what is precisely meant by this. Without further assumptions on the class of games, I do not really see why the base policy would be having a good expected reward before adaptation (either in average over a broad class of opponents or against the optimal opponent), or even less why it would be hard to exploit (especially after adaptation). In fact the empirical results suggest that the base policy is breaking even against a random opponent (before adaptation) at Leduc poker, which seems rather weak to me. <sep> The last point brings to a more general issue. I understand that the value of the contribution is more empirical than theoretical. Yet, it is absolutely not obvious to me whether L2E is supposed to converge at all (let alone having a clear idea about to what kind of solution it would converge). I am not a specialist of game theory, but I understand this is likely a difficult setting to analyze. At least, empirical evidence on the convergence of L2E would, in my opinion, strengthen greatly the manuscript. <sep> I find that the writing could also be improved. While the algorithm is admittedly hard to fully describe in a very succinct way, the amount of repetions or redundancies in the first 6 pages suggests that L2E could be more concisely and sharply introduced. The split between the main text and the appendices seems a bit arbitrary to me as I definitely think more content about related work should be exposed in the main document. At the very least, appendices A and B should be referenced in the main text. As it stands, there is literally no indication in the paper that the related work section and the algorithms can be found in the supplementary materials. There are also a lot of imprecisions in the form of somewhat vague claims or missing important details. In addition to the ones already mentioned, I would for instance take the example of section 3.2.2. where it is not clear to me how the first policy of diverse-OSG is generated in the absence of the hard-OSG module. And in that same paragraph the bold statement that hard-OSG helps enhance the stability (what is meant by that exactly?) is not clear at all to me. It is also claimed in 3.2. that positive returns are guaranteed against opponent without a clear style (whatever that precisely means). I see rather mild empirical envidence of this but certainly not guarantees. Another minor point is that I find the Theorem 1 to be a bit weirdly formatted. I imagine the theorem is supposed to be the statement that MMD equals 0 iff the two distributions are equal, but then, the following sentence shuld be more clearly separated (as not part of the theorem) and it should be more clearly stated that the result is not a contribution by adding the reference where this result first appeared (Gretton et al. '07, I assume). If not, the derivation of the gradient computation does not really constitute a ""theorem"". Last, there are a number of typos, or verbs missing throughout the manuscript, that should be easy to fix (sorry, it's really not convenient for me to list them without line numbers...). <sep> I'm a bit puzzled by some implementation details of the diverse-OSG. Notably, there seems to be no weighing on the MMD term in eq. 11. That seems pretty arbitrary to me. Could you elaborate on that? More specifically, I would imagine that if the MMD ways too little, the generated policies will be roughly identical while they will be diverse but potentially arbitrarily bad (in terms of expected reward). Also, while I don't have an issue with the somewhat arbitrary choice of an RBF kernel, I am a bit more puzzled by the choice of a width of 1. But I could imagine I'm missing an argument as to why this is a good choice. <sep> In Table 1, L2E is reported to have a positive average return against the oracle, which is defined as ""making decisions based on perfect information"". It's not clearly described what those decisions are but unless they are pretty bad, there is no way L2E or any policy can win against it. (And it should be pretty easy to find and implement the optimal strategy for the perfect information game.) <sep> Reasons for score <sep> While I really want to emphasize that the problem is very interesting and that I like the premise of L2E, I think the paper, in its current form, is missing the target. <sep> The main reasons can already be found in the ""cons"" that I listed. <sep> To elaborate a bit further, I think that either the selected games for the experiments are too small and toy-like for a purely empirical paper (in contrast with AlphaStar or Liberatus achieving superhuman performance at games like Starcraft 2 or heads-up no limit hold'em, although they definitely tackle a different, and probably simpler problem). <sep> In this current form, I consider the experiments as a crude proof-of-concept, which could be totally fine if there were more theoretical analysis to support the suggested approach. <sep> Questions during rebuttal period <sep> I think several questions have already been raised in the rest of my review. <sep> Most importantly, I would really love to understand how the base policy is updated in a ""real setting"", after training (see my cons #1).","Well, this paper has achieved something remarkable in this review process: The initial scores came in at fairly low scores (4, 5, 3, 6). However, as the discussions / rebuttals went back and forth, the reviewers were able understand and see the merits of the proposed methodology. Namely, the setting of L2E (Learning to Exploit), which makes use of a novel method called Opponent Strategy Generation, to quickly generate very different types of opponents to play against. One more pertinent component is the use of MMD (maximum mean discrepancy regularization) which can remove the necessity of dealing with task distributions, and does a better job in creating diverse opponents. <sep> Having understood the technical approach, three of the reviewers decided to substantially increase their scores. R4 increased 4->6, R5 increased 5->6, R3 increased 3->4, while R2 held steady with a score of 6. It was also good to see empirical favorable results compared to other baseline methods: L2E had the best return against unclear opponents, such as Rocks opponent and Nash opponent. <sep> Without any reviewer arguing strongly for acceptance, the program committee decided that the paper in its current form does not quite meet the bar, and also that it would benefit from another revision."
"Summary <sep> This paper presents a variant of MAML that aims to make the meta-learned initialisation robust to adversarial examples. The core idea of the proposed method, ADML, is to compute two task-specific parameters in the inner-loop: one on clean data and one on adversarial data. In the meta-learning step, the clean task-specific parameters are meta-learned on adversarial data while the adversarial task-specific parameters are meta-learned on clean data. The authors demonstrate robustness to a range of adversarial attacks on miniImagenet. <sep> Pros: <sep> As far as I'm aware, this is the first proposal to meta-learn for adversarial robustness <sep> Method is agnostic to the model and the form of adversarial attack <sep> Reported results demonstrate robustness to a range of adversarial attacks <sep> Cons: <sep> The method is not really explored in any depth <sep> The method is limited to MAML <sep> Considered attacks are relatively weak [see 1] <sep> Recommendation: rejection <sep> Motivation: <sep> While the idea of adversarial meta-training is well motivated and generally sound, the specific method in this paper is primarily proposed and not really explored in any depth. The authors demonstrate that a simple approach of mixing adversarial and clean data in the usual MAML update doesn't work very well, but don't go in to any details as to why this failure motivates their method. In particular, ADML formulates two task optimisation problems (Eq. 2) for the same variable (the initialisation) - one is adaptation under clean data and one under adversarial data. While intriguing, I'm missing a motivation: why would we want to consider two independent adaptation trajectories under best / worst case scenario, and not one trajectory under mixed data? <sep> Given that ADML formalises two independent optimisation problems over the same variable, it is not at all clear how they should be combined. The algorithm itself does not indicate how this is done, the text indicates that the losses are simply averaged? If so, that induces a specific optimisation problem that is neither of the two presented in Eq. 2. Have you considered what what that optimisation problem looks like? <sep> Perhaps the least obvious choice in ADML is to swap clean and adversarial data in the meta-update. While interesting, there is no apparent explanation for this and as a reader I would like to know what motivates this design choice and how much it really matters. Given that the experimental section is restricted to miniImagenet only, some form of simple analysis would have gone a long way to shed light on the proposed method. I would have liked to see an ablation on the design choices of ADML to gain insight into which of these are responsible for the observed performance gain. <sep> Finally I have a concern with the evaluation protocol: the algorithm suggests that adversarial examples are generated by drawing a fresh batch of data. In a K-shot regime, does this mean that you draw two batches of K samples per task adaptation? This would mean that ADML see twice as much data and result in unfair comparisons. <sep> Minor issues <sep> There are several grammar and spelling mistakes that makes the paper look rushed (pages 1, 2, 3) <sep> In the motivation, the authors use the phrase 'arm-wrestle'. What is meant by this analogy? <sep> Problem statement: a meta-learner (at least MAML) does not take several datasets as input, only one at a time. <sep> why are adversarial examples generated from a different batch of task data? Could they not be generated from the same batch? <sep> Algorithm: (1) the variables \\bar{D} are never used and (2) L14 makes a nonsensical double assignment to \\theta. <sep> Figure 1: the meta-update arrows should originate in the initialisation since that is what they are updating. <sep> Experimental setup: you mention that 15 adversarial attack mechanisms where leveraged in the experiments, but the tables suggests that you only use 1 (at a time)? <sep> Why does MAML-AD do worse on clean-clean? Should it not be exactly equivalent to MAML in this case? <sep> Typos: <sep> ""none of existing works have well addressed"" -> ""existing works have not yet addressed"" <sep> ""which, however, are"" -> "", which is"" <sep> ""an existing meta-learning algorithms"" -> ""[...] algorithm"" <sep> ""we show such a approach"" -> ""we show such an approach"" <sep> ""testing dataset"" -> ""test dataset"" <sep> ""In the meta-training"" -> ""In the meta-training phase"" <sep> Post Rebuttal <sep> I have read the authors rebuttal and updated manuscript. The authors have taken some steps to clarify parts of the manuscript, but my main concerns remain and thus my score is unchanged. In particular, the authors did not clarify how the two optimisation problems in Eq. 2. relates and what this means algorithmically. The authors defend their empirical setup by stating that meta-testing is still K-shot. While this is true, it is also true that their method have enjoy a greater amount of meta-training compared to baselines. What I would have liked to see is an ablation that trains the baselines for 2x meta-updates, but this is unfortunately not provided. <sep> Echoing other reviewers, if stronger attacks are considered, these should not be relegated to appendix. <sep> References <sep> [1] Goldblum et. al. Adversarially Robust Few-Shot Learning: A <sep> Meta-Learning Approach. 2020.","The paper proposes a new meta-learning algorithm which promises greater robustness to adversarial examples. I will be brief, as the fault with the paper is quite clear: the experimental results are not sufficient. The attack used (FGSM) is particularly dated and weak, and the comparison to existing defences is insufficient. Additionally, prior work (Adversarially Robust Few-Shot Learning: A Meta-Learning Approach) obtains better results, and is not compared against. The reviewers provided further criticism regarding the motivation for and explanation of the method, but the empirical aspects of the paper are where it primarily falls short of the publishable standard for *CONF*. <sep> I recommend rejection, and invite the authors to consider demonstrating robustness to a wider range of attacks (including non-gradient based), and a more thorough comparison to defence methods, before resubmitting to another conference."
"Summary: <sep> This paper makes two contributions: <sep> Formalizing the offline meta-RL paradigm, where we meta-train on pre-collected (offline) data for several RL tasks and adapt to a new task with a small amount of data. Within offline meta-RL the experiments focus on the fully offline case, where the meta-test task is also offline. It could be online with the meta-train tasks remaining offline, but then we would have to meta-learn an exploration policy, which isn't done in this work. <sep> Introducing MACAW: an algorithm for offline meta-RL that has the desirable property of being consistent (i.e. converges to a good policy if enough time and data for the meta-test task are given, regardless of meta-training). To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper-network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. <sep> Pros: <sep> The paper proposes an important area of research <sep> Most of the experiments are well executed, using good baselines and as well as providing understanding through ablations <sep> MACAW is a nice simple algorithm with good guarantees. <sep> Cons: <sep> I think the offline meta-RL paradigm is not introduced correctly. <sep> In particular, the paper largely borrows the meta-RL formulation from the online setting where task=MDP. It then treats the collection of the batch data as an obvious after-thought once the task is defined. However, in the offline setting the policy that generated the batch of data is of critical importance and should be part of the task definition. <sep> For instance, it is not the same to receive an MDP and examples from the perfect policy (where you can take supervised examples of a given s) than from a random policy (where there is no signal) or an adversarial policy that tries to act as bad as possible. This also has consequences at the meta-level: if at meta-training time I only see examples of perfect policies I may learn to imitate them, then at meta-test time I see an in-distribution MDP but data coming a bad policy and imitating it is a bad idea. In the definition of 'task' described in the paper this is fine since task=MDP. However, we would expect this to work poorly since the policy is out-of-distribution with those since at meta-training. <sep> This effect of the quality of the data is used in the experiment of figure 4 left, which makes the experiment good (a Pro), but does not detract from having (IMO) the wrong formalization. <sep> This is a big minus since this formalization is one of the big contributions of the paper and it could affect further papers in that area. However, there is a chance I am wrong since the authors have spent months on this and I've spent only some hours reviewing the paper. <sep> One of the two improvements over MAML+AWR, the weight transform, is not fully justified: <sep> From a conceptual point of view, it's not clear to me why this is the first time MAML has needed this change after being used in tens/hundreds of experiments. What is different on this task that hasn't been true in any other task in the past? I understood we're doing it to increase the representation capabilities of the gradient, but wouldn't this be useful for other meta-learning tasks? If so, why wasn't it used in the past? (specially since the bias-only version of the idea was already proposed by Finn et al. in 2017) <sep> From an experimental point of view I couldn't find the details of the ablated version on the main text appendix B, D or E. Therefore, I understood we're simply changing it to just a weight matrix of the same dimension. If that's the case one could argue that to make things comparable we should try increasing the width by a factor of c (c defined as in the appendix) to have roughly the same number of parameters, as well as possibly the depth, since that would be a simpler change with a similar latency to the weight transform version. <sep> [edited post-rebuttal to correct inaccuracy] The fact that there is a recent/concurrent paper is not ideal. However, I didn't weight it in my consideration. <sep> Clarity: pretty high <sep> Significance: somewhat high, except for similar concurrent NeurIPS work <sep> Questions: <sep> Figure 4 left I didn't understand if the quality changed at meta-test only or both at meta-train and meta-test. <sep> Any conjecture on why PEARL performance goes down in cheetah-velocity? <sep> Doesn't AWR rely on the policy providing the data being somewhat good already? Otherwise the advantage function may be very different for the policy that generated the data vs. the optimal policy. <sep> Details: <sep> Figure 3 has logarithmic x axis and figure 4 (left, center) have linear x axis. It may be better to keep them the same. <sep> ""An important property of a meta-RL algorithms is thus its robustness"" --> offline meta-RL? <sep> In Related work Kirsch 2020b,a seems to be the same paper cited twice <sep> Summary of review: because I believe the formalization has a big flaw and this paper is mainly about the formalization, I have to recommend rejection. I also have major concerns regarding the weight transform and the experiments that were done to prove its usefulness. The paper is otherwise good, interesting, well-written, and timely; I'm looking forward to the discussion and updating my review if my initial assessment was wrong. <sep> ============ <sep> Update after discussion with authors <sep> I had two main concerns: <sep> The modification to MAML was unconvincing to me. <sep> The offline meta-RL formulation should include behavior policy as part of the task definition. <sep> After a very detailed response from the authors, I am now happy with the response and extra experiments w.r.t. the MAML modification, but I still have concerns about the formulation. In particular, reading the final version I still think the policy giving the behavior data is treated as an after-thought and is instead assumed constant across all tasks. For instance, IMO figure 1 should contain multiple examples of the same ""RL task"" that are different ""offline RL tasks""; i.e. learning to swim using guidance from a 3-year-old and learning to swim using guidance from Michael Phelps. This is one of the key differences, IMO, between meta-RL and offline meta-RL and given that this paper's main contribution is introducing offline meta-RL, I feel it really should be very clear about this point. It may be fine to first introduce the correct general version and then say something like ""it may be useful to assume each RL task is given by an expert of roughly the same characteristics"", i.e. we can assume behavior policy is constant across tasks. However, right now the original formulation directly borrows from regular meta-RL and I believe that may confuse future papers in offline meta-RL. <sep> I've increased my score from 4 to 5 since I'm now less concerned about the MAML improvement, but I cannot recommend acceptance given my concern about the formulation.","The paper proposes a method for offline meta-RL, where we meta-train on pre-collected offline data for several RL tasks and adapt to a new task with a small amount of data. The paper assumes that there is no interaction with the environment either during meta-train or meta-test. In this setting, motivated by the ide of leveraging offline experience from multiple tasks to enable fast adaptation to new tasks, the paper introduces MACAW, which combines the consistent MAML and the popular offline AWR, improving upon them by adding capacity through parameterization and adding an extra objective in the policy update. As a result, the MACAW proposed for the offline meta-RL has the desirable property of being consistent, i.e., converging to a good policy if enough time and data for the meta-test task are given, regardless of meta-training. <sep> Pros: <sep> Most of the experiments are well executed, using good baselines. Extensive ablations on the various modifications to MAML+AWR confirmed the utility of the approach for the fully offline meta-RL problem. <sep> MACAW is a simple algorithm with theoretical guarantees; the modifications to the policy functions are backed by theory. <sep> Cons: <sep> The reviewers have concerns on the formulation of offline meta-RL. One major contribution of the paper is to introduce offline meta-RL. However the paper largely borrows the meta-RL formulation from the online setting where task=MDP. The reviewers think that directly borrowing from regular meta-RL as the formulation of offline meta-RL might be misleading. The reviewers suggest including behavior policy as part of the task definition for offline meta-RL formulation. <sep> Several reviewers raised concerns that the fully offline setting might be unrealistic. Although the author did add a motivation, the reviewers would be interested in seeing MACAW being adapted online at test time on in-distribution tasks. <sep> Unfortunately, the authors accidentally revealed their names in one of the modified versions."
"Summary: <sep> This paper sheds light on the impact of nondeterminism to the run-to-run variability of neural network performance---a situation many people using neural networks have experienced. The authors establish an experimental strategy to analyze the different sources of nondeterminism. Some sources of nondeterminism are parameter initialization, data shuffling, data augmentation, regularization and cuDNN. <sep> The authors make the surprising discovery that each source of nondeterminism results in an equal amount of variability and model diversity. By modifying weights by a single bit, they experimentally demonstrate that an inherent instability in the neural network optimization procedure is the main reason. They show methods such as snapshot ensembling can reduce the observed variability. <sep> ######################################## <sep> Strong points: <sep> The discovery that each nondeterminism source has a similar effect is novel and is not in the literature as far as I know. Such results are intriguing, unexpected and useful. <sep> The experimental methodology used is well-explained and fair. <sep> Linking all the nondeterminism to a change of one bit in model weights is interesting and successfully highlights the instability and sensitivity of neural network optimization. <sep> ######################################## <sep> Weak points: <sep> Please can the authors clarify the takeaways of section 4.2? At the moment the novelty or surprise is not entirely clear. If the single linear layer problem is convex, it is expected that a single bit initialization perturbation still leads to the global minimum. Due to nonconvexity, the one hidden layer networks can have different minima and so there is more variability. But how does the extent of the variability change with depth? <sep> Please can the authors elaborate on the connection between the ensembling solution and how it prevents optimization instability which was identified as a root cause? Ensembling methods in general will help variability but there is no change in the optimization and training process and ensembling gains are due to other reasons. <sep> While it is understandable that there is significant compute time required to do multiple runs, the networks used are much smaller than current state-of-the-art networks. For example, a WideResNet-28x10 can give approximately 95% test accuracy for CIFAR-10 and has a much larger capacity than the ResNet-14. The paper would be strengthened by having experiments on popular benchmark state-of-the-art networks (maybe fewer runs and not as many nondeterminism sources as in Table 1). This is especially relevant due to the huge gulf in the number of trainable parameters in state-of-the-art networks which means potentially very different functions can be learned run-to-run---do the trends in Table 6 carry through for big networks? <sep> Why were 500 epochs used for the CIFAR experiments? 500 epochs are plenty for training the ResNets for CIFAR-10 classification and is much larger than what is common in the literature. Furthermore, with five snapshots, this is like having five models trained for 100 epochs each which on its own is enough (or almost enough) for a single model. Do the authors have a more realistic set of experiments (such as a total of 200 epochs and four to five snapshots)? <sep> ######################################## <sep> Recommendation: <sep> Overall I lean towards rejection. I think the results (such as Table 1) are extremely interesting and should be known in the community, however, the current analysis of the optimization instability should be developed. Furthermore, the experiments could be improved as described elsewhere in this review. <sep> ######################################## <sep> Additional questions and clarifications that will help assessment: <sep> Please address and clarify my above questions and queries. <sep> In the experiment of Figure 2, is the cosine learning rate decay used from epoch 0? If so at later epochs when the nondeterminism is activated, the learning rate is lower and the model cannot explore and change as much and so reduction in variability may in part be due to the learning rate. <sep> Section 6 mentions that there are 18 layer ResNet experiments. Please can the authors include these? <sep> ######################################## <sep> Additional feedback that does not necessarily impact recommendation: <sep> Typo: <sep> - Page 7, first paragraph 'in this was with only' <sep> - Appendix B says that Table 7 is for MNIST, but the table says CIFAR <sep> ######### After author response ######### <sep> I thank the authors for their responses and updates to the manuscript. While I still feel that the connections between optimization instability and the observed phenomena could be developed further, the updates strengthen the paper and the experimental results are extremely interesting. I have increased my score for these reasons.","This paper investigates the topic of nondeterminism and instability in neural network optimization. The reviewers found the results on different sources of nondeterminism particularly interesting and relevant. The experiments are carried on both language and also vision, which strengthens the findings. Concerns were raised about the use of smaller non-standard models, which were somewhat mitigated by the addition of Resnet-18 experiments on CIFAR. The reviewers also noted that the measures used in the experimental protocol were already present in the literature, and that the proposed mitigation strategy is from another work. Furthermore, R2 also found that the optimization instability section should be more developed. The paper should be resubmitted with an improved discussion of related works and more developed section on instability as suggested by the reviewers."
"This paper proposes an efficient transformer variant by replacing softmax in the self-attention layer with a RELU activation and arranging the computation using element-wise products and global/local pooling. This reduces complexity to linear complexity in the non-autoregressive case and log-linear complexity in the autoregressive case. The evaluation shows that it can reach the performance of a vanilla transformer in most of the examined tasks while having fewer memory requirements in general. <sep> Strengths: <sep> The paper is reasonably well-written and clear for the most part. The problem of scaling transformers to longer sequences is an important one since transformers cannot deal otherwise with long sequences due to their quadratic complexity. <sep> The proposed idea is interesting and reminiscent of recent methods that re-arrange self-attention computation using kernels albeit it differs in the way the computation is carried out. This one is simple computation-wise and does not aim to approximate the original computation in any way. <sep> The evaluation performed on multiple tasks shows that the proposed approach can reach the quality of a vanilla transformer and be more memory-efficient. <sep> Weaknesses: <sep> (1) The motivation of AFT and the positioning with respect to prior work were somewhat weak. The introduction does not acknowledge recent efforts towards efficient transformers and what is the unique contributions of this work. What are the benefits of AFT compared to recent established efficient transformers such as Sparse Transformer (Child et al., 2019), Reformer (Kitaev et al., 2019), or Linear transformer (Katharopoulos et al., 2020)? It is unclear why one should prefer the proposed variants over existing ones both from theoretical and practical perspectives.  Related work states some previous efficient transformers without any individual discussion about their merits or limitations in comparison to AFT. <sep> (2) One major limitation that stands out from the experiments, despite their size, is that there is no head-to-head or controlled comparison with a previously established efficient transformer such as the ones mentioned above. The results compared to Sparse Transformer given in Table 3 are not directly comparable since the model size and design are quite different.  In brief, it is not very clear what are the practical benefits compared to previous efficient alternatives. <sep> (3) The memory benefits are not reflected or they are not as important when looking at the quality achieved in the tasks where a speed-quality trade-off was reported.  In language modeling,  AFT has higher perplexity (even when it uses a much larger number of parameters) which makes the memory benefits less interesting. In MT, AFT reaches the performance of the baseline but then the efficiency benefits are not present. So, I am curious is it the same in the two former tasks when comparing to the vanilla transformer? Under what circumstances we should expect AFT to reach vanilla transformer performance and still offer clear efficiency benefits when using the same setup? <sep> (4) In terms of training speed, AFT is generally slower than the vanilla transformer when the form reaches the same quality as the latter. Also, it is especially slower when the depth is small in Table 2 (~30% with 12 layers). Could the authors elaborate a bit on why that happens?  Moreover, it would be useful to show in Table 2 what is the quality (NLL or bits/dim) achieved by each model because it's hard to tell how good the speed-quality tradeoff is. <sep> (5) Recent studies have shown that it is possible to speed up inference time using efficient transformers (see above). What is the benefit of AFT during inference time?","The new non linearity proposed in this paper present interesting observations and improvements on image and text datasets. <sep> However, reviewers point out that there should've been more comparisons to other efficient transformers and on more datasets. <sep> The speed improvements are also not clear. <sep> I'd encourage the authors to revise and submit in the future."
"The paper investigates the over-parameterization of attention heads in Transformer's multi-head attention. The authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. They propose a reparameterization of multi-head attention allowing the parameters of queries and keys to be shared between heads: this is called ""collaborative attention"". This attention can be applied either instead of the standard attention during training, or as a drop-in replacement for an already trained model. To use as a drop-in replacement, the method requires to use tensor decomposition and subsequent model fine-tuning. <sep> Strengths a nice analysis of PCA components showing that individual heads are not low-rank, but their concatenation is. <sep> the paper is overall clear and the method is explained well. <sep> Weaknesses <sep> (main) While the main contribution is a more efficient attention layer without a significant drop in performance, this claim is not supported empirically. Since the method operated only within attention layers (reduces only dimensions of queries and keys), in terms of efficiency/quality trade-off it should be compared to other methods, e.g. simple head pruning. While the paper does not provide such comparison, it is clear from the results that the simple head pruning is likely to be superior (and is simpler implementation-wise). <sep> Namely, <sep> when used as a drop-in replacement, the method is more complicated than a simple head pruning, but not more effective. E.g., the proposed method reduces query/key dimension by a factor of 1.5/2/3 and keeps everything else intact. With simple head pruning, about 50% of the heads can be removed without sacrificing quality: in addition to queries and keys, this also halves values and the output projection matrix. <sep> when used in training (see the MT experiments), the results are also not better than post-hoc head pruning. The argument to support this new approach could be that the model is smaller in training, but since the parameter reduction is only in queries and keys, the decrease in the number of parameters is negligible for the whole model. For example, keeping the same quality it reduces the number of parameters from 6.110^6 to 5.410^6, which is not going to make a large difference. <sep> (minor) As a side contribution, the authors claim to report the discrepancy between the theory and implementation of attention layers. Namely, while the original definition of attention layers does not have biases in linear projections for q/k/v in attention, the authors claim that implementations contain the bias terms, and spend some time showing how to model the biases in key and query layers properly. <sep> However, the original Transformer implementation (tensor2tensor) does not have biases by default. This means that the authors are probably referring to some specific implementation, different from the original one provided by the Transformer's authors. Therefore, I can not consider this as a contribution and think that this part is misleading for a reader. <sep> P.S. Here is the tensor2tensor code I was referring to: https://github.com/tensorflow/tensor2tensor/blob/5f9dd2db6d7797162e53adf152310ed13e9fc711/tensor2tensor/layers/common_attention.py#L4416 <sep> Overall recommendation <sep> Overall, I can not recommend accepting this paper. While there are some parts of the paper that I like, the main claim is not supported empirically: both in terms of baselines and the overall decrease in the number of parameters. Additionally, the part with the biases in attention implementation is misleading. <sep> Update after author response <sep> Context and content attention <sep> Thank you for updating and saying that only some of the implementations include bias! I think now this part is not misleading and can be of interest. I still have some concerns that this part does not fit the whole story very well - but this is the matter of taste. In the current state, I think it is ok :) <sep> On the comparison with head pruning and on the paper going beyond practical realm. <sep> I agree with your comments, but I do think you should make it very clear in the paper. In the current state, the paper tried to make practical contributions and, since they mostly do not hold (e.g., head pruning is simpler in practice), it's hard to appreciate the paper's value. I think you need to modify the things you highlight, and with proper discussion it would be much better. For example, if you state explicitly that in practice pruning may be simpler, but your results say/illustrate something other than practical applications. You won't lose because of it; in fact, I think the opposite. <sep> Overall, I think the paper has improved during the discussion period. In a hope that the authors address my later comments and discuss the pruning in the text, I'm raising the score.","This paper proposes an interesting collaborative multi-head attention (MHA) method to enable heads to share projections, which can reduce parameters and FLOPs of transformer-based models without hurting performance on En-De translation tasks. For pre-trained language models, a tensor decomposition method is used to easily covert the original MHA to its collaborative version without retraining. <sep> This paper receives 3 weak reject and 1 weak accept recommendations. On one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting. On the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the paper's main claim. From the current results, it is difficult to draw a conclusion that collaborative MHA is better. <sep> Specifically, (i) From Table 2, it can be seen that the proposed method is not effective for pre-trained models, i.e., even if the model size is not reduced much, the performance can be dropped significantly. (ii) More experiments, such as QA, more translation/generated tasks will make this paper more convincing. (iii) More rigorous experiments are needed to justify the practical value of the proposed method. If the authors try to emphasize that they go beyond practical realm, then probably a careful re-positioning of the paper is needed, which may not be a trivial task. <sep> The rebuttal unfortunately did not fully address the reviewers' main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere."
"Summary: This paper focuses on undirected exploration strategies in reinforcement learning. Following the prior work, this paper proposes an exploration method unifying the step-based and trajectory-based exploration. The authors propose to perturb only the last(linear) layer of the policy for exploration, instead of perturbing all layers of the policy network. Also, the authors use analytical and recurrent integration for policy updates. Experiments show that the proposed exploration strategy mostly helps A2C, PPO and SAC in three Mujoco environments. <sep> Clarity: <sep> This paper is generally written clearly. Some details need more clarification as pointed out in 'Cons'. <sep> Originality: <sep> As far as I know, the proposed technique is novel in the literature of undirected exploration. But for the three bullet points in section 1, the first point of ""Generalizing Step-based and Trajectory-based Exploration"" should not be one of the main contributions of this paper, because this paper follows the formulation of policy in van Hoof et al. (2017) and the latter proposed the generalized exploration connecting step-based and trajectory-based exploration. The work can be viewed as an extension of van Hoof et al. (2017)  with a deep policy network. <sep> Significance: <sep> The proposed method is mathematically solid, but the main concern lies in empirical performance. Nowadays SAC is the state-of-the-art and generally used method for continuous control tasks and it is more advanced than A2C and PPO. But the proposed method does not obviously improve the performance of SAC while inducing much more complexity in policy learning. Therefore the significance of the proposed approach in practice might be limited. <sep> Pros: <sep> *The authors provide detailed mathematical derivation (in the main text and the appendix) to support the proposed method. <sep> *The proposed method significantly outperforms the baselines when investigating the on-policy methods A2C and PPO. <sep> *The authors provide ablative studies about hyper-parameter values and components of the proposed method with A2C. <sep> Cons: <sep> *In section 4.2, ""we maintain and adapt a single magnitude σ for the parameter noise"". What's the motivation of this setting different from the formulation in section 4.1? <sep> *In section 5, why the advantage of the proposed method is poor with SAC? What's the value of hyper-parameters α and δ? Is the proposed method sensitive to these hyper-parameter choices? <sep> *In section 5, apart from the comparison of the performance of the learned policy, the comparison of the complexity (which might be measured by wall time to learn the policy?) of different exploration strategies can also be interesting. <sep> *In the first two rows of Figure 1, why the baseline methods NoisyNet-A2C(PPO) and PSNE-A2C(PPO) even significantly underperform the vanilla A2C(PPO)? The intuition is that introducing exploration strategies will mostly help the agent learns more quickly. Is it possible that the baselines are not tuned well? <sep> *The experiments on a single domain (Mujoco) seems not convincing enough. It will be better if there are experiments on other more complicated domains.","Unfortunately some of the reviewers' reactions to the author feedback won't be visible to the authors. <sep> The reviewers highly appreciated the replies and revision of the paper <sep> Pros: <sep> The paper renders Generalized Exploration tractable for deep RL. <sep> The idea is applicable to many DRL methods and is potentially very valuable to deal with the headaches associated to DRL. <sep> Cons: <sep> R2 and R4 are still concerned about whether 'smart' exploration will always be advantageous, and whether the added complexity is a good trade-off for the (potentially) better performance. A comparison to 'pure' exploration would still be insightful. <sep> the new 'SAC with Deep Coherent Exploration' only partially addresses the concerns of R2 and R4, especially in terms of performance <sep> While the paper has improved drastically during the reviewing process, there are still a few too many doubts."
"This paper proposes a method for estimating the probability deinsity distribution on a low-dimensional manifold embedded in a high-dimensional space using Normalizing Flow (NF). The problem is that the universality of NF is limited because low-dimensional manifolds are not diffeomorphic with respect to high-dimensional Euclidean space. The proposal of this study is to make NF applicable by inflating low-dimensional manifolds with Gaussian noise. Then, after the transformation is obtained, the probability distribution on the original low-dimensional manifold can be obtained by deflation. <sep> There is a detailed discussion about the addition of Gaussian noise. Theoretically, any Gaussian noise e at coordinate x can be decomposed into projections on tangent space Tx and normal space Nx as e=et+en. Only normal noise en is ideal to add. However, it is not realistic to find this at each coordinate x. This paper argues that Gaussian noise e is a good approximation of en when the manifold dimension d is sufficiently smaller than the higher-dimensional Euclidean space dimension D. This also argues that the noise variance  sigma at that time should be set according to the inverse of the density. <sep> In computer experiments, simple simulation results using the von mises distribution on a circle and a sphere are shown. <sep> The gist of the manuscript is well-written, and the issues it deals with are also important and interesting. <sep> Theoretically, interesting discussions are being developed, but discussions and experimental results are somewhat weak regarding the merits of practical application. <sep> I have some questions. <sep> I couldn't understand the description about scalability well, ""our method scales to high dimensions because it is based on one NF and does not require to compute its inverse."" Isn't the scalability the same as normal NF because the proposed method basically uses normal NF? <sep> In this research, Authors propose to add noise to the data sampled from low-dimensional manifolds in a pseudo manner, but in actual measurement, noise has been included in data naturally without adding it in a pseudo manner. Is it necessary to add pseudo noise even when applying it to real world data? <sep> The denoising auto-encoder is famous as a manifold learning method that adds pseudo noise. Is there any relation to this? Also, I would like to know if there are any advantages of the proposed method over the denoising auto-encoder. <sep> VAE is a well-known method for finding the mapping of low-dimensional space to the normal distribution. Is there any relation to this? Also, I would like to know if there are any advantages of the proposed method over VAE. <sep> In relation to the above, it may be good to have a comparison experiment with denoising auto-encoder and VAE. <sep> The lack of experimental results with actual data makes that the paper is unconvincing. Especially for the description of scalability, it is better to prove it experimentally. <sep> This is a pure question, is it possible to know the dimensions of the manifold through this technique?","The paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional. Although the method is appealing, I believe the paper falls a bit short of acceptance at the conference. Too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating low-dimensional distributions living in high dimensional spaces. We encourage the authors to use the feedback contained in this round of reviews to improve their work."
"The paper proposes a method for Neural Architecture Search (NAS) with two stages of search. In the first  stage, the topology of the cell is searched with only one operator (skip connection) using graph pruning through gradient descents. In the later stage, there are two ways to search the operators. In the first approach, the found topology is equipped with some operators (e.g., 3x3 convolution,  skip connection, and 3x3 dilated convolution) and then the architecture parameters are optimized. Another approach is to replace all operators with one single operator e.g. convolution. The experiments show that the searching time is reduced significantly compared to DARTS and the results on CIFAR-10 and ImageNet are very competitive. <sep> Strengths: <sep> The search time is reduced significantly from normal DARTS and PC-DARTS. <sep> The results are competitive to the prior differentiable NAS methods with less searching time. <sep> Weaknesses: <sep> It is not very clear how exactly the topology search algorithm ensures that each node is connected? <sep> The choice of skip connection as the operator for topology search is not well-justified. Max-pooling has no learnable parameters too. Is there any particular reason why skip connection? <sep> The experiments are lacking because there is no evaluation on current benchmarks such as NAS-101, NAs1Shot1, or NAS-201 that can explicitly show how the search algorithm performs. In current experimental results, it is very difficult to judge if the proposed approach has benefits for searching 'good' architectures. <sep> The comparison with other differentiable NAS algorithms (e.g., DARTS and PC-DARTS) can be delivered in the form of a table. The information of this comparison is all scattered in the manuscript. <sep> In Section 3 ""In FTSO, this problem is almost nonexistent because, the skip connection has no kernel weights to tune"", this is not very clear how the problem exists in the DARTS formulation and how the formulation in the proposed approach can mitigate this issue. There is no discussion about the bi-level program in DARTS. <sep> It is very difficult to follow the overall approach, it would be better to have an algorithm/pseudocode. <sep> "", we take the second strategy as the default configuration because, it is much more efficient, and the large amount of kernel weights, contained in the first strategy, may lead to over-fitting, <sep> and finally leads to worse performance than the second strategy. "". How do exactly large amount of kernel weights lead to overfit in NAS? Is there any proof/citation about it? The second strategy has no clear evidence to always give better results compared to the first strategy. <sep> There is no empirical results how the algorithm works by varying the number of nodes in a cell. This will show that the limit of the proposed approach and also its capacity to handle various number of nodes. <sep> The manuscript is poorly written,  at least these sentences/paragraphs must be clarified and revised: <sep> ** In Section 3,  ""it might be possible to cluster the architectures according to their connection topologies"" <sep> ** In Subsection 4.3, ""When we do not search for the operators, after the topology search, we assign all the remaining edges a fixed operator"" and the rest of the paragraph is difficult to follow. <sep> ** In Section 3, ""Secondly. Because our supernet contains very few parameters, it can hardly over-fit the dataset. Thus, the found architecture may generalize better on new datasets."". This sentence is a claim without any evidence. <sep> ** Table 3 is not clear, some codes are not described in its caption. <sep> ** In Section 3, please fix ""Firstly. It allows the algorithm to converge within extremely few iterations... and so on..""","Three reviewers have reviewed this manuscript, and they had severe reservations regarding the presentation quality and the lack of sufficient theoretical support behind empirical observations. Even after rebuttal, the reviewers maintained that the above issues are not fully resolved. Unfortunately, this paper cannot be accepted in its current form."
"Summary: the idea of prioritized experience replay is revisited, but from a new perspective with new theoretical results. Here, the authors propose the expected value of backup (EVB) as a metric to assess the quality of a sample and its potential improvement on the policy and on the value function. The authors decompose this metric into the benefit attributed to the policy and benefit to the value function. The authors have two theorems. The first theorem shows that the surprise (aka the temporal difference error) is an upper bound of the EVB in the Q-learning setting. The second theorem shows that the surprise, multiplied by a constant that depends on the policy, is an upper bound to EVB in the soft RL setting. The authors demonstrate that the proposed tighter bound on the EVB could yield improvements in the soft RL setting. <sep> Pros of this work: <sep> the work tries to tackle an important and still not fully answered question, which is why prioritized replay works well in the DQN setting but not in the soft RL settings, making very nice connections to the existing empirical literature on the topic, and a suggestion of how to improve PER <sep> the paper is very clearly written and the motivation of addressing PER from a rigorous perspective is clear. <sep> overall, a rigorous study of the ""quality"" of samples and experience seems like a very promising direction if the goal is to develop more intelligent agents that can make better use of the information available to them the authors try to demonstrate why the approach leads to tighter bounds than standard PER with some visualization (although I point out some issues below) <sep> Cons: <sep> lack of motivation regarding the choice of EVB <sep> more thorough experimentation lack of evidence suggesting a tight upper bound (e.g. modest improvement in soft RL, maze example does not suggest a tight bound in Q-learning) <sep> Details: <sep> lack of motivation regarding the choice of EVB: while the EVB seems like an intuitive starting point for investigation, its motivation and a-priori connection to prioritized replay is not fully clear. one of the problems with EVB is that it seems to be a largely myopic measure of quality that is linked to only the current sample, e.g. (s,a,r,s'), while ignoring the effect of the sample on further backups in the rollout. Recent work on the topic [1] (PSER) suggest that a myopic approach can be significantly improved by up-weighting earlier transitions that lead to good transitions in the future. This would seem to suggest that a non-myopic metric could lead to more significant improvements than working with a myopic setting which could be fundamentally flawed. While I do agree that EVB is a good starting point for analysis, I am not convinced the results are fully conclusive, particularly given the need for stronger motivation on EVB and more conclusive experiments (see below) . I would encourage the authors to think about how to better motivate the use of EVB in the papers introduction more clearly if existing literature suggests so (I am somewhat aware of [2], although the authors there do highlight the shortcomings of EVB). Could sequential replay be the result of using an underlying metric that is less myopic than EVB, which seems to shows better promise over PER? If not, why is EVB truly the only right approach for analyzing PER? <sep> more thorough experimentation: while I concur that the current paper addresses an important theoretical question, and while the result appears trivial to implement (not more difficult than PER), the bound should also be justified experimentally if it is to have significant value in practice. I think one key detail that is missing and could greatly benefit the paper is a more careful analysis and visualization of what the modified tighter bound is actually doing in the soft-Q setting. For example, how does the ranking of experience change when using the ""tighter"" bound? How often does this bound lead to a revision or re-evaluation of experience v.s. PER? Is there a fundamental reason why PER could not be as effective for soft RL in general that cannot be explained by better myopic estimates of the value of each transition? <sep> lack of evidence suggesting a tight upper bound - Q: while the authors argue at several points in the paper that the |TD| could be a tight upper bound, this bound is only attained in some special cases. However, this does not really mean that the upper bound is tight for Q-learning, since there could be other upper bounds based on |TD| that incorporate additional value or policy information that could be tighter (same also applies for soft RL). The Figure 1 also seems to suggest that the values are scattered quite randomly and do not attain the upper bound as claimed for Q-learning. This does not suggest that |TD| is tight in any way. Can a gap be proved that effectively provides a lower bound on EVB? <sep> lack of empirical evidence supporting the tighter bounds - soft RL: The experiments on Atari also suggest that the improvements of VER are quite mild, only improving on PER with statistical confidence on two of the experiments (interestingly, these two experiments also show mild improvement by PSER, whereas PSER shows considerable improvements on other games evaluated here).  This seems to be at odds with the claims that VER is a significant improvement of PER in soft RL. In the games where VER does not improve upon PER significantly, I would encourage the authors to comment on why the results are so similar to PER. I think the current benchmark problems are fairly sufficient and complex, and would not require more evaluation, unless the authors believe this would lead to a different conclusion or if the games selected are not representative of where VER can be beneficial. <sep> Other points: <sep> while I agree with the authors that when the learning rate is held constant, |TD| is an ""upper bound"" to EVB in the Q-learning case. However, in practice we often use state-dependent learning rates that can be annealed over time with visitation counts, which can often yield improvements (as long as the usual stochastic convergence conditions are satisfied, of course). In this case, wouldn't the learning rate play a role in the bound? <sep> I think the work is very interesting and addresses a central issue, and I hope that the above comments can be useful to improve the paper. Overall, I think it is necessary to think more carefully about the connection between PER and quantifying the value of an experience (e.g. why EVB? how to reconcile moderate empirical evidence of the new bounds?). I am looking forward to the authors' response on these issues above. <sep> References: <sep> [1] Brittain, Marc, et al. ""Prioritized Sequence Experience Replay."" arXiv preprint arXiv:1905.12726 (2019). <sep> [2] Mattar, Marcelo G., and Nathaniel D. Daw. ""Prioritized memory access explains planning and hippocampal replay."" Nature neuroscience 21.11 (2018): 1609-1617.","This paper is certainly on the way to be a solid contribution: it's an interesting research question, and we need more understanding papers (rather than yet another algorithmic trick paper). <sep> The reviewers thought the paper was not yet ready. The reviewers suggested: (1) more motivation of why the proposed metrics were of interest, (2) clearer discussion and evidence of how the analysis better articulates the performance of PER, (3) missing empirical details like methodology for setting hyper-parameters, why these 9 Atari games, undefined errorbars, unspecified number of runs, and (4) conclusions not supported by evidence in Atari: with missing experiment details, likely too few runs, and overlapping errorbars in most games few scientific conclusions can be drawn. <sep> The work might be strengthen by developing the first part of the paper (and focussing on the reviewer's suggestions) and deemphasizing the novel algorithmic contribution part."
"The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework.  The algorithms function by considering cutting an edge in the graph representation of the trace invariant, yielding a matrix whose leading eigenvector provides a (up to a rotation) estimate of the signal vector v.  This algorithm appears to be very interesting and works well in a series of simulations. <sep> Unfortunately, the presentation of the paper makes it very difficult to assess the importance of the contribution.  The introduction is well-written and well-motivated, though the later segmentation of the paper into many small subsections without much exposition makes the flow of the paper and its results hard to follow.  In addition, the notation and terminology in the paper are imprecise and, with important terminology and symbology introduced without definition or background citation. <sep> Pros: <sep> The proposed algorithm is clever and appears to do well compared to existing approaches in experiments. <sep> Well written introduction (with the only complaint being some minor grammatical errors). <sep> Cons: <sep> Important notation is introduced, and is not defined; Equation 4 is an example of this, where ⟨⋅⟩ (I assume this means E?), T¯, and E0(G) are all undefined.  This occurs often in the paper and in the appendix. <sep> In the ∙,×,∙ decomposition at the start of Section 2.3, what is N? <sep> What is the variance of a graph (as in Theorem 4)?  The proof sketch of this theorem is very hard to follow. <sep> Algorithm 1 is imprecise; what does ""compare α to σ(I(N)(T)) mean?  If α>σ(I(N)(T)) then a spike is detected?  How do you compute the variance of I(N)(T)?  How would you compute this if the noise model did not have unit variance)? <sep> Both algorithms are only presented for 3-way tensors, but the Theoretical claims are for higher order tensors? <sep> The proofs of the theorems and the statement of the theorems are, in general, a bit imprecise.  For example, in the proof of Theorem 2, Chebyshev's inequality will not guarantee disjointness everywhere, but only with high probability.  This is the case if βdet is finite.  This is a finite βdet result, with a claim only holding in the limit. <sep> In Theorem 5, what are the intermediate graphs/matrices?  In addition, this section (and Appendix C discussing perfect one-factorization) are a bit opaque. <sep> Is the decomposition after equation 5 only for the melon graph?  For more complex graphs (i.e., the tetrahedral), I believe you will have additional trace-like coefficients on all terms.  In any event, I am confused about the summands.  I do not see why the all Z sum would have a β, while the cross-terms would not.  Furthermore, why would the all v sum not have a βd coefficient?  This is what is implicitly being used in the proofs? <sep> In the experiments, important details are left out.  What is the setup here: what are the v's, how many iterations of tensor power method are applied, how many MC replicates are run to produce the error bars, what is the y-axis, what are the runtimes here, what is Random in Figure 6?  More detail would help a lot to understand how your new approach compares (it appears well) with the current literature.","This paper studies the tensor principal component analysis problem, where we observe a tensor T = \\beta v^{\\otimes k} + Z where v is a spike and Z is a Gaussian noise tensor. The goal is to recover an accurate estimate to the spike for as small a signal-to-noise ratio \\beta as possible. There has been considerable interest in this problem, mainly coming from the statistics and theoretical computer science communities, and the best known algorithms succeed when \\beta \\geq n^{k/4} where n is the dimension of v. The main contribution of this paper is to leverage ideas from theoretical physics and build a matrix whose top eigenvector is correlated with v for sufficiently large \\beta using trace invariants. On synthetic data, the algorithms achieve better performance than existing methods. <sep> The main negative of this paper is that it is not so clear how tensor PCA is relevant in machine learning applications. The authors gave some references to applications of tensor methods, but I want to point out that all of those works are about using tensor decompositions, which despite the fact that they are both about tensors, are rather different sorts of tools. Many of the reviewers also found the paper difficult to follow. I do think exposition is particularly challenging when making connections between different communities, as this work needs to introduce several notions from theoretical physics. I am also not sure how novel the methods are, since a somewhat recent paper Moitra and Wein, ""Spectral Methods from Tensor Networks"", STOC 2019 also uses tensor networks to build large matrices whose top eigenvalue is correlated with a planted signal, albeit for a different problem called orbit retrieval."
"############# Summary of contributions ############## <sep> This paper introduces the problem of enforcing group-based fairness for ""invisible demographics,"" which they define to be demographic categories that are not present in the training dataset. They assume access to a ""context set,"" which is an additional unlabeled dataset that does contain the invisible demographic categories of interest. They further provide an algorithm for enforcing fairness on these invisible demographics using this context set. <sep> Specifically, their contributions are: <sep> Algorithmic: They provide an algorithm for enforcing fairness on these invisible demographics. This algorithm involves first applying clustering methods on the context set to ""balance"" it, followed by disentangled representation learning and on the ""balanced"" context set. <sep> Empirical: They provide experiments on two benchmark datasets (colored MNIST and Adult) comparing their proposed method to multiple baselines. <sep> ############# Strengths ############## <sep> The paper is organized well, and the problem of ""invisible demographics"" is described and motivated well using concrete examples. <sep> The architecture of the proposed method is documented clearly in Figure 2. <sep> Their architecture builds on state of the art techniques such as DeepSets (Zaheer et al. 2017). Using DeepSets, the discriminator in their architecture estimates the probability that a given batch of samples, as a set, has been sampled from one distribution or the other. Preserving the set invariance to permutations is useful here, and different from a typical GAN discriminator. <sep> The baselines in the experiment section are thorough. It's useful to see a comparison between their clustering + balancing + disentangling method and the baseline methods of ZSF, which has balancing + disentangling but no clustering, and ZSF + bal. (ground truth), which has ground truth clusters + balancing + disentangling. <sep> ############# Weaknesses ############## <sep> The experiments section does not describe the implementation of the comparison to Hashimoto et al. 2018. Notably, the methodology of Hashimoto et al. 2018 is not specifically meant to enforce equality of acceptance rates, true positive rates, or true negative rates -- it only minimizes the worst case loss over unknown demographics. <sep> The authors do not provide any description of hyperparameters tuned, or any use of a validation set for hyperparameter tuning. I could not find this in the appendix either. In fact, on page 7, they say that they ""repeat the procedure with five different train/context/test splits"", which suggests no validation set. The parameters for the clustering methods are not given, and I find it hard to believe that no hyperparameters were tuned. Can the authors specifically provide the hyperparameters used, whether/how they were tuned, and any validation methods used (whether it be a validation set or cross validation)? <sep> The experiments are all done with binary protected groups: purple vs. green for the colored MNIST dataset, and male vs. female for the Adult dataset. Furthermore, these groups are not hugely imbalanced in the context set to begin with. This makes the clustering task easier. It would be interesting to see experiments with protected groups with more than two categories. For example, in the Adult dataset, the race feature is highly inbalanced, with a very small proportion of examples labeled as Asian-Pac-Islander or Amer-Indian-Eskimo. It would be interesting to see how the clustering techniques compare when the context set includes more than two protected categories, there is initial strong data imbalance between those groups, and the ""invisible demographic"" has relatively few data examples in the context set. This may not be entirely necessary for acceptance this round, but could be an interesting future experiment. <sep> The notation is in multiple cases unclear/inconsistent, possibly due to typos. Examples listed below: <sep> In the last paragraph on page 5, the notation and description of the support is confusing and not well defined. First, \\mathcal{S} and \\mathcal{Y} are themselves sets as defined in Section 2.1. Can the authors more specifically define what they mean by Sup(\\mathcal{Y}tr)? Is this the set of elements from \\mathcal{Y} that are contained in the training set? If so, why not just notate this as \\mathcal{Y}tr alone? The additional ""Sup"" notation is confusing and appears unnecessary. Furthermore, what do the authors mean when they say, ""we wish to use Sup(\\mathcal{S}{ctx} \\times \\mathcal{Y}{ctx}) \\ Sup(\\mathcal{S}{tr} \\times \\mathcal{Y}{tr}) as the training signal for the encoder""? <sep> [Top of page 6: ""whenever we have |S| > 1""] -- What does this notation mean? Is this the absolute value of the random variable S? This doesn't quite make sense given that S was previously stated to be a discrete-valued protected attribute, which could be a vector with p entries. The next statement of this corresponding to the ""partial outcomes"" setting is thus also unclear. <sep> [Section 2.2: ""c_i = C(z_i)""] -- What is z_i here? Is z_i the vector of (z_s, z_y) for the input features x_i? <sep> ############# Recommendation ############## <sep> UPDATE (after author response): I appreciate the authors' response. The inclusion of the hyperparameters are helpful. I also think it's an improvement that the authors added a comparison to ZSF+bal.(ground truth) to the Adult experiment. <sep> I still have a question about the experimental comparison to Hashimoto et al. (called ""FWD"" in this paper). Is the version of ""FWD"" implemented in this paper using exactly the same fairness criterion as in the Hashimoto et al. paper? If so, am I correct in saying that the ""FWD"" comparison in the experiments section does not directly constrain for any of the measured AR ratio, TPR ratio, or TNR ratio? The authors should clarify this in a later version. <sep> Overall, I'm willing to raise my score to a 6, but still think the paper is borderline. The paper could still use some improvement in covering related work on the problem of fairness where the protected attributes are not fully known (including the references I suggested). <sep> ------------- OLDER RECOMMENDATION BELOW ------------- <sep> Overall, my recommendation is 5: Marginally below acceptance threshold. The paper states an interesting and practically relevant problem of enforcing fairness with ""invisible demographics."" The methodology is overall well documented, and the experimental baselines make sense. However, the implementation detail in the experiments section is severely lacking, including description of hyperparameters/validation methods and implementation details for the comparison to Hashimoto et al. If the authors provide some of these details and answer some of my notation questions, then I would be willing to raise my score. <sep> ############# Questions and clarifications ############## <sep> Why is there no comparison to ZSF+bal. (ground truth) on the Adult dataset? <sep> Can the authors clarify what the ZSF alone baseline is doing in the experiments section? It's not written super clearly in the text. Does ZSF alone simply replace the perfect set in Figure 2 with the context set? <sep> ############# Additional feedback ############## <sep> Below I've listed some additional related work in the setting where protected attributes are unknown. This is not factored into the review, as these settings seem different enough and some of these works are recent. <sep> Lamy et al. Noise-tolerant fair classification. NeurIPS, 2019. <sep> Awasthi et al. Equalized odds postprocessing under imperfect group information. ICML, 2020. <sep> Wang et al. Robust Optimization for Fairness with Noisy Protected Groups. arXiv:2002.09343, 2020 <sep> [page 3: ""We can all agree that this sounds unfair""] -- nit: this wording seems unnecessarily strong to me. Let's not claim that ""we would all agree"" on something, especially when the meaning of unfair has not yet been defined. <sep> [page 5]: There appear to be multiple typos in the paragraph following equation (10), where the variables V, Q, K are not written in math mode, and are instead just capital letters in the text.","The paper studies the problem of satisfying group-based fairness constraints in the situation where some demographics are not available in the training dataset. The paper proposes to disentangle the predictions from the demographic groups using adversarial distribution-matching on a ""perfect batch"" generated by a clustered context set. <sep> Pros: <sep> The problem of satisfying statistical notions of fairness under ""invisible demographics"" is a new and well-motivated problem. <sep> Creative use of recent works such as DeepSets and GANs applied to the fairness problem. <sep> Cons: <sep> Makes a strong assumption that the clustering of the context set will result in a partitioning that has information about the demographics. This requires at the very least a well-behaved embedding of the data w.r.t. the demographic groups, and a well-tuned clustering algorithm (where optimal tuning is difficult in practice on unsupervised problems) -- but at any rate, as presented, the requirements for a ""perfect batch"" is neither clear nor formalized. <sep> Lack of theoretical guarantees. <sep> Various concerns in the experimental results (i.e. proposed method does not clearly outperform other baselines, high variance in experimental results, and other clarifications). <sep> Overall, the reviewers agreed the studied problem is new, interesting and relevant to algorithmic fairness; however, there were numerous concerns (see above) which were key reasons for rejection."
"The paper presents a modification to conditional batch normalization, wherein an extra affine layer is introduced between the standardization and the conditional affine layer. The empirical results show the benefits of introducing this extra ""sandwich"" affine layer. <sep> The intuition behind the approach makes sense, but the formulation makes it difficult to see why SaBN is in fact beneficial compared to CCBN. It does not appear that the approach imposes any restrictions or regularization on the CCBN affine parameters; therefore the only difference between CCBN and SaBN seems to be a different parameterization. I would then expect both CCBN and SaBN to reach the same optimal training loss. It may be that the reparameterization provided by SaBN yields the optimization trajectories that lead to better-generalizing solutions, but it is not explained in the paper whether, or why, this happens. One way to probe this might be to reparameterize each gamma in BN or CCBN as γ=γ1γ2 (or even γ=γ12) and study the behavior of the resulting model. <sep> The paper proposes to measure the heterogeneity in the found representations (between different branches of CCBN) via the CAPV measure. In its definition, I could not find what the overbars signify but I assume it means the average over the channels. Also, the indexing of gammas from 0 to N should probably be from 1 to C, for consistency with Eq. (2). The definition of CAPV as the variance of gammas seems problematic, however, in ReLU models with batch normalization: models whose gammas differ by a constant factor represent the same function, so the variance of gammas can be arbitrarily changed without affecting the model. A more useful measure of heterogeneity would need to take this scale invariance into account. <sep> The paper shows several empirical studies, including one for architecture search -- with the main paper using DARTS, and the appendix using GDAS. This choice seems suboptimal, given that in the NAS-Bench-201 paper (https://openreview.net/pdf?id=HJxyZkBKDr), Table 5 seems to indicate that DARTS performs much worse than GDAS. In this work, the appendix devoted to GDAS seems to indicate that (1) there is no consistency on using or not using affine, between CIFAR-100 and Imagenet, and (2) GDAS-SaBN is not statistically significantly better than the better of GDAS and GDAS-affine. <sep> The results in this paper are encouraging, but I believe the paper needs to explain more clearly why SaBN is expected to work (given that it preserves the hypothesis class as well as the minimum of the training objective). Additionally, since it appears to amount to a per-dimension reparameterization, the reader might expect that some of the other reparameterizations could have a similar effect (including such simple interventions as changing the learning rates for some of the gamma or beta parameters), and compellingly demonstrating that the specific reparameterization given by SaBN outperforms such alternatives would make the paper stronger.","This work proposes a novel reparameterization of batch normalization that is hypothesized to give a better inductive bias for learning several tasks, including neural architecture search, conditional image generation, adversarial robustness and neural style transfer. The reviewers indicate that this is useful and is of interest to the *CONF* audience, but they are not satisfied with the analysis offered in the paper. Specifically, the reviewers request that the authors provide a more detailed analysis of why the proposed reparameterization improves results, given that it does not change the expressive power of the model class. Additionally, the reviewers have some concerns about the structure of the paper. I therefore recommend rejecting the paper at this time."
"This paper features two complementary contributions: <sep> The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize log⁡p(O,x∣a). As the name suggests, this is similar to the control as inference formulation with a focus on partial observability and the addition of a perception model qw. <sep> Object-based perception control (OPC), which is an instance of PCI using an object-factorized model. <sep> Incorporating a perception model into the the control-as-inference derivation to yield a single unified objective for multiple components in a pipeline is an interesting idea and complements other work on incorporating reward structure into model-learning nicely. (A discussion of some of these, such as [Farahmand et al, 2017] or [Oh et al, 2017] could serve to better highlight the difference between PCI and existing hybrid approaches, but these references are not ""missing"" in the sense that their absence is an issue.) <sep> However, the experimental evaluation does not really do the generality of this idea justice. It seems that this sort of approach would be favored only when the perception problem is too difficult for a standard maximum likelihood objective, as the joint objective would (presumably) bias the perception model to be useful for control. The main evaluation of the perception model, though, only shows four primitive white shapes on a black background being assigned labels. Unless there is underlying complexity not apparent in the image, this sort of input appears to be in line with what binary segmentation algorithms could work with, and likely would require no learning. The implicit argument made is that optimizing the joint objective causes these labels to be semantically-informed, but this is not supported by an experiment; the segmentation results in figure 4 are also what you would get by segmenting based on boundaries. <sep> Somewhat better support could come from showing what labels are assigned when two different object types are contiguous with each other (would food and poison be assigned the same label, or does the model separate them?), but in general it is difficult to support the claimed benefits with this particular environment. This is likely one of the reasons for the result described in the figure 3(c-d) ablations: when the environment is too simple (this time in terms of dynamics, likely, and not perceptual complexity), many of the design choices can become less consequential. <sep> Aside from experiments, the presentation of the core ideas could also be improved somewhat. Section 3.2 in particular is difficult to parse, and presented in such a way that many readers will likely just skip over most of it. It is certainly a good idea to work out an approach in its full generality, but some of the components taking up space in the presented derivation are immediately cut in the experiments section (like the DKL(π || p(at∣a<t) term), so you might as well present a simplified version of the result that focuses on the method actually used. <sep> Miscellaneous questions <sep> What is the role of p(a∣a<t)? Is this to account for a recursive policy? <sep> Why do you think the world models baseline never appreciably improves upon its random initialization in figure 2b? I could imagine that the joint objective would cause OPC to perform a bit better than world models at convergence, but I am surprised that the perception is apparently so difficult that a separate modeling objective never gets off the ground at all. <sep> Minor <sep> Section 3.4: ""resulting a"" → ""resulting in a"" <sep> There are a number of claims about human cognition that are not supported. For example: <sep> OPC agent can quickly learn the dynamics of a new environment without any prior knowledge, imitating the inductive bias acquisition process of humans. <sep> Moreover, in order to mimic a human's spontaneous acquisition of inductive biases throughout its life, we propose to build a model able to acquire new knowledge online, rather than a one which merely generates static information from offline training (Dehaene et al., 2017). <sep> It is hard to say whether OPC mimics humans' knowledge acquisition without any sort of evidence. <sep> Summary <sep> This generalization of the control as inference formulation could be influential given a more didactic presentation of the PCI derivation and an evaluation better suited to its claimed strengths, but it does not seem quite ready for publication yet. I would encourage the authors to try to test their approach on an environment that is perceptually complicated enough to motivate a non-standard perceptual modeling objective.","This paper introduces an object perception and control method for RL, derived from a control-as-inference formulation within a POMDP. The paper provides a theoretical derivation and experiments where the proposed joint-inference approach outperforms baselines. <sep> The discussion focussed on understanding the paper's contribution relative to prior work. The reviewers highlighted the similarities with earlier systems (R1, R2, R4), the unclear benefits of joint inference over independently trained modules in the experiments (R3), and the lack of clarity of the presentation (R1, R2, R3). The authors responded to some of these criticisms, bolstering the paper with additional experiments to show the benefits of joint inference and increasing the discussion of related work. The reviewers examined the revisions and rebuttal and found the paper still did not resolve all their original concerns. Two limitations mentioned in the final phase of the discussion were the use of a single environment to evaluate the general framework, and continuing doubts on the contribution of joint inference mechanism to the measured performance. <sep> Four knowledgeable reviewers indicate reject as their concerns were not adequately resolved. The paper is therefore rejected."
"Summary: <sep> This submission proposes an approach to modulate activations of general convolutional neural networks by means of an auxiliary network trained on additional metadata to a dataset. The specific goal is to improve out-of-distribution (OOD) generalisation. This conditional network approach is illustrated for two standard convolutional neural network (CNN) architectures, U-Net and VGG, on two benchmark datasets suitable for OOD detection, the Inria Aerial Image Labeling Dataset and the Tumor Infiltrating Lymphocytes classification dataset. The conditional network approach yields favourable results compared to competing segmentation as well as classification networks and exhibits a reduction of the generalisation gap compared to the baseline methods. <sep> Strengths: <sep> Significance / Novelty: The conditional network uses a latent / intermediate representation z of an auxiliary network solving an auxiliary task T (as usually considered in the context of self-supervised learning) to learn parameters used to perform an affine transformation of feature map activations in the main neural network solving the main task Y. The approach combines self-supervised learning with conditional normalisation of activations. As far as I can judge, the proposed idea is original and novel. <sep> A particular benefit of the proposed approach is that, at test time, only the latent representation z is required (provide by the trained auxiliary network) without the necessity of providing metadata t. <sep> Relevance / Potential impact: The presented idea seems to improve performance and training of standard CNN architectures and can be used in general CNN architectures, which might render the approach applicable in a great variety of segmentation / classification tasks and relevant for a wider audience. <sep> Clarity: In my opinion, this paper is very well-written, self-contained and offers a good description of the approach and adequate experimental evaluation of the made claims with some exceptions outlined below. <sep> Weaknesses: <sep> Technical quality:  My major concern with respect to the evaluation is the comparison to the baseline. In the paper by Huang et al. on AMLL U-Net higher IoU scores and accuracies on the transfer (test) set are reported (table 2, page 5 in their paper) compared to table 1, which in fact exceed the performance results of the proposed method. This raises the question whether the baseline methods are properly trained. From the description in the main text and appendix (section A.1), the only difference w.r.t. Huang et al. consists in using data augmentation. Is there a particular reason why the training deviates from the settings outlined in Huang et al.? <sep> In connection to the previous point, also no details on hyperparameter tuning are provided and all networks were trained with the same settings. However, the differences in the methods might require different hyperparameter settings to provide the best performance. Were other hyperparameters considered in the outlined experiments? <sep> Using the geocoordinates as metadata t_n does not strike me as a particularly useful choice, as it appears unlikely that a representation of ""location"" is learned but rather some random features are extracted which are known to be beneficial, too (as shown e.g. by Rahimi and Recht). In my opinion, relative closeness in geocoordinates should have little information on input aerial regions. The second example with t_n being the cancer type is much more convincing. <sep> Clarity: Figure 2 and 4 are not explicitly addressed, put into context with other results or discussed in the main text (fig. 4 mentioned in the appendix). After carefully studying the paper, one might make a connection between the results in table 1 and figure 2. But although paragraph ""Interpretation of conditioning features"" (p. 7) seems to relate to figure 4, the results are difficult to interpret without any further discussion or comments on it. Could the authors elaborate more on Hypothesis 2 and Figure 4? I believe a more explicit / clearer discussion would improve the quality of the paper quite a bit. <sep> In connection to the previous point, the reasoning for the following conclusion (p. 8) should be made more clearer (and maybe connected to section A.3 and figure 10 in the appendix): ""After carefully studying the network feature activations, we found that the improved generalization ability of the proposed network is not due to the ability of learning more invariant features. It appears, instead, that the conditional network learns a smaller collection of features more relevant to the task."" <sep> I might have missed it, but I am not sure what the difference between ""Cond. U-Net"" and ""Fully Cond. U-Net"" is. As the former already modulates all blocks in the encoder and decoder, what does ""fully cond."" implicate? <sep> Additional Feedback: <sep> Figure 2 caption: ""(e) Fully Cond. U-Net CGN"" should read ""(e) AMLL U-Net"". <sep> Figure 4: I would suggest using the same scale range for both plots, to make the difference more prominent. This change might be beneficial for the points mentioned above, too. <sep> Page 5, last sentences: ""[…] (see Section 4.1)."" This is a self-reference to the same section. Quite likely the reference was supposed to be ""Section 3.1"". <sep> Tables 1 and 2, (overall) IoU score on transfer set for ""U-Net + GN"": There is probably a small typo what concerns the values ""63.71"" (table 1) and ""63.79"" (table 2). If I understand correctly, these values should be exactly the same. <sep> Page 8, results paragraph, first sentence: ""[…] Tumor-infiltrating Lymphocyte […]"" while before ""[…] Tumor Infiltrating Lymphocyte […]"" was used (without the hyphen). <sep> Recommendation: <sep> I enjoyed reading this submission and think that the idea of ""conditional networks"" constitutes a novel and relevant contribution to improve OOD generalisation. In particular, I believe the proposed approach might be impactful in a variety of related methods and applications as it can be used in general CNN architectures. However, there are some concerns and questions outlined above which I believe need to be addressed / adapted in order to accept this paper. My initial rating is weak accept, but I am willing to raise my rating if the authors can address these aspects. <sep> Post-Rebuttal: <sep> I would like to thank the authors for addressing the questions and concerns. I still believe that the general idea of ""conditional networks"" might pose a relevant contribution to improving OOD generalisation. However, after rereading the submission, reading the other reviews, and taking the rebuttal into consideration, I think there are some aspects which need some revision and clarification. I comment on this in more detail below. Therefore, I stand with my initial rating of borderline, but I would like to encourage the authors to revise their paper taking the points raised by the reviewers into consideration and submit again. <sep> Section 4.2, discussion of figure 4 and hypothesis 2: I thank the authors for the added discussion. However, the results are a bit at odds with the premise of the proposed approach, in my opinion. The bullet points which detail why hypothesis 2 could be interesting, are refuted by the results presented below in that section. I believe the result that activations look different (what concerns scale of activation and which features are active) by itself is less surprising as the approach tackles the normalisation of activations explicitly. But more importantly, other than that, I would say the activation patterns for different cities look qualitatively similar in both models and does not align well with the story of the paper. So, in my opinion, the activation patterns on the left (AMLL U-Net) in figure 4 look very similar for all cities, and also the patterns on the right (Cond. U-Net) look quite alike for different cities. Therefore, the interpretation of these results in the context of conditioning on auxiliary information remains speculative. I believe this part of the submission requires a careful reconsideration. <sep> Geocoordinates as metadata t_n: A potential reason for the difficulties in the previous point could be the choice of metadata t_n in the segmentation example. I thank the authors for elaborating again on the choice. Still, I am not convinced that this is the most suitable choice to present the advantages of the proposed approach. If the conditioning network really just performs city ID classification, I would not be sure that any kind of useful (generalisable) features are extracted. This could be a potential explanation why no ""conditioning influence"" on the results in figure 4b is observed. <sep> Difference ""Cond. U-Net"": I should've been more explicit in my question. On page 6, last paragraph of ""Generalization via conditioning"" it reads: ""We identify as Cond. U-Net those models in which both the encoder and decoder are modulated, which yielded a small gain in performance over just modulating the encoder or decoder alone."" If I see correctly, ""Cond. U-Net"" should be replaced by ""Fully Cond. U-Net"", as the provided answer suggests, too. <sep> References: <sep> Rahimi and Recht, ""Random Features for Large-Scale Kernel Machines"", NeurIPS 2007. <sep> Huang et al., ""Large-scale semantic classification: outcome of the first year of inria aerial image labeling benchmark"", 2018.",The paper proposes to address the out-of-distribution generalization problem by means of conditional computation in form of a feature modulating module. <sep> While the approach is interesting and brings a new take on how to perform feature modulation (although initially felt too similar to Conditional Batch Normalization) some major concerns about the experiments and validation of the approach are raised by all reviewers. Some of the hypothesis made are also challenged due to lack of proper validation. <sep> Although the discussion clarified some points I am afraid many open questions are left unanswered and would require a more work to be fully addressed before acceptance.
