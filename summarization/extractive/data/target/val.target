This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose "spectral normalization" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues. The presented methodology is principled and well written. The authors did a good job in addressing reviewer's comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs. I recommend acceptance.
This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals. The paper is well motivated and follows a significant direction of research, as agreed by all reviewers. In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice. There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers' suggestions, raising the average rating by 2 points after the rebuttal.
Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks). The approach is simple and likely to be useful in applications. The paper is well written. <sep> + simple and efficient <sep> + high quality evaluation <sep> + strong results <sep> - novelty is somewhat limited
An interesting application of graph neural networks to robotics. The body of a robot is represented as a graph, and the agent's policy is defined using a graph neural network (GNNs/GCNs) over the graph structure. <sep> The GNN-based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components. I believe that the reviewers' concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN-based model outperforms MLPs on a dataset of 2D walkers. <sep> Overall: <sep> -- an interesting application <sep> -- modeling robot morphology is an under-explored direction <sep> -- the paper is well written <sep> -- experiments are sufficiently convincing (esp. after addressing the concerns re diversity and robustness).
The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference. The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference. <sep> The submission provides links between two seemingly different frameworks: SPENs and GANs. By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.
Thank you for submitting you paper to *CONF*. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper.
Thank you for submitting you paper to *CONF*. The reviewers agree that the paper's development of action-dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein's identity to provide a principled way to think about control variates is sensible. The revision clarified an number of the reviewers' questions and the resulting paper is suitable for publication in *CONF*.
I recommend acceptance based on the reviews. The paper makes novel contributions to learning one-hidden layer neural networks and designing new objective function with no bad local optima. <sep> There is one point that the paper is missing. It only mentions Janzamin et al in the passing. Janzamin et al propose using score function framework for designing alternative objective function. For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials. Lack of discussion about this connection is weird. There should be proper acknowledgement of prior work. Also missing are some of the key papers on tensor decomposition and its analysis <sep> I think there are enough contributions in the paper for acceptance irrespective of the above aspect.
The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack. There were missing relevant references in the original submission, but these have been added. I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn't been tested in this work. Even if you keep the title you might be more careful to frame the body in the context of CNN's.
With scores of 7-7-6 and the justification below the AC recommends acceptance. <sep> One of the reviewers summarizes why this is a good paper as follows: <sep> "This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances: <sep> - This gives a more unified way of understanding, and implementing the methods. <sep> - The paper points out situations when the methods are equivalent <sep> - The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity <sep> - The paper proposes a new objective function to measure joint sensitivity"
This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks. This metric comes with theoretical guarantees and can be efficiently computed on large-scale neural networks. <sep> Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process. The AC believes this paper will contribute to the growing body of literature in robust training of neural networks.
The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low-probability parts of image space. The work is novel, relevant to the *CONF* community, and appears to be technically sound. <sep> A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet.
All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping. The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. The experiments are not mind-boggling, but none of the reviewers seem to think that's a show stopper.
Well motivated and well written, with extensive results. The paper also received positive comments from all reviewers. The AC recommends that the paper be accepted.
This clearly written paper describes a simple extension to hard monotonic attention -- the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism. Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism. About the only "con" the reviewers noted is that the paper is a minor extension over Raffel et al., 2017, but the authors successfully argue that the strong empirical results render this simplicity a "pro."
Pros: <sep> + Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization. <sep> Cons: <sep> - While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall-clock times were given in some cases, as that will help to illustrate the utility of the approach. <sep> - The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material. <sep> - The results are not all that surprising in light of other recent papers on the subject.
This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell's formulation. Experimental results shows good perplexity results on benchmarks compared to many baselines. All reviewers agree that the paper will bring good contribution to the conference.
I fully agree with strong positive statements in the reviews. All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent's learning models of the environment. I also concur that the empirical testing of this method is quite good. There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft). This paper would make for a strong poster at *CONF*.
Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform. The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further.
All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at *CONF*.
Dear authors, <sep> Based on the comments and your rebuttal, I am glad to accept your paper at *CONF*.
This is a meta-learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network. The approach has similarities to recently proposed methods for architecture search, but is significantly different. The paper is well written and the experiments are clear and convincing. One of the reviews was unacceptable; I am not considering it (R1).
This paper proposes an interesting new idea which creates an interesting discussion.
This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. Reviewers found the paper to be clearly written, saying it "nice introduction to the topic" and noting that they "enjoyed reading this paper". In general though there was a feeling that the "substance of the work is limited". One reviewer complained that experiments were limited to small English datasets PTB and Wikitext-2 and asked why they didn't try "machine translation or speech recognition". (The author's note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) . Another felt that the "multipop model" alone was not too large a contribution. The actual experiments in the work are well done, although given the fact that the models are known there was expectation of "more "in-depth" analysis of the different models". Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area.
R1 thought the proposed method was novel and the idea interesting. However, he/she raised concerns with consistency in the experimental validation, the trade-off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability. The authors responded to these concerns, and R1 upgraded their score. R2 didn't raise major concerns or strengths. R3 questioned the novelty of the work and the experimental validations. All reviewers raised concerns with the writing. Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. I think this is a paper that could make a good workshop contribution.
This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task). Unfortunately, all reviewers raise concerns (e.g. limited relations studied) and are largely on the fence about this paper. While this paper does not propose solutions, it does present interesting "negative results" that should get some visibility in the workshop track.
This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low-dimensional projected optimization landscapes between different network architectures. <sep> - the visualisation techniques are a small variation over previous works <sep> + extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants <sep> A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims.
The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice. <sep> I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion.
The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data. The paper developed new modifications to each component and designed network structure using ideas from state-of-the-art nets. As pointed out by reviewer 1 & 2, the technical contribution is not sufficient. We hence recommend it to workshop publication.
The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR-10) the pre-processing can become prohibitive. I recommend improving the manuscript for a re-submission to another venue and an *CONF* workshop presentation.
Dear authors, <sep> While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work. <sep> Thus, I recommend it as an *CONF* workshop paper.
The authors addressed the reviewers concerns but the scores remain somewhat low. <sep> The method is not super novel, but it is an incremental improvement over existing approaches.
The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop.
Pros: <sep> - The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model. <sep> - Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting. <sep> - Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period. <sep> Cons: <sep> - Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before. <sep> - The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter's objective seems narrow. More analysis is needed to demonstrate that the approach out-performs other approaches that directly tackle this problem in ALI. <sep> - The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper). <sep> - Semi-supervised learning as a down-stream task is impressive but limited to MNSIT.
This paper proposes a way to automatically weight different tasks in a multi-task setting. The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation.
Pros and cons of the paper can be summarized as follows: <sep> Pros: <sep> * The underlying idea may be interesting <sep> * Results are reasonably strong on the test set used <sep> Cons: <sep> * Testing on the single dataset indicates that the model may be of limited applicability <sep> * As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns <sep> * There is little mathematical notation, which compounds the problems of clarity <sep> After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to *CONF* in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission.
The pros and cons of the paper can be summarized below: <sep> Pro: <sep> * The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset. <sep> Con <sep> * There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel. <sep> * Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear. <sep> As a result, I recommend that the paper not be accepted at this time.
The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (e.g., the prediction loss at each layer - also not a new idea per se). <sep> The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable. It is not even clear what is meant here by 'reasoning' as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (e.g., matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by Schlichtkrull et al). <sep> So the contribution seems a bit over-stated. Rather than introduces a new model, the work basically proposes an application of largely known model to two (not-so-hard) tasks which have not been studied in the context of GNNs. The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2). <sep> There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria. <sep> Pros: <sep> -- an interesting direction <sep> -- clarity <sep> Cons: <sep> -- the claim of generality is not well supported <sep> -- the approach is not so novel <sep> -- the approach should be better grounded in previous work
The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding. The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work. They key problems (acknowledge also by at least 2 reviewers): <sep> Pros: <sep> -- generating trees input under-explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols) <sep> Cons: <sep> -- no comparison with previous tree-decoding work <sep> -- only artificial experiments <sep> -- the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)
The reviewers tend to agree that the empirical results in this paper are good compared to the baselines. However, the paper in its current form is considered a bit too incremental. Some reviewers also suggested additional theory could help strengthen the paper.
The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results. An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs.
Thank you for submitting your paper to *CONF*. The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough. However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication. Adding a second dataset, such as mini-imagenet or the youtube faces dataset, would make the paper very strong.
Thank you for submitting you paper to *CONF*. *CONF*. Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication.
This paper presents a method for learning more flexible prior distributions for GANs by learning another distribution on top of the latent codes for training examples. It's reminiscent of layerwise training of deep generative models. This seems like a reasonable thing to do, but it's probably not a substantial enough contribution given that similar things have been done for various other generative models. Experiments show improvement in samples compared with a regular GAN, but don't compare against various other techniques that have been proposed for fixing mode dropping. For these reasons, as well as various issues pointed out by the reviewers, I don't recommend acceptance.
The reviewers feel that the novelties in the model are not significant. Furthermore, they suggest that empirical results could be improved by <sep> 1: analyses showing how the significance network functions and directly measuring its impact <sep> 2: More reproducible experiments. In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. <sep> 3: baselines that make assumptions more in line with the authors' problem setup
The paper appears unfinished in many ways: the experiments are preliminary, the paper completely ignored a large body of prior work on the subject, and the presentation needs substantial improvements. The authors did not provide a rebuttal. <sep> I encourage the authors to refrain from submitting unfinished papers such as this one in the future, as it unnecessarily increases the load on a review system that is already strained.
The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron's weights to sum to zero improves training of a 100 layers sigmoid MLP. <sep> The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it's not appplicable to RELUs and shows positive effect on optimization but not generalization).
The paper received mixed reviews with scores of 5 (R1), 5 (R2), 7 (R3). All three reviewers raise concerns about the lack of comparisons to other methods. The rebuttal is not compelling on this point. There are quite a few methods that could be used for this application available (often with source code) and should be compared to, e.g. DenseNets (Huang et al.). Given that the proposed method isn't in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach. The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time.
Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference.
Three reviewers recommended rejection and there was no rebuttal to overturn their recommendation.
The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue.
This work takes dialogue acts into account to generate responses in a human-machine conversation. However, incorporating dialogue acts into open-domain dialogue was already the focus of Zhao et al's ACL 2017 paper, Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human-machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors' response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re-work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.
This work attempts to incorporate affect information from additional resources into word embeddings. This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper.
Pros: <sep> + The paper is very clearly written. <sep> + The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures. <sep> Cons: <sep> - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors). <sep> This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced.
The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al., 2017, and not enough for a new paper. They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision. <sep> Pros: <sep> + Adaptive batch sizing is useful, especially if the larger batches license parallelization. <sep> Cons: <sep> - Small, incremental change to the algorithm from Yin et al., 2017 <sep> - Test performance did not improve over well-tuned momentum optimization, which limits the appeal of the method.
The author's revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this "toy" as well. On this toy task, the approach isn't that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences. <sep> The authors mention that they didn't have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one.
The paper presents an adversarial learning framework for reading comprehension. Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3's comments) compared to other baselines to warrant acceptance. Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern. Addressing these issues will strengthen the paper for a potential future venue.
This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain. However, the committee and most reviewers find that results are inconclusive. Results are mixed across subjects. We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here. It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper.
None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation. The response of the authors towards this criticism is also not sufficient. The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison. We think that addressing these immediate concerns would improve the quality of this paper.
The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results. However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work. The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc. While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response.
This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time. The presentation needs a lot of work. The original paper was 17 pages long and very difficult to understand. The revised paper is 12 pages long, which is still too long for the content. The paper needs to better distinguish between the major and minor points. It is still too difficult to judge the contribution.
This paper has been withdrawn by the authors.
The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse. <sep> Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models. The authors do not provide convincing arguments as to why the proposed approach should work well. The experiments presented also fail to demonstrate this. The results are limited to smaller MNIST and CIFAR10 datasets. Comparisons with approaches that directly address the mode collapse problem are missing.
The paper aims to combine Wasserstein GAN with Improved GAN framework for semi-supervised learning. <sep> The reviewers unanimously agree that: <sep> - the paper lacks novelty and such approaches have been tried before. <sep> - the approach does not make sufficient gains over the baselines and stronger baselines are missing. <sep> - the paper is not well written and experimental results are not satisfactory.
The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation. It seems that the authors largely agree and are working to improve it. <sep> PROS: <sep> 1. Improving the speed of program synthesis is a useful problem <sep> 2. Good treatment of related work, e.g. CEGIS <sep> CONS: <sep> 1. The approach likely does not scale <sep> 2. The architecture is underspecified making it hard to reproduce <sep> 3. Only 1 domain for evaluation
The reviewers have unanimously expressed strong concerns about the technical correctness of the theoretical results in the paper. The paper should be carefully revised and checked for technical errors. In its current form, the paper is not suitable for acceptance at *CONF* 2018.
Reviews are marginal. <sep> I concur with the two less-favorable reviews that the metrics for privacy protection are not sufficiently strong for preserving privacy.
This paper describes active vision for object recognition learned in an RL framework. <sep> Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation. <sep> While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper.
All 3 reviewers consider the paper insufficiently good, including a post-rebuttal updated score. <sep> All reviewers + anonymous comment find that the paper isn't well-enough situated with the appropriate literature. <sep> Two reviewers cite poor presentation - spelling /grammar errors making hte paper hard to read. <sep> Authors have revised the paper and promise further revisions for final version.
The authors propose a hierarchical VAE model with a discrete latent variable in the top-most layer for unsupervised learning of discriminative representations. While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture. Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature. This paper seems better suited for a more applied venue than *CONF*.
The submission provides an interesting way to tackle the so-called distributional shift problem in machine learning. One familiar example is unsupervised domain adaptation. The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re-weighted empirical risk on the source domain and some discrepancy between the re-weighted source domain and the target domain. The authors then use this to formulate an objective function. <sep> The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I. Especially the unsupervised domain adaptation results are very toy-ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments. I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation). <sep> Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results.
In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable "goals" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison d'etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments (a la Figure 2) showing how this method performs on complicated tasks. <sep> I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases).
This paper proposes a tree-structured tensor factorisation method for parameter reduction. The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative. This seems like a promising research direction which needs more empirical work, but is not ready for publication as is.
This paper combines multiple existing ideas in Bayesian optimization (continuous-fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method. While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to *CONF*. Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS. The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling). <sep> Pros: <sep> - The paper is clear and writing is of high quality <sep> - Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful <sep> - Outperforms existing methods on the chosen benchmarks <sep> Cons: <sep> - Is an incremental combination of existing methods <sep> - The paper claims too much
This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations. In general the reviewers thought that the paper was well written and easy to understand. However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently. Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach. <sep> Pros: <sep> - Well written and easy to follow <sep> - An interesting idea <sep> - Regularizing RNNs is an interesting and active area of research in the community <sep> Cons: <sep> - The experiments are not compelling and are questioned by all the reviewers <sep> - The writing does not cite relevant related work <sep> - The work seems underexplored (empirically and methodologically)
The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited.
The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading. "Approximate Confidence Interval" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >= 0 or 1).
The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this.
Dear authors, <sep> The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication. <sep> Regarding the theoretical part, here are the main concerns: <sep> - Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al. <sep> - The isotropic approximation is very strong and I don't know settings where this would hold. Since it seems central to your statements, I wonder what can be deduced from the obtained results. <sep> - I do not think the Gaussian assumption is unreasonable and I am fine with it. Though there are clearly cases where this will not be true, it will probably be OK most of the time. <sep> I encourage the authors to focus on the experimental part in a resubmission.
Dear authors, <sep> Thank you for your submission to *CONF*. Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results. Thus, your paper cannot be accepted to *CONF*.
The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state. There are a few problems with this paper. First, the method lacks novelty - it is very similar to DQN. Second, the claims of learning physical intuitions is not borne out by the method or experimental results. Third, the tasks are very simple and there is no held-out test set of target configurations.
The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice.
The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights. The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper.
The paper presents some reasonable experiments and approaches for unsupervised time series. However as mentioned by R2 there is several issues. The paper also overclaims a bit some of the novelty. As noted by R1 and the metareviewer there is other works using triplet loss for timeseries, a relatively common approach in temporal dataset (e.g. video, audio), the popular causal convolution structure from wavenet is also quite well known, contributions should be more clear.
The paper has some relatively minor issue but an overall interesting concept
All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs.
Well-written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks. Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures. <sep> Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes. The paper was revised accordingly. Another important suggestion was considering ACT as a baseline. Authors explained clearly why it wasn't considered as a baseline, and updated the paper to include references and explanations in the paper as well.
The authors propose a method for low-resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a "content" (task-specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source --> target). Experiments are conducted on both supervised as well as unsupervised settings. <sep> The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low-resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version.
The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods.
The paper investigates a novel initialisation method to improve Equilibrium Propagation. In particular, the results are convincing, but the reviewers remain with small issues here and there. <sep> An issue with the paper is the biological plausibility of the approach. Nonetheless publication is recommended.
Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study. <sep> Weaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. <sep> Contention: Authors point to novelty in 3D convolutions inside the RNN. <sep> Consensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement.
see my comment to the authors below
This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak. <sep> Pros: <sep> The paper introduces a new dataset for source code edits. <sep> Cons: <sep> Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6. <sep> Verdict: <sep> Possible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak.
This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework. Significant gains are found through semi-supervised learning. <sep> The largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility. <sep> Overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.
All reviewers recommend accept. <sep> Discussion can be consulted below.
Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns. As a result, all the reviewers ultimately recommended acceptance. They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough. All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference. One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version.
There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers.
The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with <sep> prior work on optimal control. It designs input convex recurrent neural networks to capture temporal behavior of <sep> dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem. <sep> There were initial critiques regarding some of the claims. These have now been clarified. <sep> Also, there is in the end a compromise between the (necessary) approximations of the input-convex model and the true dynamics, and being able to compute an optimal result. <sep> Overall, all reviewers and the AC are in agreement to see this paper accepted. <sep> There was extensive and productive interaction between the reviewers and authors. <sep> It makes contributions that will be of interest to many, and builds interesting bridges with known control methods.
This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers. The idea is general, and admirably simple. The explanation is clear, and the results are impressive. The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference. While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form.
The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network. The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. The experiments have sufficient depth to support the claims. Overall, the method seems to be a simple but effective technique for learning very deep residual networks. <sep> While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network. <sep> The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. The main concerns by the reviewers were addressed by the author responses. The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper.
The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto-encoder with a WaveNet-like domain-specific decoder and a shared encoder, trained with an adversarial "domain confusion loss". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.
The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable. The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training. <sep> Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re-introduction of SAT in a machine learning context using deep networks. It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems. The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach. <sep> The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting.
This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions. For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned. All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks. Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature.
The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC's main concern of the experimental part of the paper is that it doesn't outperform or match the performance of the "vanilla" neural network compression algorithms such as Han et al'15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don't include state-of-the-art compression algorithms.
This paper proposes a solution for the well-known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible. <sep> A downside of the proposed method is the introduction of hyper-parameters controlling the degree of regularization. The empirical results show improvements on various baselines. <sep> The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing.
The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention-based weighted averaging (where discriminative parts/frames might get disportionately high attentions). A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation. The experimental results show that the proposed attention mechanism improves over prior methods (e.g., STPN) on THUMOS14 and ActivityNet1.3 datasets. In terms of weaknesses: (1) the computational cost may be quite significant. (2) the proposed method should be evaluated over several tasks beyond activity recognition, but it's unclear how it would work. <sep> The authors provided positive proof-of-concept results on weakly supervised object localization task, improving over CAM-based methods. However, CAM baseline is a reasonable but not the strongest method and the weakly-supervised object recognition/segmentation domains are much more competitive domains, so it's unclear if the proposed method would achieve the state-of-the-art by simply replacing the weighted-averaging-attentional-pooling with the proposed attention mechanism. In addition, the description on how to perform attentional pooling over images is not clearly described (it's not clear how the 1D sequence-based recurrent attention method can be extended to 2-D cases). However, this would not be a reason to reject the paper. <sep> Finally, the paper's presentation would need improvement. I would suggest that the authors give more intuitive explanations and rationale before going into technical details. The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part. Overall, there are interesting technical contributions with positive results, but there are issues to be addressed.
The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections. <sep> Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community. <sep> The proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.
This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as "maximizing the complement entropy." Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.
This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher-than-average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well-presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to "probe" them, that would likely be of high interest to many people at *CONF*.
This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance.
The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted.
The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results. There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version.
The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. <sep> Strengths: <sep> - The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations <sep> - The authors put a lot of care into the robustness evaluation <sep> Weaknesses: <sep> - Some of the "shortcomings" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about <sep> Overall, this looks like a valuable and interesting contribution.
The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k-means) so that one can get a better understanding of the difficulty of these task. <sep> This paper is clearly above the acceptance threshold at *CONF*.
The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. <sep> The architecture of GANs used in the paper is standard, yet the defensive performance seems good. The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits. In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers. <sep> The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments. The understanding on why it works may need further explorations. Thus the paper is proposed to be borderline lean accept.
The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. Strong empirical results are presented which support the use of optimal transport in conjunction with log-likelihood for training sequence models. I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version.
All reviewers gave an accept rating: 9, 7 &6. <sep> A clear accept -- just not strong enough reviewer support for an oral.
Strengths <sep> The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well. <sep> The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the <sep> separation of internal state from external state is a clean principle that can potentially be broadly employed. <sep> The method does well in outperforming the alternative baselines. <sep> Weaknesses <sep> There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses <sep> a policy ensemble; phase info is used in DeepLoco/DeepMimic; methods such as "Virtual Windup Toys for Animation" <sep> exploited periodicity (25y ago); More comparisons with prior work such as Florensa et al. would help. <sep> The separation of internal and external state is an assumption that may not hold in many cases. <sep> The results are locomotion focussed. There are only two timescales. <sep> Decision <sep> The reviewers are largely in agreement to accept the paper. <sep> There are fairly-simple-but-useful lessons to be found in the paper <sep> for those working on HRL problems, particularly those for movement and locomotion. <sep> The AC sees the novely with respect to different pieces of related work is the weakest point of the paper. <sep> The reviews contain good suggestions for revisions and improvements; the latest version may take care <sep> of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution <sep> to *CONF* 2019.
The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.
This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. <sep> The reviewers found the contribution interesting for the *CONF* community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted.
The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk-averse. <sep> Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns. In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time). Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning. A title change was proposed and accepted to reflect the focus on the application of risk aversion (I'd ask that the authors update the paper OpenReview metadata to reflect this). <sep> At a high level, I believe this is an original and interesting contribution to the literature. I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance.
The paper was judged by the reviewers as providing interesting ideas, well-written and potentially having impact on future research on NN optimization. The authors are asked to make sure they addressed reviewers comments clearly in the paper.
as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling. although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow-up. <sep> r3's concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair.
This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates - which is standard in many proximal methods. The paper combines this with the analysis for non-smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue. The main weakness is that the results only holds for \\mu weekly convex functions and the algorithm depends on the knowledge of \\mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version.
The paper proposes an interesting idea (using "reliable" samples to guide the learning of "less reliable" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects. <sep> On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less-reliable samples is domain-specific (it makes sense for object detection tasks, but it's unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to "Feature Intertwiner for Object Detection" to alleviate such criticisms. <sep> Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers.
Reviewers mostly recommended to accept after engaging with the authors. I have decided to reduce the weight of AnonReviewer3 because of the short review. Please take reviewers' comments into consideration to improve your submission for the camera ready.
Dear authors, <sep> All reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization. <sep> I strongly encourage you to spend the extra effort making your work more accessible for the final version.
This paper proposes a new kernel learning framework for change point detection by using a generative model. The reviewers agree that the paper is interesting and useful for the community. One of the reviewer had some issues with the paper but those were resolved after the rebuttal. The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature. Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted.
This paper introduces an unsupervised algorithm to learn a goal-conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control.
This paper generated a lot of discussion (not all of it visible to the authors or the public). <sep> R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection. <sep> After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature.
The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning. Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro-modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity. The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non-modulated plasticity fails completely but where the proposed approach succeeds. On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS. The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks. The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high-quality and interesting to the community.
This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP. I really like this general line of work, and the reviewers broadly speaking did as well. The one holdout is reviewer 3, who raises important concerns about the need for further evaluation. I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow-on work to focus on. Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given. Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP-ML hybrids.
+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space <sep> + As the space is low-dimensional (2D), it can be directly visualized. <sep> + I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings <sep> + Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality / multiple senses are captured (except for models which capture discrete senses) <sep> + The paper is very well written <sep> - The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset) <sep> - The approach is not very scalable (hence evaluation on 17M corpus) <sep> - The method cannot be used to deal with data sparsity, though (very) interesting for visualization <sep> - This is mostly an empirical paper (i.e. an interesting application of an existing method) <sep> The reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting.
This paper proposes a method to encourage diversity of Bayesian dropout method. A discriminator is used to facilitate diversity, which the method deal with multi-modality. Empirical results show good improvement over existing methods. This is a good paper and should be accepted.
The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).
The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction. Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques. More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning. No responses were offered to counter such an opinion.
The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology. Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications. Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families. <sep> The authors did not participate in the discussion, and address concerns. The reviewer consensus is a rejection.
The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3's comments.）
The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality. They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid. However, the fact that the results are not very convincing -resulting 3D shapes are very coarse- raises questions regarding the usefulness of the proposed problem formulation. <sep> Thus, the problem formulation novelty alone is not enough for acceptance. Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission. Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase.
Strengths <sep> The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling) <sep> approach to learning the state transition function. The paper is clearly written. <sep> Weaknesses <sep> All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation, <sep> and aspects of the paper are difficult to follow or are sparse on details. <sep> No revisions have been posted. <sep> Summary <sep> All reviewers are in agreement that the paper requires significant work and that it is not ready for *CONF* publication.
The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions. Reviewers unanimous reject, no rebuttal from authors.
The current version of the paper receives a unanimous rejection from reviewers, as the final proposal.
The paper addresses the problem semantic segmentation using a sequential patch-based model. I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account and improve the paper.
This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images. After the discussion phase, reviewers rate the paper close to the acceptance threshold. <sep> AnonReviewer3, who initially stated "My second concern is the results are all on synthetic data, and most shapes are very simple", remains concerned after the rebuttal, stating "all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images." <sep> The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper. Particularly relevant is the criticism that "While the paper is called 'pix2scene', it's really about 'pix2object' or 'pix2shape'."
The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue. While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper.
There is a clear reviewer consensus to reject this paper so I am also recommending rejecting it. The paper is about an interesting and underused technique. However, ultimately the issue here is that the paper does not do a good enough job of explaining the contribution. I hope the reviews have given the authors some ideas on how to frame and sell this work better in the future. <sep> For instance, from my own reading of the abstract, I do not understand what this paper is trying to do and why it is valuable. Phrases such as "we exploit the sparsity" do not tell me why the paper is important to read or what it accomplishes, only how it accomplishes the seemingly elided contribution. I am forced to make assumptions that might not be correct about the goals and motivation. It is certainly true that the implicit one-hot representation of words most common in neural language models is not the only possibility and that random sparse vectors for words will also work reasonably well. I have even tried techniques like this myself, personally, in language modeling experiments and I believe others have as well, although I do not have a nice reference close to hand (some of the various Mikolov models use random hashing of n-grams and I believe related ideas are common in the maxent LM literature and elsewhere). So when the abstract says things like "We show that guaranteeing approximately equidistant vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn" my immediate reaction is to ask why this would be surprising or why it would matter. Based on the reviews, I believe these sorts of issues affect other parts of the manuscript as well. There needs to be a sharper argument that either presents a problem and its solution or presents a scientific question and its answer. In the first case, the problem should be well motivated and in the second case the question should not yet have been adequately answered by previous work and should be non-obvious. I should not have to read beyond the abstract to understand the accomplishments of this work. <sep> Moving to the conclusion and future work section, I can see the appeal of the future work in the second paragraph, but this work has not been done. The first paragraph is about how it is possible to use random projections to represent words, which is not something I think most researchers would question. Missing is a clear demonstration of the potential advantages of doing so.
The reviewers have agreed this work is not ready for publication at *CONF*.
The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation.
The paper presents a novel architecture, reminescent of mixtures-of-experts, <sep> composed of a set of advocates networks providing an attention map to a <sep> separate "judge" network. Reviewers have several concerns, including lack <sep> of theoretical justification, potential scaling limitations, and weak <sep> experimental results. Authors answered to several of the concerns, which did <sep> not convinced reviewers. The reviewer with the highest score was also the least <sep> confident, so overall I will recommend to reject the paper.
This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. <sep> These concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction.
The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone.
This paper proposes a combination of three techniques to improve the learning performance of Atari games. Good performance was shown in the paper with all three techniques together applied to DQN. However, it is hard to justify the integration of these techniques. It is also not clear why the specific decisions were made when combining them. More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape-X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5). Overall, this paper is not ready for publication.
This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance.
This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond "filling the gap" in the literature. <sep> All reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. <sep> Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty.
While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet).
Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with "revise and resubmit".
This paper introduces a novel idea, and demonstrates its utility in several simulated domains. The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on-policy. <sep> They key weakness is not better investigating the idea of making the ER buffer more on-policy, and the effect of doing so. The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3. Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals. However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref-ER. With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger. <sep> For more context, the authors rightly mention "It is commonly believed that off-policy methods (e.g. Q-learning) can handle the dissimilarity between off-policy and on-policy outcomes. We provide ample evidence that training from highly similar-policy experiences is essential to the success of off-policy continuous-action deep RL." Q-learning can significantly suffer from changing the state-sampling distribution. However, adjusting sampling in the ER buffer using rho_t does not change the state-sampling distribution, and so that mismatch remains a problem. Changing the policy more slowly (Point 3) could help with this more. In general, however, these play two different roles that need to be better understood. The introduction more strongly focuses on classifying samples as more on or off-policy, to solve this problem, rather than the strategy used in Point 3. So, from the current pitch, its not clear which component is solving the issues claimed with off-policy updates. <sep> Overall, this paper has some interesting results and is well-written. With more clarity on the roles of the two components of Ref-ER and what they mean for making the ER buffer more on-policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control.
The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm. <sep> Yet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation. <sep> While R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments. <sep> The AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to "revise and resubmit" the paper.
The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop. <sep> This was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at *CONF*. <sep> There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers. <sep> There was a lot of back-and-forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. <sep> Unfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). <sep> Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at *CONF*.
As the reviewers point out, the paper seems to be below the *CONF* publication bar due to low novelty and limited significance.
This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted. <sep> Some of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper. <sep> Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work.
The paper presents "deep deducing", which means learning the state-action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time. <sep> The paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims. <sep> The paper is not ready for publication at this time.
The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point - as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method's applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed.
The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors' rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at *CONF*.
The paper analyzes the performance of CNN models when data is mislabelled in different manners. <sep> The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of *CONF*. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper presents an heuristic method to detect periodicity in a time-series such that it can handle noise and multiple periods. <sep> All reviewers agreed that this paper falls off the scope of *CONF* since it does not discuss any learning-related question. Moreover, the authors did not provide any response nor updated manuscript addressing the reviewers remarks. The AC thus recommends rejection.
Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors. The most positive reviewer is the least confident one. Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold. There is significant literature already on this topic. The "thought delta" created by this paper and the empirical results are also not sufficient for acceptance.
Reviewers are in full agreement for rejection.
There is no author response for this paper. The paper formulates a definition of easy and hard examples for training a neural network (NN) in terms of their frequency of being classified correctly over several repeats. One repeat corresponds to training the NN from scratch. Top 10% and bottom 10% of the samples with the highest and the lowest frequency define easy and hard instances for training. The authors also compare easy and hard examples across different architectures of NNs. <sep> On the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training NNs, and R1 was ready to improve his/her initial rating if the authors revisited the paper. <sep> On the other hand, all the reviewers and AC agreed that the paper requires (1) major improvement in presentation clarity -- see detailed comments of R1 on how to improve as well as comments/questions from R3 and R2; try to avoid confusing terminology such as 'contradicted patterns'. <sep> R1 raised important concerns that the proposed notion of easiness is drawn from the experiment in Fig. 1 of Arpit et al (2017) which is not properly attributed. R3 and R2 agreed that in its current state the experimental results are not conclusive and often non informative. To strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples (see R3's suggestions). <sep> We hope the reviews are useful for improving the paper.
The paper proposes an interesting idea for efficient exploration of on-policy learning in sparse reward RL problems. The empirical results are promising, which is the main strength of the paper. On the other hand, reviewers generally feel that the proposed algorithm is rather ad hoc, sometimes with not-so-transparent algorithmic choices. As a result, it is really unclear whether the idea works only on the test problems, or applies to a broader set of problems. The author responses and new results are helpful and appreciated by all reviewers, but do not change the reviewers' concerns.
This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design. The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application. The topic is relatively under-explored in deep learning and the paper appears to attempt to set a valuable baseline. However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity. As such the paper would benefit from a revision and a stronger resubmission.
Both authors and reviewers agree that the ideas in the paper were not presented clearly enough.
The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator's features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed. R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al. (2016). The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient. The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO.
The paper studies the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior. <sep> The reviewers and AC note the potential weaknesses of experimental results: (1) lack of sufficient datasets with moderate-to-high dimensional inputs, (2) arguable choices of hyperparameters and (3) lack of direct evaluations, e.g., measuring network calibration is better than active learning. <sep> The paper is well written and potentially interesting. However, AC decided that the paper might not be ready to publish in the current form due to the weakness.
Strengths of the paper: <sep> Based on previous work suggesting that radial basis features can help defend against adversarial attacks, the paper proposes a concrete method for incorporating them in deep networks. The paper evaluates the method on multiple datasets, including MNIST and ISBI International Skin Imaging Collaboration (ISIC) Challenge. <sep> Weaknesses: <sep> Reviewers 2 and 3 felt that the paper was not clearly written, and cited several concrete questions about the method that could not be understood from the paper. There were additional concerns of lacking comparison to existing methods, and Reviewer 1 pointed out that a competing method gave higher performance, although this was not reported in the present submission. <sep> Points of contention: <sep> The authors did not provide a response to the reviewer concerns. <sep> Consensus: <sep> All reviewers recommended that the paper be rejected, and the authors did not provide a rebuttal.
This paper presents an interesting approach to image compression, as recognized by all reviewers. However, important concerns about evaluating the contribution remains: as noted by reviewers, evaluating the contribution requires disentangling what part of the improvement is due to the proposed approach and what part is due to the loss chosen and evaluation methods. While authors have done a valuable effort adding experiments to incorporate reviewers suggestions with ablation studies, it does not convincingly show that the proposed approach truly improves over existing ones like Balle et al. Authors are encouraged to strengthen their work for future submission by putting particular emphasis on those questions.
This paper shows that combining GAN and VAE for video prediction allows to trade off diversity and realism. The paper is well-written and the experimentation is careful, as noted by reviewers. However, reviewers agree that this combination is of limited novelty (having been used for images before). Reviewers also note that the empirical performance is not very much stronger than baselines. Overall, the novelty is too slight and the empirical results are not strong enough compared to baselines to justify acceptance based solely on empirical results.
The paper aims to clean data samples with label noise in the training procedure. <sep> The reviewers and AC note the following potential weaknesses: (1) the assumption of uniform noise, which is not the case in practice, (2) marginal gains under real-world datasets and (3) highly empirical and ad-hoc approach. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work.
This paper proposes a document classification algorithm based on partitioned word vector averaging. <sep> I agree with even the most positive reviewer. More experiments would be good. This is a very developed old area.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The reviewers are unanonymous in their assessment that the paper is not *CONF* quality in its current form.
This work studies the performance of several end-to-end CNN architectures for the prediction of biomedical assays in microscopy images. One of the architectures, GAPnet, is a minor modification of existing global average pooling (GAP) networks, involving skip connections and concatenations. The technical novelties are low, as outlined by several reviewers and confirmed by the authors, as most of the value of the work lies in the empirical evaluation of existing methods, or minor variants thereof. <sep> Given the low technical novelty and reviewer consensus, recommend reject, however area chair recognizes that the discovered utility may be of value for the biomedical community. Authors are encouraged to use reviewer feedback to improve the work, and submit to a biomedical imaging venue for dissemination to the appropriate communities.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
Multiple reviewers had concerns about the clarity of the presentation and the significance of the results.
This paper proposes a new method for speeding up convolutional neural networks. It uses the idea of early terminating the computation of convolutional layers. It saves FLOPs, but the reviewers raised a critical concern that it doesn't save wall-clock time. The time overhead is about 4 or 5 times of the original model. There is not any reduced execution time but much longer. The authors agreed that "the overhead on the inference time is certainly an issue of our method". The work is not mature and practical. recommend for rejection.
The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft-max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community. <sep> The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission.
This paper extends the transformer model of Vashwani et al. by replacing the sine/cosine positional encodings with information reflecting the tree stucture of appropriately parsed data. According to the reviews, the paper, while interesting, does not make the cut. My concern here is that the quality of the reviews, in particular those of reviewers 2 and 3, is very sub par. They lack detail (or, in the case of R2, did so until 05 Dec(!!)), and the reviewers did not engage much (or at all) in the subsequent discussion period despite repeated reminders. Infuriatingly, this puts a lot of work squarely in the lap of the AC: if the review process fails the authors, I cannot make a decision on the basis of shoddy reviews and inexistent discussion! Clearly, as this is not the fault of the authors, the best I can offer is to properly read through the paper and reviews, and attempt to make a fair assessment. <sep> Having done so, I conclude that while interesting, I agree with the sentiment expressed in the reviews that the paper is very incremental. In particular, the points of comparison are quite limited and it would have been good to see a more thorough comparison across a wider range of tasks with some more contemporary baselines. Papers like Melis et al. 2017 have shown us that an endemic issue throughout language modelling (and certainly also other evaluation areas) is that complex model improvements are offered without comparison against properly tuned baselines and benchmarks, failing to offer assurances that the baselines would not match performance of the proposed model with proper regularisation. As some of the reviewers, the scope of comparison to prior art in this paper is extremely limited, as is the bibliography, which opens up this concern I've just outlined that it's difficult to take the results with the confidence they require. In short, my assessment, on the basis of reading the paper and reviews, is that the main failing of this paper is the lack of breadth and depth of evaluation, not that it is incremental (as many good ideas are). I'm afraid this paper is not ready for publication at this time, and am sorry the authors will have had a sub-par review process, but I believe it's in the best interest of this work to encourage the authors to further evaluate their approach before publishing it in conference proceedings.
With positive unlabeled learning the paper targets an interesting problem and proposes a new GAN based method to tackle it. All reviewers however agree that the write-up and the motivation behind the method could be made more clear and that novelty compared to other GAN based methods is limited. Also the experimental analysis does not show a strong clear performance advantage over existing models.
Dear authors, <sep> Thank you for submitting your work to *CONF*. The original goal of using smaller models to train a bigger one is definitely interesting and has been the topic of a lot of works. <sep> However, the reviewers had two major complaints: the first one is about the clarity of the paper and the second one is about the significance of the tasks on which the algorith is tested. For the latter point, your rebuttal uses arguments which are little known in the ML community and so should be expanded in a future submission.
Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for *CONF*. The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup.
This paper suggests a method for defending against adversarial examples and out-of-distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold. <sep> The paper is well-written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines). <sep> After rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness.
The reviewers agree the paper is not ready for publication at *CONF*.
This paper proposes search-guided training for structured prediction energy networks (SPENs). <sep> The reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method. <sep> R1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection. Given the space restriction to this year's conference, we have to reject some borderline papers. The AC thus recommends the authors to take the reviewers comments in consideration for a "revise and resubmit".
The paper proposes a filtering technique to use less training examples in <sep> order to train faster; the filtering step is done with an autoencoder. <sep> Experiments are done on CIFAR-10. Reviewers point to a lack of convincing <sep> experiments, weak evidence, lack of experimental details. <sep> Overall, all reviewers converge to reject this paper, and I agree with them.
This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning. All three reviewers seemed happy to see an attempt to unify these two closely related relation-learning problems. However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence-pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they're tested on. <sep> I'm happy to accept the authors' arguments that sentence-to-vector models have practical value, and I'm not placing too much weight on the reviewer's comments about the choice to use that modeling framework. I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously. <sep> I'm mostly concerned, though, about the lack of an impactful positive contribution: I'd have hoped for a paper of this kind to offer a a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind.
The paper introduces a modification of batch normalization technique. In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average <sep> of mean and standard deviation from the current and all previous minibatches. The authors then provide some theoretical justification for the superiority of their variant of BatchNorm. <sep> Unfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing.
This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing. <sep> As is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers. Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors' disagreement.
1. Describe the strengths of the paper. As pointed out by the reviewers and based on your expert opinion. <sep> - The problem and approach, steganography via GANs, is interesting. <sep> - The results seem promising. <sep> 2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision. <sep> The original submission was imprecise and difficult to follow and, while the AC acknowledges that the authors made significant improvements, the current version still needs some work before it's clear enough to be acceptable for publication. <sep> 3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it's a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately. <sep> Concerns varied by reviewer and there was no main point of contention. <sep> 4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. <sep> The reviewers did not reach a consensus. The final decision is aligned with the less positive reviewers, one of whom was very confident in his/her review. The AC agrees that the paper should be made clearer and more precise.
perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long-term dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean (per unit?) and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups. <sep> this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances.
The paper proposes a nice approach to massively multi-label problems with rare labels which may only have a limited number of positive examples; the approach uses Bayes nets to exploit the relationships among the labels in the output layer of a neural nets. The paper is clearly written and the approach seems promising, however, the reviewers would like to see even more convincing empirical results.
This work examines how to deal with multiple classes. Unfortunately, as reviewers note, it fails to adequately ground its approach in previous work and show how the architecture relates to the considerable research that has examined the question beforehand.
This paper proposes an anomaly-detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated. <sep> Revisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at *CONF*. For this reason, I recommend to reject this paper in its current state.
The reviewers agree the paper is not ready for publication.
This paper considers "prototypes" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies. <sep> The reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper. <sep> Although the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real-world applications. <sep> For these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.
The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA). The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details. The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior. Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high-dimensional observations. Therefore, it is difficult to evaluate the significance of ISA-VAE. The authors are encouraged to carefully revise their paper to address these concerns.
This work presents extensions of dialogue systems to simultaneously capture speakers' "personas" (in the framing of Li et al's work) and adapt to them. While the ideas are interesting, reviewers note that the incremental contribution compared to previous work is a bit too limited for *CONF*'s expectation, without being offset by strongly convincing experimental results. Authors are encouraged to incorporated their ideas into future submissions after having combined them with other insights to provide a stronger overall contribution.
The paper studies an narrowly focused but interesting problem -- if the Visual Question answering model "FILM" from Perez et al (2018) is able to decide if "most" of the objects have a certain attribute or color. While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task). We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline.
The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias. To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.
The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated. This differentiates the authors' work from many other works which compress the weights independent of the task/domain. <sep> Strengths: <sep> Clearly written paper <sep> PFA-KL does not require additional hyperparameter tuning (apart from those implicit in choosing \\psi) <sep> Experiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task <sep> Weaknesses: <sep> Results on large-scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period) <sep> Compression after the fact may not be as good as training with a modified loss function that does compression jointly <sep> Insufficient comparisons on ResNet architectures which make comparisons against previous works harder <sep> Overall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold. In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions. However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission.
This paper proposes an approach for incremental learning of new classes using meta-learning. <sep> Strengths: The framework is interesting. The reviewers agree that the paper is well-written and clear. The experiments include comparisons to prior work, and the ablation studies are useful for judging the performance of the method. <sep> Weaknesses: The paper does not provide significant insights over Gidaris & Komodakis '18. Reviewer 1 was also concerned that the motivation for RBP is not entirely clear. <sep> Overall, the reviewers found that the strengths did not outweigh the weaknesses. Hence, I recommend reject.
The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation. However, they did not seem entirely convinced that the improvements over the baseline are statistically significant. Reviewer 1 has lingering concerns about the experimental conditions and whether propensity-score matching within a minibatch would provide a substantial improvement over propensity-score matching across the dataset. Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community. However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance.
This work examines the AlphaGo Zero algorithm, a self-play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games. The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross-entropy minimization in the supervised learning-inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a "robust MDP" view of a 2 player zero-sum game played between the agent and nature. <sep> R3 found the paper well-structured and the results presented therein interesting. R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions). <sep> The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ's convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the "robust MDP" view is less novel than the authors claim based on the known relationships between 2 player zero-sum games and minimax robust control. <sep> I find R1's complaints, in particular with respect to "robust MDPs" (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.
The paper proposes an attention mechanism to focus on robust features in the <sep> context of adversarial attacks. Reviewers asked for more intuition, more <sep> results, and more experiments with different attack/defense models. Authors <sep> have added experimental results and provided some intuition of their proposed <sep> approach. Overall, reviewers still think the novelty is too thin and recommend <sep> rejection. I concur with them.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.
Strengths: <sep> -- Solid experiments <sep> -- The paper is well written <sep> Weaknesses: <sep> -- The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already <sep> suggested that LM objectives are preferable and also using LM objective for pretraining is already the standard practice (see details in R1 and R3). <sep> There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.
The paper studies the convergence of a primal-dual algorithm on a special min-max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non-convex generators. Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi-task learning. <sep> The major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non-convex mini-max saddle point problem in GANs. Linear discriminator implies that the maximization part in min-max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non-convex optimization instance and proves its first order convergence with descent lemma. This technique however can't be applied to general non-convex saddle point problem in GANs. Also the experimental studies are also not strong enough. Therefore, current version of the paper is proposed as borderline lean reject.
The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations. The method is evaluated on relatively simple vision and language tasks. <sep> The idea is nice, but seems to be a special case of previously published work; and the results are not convincing. Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches.
The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions. <sep> The reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of *CONF*. <sep> AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper presents an interesting strategy of curriculum learning for training neural networks, where mini-batches of samples are formed with a gradually increasing level of difficulty. <sep> While reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large-scale) and with state-of-the-art methods would substantially strengthen the evaluation (R3); see also R2's concerns regarding the comprehensive study; (2) important references and baseline methods are missing – see R2's suggestions how to improve; (3) limited technical novelty -- R1 has provided a very detailed review questioning novelty of the proposed approach w.r.t. Weinshall et al, 2018. <sep> Another suggestions to further strengthen and extend the manuscript is to consider curriculum and anti-curriculum learning for increasing performance (R1). <sep> The authors provided additional experiment on a subset of 7 classes from the ImageNet dataset, but this does not show the advantage of the proposed model in a large-scale learning setting. <sep> The AC decided that addressing (1)-(3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them.
The authors addressed the issues raised by the reviewers; I suggest to accept this paper.
This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement. <sep> Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable. <sep> Hence I recommend acceptance.
The authors propose a scale-invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance.
This paper proposes a new gradient-based stochastic optimization algorithm by adapting theory for proximal algorithms to the non-convex setting. <sep> The majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.
The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, "a fairly creative combination of existing approaches"). <sep> Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction. The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h). I believe it's important to note that most truly large scale results in the literature revolve around purely feedforward models that don't require expensive to compute approximations; that said, MNIST experiments would have been nice. <sep> Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable.
The paper proposes a novel model-free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL. The method is principled and provides good empirical results on a set of experiments that relatively comprehensive. I would have liked to see more POMDP tasks instead of Atari, but the results are good. Overall this is good work.
The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning. I concur. Reviewers mention clarity, but I find the latest paper clear enough.
This paper studies the effectiveness of self-supervised approaches by characterising how much information they can extract from a given dataset of images on a per-layer basis. Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation. Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self-supervision). <sep> Overall, the reviews agree that the paper is well written and timely given the growing popularity of self-supervised methods. Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance. We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message).
This paper proposes RTFM, a new model in the field of language-conditioned policy learning. This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments. <sep> Reviewers appreciate the importance of the problem and the effective approach. After the author response which addressed some of the major concerns, reviewers feel more positive about the paper. They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth. <sep> I thank the authors for submitting this paper.
This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don't really need to be even better than the best opponent. Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies. The authors present a physically realistic threat model and demonstrate that adversarial policies can exist in this threat <sep> model. <sep> The reviewers agree with this paper presents results (proof of concept) that is "timely" and the RL community will benefit from this result. Based on reviewers comment, I recommend to accept this paper.
The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius. <sep> Strengths: <sep> -The majority opinion is that this work is a non-trivial extension of prior work to provide radius certification. <sep> -The work is more efficient that strong recent baselines and provides better performance. <sep> -It successfully achieves this while avoiding adversarial training, which is another novel aspect. <sep> Weaknesses: <sep> -There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion. <sep> AC shares the majority opinion and recommends acceptance.
This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning. The underlying hypothesis is that examples with label noise require memorization. The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters. This idea is developed in several candidate ensemble algorithms. One of the proposed ensemble methods exceeds the performance of state-of-the-art methods on MNIST, CIFAR-10 and CIFAR-100. <sep> The reviewers found several strengths and a few weaknesses in the paper. The paper was well motivated and clear. The proposed solution was novel and plausible. The experiments were comprehensive. The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and <sep> extended experiments. The author response addressed the reviewer questions directly and also in a revised document. In the discussion phase, the reviewers were largely satisfied that their concerns were addressed. <sep> This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method's merits.
The paper extends the work on randomized smoothing for certifiably robust classifiers developed in prior work to a weaker specification requiring that the set of top-k predictions remain unchanged under adversarial perturbations of the input (rather than just the top-1). This enables the authors to achieve stronger results on robustness of classifiers on CIFAR10 and ImageNet (where the authors report the top-5 accuracy). <sep> This is an interesting extension of certified defenses that is likely to be relevant for complex prediction tasks with several classes (ImageNet and beyond), where top-1 robustness may be difficult and unrealistic to achieve. <sep> The reviewers were in consensus on acceptance and minor concerns were alleviated during the rebuttal phase. <sep> I therefore recommend acceptance.
This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in *CONF*-2020.
This paper introduces an algorithm for online Bayesian learning of both streaming and non-stationary data. The algorithmic choices are heuristic but motivated by sensible principles. The reviewers' main concerns were with novelty, but because the paper was well-written and addressing an important problem they all agreed it should be accepted.
Main content: BasiGAN, a novel method for introducing stochasticity in conditional GANs <sep> Summary of discussion: <sep> reviewer1: interesting work and results on GANs. Reviewer had a question on pre-defned basis but i think it was answered by the authors. <sep> reviewer3: interesting and novel work on GANS, wel-written paper and improves on SOTA. The main uestion is around bases again like reviewer 1, but it seems the authors have addressed this. <sep> reviewer4: Novel interesting work. Main comments are around making Theorem 1 more theoretically correct, which it sounds like the authors addressed. <sep> Recommendation: Poster. Well written and novel paper and authors addressed a lot of concerns.
This paper is consistently supported by all three reviewers during initial review and discussions. Thus an accept is recommended.
All three reviewers gave scores of Weak Accept. AC has read the reviews and rebuttal and agrees that the paper makes a solid contribution and should be accepted.
This paper proposes a new method to learning heuristics for quantified boolean formulas through RL. The focus is on a method called backtracking search algorithm. The paper proposes a new representation of formulas to scale the predictions of this method. <sep> The reviewers have an overall positive response to this paper. R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper. R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt. They still have one more comment and I encourage the authors to address this in the final version of the paper. <sep> R3 meant to increase their score but somehow this is not reflected in the current score. Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for *CONF*. Therefore, I recommend to accept this paper.
This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN. Generally, the paper is well-organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions. <sep> Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments. We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I'd like to recommend acceptance.
All reviewers come to agreement that this is a solid paper worth publishing at *CONF*; the authors are encouraged to incorporate additional comments suggested by reviewers.
This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model's creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset. <sep> Please incorporate the revisions suggested in the reviews to add clarity to the overlearning versus censoring confusion addressed by the reviewers.
This paper focuses on studying neural network-based denoising methods. The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not "being very well theoretically motivated". These concerns seem to have at least partially been alleviated during the discussion period. I agree with the reviewers. I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to *CONF*. I recommend acceptance. I suggest that the authors continue to further improve the paper based on the reviewers' comments.
This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping. <sep> Overall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.
The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters. Results on language modeling and machine translation are promising. Pros: Interesting idea and nice results. New model may have some independent value beyond NLP. Cons: Empirical comparisons could be more thorough. For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units.
This paper describes a method for bounding the confidence around predictions made by deep networks. Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted.
The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output. This can be useful for identifying possible adversarial attacks. The majority of reviewers agrees that this is a solid technical and experimental contribution.
The submission presents an approach to single-view 3D reconstruction. The approach is quite creative and involves predicting the weights of a network that is then applied to a point set. The presentation is good. The experimental protocol is well-informed and the results are convincing. The reviewers' concerns have largely been addressed by the authors' responses and the revision. In particular, R2, who gave a "3", posted "I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers." This means that all three reviewers recommend accepting the paper. The AC agrees.
Thanks for an interesting discussion. The authors present a supposedly task-independent evaluation metric for generation tasks with references that relies on BERT or similar pretrained language models and a BERT-internal alignment. Reviewers are moderately positive. I encourage the authors to think about a) whether their approach scales to language pairs where wordpieces are less comparable; b) whether second order similarly, e.g., using RSA, would be better than alignment-based similarity; c) whether this metric works in the extremes, e.g., can it distinguish between bad output and super-bad output (where in both cases alignment may be impossible), and can it distinguish between good output and super-good output (where BERT scores may be too biased by BERT's training objective).
This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks. <sep> There were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.
All reviewers found the work interesting but worried about the extension to non-bilinear games. This is a point the authors should explicitly address in their work before publication.
This paper studies numerous ways in which the statistics of network weights evolve during network training. Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process. Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.
This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret. <sep> The only concern with the paper is that *CONF* may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.
The authors propose a novel model-based reinforcement learning algorithm. The key difference with previous approaches is that the authors use gradients through the learned model. They present theoretical results on error bounds for their approach and a monotonic improvement theorem. In the small sample regime, they show improved performance over previous approaches. <sep> After the revisions, reviewers raised a few concerns: <sep> The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would. <sep> The results would be stronger as the experiments were run with more than 3 random seats. <sep> In the revised version of the text, it's unclear if the authors are using target networks. <sep> Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches. I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence. Currently, I don't see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches.
The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. The reviewers all find the proposed method interesting, and believes that this is a paper with reasonable contributions. One comment pointed out that the connection between Time2Vec and harmonic analysis has been discussed in the previous work, and we suggest the authors to include this discussion/comparison in the paper.
This paper proposed an improved ensemble method called BatchEnsemble, where the weight matrix is decomposed as the element-wise product of a shared weigth metrix and a rank-one matrix for each member. The effectiveness of the proposed methods has been verified by experiments on a list of various tasks including image classification, machine translation, lifelong learning and uncertainty modeling. The idea is simple and easy to follow. Although some reviewers thought it lacks of in-deep analysis, I would like to see it being accepted so the community can benefit from it.
This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling. <sep> Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance. The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting. Authors provided convincing feedbacks on this key point. Details requested by the reviewers were all well addressed in the revision. <sep> Hence I recommend acceptance.
The paper lies on the borderline. An accept is suggested based on majority reviews and authors' response.
Four knowledgable reviewers recommend accept. Good job!
The authors introduce an approach to learn a random forest model and a representation simultaneously. The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated. The authors evaluate the technique empirically and show some modest gains. While the reviews were mixed, the approach is quite different from the usual approaches published at *CONF* and so I think it's worth highlighting this work.
This paper deals with the under-sensitivity problem in natural language inference tasks. An interval bound propagation (IBP) approach is applied to predict the confidence of the model when a subsets of words from the input text are deleted. The paper is well written and easy to follow. The authors give detailed rebuttal and 3 of the 4 reviewers lean to accept the paper.
The paper studies out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room, and analyzes factors which promote combinatorial generalization in such environment. <sep> The paper is a very thought provoking work, and would make a valuable contribution to the line of works on systematic generalization in embodied agents. The draft has been improved significantly after the rebuttal. After the discussion, we agree that it is worthwhile presenting at *CONF*.
A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games. <sep> Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers. <sep> This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation.
This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods. <sep> The reviewers ultimately decided this paper is well-written and has content which is of general interest to the *CONF* community.
This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).
This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation). The authors analyze the space of exact and approximated solutions under automorphisms. <sep> Reviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.
The paper describes a method to train a convolutional network with large capacity, where channel-gating (input conditioned) is implemented - thus, only parts of the network are used at inference time. The paper builds over previous work, with the main contribution being a "batch-shaping" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization. The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs. Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time - which would further validate the practicality of the approach.
The paper proposes an approach for N-D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation. All reviewers found the paper to be a novel and useful contribution towards the problem of N-D continuous convolution on unordered particles. I recommend acceptance.
The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition. The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating. I think the authors have adequately addressed the reviewer concerns. The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.
The paper proposes an embedding for nodes in a directed graph, which takes into account the asymmetry. The proposed method learns an embedding of a node as an exponential distribution (e.g. Gaussian), on a statistical manifold. The authors also provide an approximation for large graphs, and show that the method performs well in empirical comparisons. <sep> The authors were very responsive in the discussion phase, providing new experiments in response to the reviews. This is a nice example where a good paper is improved by several extra suggestions by reviewers. I encourage the authors to provide all the software for reproducing their work in the final version. <sep> Overall, this is a great paper which proposes a new graph embedding approach that is scalable and provides nice empirical results.
This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training. The method is evaluated against prior works that use full precision gradients and shown comparable performance. Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept. Therefore, I recommend accepting this submission.
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.
Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.
This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature. <sep> If possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (e.g., 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors).
This paper proposes to discover causal mechanisms through meta-learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers' other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to *CONF*.
The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator. Given the stochastic augmentation procedure T(x) the loss is just a penalty on D. The main unsolved question why it help to make discriminator "smoother" in the consistency case for a standard GAN (since typically, no constraints are enforced). Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength. The reviewer all agree to accept, and so do I.
This paper presents a number of improvements on existing approaches to neural logic programming. The reviews are generally positive: two weak accepts, one weak reject. Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept. The main reason Reviewer <sep> 1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a "fundamental contribution", but otherwise is very positive. I personally feel that <sep> a) most work extends existing work <sep> b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc). <sep> Reviewer 3 is somewhat unconvinced by the nature of the evaluation. While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations. <sep> Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic-based programming.
The paper provides a language for optimizing through physical simulations. The reviewers had a number of concerns related to paper organization and insufficient comparisons to related work (jax). During the discussion phase, the authors significantly updated their paper and ran additional experiments, leading to a much stronger paper.
The authors study neural networks with binary weights or activations, and the so-called "differentiable surrogates" used to train them. <sep> They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability. <sep> The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions. <sep> The authors imporved the readability of the manuscript in the rebuttal. <sep> This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. <sep> Not being familiar with this line of work, I recommend acceptance following the average review score.
This paper introduces a way to measure dataset similarities. Reviewers all agree that this method is novel and interesting. A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept.
The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias. <sep> The reviewers unanimously recommend acceptance of this paper.
The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result -- they show that there exist MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning. Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work. Thus, I recommend this paper for acceptance.
The paper proposes a new algorithm for adversarial training of language models. This is an important research area and the paper is well presented, has great empirical results and a novel idea.
The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments. Therefore, I recommend acceptance.
The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. <sep> All reviewers and AC agree that this work is clearly of interest to *CONF*, however the reviewers have noted the following potential weaknesses: (1) presentation clarity -- see R3's detailed suggestions e.g. comparison to Dwork et al, see R2's comments on how to improve, (2) empirical evaluations -- see R1's question about using more complex models, see R3's question on the usefulness of the word embeddings. <sep> Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted - congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.
This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training. They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole. <sep> The reviewers agree this paper is well-presented and of general interest to the community. Therefore, we recommend that the paper be accepted.
The paper proposes a data-driven approach to learning atomic-resolution energy functions. Experiment results show that the proposed energy function is similar to the state-of-art method (Rosetta) based on physical principles and engineered features. <sep> The paper addresses an interesting and challenging problem. The results are very promising. It is a good showcase of how ML can be applied to solve an important application problem. <sep> For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work.
The authors develop a framework for off-policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations!
This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG-MCMC). The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller). SG-MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models. However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis-Hastings accept / reject step). The cyclic learning rate intuitively helps the sampler escape local modes. This property is demonstrated on synthetic problems in comparison to existing SG-MCMC baselines. The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems. The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept. Some questions remain for future work. E.g. all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior. Why is such a low temperature needed for reasonable performance? In any case a very nice paper.
This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches. The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.
This paper concerns the problem of defending against generative "attacks": that is, falsification of data for malicious purposes through the use of synthesized data based on "leaked" samples of real data. The paper casts the problem formally and assesses the problem of authentication in terms of the sample complexity at test time and the sample budget of the attacker. The authors prove a Nash equillibrium exists, derive a closed form for the special case of multivariate Gaussian data, and propose an algorithm called GAN in the Middle leveraging the developed principles, showing an implementation to perform better than authentication baselines and suggesting other applications. <sep> Reviewers were overall very positive, in agreement that the problem addressed is important and the contribution made is significant. Most criticisms were superficial. This is a dense piece of work, and presentation could still be improved. However this is clearly a significant piece of work addressing a problem of increasing importance, and is worthy of acceptance.
The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information. They then iteratively merge nodes based spectral similarities to obtain coarser graphs. They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs. They demonstrate the performance of their method on standard graph datasets. <sep> This paper has received positive reviews from all reviewers. The authors did a good job of addressing the reviewers' concerns and managed to convince the reviewers about their contributions. I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.
Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP. However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences. This paper proposes locality-sensitive hashing to reduce the sequence-length complexity, as well as reversible residual layers to reduce storage requirements. Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community. <sep> Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well. Note that this paper was also vetted by several detailed external commenters. In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger.
The authors introduce a framework for automatically detecting diverse, self-organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns. <sep> This work is really well-presented, both in the paper and on the associated website, which is interactive and features source code and demos. Reviewers agree that it's well-written and seems technically sound. I also agree with R2 that this is an under-explored area and thus would add to the diversity of the program. <sep> In terms of weaknesses, reviewers noted that it's quite long, with a lengthy appendix, and could be a bit confusing in areas. Authors were responsive to this in the rebuttal and have trimmed it, although it's still 29 pages. My assessment is well-aligned with those of R2 and thus I'm recommending accept. In the rebuttal, the authors mentioned several interesting possible applications for this work; it'd be great if these could be included in the discussion. <sep> Given the impressive presentation and amazing visuals, I think it could make for a fun talk.
The authors presented a Federate Learning algorithm which constructs the global model layer-wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms <sep> This paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers.
This paper proves that fully-connected wide ReLU-NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization. The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof, and (3) lack the comparison with prior work. Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response. Thus I recommend reject.
This paper makes a claim that the iid assumption for NN parameters does not hold. The paper then expresses the joint distribution as a Gibbs distribution and PoE. Finally, there are some results on SGD as VI. Reviewers have mixed opinion about the paper and it is clear that the starting point of the paper (regarding iid assumption) is unclear. I myself read through the paper and discussed this with the reviewer, and it is clear that there are many issues with this paper. <sep> Here are my concerns: <sep> - The parameters of DNN are not iid *after* training. They are not supposed to be. So the empirical results where the correlation matrix is shown does not make the point that the paper is trying to make. <sep> - I agree with R2 that the prior is subjective and can be anything, and it is true that the "trained" NN may not correspond to a GP. This is actually well known which is why it is difficult to match the performance of a trained GP and trained NN. <sep> - The whole contribution about connection to Gibbs distribution and PoE is not insightful. These things are already known, so I don't know why this is a contribution. <sep> - Regarding connection between SGD and VI, they do *not* really prove anything. The derivation is *wrong*. In eq 85 in Appendix J2, the VI problem is written as KL(P||Q), but it should be KL(Q||P). Then this is argued to be the same as Eq. 88 obtained with SGD. This is not correct. <sep> Given these issues and based on reviewers' reaction to the content, I recommend to reject this paper.
This paper describes a method for generating adversarial examples from images and text such that they maintain the semantics of the input. <sep> The reviewers saw a lot of value in this work, but also some flaws. The review process seemed to help answer many questions, but a few remain: there are some questions about the strength of the empirical results on text after the author's updates. Wether the adversarial images stay on the manifold is questioned (are blurry or otherwise noisy images "on manifold"?). One reviewer raises good questions about the soundness of the comparison to the Song paper. <sep> I think this review process has been very productive, and I hope the authors will agree. I hope this feedback helps them to improve their paper.
This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN 'message passing' function. GNN with edge features have already been proposed in the literature. Furthermore, the reviewers think the paper needs to improve further in terms of explain more clearly the motivation and rationale behind the method.
The paper suggests an RL-based approach to design a data valuation estimator. The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study. <sep> The authors provided a rebuttal to address these concerns. It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough.
The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in-depth.
This paper considers solving the minimax formulation of adversarial training, where it proposes a new method based on a generic learning-to-learn (L2L) framework. Particularly, instead of applying the existing hand-designed algorithms for the inner problem, it learns an optimizer parametrized as a convolutional neural network. A robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The idea is using L2L is sensible. However, main concerns on empirical studies remain after rebuttal.
Borderline decision. The idea is nice, but the theory is not completely convincing. That makes the results in this paper not be significant enough.
The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors' response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers' suggestions and resubmitting.
The submission presents an approach to accelerating convolutional networks. The framework is related to depthwise separable convolutions. The reviews are split. R3 expresses concerns about the experimental evaluation and results. The AC agrees with these concerns. The AC also notes that the submission is 10 pages long. Taking all factors into account, the AC recommends against accepting the paper.
The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi-sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE-STL and IWAE-DREG. <sep> Generally reviewers found the paper to be well-written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways. <sep> The authors responded by explaining that "the main "practical" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks." I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted. <sep> This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point.
The authors focus on low-resource text classifications tasks augmented with "rationales". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights. <sep> Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers' reservations were in terms of novelty and extent of technical contribution. <sep> Given the large number of submissions this year, I am recommending rejection for this paper.
This paper proposes two contributions to improve uncertainty in deep learning. The first is a Mahalanobis distance based statistical test and the second a model architecture. Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn't understand the connection between these two contributions. A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood. Some empirical justification of this should be presented.
The paper theoretically shows that the data (embedded by representations learned by GANs) are essentially the same as a high dimensional Gaussian mixture. The result is based on a recent result from random matrix theory on the covariance matrix of data, which the authors extend to a theorem on the Gram matrix of the data. The authors also provide a small experiment comparing the spectrum and principle 2D subspace of BigGAN and Gaussian mixtures, demonstrating that their theorem applies in practice. <sep> Two of the reviews (with confident reviewers) were quite negative about the contributions of the paper, and the reviewers unfortunately did not participate in the discussion period. <sep> Overall, the paper seems solid, but the reviews indicate that improvements are needed in the structure and presentation of the theoretical results. Given the large number of submissions at *CONF* this year, the paper in its current form does not pass the quality threshold for acceptance.
This paper makes a connection between one-class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss. An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution. The technical contribution of the paper is novel and brings an increased understanding into one-class neural networks. The equations and the modeling present in the paper are sound and the paper is well-written. <sep> However, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2. Since its submission the paper has not yet been updated to incorporate these comments. Thus, for now, I recommend rejection of this paper, however on improvements I'm sure it can be a good contribution in other conferences.
This paper explores a post-processing method for word vectors to "smooth the spectrum," and show improvements on some downstream tasks. <sep> Reviewers had some questions about the strength of the results, and the results on words of differing frequency. The reviewers also have comments on the clarity of the paper, as well as the exposition of some of the methods. <sep> Also, for future submissions to *CONF* and other such conferences, it is more typical to address the authors comments in a direct response rather than to make changes to the document without summarizing and pointing reviewers to these changes. Without direction about what was changed or where to look, there is a lot of burden being placed on the reviewers to find your responses to their comments.
This paper presents a new graph pooling method, called HaarPooling. Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework. <sep> One major concern of reviewers is the experiment design. Authors add a new real world dataset in revision. Another concern is computational performance. The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems. <sep> Overall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish. Based on the reviewers' comments, I choose to reject the paper.
This paper introduces a new adaptive variational dropout approach to balance accuracy, sparsity and computation. <sep> The method proposed here is sound, the motivation for smaller (perhaps sparser) networks is easy to follow. The paper provides experiments in several data-sets and compares against several other regularization/pruning approaches, and measures accuracy, speedup, and memory. The reviewers agreed on all these points, but overall they found the results unconvincing. They requested (1) more baselines (which the authors added), (2) larger tasks/datasets, and (3) more variety in network architectures. The overall impression was it was hard to see a clear benefit of the proposed approach, based on the provided tables of results. <sep> The paper could sharpen its impact with several adjustments. The results are much more clear looking at the error vs speedup graphs. Presenting "representative results" in the tables was confusing, especially considering the proposed approach rarely dominated across all measures. It was unclear how the variants of the algorithms presented in the tables were selected---explaining this would help a lot. In addition, more text is needed to help the reader understand how improvements in speed, accuracy, and memory matter. For example in LeNet 500-300 is a speedup of ~12 @ 1.26 error for BB worth-it/important compared a speedup of ~8 for similar error for L_0? How should the reader think about differences in speedup, memory and accuracy---perhaps explanations linking to the impact of these metrics to their context in real applications. I found myself wondering this about pretty much every result, especially when better speedup and memory could be achieved at the cost of some accuracy---how much does the reduction in accuracy actually matter? Is speed and size the dominant thing? I don't know. <sep> Overall the analysis and descriptions of the results are very terse, leaving much to the reader to figure out. For example (fig 2 bottom right). If a result is worth including in the paper it's worth explaining it to the reader. Summary statements like "BB and DBB either achieve significantly smaller error than the baseline methods, or significant speedup and memory saving at similar error rates." Is not helpful where there are so many dimensions of performance to figure out. The paper spends a lot of time explaining what was done in a matter of fact way, but little time helping the reader interpret the results. <sep> There are other issues that hurt the paper, including reporting the results of only 3 runs, sometimes reporting median without explanation, undefined metrics like speedup ,%memory (explain how they are calculated), restricting the batchsize for all methods to a particular value without explanation, and overall somewhat informal and imprecise discussion of the empirical methodology. <sep> The authors did a nice job responding to the reviewers (illustrating good understanding of the area and the strengths of their method), and this could be a strong paper indeed if the changes suggested above were implemented. Including SSL and SVG in the appendix was great, but they really should have been included in the speedup vs error plots throughout the paper. This is a nice direction and was very close. Keep going!
The paper is rejected based on unanimous reviews.
main summary: method for quantizing GAN <sep> discussion: <sep> reviewer 1: well-written paper, but reviewer questions novelty <sep> reviewer 2: well-written, but some details are missing in the paper as well as comparisons to related work <sep> reviewer 3: well-written and interesting topic, related work section and clarity of results could be improved <sep> recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation. Marking paper as reject.
The paper proposes an RL-based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets. <sep> The reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of "biologically plausible" used by the authors. One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow. <sep> For this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.
Reviewers raise the serious issue that the proof of Theorem 2 is plagiarized from Theorem 1 of "Demystifying MMD GANs" (https://arxiv.org/abs/1801.01401). With no response from the authors, this is a clear reject.
The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi-view. The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient-based training used for the supervised loss. Pros: This is an important problem with many applications. Cons: The novelty is minimal. Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting. The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work. The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application-oriented conference/journal.
The submission proposes a dynamic approach to training a neural net which switches between half and full-precision operations while maintaining the same classifier accuracy, resulting in a speed up in training time. Empirical results show the value of the approach, and the authors have added additional sensitivity analysis by sweeping over hyperparameters. <sep> The reviewers were concerned about the novelty of the approach as well as the robustness of the claims that accuracy can be maintained even in the accelerated, dynamic regime. After discussion there were still concerns about the sensitivity analysis and the significance of the results. <sep> The recommendation is to reject the paper at this time.
This article studies universal approximation with deep narrow networks, targeting the minimum width. The central contribution is described as providing results for general activation functions. The technique is described as straightforward, but robust enough to handle a variety of activation functions. The reviewers found the method elegant. The most positive position was that the article develops non trivial techniques that extend existing universal approximation results for deep narrow networks to essentially all activation functions. However, the reviewers also expressed reservations mentioning that the results could be on the incremental side, with derivations similar to previous works, and possibly of limited interest. In all, the article makes a reasonable theoretical contribution to the analysis of deep narrow neural networks. Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year's *CONF*.
(I acknowledge reading authors' recent note on decaNLP.) <sep> This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression. Paper includes a significant number of experiments (including low-resource and multi-tasking experiments) on multiple benchmarks. The reviewers are concerned about lack of support on author's claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results. Hence, I suggest rejecting the paper.
The authors provide an analysis of a cross-lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine-tuning, it is more beneficial to train on the cross-lingual hypotheses than on the in-language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP-focussed venue.
The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original. The approach is simple and a clear improvement over vanilla schedule sampling. However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this. The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.
This paper proposes applying potential flow generators in conjunction with L2 optimal transport regularity to favor solutions that "move" input points as little as possible to output points drawn from the target distribution. The resulting pipeline can be effective in dealing with, among other things, image-to-image translation tasks with unpaired data. Overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms (e.g., GANs, etc.). <sep> After the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance. With these borderline/mixed scores, this paper was discussed at the meta-review level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue. As one important lingering issue, R1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space. The rebuttal concedes this point, but suggests that the method still seems to work. But as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue. However, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology. <sep> Additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision. For example, the image-to-image translation experiments with CelebA were based on a linear (PCA) embedding and feedforward networks. It would have been nice to have seen a more sophisticated setup for this purpose (as discussed in Section 5), especially for a non-theoretical paper with an ostensibly practically-relevant algorithmic proposal. And consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.
This paper studies the trade-off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers. The paper is well motivated and draws interesting observations but can be improved in terms of evaluation. It is a borderline case and rejection is made due to the high competition.
The paper proposed an efficient way of generating graphs. Although the paper claims to propose simplified mechanism, the reviewers find that the generation task to be relatively very complex, and the use of certain module seems ad-hoc. Furthermore, the results on the new metric is at times inconsistent with other prior metrics. The paper can be improved by addressing those concerns concerns.
This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue.
The paper studies Positron Emission Tomography (PET) in medical imaging. The paper focuses on the challenges created by gamma-ray photon scattering, that results in poor image quality. To tackle this problem and enhance the image quality, the paper suggests using generative adversarial networks. Unfortunately due to poor writing and severe language issues, none of the three reviewers were able to properly assess the paper [see the reviews for multiple examples of this]. In addition, in places, some important implementation details were missing. <sep> The authors chose not to response to reviewers' concerns. In its current form, the submission cannot be well understood by people interested in reading the paper, so it needs to be improved and resubmitted.
This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3's comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors). I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).
This paper proposes a new method for code generation based on structured language models. <sep> After viewing the paper, reviews, and author response my assessment is that I basically agree with Reviewer 4. (Now, after revision) This work seems to be (1) a bit incremental over other works such as Brockschmidt et al. (2019), and (2) a bit of a niche topic for *CONF*. At the same time it has (3) good engineering effort resulting in good scores, and (4) relatively detailed conceptual comparison with other work in the area. Also, (5) the title of "Structural Language Models for Code Generation" is clearly over-claiming the contribution of the work -- as cited in the paper there are many language models, unconditional or conditional, that have been used in code generation in the past. In order to be accurate, the title would need to be modified to something that more accurately describes the (somewhat limited) contribution of the work. <sep> In general, I found this paper borderline. *CONF*, as you know is quite competitive so while this is a reasonably good contribution, I'm not sure whether it checks the box of either high quality or high general interest to warrant acceptance. Because of this, I'm not recommending it for acceptance at this time, but definitely encourage the authors to continue to polish for submission to a different venue (perhaps a domain conference that would be more focused on the underlying task of code generation?)
This work proposes a VAE-based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time-sequence). All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model. After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations.
The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data. Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community. This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I'm recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.
The authors present an approach to multi-task learning. Reviews are mixed. The main worries seem to be computational feasibility and lack of comparison with existing work. Clearly, one advantage to Cross-stitch networks over the proposed approach is that their approach learns sharing parameters in an end-to-end fashion and scales more efficiently to more tasks. Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture - or DARTS [https://arxiv.org/abs/1806.09055], maybe - since the offline RSA computations only seem worth it if better than *anything* you can do end-to-end. I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work. I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine. Finally, some work has suggested that benefits from MTL are a result of easier optimization, e.g., [3]; if that is true, will you not potentially miss out on good task combinations with your approach? <sep> Other related work: <sep> [0] https://www.aclweb.org/anthology/C18-1175/ <sep> [1] https://www.aclweb.org/anthology/P19-1299/ <sep> [2] https://www.aclweb.org/anthology/N19-1355.pdf - a somewhat similar two-stage approach <sep> [3] https://www.aclweb.org/anthology/E17-2026/
This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end. They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model. <sep> Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations. The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data. I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).
The reviewers have uniformly had significant reservations for the paper. Given that the authors did not even try to address them, this suggests the paper should be rejected.
The paper proposes a platform for benchmarking, and in particular hardware-agnostic evaluation of machine learning models. This is an important problem as our field strives for more reproducibility. <sep> This was a very confusing paper to discuss and review, since most of the reviewers (and myself) do not know much about the area. Two of the reviewers found the paper contributions sufficient to be (weakly) accepted. The third reviewer had many issues with the work and engaged in a lengthy debate with the authors, but there was strong disagreement regarding their understanding of the scope of the paper as a Tools/Systems submission. <sep> Given the lack of consensus, I must recommend rejection at this time, but highly encourage the authors to take the feedback into account and resubmit to a future venue.
The paper combines graph convolutional networks with noisy label learning. The reviewers feel that novelty in the work is limited and there is a need for further experiments and extensions.
The paper pursues an interesting approach, but requires additional maturation. The experienced reviewers raise several concerns about the current version of the paper. The significance of the contribution was questioned. The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies. The quality and clarity of the technical exposition was also criticized. The comments submitted by the reviewers should help the authors strengthen the paper.
This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new "matrix–based Renyi's entropy coupled with ´tensor kernels over convolutional layers" to solve this problem. The methods seems to be related to an existing approach but derived using a different "starting point". Overall, the method is able to show improvements in high-dimensional case. <sep> Both R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high-dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3's another comment makes an interesting point regarding "the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that's not justified in the paper." I could not find an answer in the rebuttal regarding this. <sep> R1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently. <sep> Overall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers' had this time. <sep> For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.
This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. Empirical results are provided which suggests improved performance. <sep> The reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Reviewers were also unconvinced by the provided empirical evaluation results.
This submission proposes a method for detecting adversarial attacks using saliency maps. <sep> Strengths: <sep> -The experimental results are encouraging. <sep> Weaknesses: <sep> -The novelty is minor. <sep> -Experimental validation of some claims (e.g. robustness to white-box attacks) is lacking. <sep> These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject.
The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features. They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets. <sep> While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below: <sep> 1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward. The suggested modifications in the form of Bilinear similarity and max-pooling were viewed as incremental contributions. <sep> 2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later). <sep> 3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components. One reviewer also pointed that the authors should control form model complexity to ensure an apples-to-apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) . <sep> IMO, the above comments are important and the authors should try to address them in subsequent submissions. <sep> Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted.
The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at *CONF*.
This paper studies the "suspended animation limit" of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes' raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion. I thus recommend reject.
The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem. The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication. Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper.
This paper highlights the problem of penalizing the total correlation of sampled latent variables for unsupervised learning of disentangled representations. Authors prove a theorem on how sample representations with bounded total correlation may have arbitrarily large total correlation when computed with the underlying mean. As a fix, the authors propose RTC-VAE method that penalizes total covariance of sampled latent variables. <sep> R2 appreciated the simplicity of the idea, making it easy to understand and implement, but raises serious concerns on empirical evaluation of the method. Specifically, very limited datasets (initially dsprites and 3d shapes) and with no evaluation of disentanglement performance and no comparison against other disentangling methods like DIP-VAE-1. While the authors added another dataset (3d face) in their revised versions, the concerns about disentanglement performance evaluation and its comparison against baselines remained as before, and R2 was not convinced to raise the initial score. <sep> Similarly, while R1 and R3 appreciate author's response, they believe the response was not convincing enough for them, and maintained their initial ratings. <sep> Overall, the submission has room for improvement toward a clear evaluation of the proposed method against related baselines.
The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain. <sep> The reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well-motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine-tuning would not be sufficient. <sep> The recommendation is to reject the paper at this time.
The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct. <sep> All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach. However, the reviewers also found the presentation a bit hard to follow. While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3's comments). <sep> I think this is a really promising direction of research and I appreciate the authors' efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score). This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.
This paper proposes a GAN-based approach to producing poisons for neural networks. While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST). During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR-10 that it is not clear whether the method benefits from using a GAN. It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (e.g., poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA.
This paper proposes an MCTS method for neural architecture search (NAS). Evaluations on NAS-Bench-101 and other datasets are promising. Unfortunately, no code is provided, which is very important in NAS to overcome the reproducibility crisis. <sep> Discussion: <sep> The authors were able to answer several questions of the reviewers. I also do not share the concern of AnonReviewer2 that MCTS hasn't been used for NAS before; in contrast, this appears to be a point in favor of the paper's novelty. However, the authors' reply concerning Bayesian optimization and the optimization of its acquisition function is strange: using the ConvNet-60K dataset with 1364 networks, it does not appear to make sense to use only 1% or even only 0.01% of the dataset size as a budget for optimizing the acquisition function. The reviewers stuck to their rating of 6,3,3. <sep> Overall, I therefore recommend rejection.
One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment.
This paper presents and evaluates a technique for semantic parsing, and in particular proposes a model to re-rank the candidates generated by beam search. The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews identified strengths of the paper but also significant concerns, mostly centered around the experimental evaluation (including choice of datasets, lack of direct comparison to baselines, need for more methodical and quantitative analysis, need for additional analysis, etc.) and some questions about the design of the technical approach. The authors submitted responses that addressed some of these concerns, but indicated that additional experimentation would be needed to address all of them. In light of these reviews, we are not able to recommend acceptance at this time, but I hope authors use the detailed, constructive feedback to improve the paper for another venue.
This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features. <sep> The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.
The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints. <sep> All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal. In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons. <sep> I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue.
This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1's comment that it is overall a "potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper," and feel the paper really needs a revision and another round of peer review before publication.
This paper introduces a new variant of autoencoders with an topological loss term. <sep> The reviewers appreciated part of the paper and it is borderline. However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference. <sep> Rejection is recommended.
This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal. The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (i.e. theoretical analysis plus timing). Other concerns point to overlaps with Baydin et al. 2015 and the question about the validity of Theorem 1. On balance, this paper requires further work and it cannot be accepted to *CONF*2020.
The paper is not overly well written and motivated. A guiding thread through the paper is often missing. Comparisons with constrained BO methods would have improved the paper as well as a more explicit link to multi-objective BO. It could have been interesting to evaluate the sensitivity w.r.t. the number of samples in the Monte Carlo estimate. What happens if the observations of the function are noisy? Is there a natural way to deal with this? <sep> Given that the paper is 10+ pages long, we expect a higher quality than an 8-pages paper (reviewing and submission guidelines).
The reviewers all agree that this is an interesting paper with good results. The authors' rebuttal response was very helpful. However, given the competitiveness of the submissions this year, the submission did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at *CONF* in its present form. <sep> Concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.
The paper proposes two methods for link prediction in knowledge hypergraphs. The first method concatenates the embedding of all entities and relations in a hyperedge. The second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. The authors demonstrate on two datasets (derived by the authors from Freebase), that the proposed methods work well compared to baselines. The paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions. <sep> The authors should be commended for providing the source code for reproducibility. One of the reviewers (who was unfortunately also the most negative), was time pressed. Unfortunately, the discussion period was not used by the reviewers to respond to the authors' rebuttal of their concerns. <sep> Even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to *CONF*, it unfortunately falls below the acceptance threshold in its current form.
The paper presents a semi-supervised data streaming approach. The proposed architecture is made of a layer-wise k-means structure (more specifically a epsilon-means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers. <sep> Some reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper-parameters. Some claims ("the distribution associated to a class may change with time") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification. <sep> Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay. <sep> The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting ("none of the existing approaches in the literature are directly applicable to the UPL problem"), preventing the authors from comparing their results with baselines. <sep> However, after a short bibliographic search, some related approaches exist under another name: <sep> * Incremental Semi-supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018; <sep> * Incremental Semi-Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018; <sep> * Online data stream classification with incremental semi-supervised learning, Loo et al., 2015. <sep> The above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.
This paper proposes a response generation approach that aims to tackle the generic response problem. The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses. The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (i.e., were not convinced) even after the rebuttal. Hence, I suggest a reject for this paper.
The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak.
Novelty of the proposed model is low. Experimental results are weak.
The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration. After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference. I thus recommend rejecting this submission for now.
This paper addresses the classic medial image segmentation by combining Neural Ordinary Differential Equations (NODEs) and the level set method. The proposed method is evaluated on kidney segmentation and salient object detection problems. Reviewer #1 provided a brief review concerning *CONF* is not the appropriate venue for this work. Reviewer #2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet. Reviewer #3 raises concerns on whether the methods are presented properly. The authors did not provide responses to any concerns. Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.
This paper introduces a new ECG dataset. While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad *CONF* community. As such, I suggest this paper to be submitted to a more specialised venue.
While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation. Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity.
This paper presents Capacity-Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity. <sep> The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing. <sep> Although the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection.
The article studies a student-teacher setting with over-realised student ReLU networks, with results on the types of solutions and dynamics. The reviewers found the line of work interesting, but they also raised concerns about the novelty of the presented results, the description of previous works, settings and claims, and experiments. The revision clarified some of the definitions, the nature of the observations, experiments, and related works, including a change of the title. However, the reviewers still were not convinced, in particular with the interpretation of the results, and keep their original ratings. With many points that were raised in the original reviews, the article would benefit from a more thorough revision.
This paper proposes an end-to-end deep reinforcement learning-based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines. <sep> The presentation is clear and the results are interesting, but the novelty seems insufficient for *CONF*. The proposed model is based on transformer with the following changes: <sep> * encoder: position embedding is removed, state embedding is added to the multi-head attention layer and feed forward layer of the original transformer encoder; <sep> * decoder: three decoders one for the three steps, namely selection, rotation and location. <sep> * training: actor-critic algorithm
The paper proposed an attention-forcing algorithm that guides the sequence-to-sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher-forcing model, which can be expensive. <sep> The major concern about this paper is the experimental justification is not sufficient: <sep> * lack of evaluations of the proposed method on different tasks; <sep> * lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc; <sep> * lack of comparisons to related existing supervised attention mechanisms.
Using ideas from mean-field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks. This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities. In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period. And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons. <sep> First, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers. Given that this is not a purely theoretical paper, but rather one suggesting practically-relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about. In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data). <sep> Secondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic. Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem. *CONF* is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message. At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution. My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future.
This is an interesting paper on an important topic. The reviewers identified a variety of issues both before and after the feedback period; I urge the authors to consider their comments as they continue to refine and extend their work.
The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch. The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources. However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at *CONF* in its current form. Concerns included that the analysis was not sufficiently focused and the experiments too small scale. As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.
The paper is on the borderline. A rejection is proposed due to the percentage limitation of *CONF*.
This paper addresses the problem of differential private data generator. The paper presents a novel approach called G_PATE which builds on the existing PATE framework. The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator. <sep> Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. I believe upon making significant changes to the paper, this could be a good contribution. Thus, as of now, I am recommending a Rejection.
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. <sep> However, the novelty of this paper is rather marginal and given the high competition at *CONF*2020, this paper is unfortunately below the bar. <sep> We hope that the reviewers' comments are useful for improving the paper for potential future publication.
The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space. The authors achieve state of the art results on two datasets. <sep> All reviewers agreed that the idea was vert interesting and has a lot of potential. Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements. I thank the authors for taking this feedback positively and significantly revising the writeup. However, even after revising the writeup some of the ideas are still not clear. In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing. It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound. Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the "clarity" barrier there wasn't enough time to discuss the other technical details of the paper. As a result, even though the paper seems interesting, the initial lack of clarity went against the paper. <sep> In summary, based on the reviewer comments, I recommend that the paper cannot be accepted.
This paper presents a method to compress DNNs by quantization. The core idea is to use NAS techniques to adaptively set quantization bits at each layer. The proposed method is shown to achieved good results on the standard benchmarks. <sep> Through our final discussion, one reviewer agreed to raise the score from 'Reject' to 'Weak Reject', but still on negative side. Another reviewer was not satisfied with the author's rebuttal, particularly regarding the appropriateness of training strategy and evaluation. Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript. Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing. As there was no strong opinion to push this paper, I'd like to recommend rejection.
This manuscript outlines procedures to address fairness as measured by disparity in risk across groups. The manuscript is primarily motivated by methods that can achieve "no-harm" fairness, i.e., achieving fairness without increasing the risk in subgroups. <sep> The reviewers and AC agree that the problem studied is timely and interesting. However, in reviews and discussion, the reviewers noted issues with clarity of the presentation, and sufficient justification of the results. The consensus was that the manuscript in its current state is borderline, and would have to be significantly improved in terms of clarity of the discussion, and possibly improved methods that result in more convincing results.
This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation. <sep> While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified. For example, in the discussion phase a reviewer asked how can one infer "hider learns to first turn away from the seeker then run away" from a single transition frequency? Or, the rebuttal mentions "The agent with visibility reward does not get the chance to learn features of self-visibility because of the limited speed hence the model received samples with significantly less variation of its self-visibility, which makes learning to discriminate self-visibility difficult". What is the justification for this? There could be more details in the paper and I'd also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms. <sep> I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper.
This paper uses GAN for data augmentation to improve the performance of knowledge distillation. <sep> Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details. <sep> Hence, I recommend rejection.
The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e-commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers' concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
This paper proposes a new type of Polynomial NN called Ladder Polynomial NN (LPNN) which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout. Experiments show it works better than FMs with simple classification and regression tasks, but no experiments are done in more complex tasks. All reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve.
This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states. The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions). Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. Therefore, I am recommending rejection.
The paper addresses image translation by extending prior models, e.g. CycleGAN, to domain pairs that have significantly different shape variations. The main technical idea is to apply the translation directly on the deep feature maps (instead of on the pixel level). <sep> While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns: <sep> (1) ill-posed formulation of the problem and what is desirable, (2) using fine-tuned/pre-trained VGG features, (3) computational cost of the proposed approach, i.e. training a cascade of pairs of translators (one pair per layer). <sep> AC can confirm that all three reviewers have read the author responses. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.
This paper proposes to use more varied geometric structures of latent spaces to capture the manifold structure of the data, and provide experiments with synthetic and real data that show some promise in terms of approximating manifolds. <sep> While reviewers appreciate the motivation behind the paper and see that angle as potentially resulting in a strong paper in the future, they have concerns that the method is too complicated and that the experimental results are not fully convincing that the proposed method is useful, with also not enough ablation studies. Authors provided some additional results and clarified explanations in their revisions, but reviewers still believe there is more work required to deliver a submission warranting acceptance in terms of justifying the complicated architecture experimentally. <sep> Therefore, we do not recommend acceptance.
This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology. The paper was reviewed by three experts. R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details. R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results. R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is "severely undermined" by a recently-published paper that proposes a very similar approach. Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for *CONF*, we would not have expected the authors to cite the paper. Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance. We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue.
The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that "Bayesian ideas" explain what's going on seems obviously wrong to me. The third reviewer seems to think there's a tautology lurking here and that doesn't seem to be true to me either. It is disappointing that the reviewers did not re-engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year. <sep> Even though I'm inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse. <sep> [1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611
This paper introduces the idea of "empathy" to improve learning in communication emergence. The reviewers all agree that the idea is interesting and well described. However, this paper clearly falls short on delivering the detailed and sufficient experiments and results to demonstrate whether and how the idea works. <sep> I thank the authors for submitting this research to *CONF* and encourage following up on the reviewers' comments and suggestions for future submission.
This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (e.g. adversarial perturbation). <sep> Strengths: <sep> -The work is interesting and the theoretical analysis is insightful. <sep> Weaknesses: <sep> -The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings. <sep> -The paper clarity could be improved. <sep> Both weaknesses were not sufficiently addressed in the rebuttal. All reviewer recommendations were borderline to reject.
Quoting from R3: "This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices." With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores. <sep> The approach and idea are interesting. The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real-world data. The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real-world problem. Experiments are shown on synthetic data and MNIST. As a stand-alone theoretical result, it leaves open questions as to the proposed utility.
This submission proposes a statistically consistent saliency estimation method for visual model explainability. <sep> Strengths: <sep> -The method is novel, interesting, and passes some recently proposed sanity checks for these methods. <sep> Weaknesses: <sep> -The evaluation was flawed in several aspects. <sep> -The readability needed improvement. <sep> After the author feedback period remaining issues were: <sep> -A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying "pixels that are crucial for the predictions". However, the authors use Figure 7 to argue that LEG is better at identifying salient "pixels that are more likely to be relevant for the prediction". These two observations are contradictory and should be resolved. <sep> -The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don't describe how many images are included in their sanity check analysis or how those images were chosen. <sep> -The new discussion section is not actually a discussion section but a conclusion/summary section. <sep> Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance. <sep> Reviewers largely agreed with the stated shortcomings but were divided on their significance. <sep> AC shares the recommendation to reject.
This manuscript proposed biologically-inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer. The authors' results indicate that the modifications improve performance as well as improved robustness to adversarial attacks. <sep> The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results. In the opinion of the AC, the manuscript in its current state is borderline and could be improved with more convincing empirical justification.
The paper proposed a method for training multiple transformers with tied parameters and enabling dynamic choice of the number of encoder and decoder layers. The method is evaluated in neural machine translation and shown to reduce decoding costs without compromising translation quality. The reviewers generally agreed that the proposed method is interesting, but raised issues regarding the significance of the claimed benefits and the quality of overall presentation of the paper. Based on a consensus reached in a post rebuttal discussion with the reviewers, I am recommending rejecting this paper.
This paper offers likely novel schemes for image resizing. The performance improvement is clear. Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue. The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately.
The submission performs empirical analysis on f-VIM (Ke, 2019), a method for imitation learning by f-divergence minimization. The paper especially focues on a state-only formulation akin to GAILfO (Torabi et al., 2018b). The main contributions are: <sep> 1) The paper identifies numerical proplems with the output activations of f-VIM and suggest a scheme to choose them such that the resulting rewards are bounded. <sep> 2) A regularizer that was proposed by Mescheder et al. (2018) for GANs is tested in the adversarial imitation learning setting. <sep> 3) In order to handle state-only demonstrations, the technique of GAILfO is applied to f-VIM (then denoted f-VIMO) which inputs state-nextStates instead of state-actions to the discriminator. <sep> The reviewers found the submitted paper hard to follow, which suggests a revision might make more apparent the author's contributions in later submissions of this work.
This paper proposes a very interesting alternative to feed-forward network layers, based on Quaternion methods and Hamilton products, which has the benefit of reducing the number of parameters in the neural network (more than 50% smaller) without sacrificing performance. They conducted extensive experiments on language tasks (NMT and NLI, among others) using transformers and LSTMs. <sep> The paper appears to be clearly presented and have extensive results on a variety of tasks. However all reviewers pointed out that there is a lack of in-depth analysis and thus insight into why this approach works, as well as questions on the specific effects of regularization. These concerns were not addressed in the rebuttal period, instead leaving it to future work. My assessment is that, with further analysis, ablation studies, and comparison to alternative methods for reducing model size (quantization, etc), this paper has the potential to be quite impactful, and I look forward to future versions of this work. As it currently stands, however, I don't believe it's suitable for publication at *CONF*.
All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. A common complaint was lack of clarity being a major problem. Unfortunately, the paper cannot be accepted in its current form. The authors are encouraged to improve the presentation of their approach and resubmit to a new venue.
All the reviewers reach a consensus to reject the current submission. <sep> In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases. <sep> 1) Between Eq (16) and (17), the authors assumed the 'extended restricted strong convexity' given by the un-numbered equation. <sep> 2) In Eq. (25), the authors assume the existence of \\sigma making the inequality true. <sep> However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, e.g. even the square or logistic loss. The authors need to address these assumptions explicitly rather than using them from nowhere.
The authors made no response to reviewers. Based on current reviews, the paper is suggested a rejection as majority.
Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication. <sep> This can be repaired, and the authors should try again after a thorough revision and rewrite.
The paper proposes a text normalisation model for Amharic text. The model uses word classification, followed by a character-based GRU attentive encoder-decoder model. The paper is very short and does not present reproducible experiments. It also does not conform to the style guidelines of the conference. There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1. It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind.
The paper proposed a new seq2seq method to implement natural language to formal language translation. Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder. Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods. Intensive discussions happened between the authors and reviewers. Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the theory and the implementation in this paper. The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers.
This work considers the popular LQR objective but with [A,B] unknown and dynamically changing. At each time a context [C,D] is observed and it is assumed there exist a linear map Theta from [C,D] to [A,B]. The particular problem statement is novel, but is heavily influenced by other MDP settings and the also follows very closely to previous works. The algorithm seems computationally intractable (a problem shared by previous work this work builds on) and so in experiments a gross approximation is used. <sep> Reviewers found the work very stylized and did not adequately review related work. For example, little attention is paid to switching linear systems and the recent LQR advances are relegated to a list of references with no discussion. The reviewers also questioned how the theory relates to the traditional setting of LQR regret, say, if [C,D] were identity at all times so that Theta = [A,B]. <sep> This paper received 3 reviews (a third was added late to the process) and my own opinion influenced the decision. While the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods.
The paper proposes a new problem setting of predicate zero-shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it. <sep> All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero-shot methods. <sep> Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.
The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low-resolution datasets. <sep> The reviewers are unanimous in their judgement that the method is not novel enough, and the authors' rebuttals have not convinced the reviewers or me about the opposite.
The authors propose a hybrid model-free/model-based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance. They evaluate their approach on a collection of benchmark tests. <sep> The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation. The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported. Even then, the reported improvements were small. <sep> Given the issues raised by the reviewers, this paper is not ready for publication at *CONF*.
The authors present an approach to large scale bitext extraction from Wikipedia. This builds heavily on previous work, with the novelty being somewhat minor efficient approximate K-nearest neighbor search and language agnostic parameters such as cutoffs. These techniques have not been validated on other data sets and it is unclear how well they generalise. The major contribution of the paper is the corpus created, consisting of 85 languages, 1620 language pairs and 135M parallel sentences, of which most do not include English. This corpus is very valuable and already in use in the field, but IMO *CONF* is not the right venue for this kind of publication. There were four reviews, all broadly in agreement, and some discussion with the authors.
The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data. Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data. The results are promising, but the experiments are fairly limited. The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work.
This paper tackles the problem of exploration in RL. In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals. The empirically show that agents using this method uniformly visit all valid states under certain conditions. They also show that these agents are able to learn behaviours without providing a manually-defined reward function. <sep> The drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques. Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper. This dampers the contribution, hence I recommend to reject this paper.
This paper proposes a new formulation of the non-local block and interpret it from the graph view. The idea is interesting and the experimental results seems to be promising. <sep> Reviewer has two major concerns. The first is the presentation, which is not clear enough. The second is the experimental design and analysis. The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video-based applications. <sep> Overall, the idea of non-local block from graph view is interesting. However, the presentation of the paper needs further polish and thus does not meet the standard of *CONF*
This paper was assessed by three reviewers who scored it as 6/3/6. <sep> The reviewers liked some aspects of this paper e.g., a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre-processing step rather than the proposed method, suspected overfitting. Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by *CONF*2020, thus it cannot be accepted at this time. AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue.
The paper considers planning through the lenses both of a single and multiple objectives. The paper then discusses the pareto frontiers of this optimization. While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.
The paper proposes to get universal adversarial examples using few test samples. The approach is very close to the Khrulkov & Oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim. Overall, all reviewers recommend rejection, and I agree with them.
The paper is proposed a rejection based on majority reviews.
The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items. The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions: empirically, it seemed like there should be more truly large-scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP-hard problem seemed unimportant as they are routinely used for this purpose in ML problems. With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference.
This paper proposes to address the issue of biases and artifacts in benchmark datasets through the use of adversarial filtering. That is, removing training and test examples that a baseline model or ensemble gets wright. <sep> The paper is borderline, and could have flipped to an accept if the target acceptance rate for the conference were a bit higher. All three reviewers ultimately voted weakly in favor of it, especially after the addition of the new out-of-domain generalization results. However, reviewers found it confusing in places, and R2 wasn't fully convinced that this should be applied in the settings the authors suggest. This paper raises some interesting and controversial points, but after some private discussion, there wasn't a clear consensus that publishing it as is would do more good than harm.
The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area. Also, after discussions the usefulness of the method remains a bit unclear - it seems some engineering (related to sparse operations) is still required to validate the viability of the approach.
This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases. Further more, it focuses on the data-free setting, where such a perturbation is found without having access to data (images) from the distribution that train- and test data comes from. <sep> The reviewers were very conflicted about this paper. Among others, the strong experimental results and the clarity of writing and analysis were praised. However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis. <sep> In the end, the paper seems in current form not convincing enough for me to recommend acceptance for *CONF*.
A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper. <sep> In sum, this paper is not currently at a stage where it can be accepted.
This paper demonstrates a framework for optimizing designs in auction/contest problems. The approach relies on considering a multi-agent learning process and then simulating it. <sep> To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty. The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced. Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis.
The paper proposes ATR-CSPD, which learns a low-dimensional representation of seasonal pattern, for detecting changes with clustering-based approaches. <sep> While ATR-CSPD is simple and intuitive, it lacks novel contribution in methodology. It is unclear how it is different from existing approaches. The evaluation and the writing could be improved significantly. <sep> In short, the paper is not ready for publication. We hope the reviews can help improve the paper for a strong submission in the future.
This work presents a method for generating an (approximately) minimal adversarial perturbation for neural networks. During the discussion period, the AC raised additional concerns that were not originally addressed by the reviewers. The method is an iterative first order method for solving constrained optimization problems, however when considered as a new first order optimization method the contribution seems minimal. Most of the additions are rather straightforward---e.g. using a line search at each step to determine the optimal step size---and the reported gains over PGD are unconvincing. PGD can be considered as a "universal" first order optimizer [1], as such we should be careful that the reported gains are substantial and not just a question of tuning. Given that using a line search at each step increases the computational cost by a multiplicative factor, the comparison with PGD should take this into account. <sep> The AC notes several plots in the Appendix show PGD having better performance (particularly on restricted Imagenet), and for others there remain questions on how PGD is tuned (for example the CIFAR-10 plots in Figure 5). One of two things explains the discrepancies in Figure 5: either PGD is finding a worse local optimum than FAB, or PGD has not converged to a local optimum. There needs to be provided experiments to rule out the second possibility, as this is evidence that PGD is not being tuned properly. Some standard things to check are the step size and number of steps. Additionally, enforcing a constant step size after projection is an easy way to improve the performance of PGD. For example, if the gradient of the loss is approximately equal to the normal vector of the constraint, then proj(x_i+ lambda * g) ~ x_i will result in an effective step size that is too low to make progress. <sep> Finally, it is unclear what practical use there is for a method that finds an approximately minimum norm perturbation. There are no provable guarantees so this cannot be used for certification. Additionally, in order to properly assess the security and reliability of ML systems, it is necessary to consider larger visual distortions, occlusions, and corruptions (such as the ones in [2]) as these will actually be encountered in practice. <sep> 1. https://arxiv.org/pdf/1706.06083.pdf <sep> 2. https://arxiv.org/abs/1807.01697
Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better. <sep> However, after all, we decided not to accept your paper due to weak justification and limited experimental validation. Writing should also be improved significantly. We hope that the feedback from the reviewers help you improve your paper for potential future submission.
This paper proposes a relation-based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero-shot learning. <sep> Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited. Thus the authors are encouraged to further improve in these directions. In particular: <sep> - The idea of using a variant of the widely-used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero-shot learning. How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL. These problems need further elaboration. <sep> - The proposed model itself is not a real "VAE", making the value of an extensive derivation based on variational inference less prominent. <sep> - There is also the need to compare with the baselines mentioned by the reviewers. <sep> Overall, this is a borderline paper. Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection.
This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning. Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods. <sep> All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation. They also agree that the writing of the paper can be improved (details included in their reviews below). <sep> The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for *CONF*.
A nice paper, but quite some unclarities; it's unclear in particular if the paper improves w.r.t. SOTA. Esp. scaling is an issue here. Also, the understandability is below par and more work can make this into an acceptable submission.
The authors discuss how to predict generalization gaps. Reviews are mixed, putting the submission in the lower half of this year's submissions. I also would have liked to see a comparison with other divergence metrics, for example, L1, MMD, H-distance, discrepancy distance, and learned representations (e.g., BERT, Laser, etc., for language). Without this, the empirical evaluation of FD is a bit weak. Also, the obvious next step would be trying to minimize FD in the context of domain adaptation, and the question is if this shouldn't already be part of your paper? Suggestions: The Amazon reviews are time-stamped, enabling you to run experiments with drift over time. See [0] for an example. <sep> [0] https://www.aclweb.org/anthology/W18-6210/
The paper received mixed reviews. On one hand, there is interesting novelty in relation to biological vision systems. On the other hand, there are some serious experimental issues with the machine learning model. While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns. However, concerns about experiments remained.
The author propose a method to first learn policies for intrinsically generated goal-based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework. The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal. However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains. These concerns persisted after the rebuttal and discussion. Thus, I recommend rejection at this time.
This paper proposes a new method for zero-shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection.
This paper studies the impact of embedding complexity on domain-invariant representations by incorporating embedding complexity into the previous upper bound explicitly. <sep> The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well-written. However, Reviewers and AC generally agree that the current version can be significantly improved in several ways: <sep> - The proposed upper bound has several limitations such as looser than existing ones. <sep> - The embedding complexity is only addressed implicitly, which shares similar idea with previous works. <sep> - The claim of implicit regularization has not been explored in-depth. <sep> - The proposed MDM method seems to be incremental and related closely with the embedding complexity. <sep> - There is no analysis about the generalization when estimating this upper bound from finite samples. <sep> There are important details requiring further elaboration. So I recommend rejection.
I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation. <sep> Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating "semantically meaningful" perturbations goes against the standard definition of adversarial attacks. This left me with two questions: <sep> 1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this. <sep> 2. In what situation would such an attack method would be practically useful? <sep> Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here. <sep> While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!
The paper proposes a top-down approach to train deep neural networks -- freezing top layers after supervised pre-training, then re-initializing and retraining the bottom layers. As mentioned by all the reviewers, the novelty is on the low side. The paper is purely experimental (no theory), and the experimental section is currently too weak. In particular: <sep> - Experiments on different domains should be performed. <sep> - Different models should be evaluated. <sep> - Ablation experiments should be performed to understand better under which conditions the proposed approach works. <sep> - For speech recognition, WER should be reported - even if it is without a LM - such that one can compare with existing work.
The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision.
This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error. The approach was evaluated on several Atari games, Super Mario, and VizDoom. <sep> There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it's an interesting approach. R1 thought it was well-written and quite easy to follow. I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for. <sep> The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen. R3 points out that these 5 Atari games are not known for being hard exploration games. Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn't show significant improvement over baselines. <sep> I appreciate the authors' argument that every method has "its niche", but the environments chosen must still be properly motivated. I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks. For instance, they state in the rebuttal that "The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (i.e., obtainable scores) of the agent." But it doesn't seem like this was assessed in any quantitative way. Without this understanding, it'd be difficult for an outsider to know which tasks are appropriate to use with this approach. I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games. I still think this is a very interesting approach and look forward to future versions of this paper.
The paper investigates a new approach to classification of irregularly sampled and unaligned multi-modal time series via set function mapping. Experiment results on health care datasets are reported to demonstrate the effectiveness of the proposed approach. <sep> The idea of extending set functions to address missing value in time series is interesting and novel. The paper does a good job at motivating the methods and describing the proposed solution. The authors did a good job at addressing the concerns of the reviewers. <sep> During the discussion, some reviewers are still concerned about the empirical results, which do not match well with published results (even though the authors provided an explanation for it). In addition, the proposed method is only tested on the health care datasets, but the improvement is limited. Therefore it would be worthwhile investigating other time series datasets, and most important answering the important question in terms of what datasets/applications the proposed method works well. <sep> The paper is one step away for being a strong publication. We hope the reviews can help improve the paper for a strong publication in the future.
The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.
The present paper addresses the problem of imitation learning in multi-modal settings, combining vision, language and motion. The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization. This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase. Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work. As a result, the depth of insights is not yet sufficient for publication.
The paper is proposed a rejection based on majority reviews.
The submission proposes a method for learning a graph structure and node embeddings through an iterative process. Smoothness and sparsity are both optimized in this approach. The iterative method has a stopping mechanism based on distance from a ground truth. <sep> The concerns of the reviewers were about scalability and novelty. Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. The improvement over LDS, the most similar approach, is relatively minor. <sep> Although the paper is promising, more work is required to establish the contributions of the method. Recommendation is for rejection.
The paper studies the problem of modeling inter-object dynamics with occlusions. It provides proof-of-concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object-level segmentation masks and depth information. However, the technical novelty is not high and the requirement of such structured information seems impractical real-world applications which thus limits the significance of the proposed method.
This paper provides empirical evidence on synthetic examples with a focus on understanding the relationship between the number of "good" local minima and number of irrelevant features. The reviewers find the problem discussed to be important. One of the reviewers has pointed out that the paper does not present deep insights and is more suitable for workshops. The authors did not provide a rebuttal, and it appears that the reviewers opinion has not changed. <sep> The current score is clearly not sufficient to accept this paper in its current form. Due to this reason, I recommend to reject this paper.
This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel. <sep> However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing. <sep> I sincerely thank the authors for submitting to *CONF* and hope to see a revised paper in a future venue.
A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model-free with a model-based approach to deal with this setting. <sep> While the idea is interesting, the main claims are insufficiently demonstrated. A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions. The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments.
The paper presents a method for continual learning with a variant of VAE. The proposed approach is reasonable but technical contribution is quite incremental. The experimental results are limited to comparisons among methods with generative replay, and experimental results on more complex datasets (e.g., CIFAR 100, CUB, ImageNet) are missing. Overall, the contribution of the work in the current form seems insufficient for acceptance at *CONF*.
An actor-critic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model. There is disagreement among the reviewers regarding the significance of this paper. Two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence. In particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper. I recommend that the authors provide more empirical evidence to back up their claims and then resubmit.
The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation. The paper provides quantitative and qualitative evidence that the proposed method works. <sep> The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period. <sep> The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers. Unfortunately, the submission as it stands is not yet suitable for *CONF*.
The paper shows an automatic piano fingering algorithm. The idea is good. But the reviewers find that the novelty is limited and it is an incremental work. All the reivewers agree to reject.
This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder. <sep> I side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting. I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up.
This submission proposes a new gating mechanism to improve gradient information propagation during back-propagation when training recurrent neural networks. <sep> Strengths: <sep> -The problem is interesting and important. <sep> -The proposed method is novel. <sep> Weaknesses: <sep> -The justification and motivation of the UGI mechanism was not clear and/or convincing. <sep> -The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well-reflected in the quantitative results. <sep> -The submission was hard to read and some images were initially illegible. <sep> The authors improved several of the weaknesses but not to the desired level. <sep> AC agrees with the majority recommendation to reject.
This paper proposes a solution to the decentralized privacy preserving domain adaptation problem. In other words, how to adapt to a target domain without explicit data access to other existing domains. In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains. <sep> The reviewers has split scores for this work with two recommending weak accept and two recommending weak reject. However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for *CONF* 2020). The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work. The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA. The authors also provided extensive revisions in response to the reviewers comments. Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations. <sep> Therefore, this paper is not recommended for acceptance in its current form. We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers.
This paper proposes using non-Euclidean spaces for GCNs, leveraging the gyrovector space formalism. The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings. <sep> Reviewers got mixed impressions on this paper. Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing. In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work.
This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages) and language-specific aspects. The authors evaluate their embeddings in a non-parametric way (i.e., on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods. The main concern that both reviewers (and myself) have about this work relates to its evaluation part. While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (e.g., as in GLUE). The authors comment: "[their model in producing embeddings] it isn't as strong when using classification for final predictions. This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible". If this is true, why is it the case and isn't it quite restrictive? I think this work is interesting with a nice analysis but the current empirical results are borderline (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks). As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks.
The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression. While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results.
This paper proposes a framework for privacy-preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX. The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed. In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases. The reviewers found that this made it difficult to make sense of the results. The writing of this paper should be streamlined, along with the experiments before resubmission.
The authors present a multiple instance learning-based approach that uses weak supervison (of which skills appear in any given trajectory) to automatically segment a set of skills from demonstrations. The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis. Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear. Thus, I recommend rejection at this time.
The authors present a self-supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high-level planner over learned latent goals with a shared low-level goal-completing control policy. The reviewers had significant concerns about both problem positioning (w.r.t. existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work. While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning w.r.t. prior work and experimental comparisons were sufficiently resolved. Thus, I recommend to reject this paper at this time.
This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others. The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future. <sep> The paper is difficult to follow, uses an incorrect *CONF* format, and is full of typos. <sep> All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state. The authors did not provide a rebuttal to reviewers' concerns. <sep> I recommend rejecting this paper for *CONF*.
This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set. This substantially improves upon existing work. The proposed method is well supported both with theory and experiments. All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion. The determinantal point process might not be one of the most popular topics in the *CONF* community today but certainly is relevant.
The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field.
This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion. The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder. A number of techniques including adversarial training and soft DTW are applied to improve the training. The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors. After the rebuttal and discussion, all reviewers are supportive on accepting the paper.
The paper studies the problem of being able to control text generated by pre-trained language models. <sep> The problem is timely and important. The paper frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm, Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact.
The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF. Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network. <sep> Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders. However, I believe the authors address the criticisms of R4 satisfactorily. <sep> Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper. <sep> I do have a few quips myself and some comments that may help the authors to further improve the paper. <sep> Re: the design that models each parameter as a spline. <sep> This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t. <sep> If you use a B-spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note, the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible. <sep> I am not sure how this method would compare to naive (non-deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.
The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper: <sep> The experiments are exhaustive, identifying many domains where the approach can be applied <sep> The presented results are compelling <sep> The paper is well written <sep> The paper introduces a new problem setup that has not been studied before <sep> I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.
This paper introduces a novel game-theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large-scale dataset (ResNet-200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy-to-follow reasoning leading to the proposed game-theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper. <sep> What I found particularly interesting in their game-theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints u^j⊤u^i=0 have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient-ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable. <sep> Pros: <sep> Provides a novel game-theoretic reformulation of PCA. <sep> Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game-theoretic reformulation. <sep> Provides theoretical guarantee for the global convergence of the sequential algorithm. <sep> Demonstrates that the proposed decentralized algorithm is scalable to large-scale problems. <sep> Cons: <sep> The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high-dimensional settings. <sep> Significance of the proposed game-theoretic formulation in the context of game theory does not seem to be well explored.
This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally-specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow-up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.
This paper provides theoretical justifications on why the data augmentation technique, Mixup (convex combinations of pairs of data examples) , can help in improving robustness and generalization of GLMs and ReLUs. The authors rewrote a Mixup loss function as the summation of a standard empirical loss and some regularization terms regularizing gradient, Hessian and some higher order terms. Using the quadratic approximation of the Mixup loss (ignoring the higher order terms), the authors proved that the quadratic approximation of the Mixup loss was equivalent to an upper bound of the second order Taylor expansion of an adversarial loss, providing justifications for why Mixup loss training could improve robustness against small attacks. Using the same quadratic approximation of the Mixup loss, the regularization term controlled the hypothesis class to have a smaller Rademacher complexity. <sep> Overall, the paper provides insightful theoretical interpretations for a commonly used data augmentation technique in DL. The paper also supports its claims by numerical experiments. Although there is some minor concerns on using the quadratic approximation of the Mixup loss, as well as R3 term's regularization effect on a broader family of models, the paper provides unique and novel insights on Mixup.; all reviewers acknowledge the authors applying the existing proof techniques to analyze Mixup's effect on robustness and generalization. <sep> Therefore, I recommend accepting this paper.
This paper describes a new and experimentally useful way to propose masked spans for MLM pretraining, by masking spans of text that co-occur more often than would be expected given their components - ie that are statistically likely to be non-compositional phrases. <sep> The authors should make some attempt to connect their PMI heuristic with prior methods for statistical phrase-finding and term recognition, eg https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-439.pdf or https://link.springer.com/chapter/10.1007/978-3-540-85287-2_24 in the final paper.
The paper introduces a learning framework for solving incompressible Navier-Stokes fluid using a physics informed loss formulation. The PDE is solved on a grid, and the model, implemented via convolutions and a U-Net, is trained to minimize the NS residual. The model is trained on a variety of randomized contexts, in a way that allows training to explore a large number of configurations. The paper presents original contributions compared to previous Physics informed framework (discrete formulation, conditioning on the domain conditions, …). All the reviewers agree that the detailed rebuttal provides answers to their questions and that the contribution is significant, they all have a positive assessment of the paper.
The paper describes a new data augmentation approach for image based RL. The approach is both simple and effective. It improves significantly the performance of several algorithms across a number of tasks. The reviewers were unanimous about the benefits of the proposed technique. This represents an important advance for RL.
This paper presents a density ratio estimation approach to make the early decision for sequential data. The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7). However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating. Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward. Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle). Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm---making early predictions on sequential data---because the method requires "plotting the speed-accuracy tradeoff curve on the test dataset." This response implies that it at least requires a withheld dataset. Although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria. <sep> Considering all these facts--high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation.
This paper addresses a central problem in inference in implicit models-- classical approaches on such problems ('ABC') rely on computation of summary statistics, and multiple methods for automatically finding summary statistics have been proposed. This paper provides a fresh take on this classical problem, by providing a methods for finding information-maximising summary stats. The work is original, likely impactful, and carried out rigorously and carefully. The reviewers flagged some issues with empirical comparisons, as well as discussion or relevant work-- those issues mainly seem to have been resolved in the review process. Moreover, given the originality of the approach, and provided that the description of empirical comparisons and relationship with other work are carefully and conservatively worded, I believe this will be worth publishing even if it is not always the 'best' method on all problems.
The paper discusses a new threat model for multi-exit DNNs: attacks against efficiency of inference. The proposed attack increases the inference time of such networks by the factor of 1.5-5, while at the same reducing the accuracy of attacked networks. Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training. <sep> Overall, the paper exhibits a novel contribution, is well written and methodically sound. Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible. However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution.
The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise-linear neural network is constant in an Lp ball around a given point. This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees. The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return "I don't know"). The experiments show good results in practice. The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result. Overall, this is an important contribution worth communicating to the *CONF* community, so I'm happy to recommend acceptance.
This paper proposes a new idea for performing knowledge distillation by leveraging teacher's classifier to train student's penultimate layer feature via proposing suitable loss functions. Reviewers appreciate the simultaneous simplicity and effectiveness of the method. A comprehensive set of studies are performed to empirically show the effectiveness of the method. Specifically, the proposed distillation method is shown to outperform state-of-the-art across various network architectures, teacher-student capacities, datasets, and domains. The paper is well-written and is easy to follow. All reviewers rate the paper on the accept side (after the rebuttal) and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact. I concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept.
The paper considers the problem of learning a new task with few examples by using related tasks which can exploit shared representations for which more data is available. The paper proves a number of interesting (primarily theoretical) results.
The authors provide four rigorous upper bounds on the operator norm of the linear transformation associated with a 2D convolutional layer of a neural network. One of these is a heuristic proposed in earlier work by Miyato et al, and widely used, so, among other things, their result provides theoretical context for that method which will be of broad interest. All four of their bounds can be efficiently computed and have easily computed gradients, so they propose using the minimum of the four bounds for various purposes. Since, for standard architectures, the Lipschitz constant of a network can be bounded above by the product of the operator norms of its layers, there are a variety of applications of differentiable bounds on these operator norms. They show that their new bound is sometimes much tighter than the bound of Miyato et al, and can be computed much more efficiently than two known methods for exact computation. The paper is written well, which will facilitate future work building on this work. The analysis builds on earlier work, but insight was required to obtain the new results; the fundamental novelty of the mathematical development was confirmed by an expert reviewer. <sep> While they experimentally compared the accuracy of their approximations to those of the method of Miyato, et al, the case for the practical utility of their method would have been stronger if they had shown that their regularizer led to better results for some tasks. However, I believe that the paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic, and, even if it cannot be directly applied, seems like to inspire practically useful methods in the future.
This paper proposes an interesting unified framework for meta-learning with commentaries, which contains information helpful for learning about new tasks or new data points. The authors present three kinds of different instantiations, i.e., example weighting, example blending, and attention mask, and show the effectiveness with the extensive experiments. The proposed method has a potential to be used for a wide variety of tasks.
This work explores the distillation of language models using MixUp for data augmentation. Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out. The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the *CONF* audience. I therefore recommend accepting this paper for a poster presentation.
The paper identifies the phenomenon of oversquashing in GNNs and relate it to bottleneck. While this phenomenon has been previously observed, the analysis is new and insightful. The authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and long-range dependencies, and propose a solution in the form of a fully-adjacent layer. While the paper does not offer much methodologically, it is the observation of bottleneck that is of importance. <sep> We therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution "too simple" rather unsubstantiated. The authors have well addressed these issues in their rebuttal. The AC recommends accepting the paper.
The paper offers a new take on generalization, motivated by the empirical success of self-supervised learning. Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it). Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews. <sep> I agree with the positive reviewers and would like to recommend acceptance.
All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written. However, I agree with R3 that the paper need further re-working the theoretical part (see the post-rebuttal comments of R4). Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper.
This paper studies the reasons for failure of trained neural network models on out of distribution tasks. While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets. The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent. Further, there are interesting insights in the paper to merit acceptance.
In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode-level exploratory behaviors. The innovation is in the scoring and learning from these episode-level behaviors rather than trying to come up with shorter timescale proxies of exploration. In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (i.e. good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states. To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level. <sep> A weakness is that the paper uses low-level state in grid worlds and there is some ambiguity in applying this to settings with continuous states. The authors discuss general strategies for dealing with these limitations as potential future work. <sep> The reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective. The authors put in solid effort to address reviewer concerns. The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces. <sep> Overall, this paper had a consensus "accept" rating (7,7,7,6), and I endorse this as my decision.
The paper explores how to effectively conduct negative sampling in learning for text retrieval. The paper shows that negative examples sampled locally are not informative, and proposes ANCE, a new learning mechanism that samples hard negative examples globally, using an asynchronously updated ANN index. <sep> Pros • The problem studied is important. • Paper is generally clearly written. • Solid experimental results. • There is theoretical analysis. <sep> Cons • The idea might not be so new. The contribution is mainly from its empirical part. <sep> During rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. They have also added additional experimental results.
The paper introduces LEAD, a decentralized optimizer with communication compression that can achieve linear convergence rate in the strongly convex setting. In terms of novelty, the authors should still add a discussion of Magnusson et al., 2019, On Maintaining Linear Convergence of Distributed Learning and Optimization under Limited Communication, which is a related linear convergence result in the deterministic (full gradient) case, and relates to the analysis here which is stochastic but also exploits the deterministic case. Nevertheless, reviewers reached consensus-with communication compression in the given time-that the paper in its current form is well written and the results are presented clearly in both experiments and theory (which builds up on the earlier NIDS algorithm). The presentation of the algorithm can be slightly improved. We hope the authors will incorporate the remaining smaller open points such as mentioned by R1, such as making the constants in the convergence bounds explicit when comparing with other methods.
This paper proposes an input-dependent dropout strategy, using variational inference to infer the rates. While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large-scale experiments (e.g. ImageNet). The reviewers unanimously agreed on accepting the paper.
This work presents a novel approach to improving text decoding. This is backed up by a solid analysis of cross-entropy growth with top-k vs top-p and an interesting demonstration of repetition correlating with probability. The paper is well written and well organized. The authors' rebuttal was effective in convincing the reviewers. The human evaluation (added during the rebuttal phase) is a good demonstration of the effectiveness of the approach and so this paper's proposed decoding algorithm is likely to be impactful. <sep> Pros: <sep> Well written. <sep> Solid theoretical analysis of cross-entropy and its relation to top-p and top-k decoding. Good demonstration of how repetition is related to probability. <sep> Interesting, novel and effective decoding algorithm. <sep> Human evaluation of the algorithm's output. <sep> Cons: <sep> The approach has not been tested with a variety of language models. <sep> Decoding quality still depends on a target perplexity which may need to be tuned. <sep> Unnecessary dependence on Zipf's law in the basic decoding algorithm.
This paper studies the robustness of CapsNets under adversarial attacks. It is found that the votes from primary capsules in CapsNets are manipulated by adversarial examples and that the computationally expensive routing mechanism in CapsNets incurs high computational cost. As such, a new adversarial attack is specially designed by attacking the votes of CapsNets without having to involve the routing mechanism, making the method both effective and efficient. <sep> Strengths: <sep> This is the first work which proposes an attack specifically designed for CapsNets by exploiting their special properties. <sep> The proposed vote attack is more effective and efficient than the other attacks originally proposed for CNNs rather than CapsNets. <sep> The paper is generally well written. <sep> The experimental study is quite comprehensive. <sep> The code will be made available to facilitate reproducibility. <sep> Weaknesses: <sep> The study is mostly for only one type of CapsNets. It is not clear whether the observations in this paper still hold generally for other types of CapsNets even after some additional experiments have been added. <sep> The presentation of the paper has room for improvement. <sep> The authors are recommended to proofread the references thoroughly to ensure style consistency such as the consistent use of capitalization, e.g. <sep> "Star-caps" -> "STAR-Caps" <sep> "ieee symposium on security and privacy (sp)" -> "IEEE Symposium on Security and Privacy (SP)" <sep> Despite its weaknesses especially those pointed out by Reviewer 2, this paper would be of interest to other researchers as it is the first paper that studies adversarial attacks on CapsNets.
The paper introduces "Concept Embeddings" to Prototypical Network, which are part-based representations and are learnt by a set of independent networks (which can share weights). The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross-organ cell type classification. The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few-shot learning methods. <sep> Although some reviewers raised concerns about not using other few-shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the "part-based concepts" to reasonably span the space of all images which is a characteristic of fine-grained image classification problem. Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient.
The paper's main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable. This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance. The initial version of the paper was not well written, but the authors improved the writing significantly. The paper includes a thorough empirical evaluation to support the main message. I have read the paper and I believe that this work will be of interest to a diverse audience.
The paper attempts to improve retrieval in open domain question answering systems, which is a very important problem. In this regards, the authors propose to utilize cross-attention scores from a seq2seq reader models as signal for training retrieval systems. This approach overcomes typical low amount of labelled data available for retriever model. The reviewers reached a consensus that the proposed approach are interesting and novel. The proposed approach establish new state-of-the-art performance on three QA datasets, although the improvements over previous methods are marginal. Overall, reviewers agree that the paper will be beneficial to the community and thus I recommend an acceptance to *CONF*.
There was a predominantly positive feedback from the reviewers so I recommend acceptance of the paper. It is well-written and well-motivated tackling an important problem: That in self-supervised learning one might encode different invariances by default, even if some of these invariances are useful for downstream tasks (e.g. being rotation invariant may be detrimental to predicting if an image has the correct rotation on a phone). For this, they propose a simple, yet elegant approach and validate it on many downstream tasks. Given the recent interest in self-supervised learning, this appears to be a relevant and interesting paper for the *CONF* community.
This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper.
This paper examines under what conditions influence estimation can be applied to deep networks and finds that, among of items, that influence estimates are poorer for deeper architectures, perhaps due to poor inverse Hessian vector approximations for poor for deeper models. The authors provide an extensive experimental evaluation across datasets and architectures, and demonstrates the fragility of influence estimates in a number of conditions. Although the reviewers noted that these issues are now "folk knowledge", there has been less scientific effort in identifying these failures. <sep> Of course, more theoretical understanding would help the community better understand where these fragilities lie, but the experimental evaluation is sufficiently strong to be of broad interest to the community.
The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. These concerns were well-addressed in the rebuttal. Both of the reviewers that originally rated the paper below the bar raise the scores. After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper.
This paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of "usable information". This is effectively an indirect measure of how much information the network maintains about a given categorical variable, Y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network's representations have with Y. The authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to "minimal sufficient representations". <sep> The initial reviews were mixed. A common theme in the critiques was the lack of evidence of the generalization and scalability of these results. The authors addressed these concerns by including new experiments on different architectures and the CIFAR datasets, leading one reviewer to increase their score. The final scores stood at 3, 7 ,7, 7. Given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the AC's opinion.
The paper proposes an approach for solving constrained optimization problems using deep learning. The key idea is to separate equality and inequality constraints and "solve" for the equality constraints separately. Empirical results are given for convex QPs and for a non-convex problem that arises in AC optimal power flow. <sep> There was much discussion of this paper between the reviewers and the area chair. THe key question was whether the empirical evaluation is sufficient to convince that the method is more effective than existing solvers. The current experiments do not show that the method achieves better solutions than existing solvers. For the convex case this is to be expected since solvers are optimal. But in the non-convex case, it would have been nice to see that the method indeed can find better solutions. <sep> This leaves the advantage of the method in its speedup over existing methods. However, as the authors acknowledge, it is possible that this speedup is due to better use of parallelization than the methods they compare to. It is true that deep learning is particularly easy to parallelize, but this is not impossible for other methods (e.g., for linear algebra operations etc). <sep> Thus, taken together the empirical support for the current method is somewhat limited. The method itself does make sense, and this was indeed appreciated by the reviewers.
The paper is presenting an important empirical finding. When the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and low-error paths. Motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization. The paper received unanimously good scores. I agree with the reviews and recommend acceptance.
Reviewers concurred that this is an interesting paper with contributions worthy of publication. The authors also provided many details in the rebuttal which makes the paper even more strong.
The paper proposes a sensitivity-based pruning method at initialization. For fully connection and and convolutional neural networks, it shows that the model is trainable only when the initialization satisfies Edge of Chaos (EOC). The paper also provided a rescaling method so that the pruned network is initialized on the EOC. For Resnet, the paper shows that the proposed pruning satisfies the EOC condition by default and further provides re-parameterization method to tackle exploding gradients. The experiments show the performance of the proposed method on fully connected and convolution neural network, as well as ResNet. There were some concerns about the contribution of the paper compared to that of [1]. I read the two papers carefully and while both papers aim at addressing a similar problem, i.e., pruning at initialization while avoiding layer collapse, the paper provides a different perspective on the problem, and provides enough theoretical contribution and insights to be found helpful and interesting by the community.
This paper proposes a modular RNN architecture called SCOFF. The work was inspired by cognitive science(object file and schema) and was built upon previous work RIMs. The method is validated on tasks having multiple objects of the same type. <sep> Pros: <sep> It addresses an important problem in DNN -- systematic generalization. <sep> The proposal makes sense and is more flexible than RIM. <sep> Experimental results outperform baselines. <sep> Cons before rebuttal: <sep> The presentation of the algorithm is not very clear due to some confusing notations and missing details of algorithm steps. <sep> The comparison with baselines might not be fair due to extra parameters. <sep> The novelty is limited, because the only difference from RIM is weight sharing. <sep> The reviewers raised concerns listed in Cons. The authors successfully addressed concerns: they indicated that the comparison was fair with the same input to both; SCOFF is more flexible than RIM, and there is spatial attention to input. <sep> The authors added the missing details in the revised version. <sep> All reviewers agree that the problem is important and the idea is interesting. Since the authors' rebuttal was very helpful in clarifying the questions raised, I recommend accept.
The authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression. The framework contains several methods, some that use sub-sampling on data to calculate the estimation, and some that use sub-sampling on the trees within one single gradient boosting model (i.e. virtual ensemble) to calculate the estimation. The different methods reveal the trade-off between faster calculation and good uncertainty estimation. The authors conduct extensive empirical study to demonstrate the validity of the designed framework. <sep> The reviewers agree that the paper is well-written on a very important topic of machine learning in practice. The authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples. The reviewers believe that the work marks a good starting point for addressing this important topic. Nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging.
This paper proposes a new sparsity-inducing activation function, and demonstrates its benefits on continual learning and reinforcement learning tasks. <sep> After the discussion period, all reviewers agree that this is a solid paper, and so do I. I am thus recommending it for acceptance as a poster. Hopefully, such visibility (combined with the open source release of the code) will encourage other researchers to try this new technique, and we will see more evidence confirming its usefulness in more varied settings and versus stronger baselines (that remain somewhat limited in the current work: this is the main weakness of the paper).
This paper studies how to statistically test if a given model violates the constraint of individual fairness. This is an interesting and novel problem, and the paper leverages the technique of gradient flow to identify a "witness" pair for individual fairness violation. <sep> During the rebuttal, the authors have addressed many concerns raised in the reviews. The author should also consider discussing the runtime and improving the exposition to resolve some of the presentation issues raised in the reviews.
Many papers have been written on calibrating neural networks recently. This paper presents a definition of calibration that is more robust than the popular ECE measure while also being more discerning than the Brier score. Then it proposes a practical spline-based method of post-editing the output softmax scores to make them more calibrated. The method is shown to be better than existing methods both on their measure and established measure (thanks to reviewer's questions on that.). <sep> The paper should be of much interest to the community.
This paper proposes a graph information bottleneck (GIB) framework for subgraph recognition, including the proposal of a MI objective as well as a bi-level optimization scheme for minimizing said objective. The paper receive mixed reviews, with two reviewers in favor of acceptance and two reviewers in favor of rejection. <sep> One negative reviewer was too short to judge and had low-confidence. I think most of the concerns arise from lack of understanding of the work and the authors adequately address this on the rebuttal. The authors are encouraged to make minor modifications for clarity. In particular, classical IB considers random variables x, z, y, and learns latent representation z that is maximally informative about output y and sufficiently informative about input x. Therefore, it is natural to expect that the input to GIB is a random graph. <sep> The other negative reviewer finds the paper lacks novelty and points to multiple references. The positive reviewers also ask about the connection with additional references. Im my opinion, the authors do an excellent job at clarifying the differences with all prior work mentioned by the reviewers, including the closest one, a GIB paper in NerurIPS 2020. In my view, the present submission contains sufficient novelty relative to prior work, specifically as it focuses on a different problem (sub-graph) and proposes a different optimization method. That being said, I think it is absolutely essential that the author responses be added to the paper. In other words, the final version must add citations to the relevant work mentioned by the reviewers and clarify the differences. <sep> All other comments from the two remaining reviewers are very positive: the reviewers find the paper contributes with "quite interesting information theoretic objective functions that actually work on multiple graph learning tasks" and "makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation." I share the views of the positive reviewers and recommend acceptance, subject to the authors incorporating their responses to the reviewers' comments.
This paper has been thoroughly evaluated by four expert reviewers and it had received one public comment. The authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers. Even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, I consider this paper worthy of inclusion in the program of *CONF* 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations.
The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published. <sep> In the final version, the authors should discuss briefly "BERTology Meets Biology: Interpreting Attention in Protein Language Models"(https://openreview.net/forum?id=YWtLZvLmud7) and "Improving Generalizability of Protein Sequence Models via Data Augmentations" (https://openreview.net/forum?id=Kkw3shxszSd). However, the authors should also make sure that the final version respects the *CONF* length limits. <sep> I am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.
The paper considers the use of adversarial self-supervised learning to render robust data representations for various tasks, in particular to integrate the Bootstrap Your Own Robust Latents (BYOL) with adversarial training, where a small amount of labeled data is available together with a sizable unlabeled dataset. Especially the low-data regime is of interest. It extends a previous method with a new adversarial augmentation technique, it is compared against several methods, and the robust representations are shown to be useful more generally. There were some confusing presentations and questions that were resolved in a detailed discussion with the reviewers.
This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting <sep> more learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self-training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights. <sep> The clarity and novelty are above the bar of *CONF*. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all <sep> comments in the final version.
This paper presents a hyperparameter optimization (HPO) method in which two search strategies: global and local optimizations, are effectively combined. All reviewers evaluated the proposed method positively. The experimental results clearly show the effectiveness of the proposed method, and it could be an important contribution to the AutoML research community. On the other hand, since there is no theoretical justification for the proposed method, it is not clear why the performance of the proposed method is improved so much. The author's rebuttal has alleviated some of our concerns on this point, but the further theoretical analysis is desirable.
This paper proposes benchmark tasks for offline policy evaluation. The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error. The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines. All of the reviewers are in favor of the paper.
This paper presents a method named LowKey, which is designed to protect user privacy. This is done by taking advantage of adversarial attacks to pre-process facial images against the black-box facial recognition system in social media, yet the processed facial images remain visually acceptable. The paper experimentally illustrates that it is effective against two existing commercial facial recognition APIs. <sep> The reviewers unanimously agree that this is an interesting and important problem, and recommend the paper for acceptance. The ACs agree.
The authors appreciated this submission because (a) the aspect of explainability is novel, (b) its strong performance, (c) the clarity of the paper. I urge the authors to double check all of the reviewer comments to make sure they are all addressed in the updated version. I vote to accept.
This paper provides a natural combination of conditional neural processes with LieConv models. It is a good step forward for stochastic processes with equivariances. While there is still room to improve the experiments, the authors provided a good response to reviewers, and the paper is a nice contribution.
This paper proposes a method for regularizing the pre-training of an embedding function for relation extraction from text that encourages well-formed clusters among the relation types. Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state-of-the-art, including MTB (Soares et al., 2019), which was the strongest. The key, novel idea is to model prototype representations for target relations as part of the learning process. A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few-shot learning. This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research. <sep> Reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre-training and target tasks, and the need for several additional baselines. The authors were able to address these concerns, and the reviewers did not raise any follow-up concerns.
This work improves deep generative models by applying Langevin dynamics to sample in the latent space. The authors test their method under different configurations (different loss functions) and various generative models (VAE, flow, besides GAN). Experimental results demonstrate the benefits of the proposed method in different generative tasks. <sep> I tend to accept this solid work. I just have two suggestions: 1) the authors should discuss the connections and the differences between the proposed method and the energy-based methods like (Arbel et al., 2020) in-depth; 2) it may be more suitable to replace "Wasserstein gradient flow" with "Discriminator gradient flow" in the title.
Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics. It contributes to the growing interest in continuous-time RNN formulations that can deal with exploding gradient problem, and worthy of *CONF* poster presentation. Three reviewers were positive and one was slightly negative. Authors added additional experiments and strengthened the manuscript significantly during the review process.
This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns. <sep> Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.
This paper investigates knowledge distillation in the context of non-autoregressive machine translations. All reviewers are supportive of acceptance, especially after the thoughtful author responses. A well motivated and simple to implement approach that is giving good empirical results.
The reviewers agree that the submitted paper is of high quality and provides a promising approach/framework for Bayesian IRL. Certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers. For the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards (this is already partly done by the added grid world experiments) and discuss relations to VAEs as the initially had in mind and even in the paper title. Beyond that, the should of course also address other reviewers' comments.
The authors did a good job responding to reviewer concerns. While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality. imo the authors' attention detailed ablations and analysis post-review makes the paper worth including in the conference.
Reviewers agree that the paper excels in providing a principle pipeline that combines CNNs and GPs with a Poisson-Gamma distribution to provide a generic approach for multiresolution modelling of tumour mutation rates. As a whole such combination of techniques addresses a key challenge in computational biology that also scales to large datasets.
This paper provides some theoretical perspective on the use of data augmentation in consistency regularization-based semi-supervised learning. The framework used in the paper argues that high-quality data augmentation should move along the data manifold. This generic view allows the paper's ideas to be applied across datasets (as opposed to image-specific data augmentation used in state-of-the-art semi-supervised learning algorithms). I am not aware of any other work raising these points, and indeed this paper is significant in that it provides a new and potentially useful perspective on the most performative semi-supervised learning approach. Reviewers agreed that the paper was clear and useful. The main concern was that the paper only included experiments in toy settings. Indeed, it would have been much more impactful to apply these ideas to state-of-the-art semi-supervised learning methods, but I think it can be excused given the theoretical focus of the work.
The paper studies a hierarchical or multi-level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and (Jiang et al. 2019) among others. It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval. The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow-up research in the future. <sep> Smaller concerns remained that the presented multi-level results cannot exactly recover local SGD as a special case. Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar. In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated.
This paper proposes an autoregressive flow-based network, Flowtron, for TTS with style transfer. It integrates the Tacotron architecture with the flow-based generative model. Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles. All reviewers consider the work interesting. There are concerns raised on technical details which mostly have been cleared by the authors' rebuttal. The exposition also has been greatly improved based on the reviewers' suggestions and questions. Overall, this is an interesting paper and I would recommend acceptance.
This paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth before-after state ("gold" setting, like reconstruction error of auto-encoder) or from the before-after state of an analogous edit. The problem setting follows mostly from Yin et al (2019). <sep> There are several shortcomings of this paper: <sep> The technical novelty of the model is somewhat limited, as it's an assembly of components that have been used in related work. Authors insist in the discussion on the novelty of the tree edit encoder (Sec 3.2), but I think this is overstated. The related tree-edit models (e.g., Tarlow et al (2019)) perform a very similar encoding in the decoder when training with teacher-forcing. While it's true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacher-forced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits. AFAIU, the proposal is basically to use this hidden representation as the edit encoder. <sep> The claim that the approach is more language agnostic than Dinella et al (2020) also seems shaky, as the authors admit in their response that language-specific grammars need to be handled specially. E.g., I expect that the authors of Dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach. <sep> The submission relies too heavily on the "gold" setting (where the target output is fed as an input), and I'm skeptical of their characterization of Yin et al's intentions when the authors say in comments, "Because of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently. This was the original motivation expressed by Yin et al. (2019)." I don't see this stated in the Yin et al paper. I see Yin et al. characterizing this setting as an upper bound and saying "better performance with the gold-standard edit does not necessarily imply better (more generalizable) edit representation." (Yin et al., 2019). It's worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it. <sep> Having said this, (1) is not a standard way to think about encoding edits, (2) is debatable, and we can hope that future work does not treat improvements in the "gold" setting as a valid research goal. Further, there is another contribution around imitation learning that the reviewers appreciate. In total, reviewers did an excellent job and generally believe the paper should be accepted. I won't go against that recommendation.
Paper summary <sep> This paper investigates theoretically and empirically the effect of increasing the number of parameters ("overparameterization") in GAN training. By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically). This paper provides an explicit threshold for the width of a 1-layer ReLU network generator so that gradient-ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data). The authors also provides a more general theorem that generalizes this result to deeper networks. <sep> Evaluation <sep> The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of "overparameterization". After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator distribution (produced from a fixed set of latent variables z_i) and the empirical mean of the data. R3 argues that this is not sufficient to "understanding the training of GANs". At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)? While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance. And importantly, this paper is the first to study this question theoretically. <sep> I also read the paper in more details. I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent-ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting. I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance. See below for additional comments to be taken in consideration for the camera ready version. <sep> Some detailed comments <sep> Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it. The fact that this is standard in some other papers is not a reason to not be clear about it. <sep> Theorem 2.2: in the closed form of the objective when d is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression. The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning). This should also a bit more clear in the introduction -- it seems to me that the main novelty of the work is to look at the gradient-descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to d). <sep> p.6 equation (8): typo -- the −μdt term is redundant and should be removed as already included from ∇dh(d,θ). <sep> p.7 "numerical validations" paragraph: Please describe more clearly what is the meaning of "final MSE". Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (e.g. after a fixed number of iterations, or according to some stopping criterion?). Please clarify.
This work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions. Several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization. Of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved. Reviewers agreed that this is an interesting counter-example to the shape-bias hypothesis and improves our understanding of why stylization improves robustness. Given the carefully designed experiments investigating an important topic I recommend accept.
The paper provides a new distance preserving embedding based on a recent result called sigma-delta quantization. The authors notice that in many realistic scenarios, the input vectors are well-spread and under assumptions regarding the spreadness provide a fast technique to convert the input vectors into binary vectors, possibly of lower dimension. For completeness, the authors analyze the setting where the vectors are not spread and show that by using a randomized Walsh-Hadamard transform, their results still apply. <sep> The authors do not provide a completely novel approach, to quote R2 "On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail". That being said, they show that a natural idea indeed works out by providing both a theoretical analysis and experimental results. The experiments can be more thorough but do convey the point that the result indeed works and moreover is somewhat robust in that it works well even when the formal requirements do not entirely hold. <sep> There are a few issues mentioned by the reviewers that should be addressed: A clearer exposition of the guarantees and assumptions, some comparison with previous papers. However given the responses and discussions these seem minor and fixable towards a camera ready version. I recommend accepting the paper
This paper presents a model for video action recognition. The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling. After reading the authors' responses, the reviewers converged on an accept rating. The solid empirical results and analysis, the fact that is is a plug-in method that could be used in other models, and the clear exposition were deemed to be positives. As such, this paper is accepted to *CONF* 2021.
This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to *CONF* 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes. It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/).
This paper addresses the problem of visual object navigation by defining a novel visual transformer architecture, where an encoder consisting of a pretrained object detector extracts objects (i.e. their visual features, position, semantic label, confidence) that will serve as keys in an attention-based retrieval mechanism, and a decoder computes global visual features and positional descriptors as a coarse feature map. The visual transformer is first pretrained (using imitation learning) on simple tasks consisting in moving the state-less agent / camera towards the target object. Then an RL agent is defined by adding an LSTM to the VTNet and training it end-to-end on the single-room subset of the AI2-Thor environment where it achieves state-of-the-art performance. <sep> After rebuttal, all four reviewers converged on a score of 6. The reviewers praised the novelty of the method, extensive evaluation with ablation studies, and the SOTA results. Main points of criticism were about clarity of writing and some explanations (which the authors improved), using DETR vs. Faster R-CNN, and the relative simplicity of the task (single room and discrete action space). There were also minor questions, a request for more recent transformer-based VLN bibliography, and a request for a new evaluation on RoboThor. One area of discussion -- where I empathise with the authors -- was regarding the difficulty of pure RL training of transformer-based agents and the necessity to pre-train the representations. <sep> Taking all this into account, I suggest this paper gets accepted.
This paper proposes an lightweight method for cross-domain few-shot learning, using a meta-learning approach to predict batch normalization statistics. <sep> After the extensive paper revisions and discussion, the reviewers all agreed that this paper is above the bar for acceptance, assuming that the authors will include results for both the standard and expanded target set size in the final version of the paper. The authors are strongly encouraged to include these results in the camera-ready version of the paper.
This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs. The reviewers were divided in their evaluation. On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting. On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked. Furthermore, its applicability is unclear in ML, since they aren't applicable to the usual nonlinear RNNs. However, given that the theoretical contributions are clear, the final decision was to accept.
There was some positive consensus towards this paper, which slightly improved after the very strong author rebuttal. Reviewers, in general, appreciate the simplicity of the approach as well as its effectiveness. The most acute criticisms derived from several theoretical and technical points, similarity with [Mizadeh, 2020], and missing baseline comparisons. The author rebuttal responds to each of these points very clearly and convincingly, as well as with new experimental baseline comparisons that clearly demonstrate the effectiveness of the CPR approach. I encourage the authors to include the extensive comparison with [Mizadeh, 2020] provided in the rebuttal, especially given the similarity to the proposed approach. and to also tone down the strong claims of novelty in light of the similarities.
Most reviewers found the method proposed to be technically sound, well-motivated and particularly interesting due to the interpretability of its results. Indeed, the extraction of interpretable motifs from NAS is a valuable contribution. One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients. We thank both the reviewer and the authors for the detailed discussion on these points. Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns.
This paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting. The authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects. The method is interesting and looks effective, although this assumption may not hold always true (e.g., in some domains, some causal influences may disappear, leading to extra conditional independence relations). Hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work.
The reviewers agreed that the paper presents interesting ideas but the presentation of the paper needs be improved. Also, the experiments and the related work section need be improved.
This paper deals with a problem of feature compatible learning, where the features produced by new model should be compatible with old features. As pointed out by the reviewers, there are several weaknesses with this paper: (a) the novelty is not strong enough, (b) the experimental results should be better explained and be more thorough, (c) the formulation is not well motivated.
The authors consider view-consistency when learning graph neural networks. However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing. More deep discussions about related papers and analytic experiments are required to support this work. Additionally, I have concerns about the scalability of the method --- whether it can deal with more than two views and how it will perform are not studied in this work. I tend to reject it based on its current status.
The paper presents a new online convex optimization algorithm that uses per-coordinate learning rates. The learning rates are changed over time using information coming from the gradients. A regret upper bound is proved and the algorithm is empirically validated on deep learning experiments. <sep> While the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version AdaGrad with diagonal matrices. Also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than AdaGrad. Overall, the theoretical contribution appears to be not sufficient. <sep> On the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms. <sep> More generally, this paper suffers from the same problem of many other similar papers: There is a complete disconnect from the theory proven under restrictive assumptions (convexity, bounded domains, no stochasticity) and the experiments (non-convex functions, no projection on bounded domain, stochastic setting). Unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field. In this view, I strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds. Such analysis would also suggest better ways to design new optimization algorithms better suited to non-convex problems.
I thank the authors and reviewers for the lively discussions. Although reviewers mentioned the work has potentials to improve adversarial robustness, they agreed that the current draft needs a bit more work specially to strengthen its experimental results and comparisons with related works.
This paper proposes a contribution aiming at understanding the cause of errors in few-shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. <sep> Hence, I propose rejection.
All reviewers agree that the current approach is very similar to traditional uncertainty-based active learning, and that the empirical results are inconclusive, so at this point the paper is not ready for publication.
The authors empirically analyse the properties of datasets which lead to poor calibration. In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per-class calibration. While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for *CONF*. To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship -- is there a tradeoff? For the latter, the reviewers pointed to a concrete extension with structured label noise. Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. Therefore, I will recommend rejection.
During the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns: <sep> Lack of theoretical or empirical justification/proof for the key statement: "the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima". <sep> Lack of comparisons with newer methods from e.g. ECCV2020 etc. <sep> In particular, the first point is crucial. As the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically and/or empirically. However, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing. <sep> About the second point, although I think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness. <sep> Overall, the contribution of this paper it not significant enough for publication. Hence I will reject the paper.
Since the authors have decided to withdraw this submission, it has been rejected from the conference.
Although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews. <sep> During the discussion, reviewers agree with that this submission is not ready for publication. In particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing. <sep> I will therefore reject the paper. <sep> For future submission, I strongly recommend the authors to do author response. There are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.
This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea. However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets. Doing one of these would have substantially strengthened the paper. Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state.
The authors propose a new dataset and compositional task based on the EPIC Kitchens dataset. The goal is to test novel compositions and to build a transformer based network specifically for this inference (by analogy). Specifically, the analogy here references the use of nearest neighbors in the dataset. There are a lot of concerns raised by reviewers which require a large number of changes to the presentation of the manuscript and they are not at present convinced by the current setup or experiments. Explicitly motivating which pretraining methods do or do not violate which aspects of composition and what role other factors like synonymy play in generalization is necessary. Several aspects of the claims made in the paper and in the discussion are big claims that require substantial discussion and analysis (e.g. the surprising weakness of pretrained models) which the reviewers do not feel can be so easily explained away (e.g. by domain shift).
This paper proposes an anonymization method for federated learning based on the Indian buffet process. The reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details): <sep> Motivation and terminology needs clarification <sep> Better comparison with secure aggregation methods <sep> Missing privacy guarantees <sep> Overall the reviewers of this paper are borderline. I hope the authors will take the reviewers' feedback into account when revising the paper.
While this paper was perceived as being fairly well written, the level of novelty and the evaluation were seen as weak by many reviewers. The aggregate opinions across reviewers is just too low to warrant an acceptance rating by the AC. The AC recommends rejection.
This paper proposed an unsupervised domain adaptation method for 3D lidar-based object detection. Four reviewers provided detailed reviews: 3 rated "Marginally above acceptance threshold", and 1 rated "Ok but not good enough - rejection". The reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper. However, major concerns are also raised regarding the core technical contributions on the proposed approach. The ACs look at the paper, the review, the rebuttal, and the discussion. Given the concerns on the core technical contributions, the high competitiveness of the *CONF* field, and the lack of enthusiastic endorsements from reviewers, the ACs believe this work is not ready to be accepted to *CONF* yet and hence a rejection decision is recommended.
This paper proposes a hybrid algorithm that combines RL and population-based search. The work is interesting and well-written. But, the contribution of the work is very limited, in comparison with the state-of-the-art.
This paper analyzes dropout and shows it selectively regularizes against learning higher-order interactions. The paper received mixed reviews, with two in favor of rejection and one in favor of acceptance. Specifically, while all reviewers find the intuitions and ideas in the paper adequate/plausible, two reviewers didn't find sufficient evident that supports the conclusions. The reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published.
This submission generated a lot of discussion. <sep> The main strengths of the paper: <sep> It is an interesting application of meta-learning (to 3D shape completion), and a novel one. <sep> It appears to work well: it is remarkable that the proposed model can reconstruct shapes as well as it does given a point cloud with only 50 input points. <sep> The main concern (raised by two reviewers) is that while the method is described using the language of meta-learning, the proposed architecture ends up looking very similar to a variational autoencoder: a point cloud encoder that outputs some distribution in a latent space, which is then sampled from to produce a code which drives an implicit surface decoder. The only difference appears to be that the proposed method uses a factored distribution in latent space, whereas a traditional VAE uses a non-factored one (i.e. a single multivariate Gaussian over all dimensions of the latent code). <sep> One reviewer engaged the authors in a discussion about this point, but the resulting conversation was not satisfactory. One interpretation of the authors' response is that they are simply not aware of how a VAE could be trained using variable-sized point clouds as input (which is quite possible using many standard point cloud processing networks). However, at other points, they do seem to grasp this, when they write that "This flexible representation of the uncertainty [i.e. the one proposed by the authors] cannot be attained by the mere average or max pooling aggregation [what a PointNet encoder would do]." They even go on to provide an additional ablation study where they replace their factored probabilistic encoder with a deterministic mean/max pool encoder, and show worse results. Unfortunately, they never compare against probabilistic variants of such encoders (i.e. where the mean/max pool output is then used to compute a mean and variance). <sep> Without seeing this comparison, the reviewers believe that this paper cannot be accepted, and I am inclined to agree. <sep> On a related note: one reviewer pointed out an issue with unfair comparisons, in that baselines were trained on high density point clouds and evaluated on low density ones. The reviewer noted that these methods could be (and should have been) trained on point clouds of varying density. Perhaps this relates to my hypothesis that the authors initially did not understand that training such encoders on variable-sized point clouds was possible. In any event, in their rebuttals, they have reported some preliminary results from experiments which do this type of training, but these results are not conclusive. Complete, conclusive results from these experiments would also need to be presented before this paper could be accepted.
The paper proposes a new adaptive optimization algorithm which is claimed to have better convergence properties and lower susceptibility to gradient variance. Reviewers found the idea of normalizing on the fly to be interesting, but raised some important concerns. Although similar to AdaGrad, Expectigrad has a very important differentiation due to division by nt. Assuming β=0 in my opinion is also ok and many papers assume this for analysis. Even after accounting for these two facts, during discussions the reviewers considered the work to be incremental and a more thorough evaluation is needed to determine the benefits of algorithm. Specifically, please compare to important and relevant baselines (like AdamNC and Yogi), because sometimes it felt like baselines were picked and dropped randomly. The empirical improvement provided by Expectigrad compared to SOTA is not clear (both on synthetic problems from Reddi et al and real problems). Thus, unfortunately, I cannot recommend an acceptance of the paper in the current form. However, I would strongly encourage authors to resubmit after improving according to reviewer suggestions. <sep> Some other minor points that came up during discussion are: <sep> choice of hyperparameters was not clear to reviewers, e.g. different optimizer may behave very differently for same set of hyperparameters, so it would not be fair to compare them as is. <sep> gradients would never be exactly zero in deep networks, so is current definition of nt good enough?
The paper introduces a new variant (SREDA-Boost) of a variance-reduced method SEDRA for nonconvex-strongly-concave min-max optimization. Given that SEDRA is already optimal in the worst case, the proposed modification is intended to improve practical performance of the method, by relaxing conditions needed at initialization and allowing larger step sizes. While the reviewers appreciated the main ideas of the paper, they shared concerns about the significance of the paper's technical contributions, which were ultimately not addressed by the authors in the rebuttal phase.
The reviewers raised a number of concerns, but <sep> the authors provided no rebuttal to the reviewers' comments. <sep> One reviewer felt the experimental fitting was not thorough enough. <sep> Suppose one used layers of oriented bandpass filters, separated by <sep> non-linearities, would that perform well on the task convnets are <sep> trained on? <sep> The AC doesn't agree with the arguments of R3. I hope the comments of <sep> the reviewers, particularly the many specific comments of reviewers R1 and <sep> R2, will be helpful to you as you revise the manuscript. <sep> The AC feels a more thorough experimental evaluation, and following-up <sep> on many of the suggestions of the reviewers will lead to a strong <sep> paper. As it stands, however, with 3 recommendations for rejection (1 <sep> weak), and only 1 weak recommendation for acceptance, we need to reject.
Overall the reviewers had various positive things to say about the paper, including that it was well written and easy to understand, topical, that the method was sensible, novel and interesting and that the computational efficiency (i.e. real time) was appealing. However, all the reviewers thought it wasn't quite ready for acceptance, mainly citing concerns with the empirical evaluation. It seems they had trouble interpreting the empirical results and placing the work with respect to other relevant methods. <sep> It seems in the author response, the authors did much to add to the experiments, but ultimately the reviewers were not comfortable with acceptance. Taking the reviewers' feedback into account and adding the desired empirical evaluation would make this a much stronger submission to a subsequent conference.
The reviewers overall appreciated the efforts of the authors in making NAS more computationally efficient. The paper could greatly benefit from further editing/restructuring with the goal of improving clarity, as it's currently hard to navigate and understand in places. Future submissions of this work would benefit from more extensive empirical validation that motif networks mimic the original network. The reviewers also agreed that for the method to be appealing/useful, a general way to generate motif networks is needed. Overall, the outcome was that this is a very interesting idea but needs further development along the directions outlined above.
This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long-range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN). The experimental results show incorporating topic information can further improve the performance of Transformers. While this is an interesting idea, reviewers pointed out some weaknesses: <sep> GLUE evaluation is not a test of long-term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task. <sep> The improvement over the baseline does not seem to be significant. <sep> The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer. <sep> A comparison of the model performance for different lengths of input sequences would be helpful. <sep> There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic-based method. <sep> Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.
This work proposes a fully-explored masking strategy, segmenting the input text, which maximizes the Hamming distance between any two sampled masks on a fixed text sequence. The hope is to reduce the large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM lead to undesirably large gradient variance, which typically hurts training efficiency with stochastic gradient optimization algorithms. <sep> Pro <sep> A clear and interesting, novel leading theoretical idea. The paper has one good theme that it pursues. <sep> A mostly well-written paper <sep> Contains good theoretical discussion <sep> Experiments support the idea <sep> Con <sep> The experiments could be better, especially they don't actually measure a reduction in gradient variance only accuracy <sep> The proofs are at best loose <sep> Alternative methods of reducing variance like using large mini batches are not considered <sep> The results might go away with use of stronger (larger) contextual LMs <sep> There isn't good comparison to other methods of masking like span masking and salient term masking <sep> Some of the things included seem quite haphazard (the ablations don't seem to the point, it's not really clear what this has to do with continual learning) <sep> Overall, this paper feels to be in a premature state. The idea is interesting, but the idea and the paper needs to be developed more, with stronger results. I think it doesn't deserve to be accepted at this time.
This paper studies the following broad question: How can we predict model performance when the data comes from different sources? The reviewers agreed that the direction studied is very interesting. While the results presented in this work are promising, several reviewers pointed out some weaknesses in the paper, including a confusion between absolute loss and excess loss, and the limited scope of the experiments. Overall, this paper does not appear to be ready for publication in its current form. In my personal opinion, if the concerns raised by the reviewers are appropriately addressed, this work could be publishable in a high quality venue.
In this paper, the authors propose a theoretically principled neural network that inherently resists ℓ∞ perturbations without the help of adversarial training. Although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis.
The reviewers are in consensus that this paper is not ready for publication: cited concerns include simple (interesting) ideas but need to be carefully analyzed empirically, contextualized (other similar studies exist), identifying convincing empirical evidences,. etc. <sep> The AC recommends Reject.
Taking all reviews and the work in consideration, unfortunately the work does not present the breadth it needs to sustain the claims it makes. In particular, there work requires to analyse more architectures/variations of datasets with different properties and to provide more careful ablation studies that shows the efficiency of the 3 different proposed methods. Potentially removing one of this methods in order to give more space to analyse the others that seem more promising.
The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold. The paper proposes a method using a gradient that is unbiased and consistent. <sep> Pros: <sep> Problem setting is new and this paper is one of the first works exploring it. <sep> The procedure comes with some unbiasedness and consistency guarantees. <sep> Experimental results on a wide variety of datasets and domains. <sep> Cons: <sep> Novelty and technical contribution is limited. <sep> Motivation of the problem setting was found to be unclear. <sep> Some gaps in the experimental section (i.e. needing the use of synthetic data or synthetic modifications of the real data). <sep> Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one-sided regression problem as important or relevant in practice, which was a key reason for rejection. The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation.
This paper mostly received negative scores. A few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting. However, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing (both in terms of execution as well as exploration of the results). The authors did not provide a response. Therefore, I recommend reject.
This paper proposed to theoretically explain why a pre-trained embedding network with self-supervised training (SSL) can provide representation for downstream few-shot learning (FSL) tasks. The review process finds that the paper may over-claim the results and that the results seem unsatisfactory. Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. The paper needs a substantial revision to improve clarity and accessibility. As pointed out by Nikunj Saunshi's public comment, this paper may benefit from discussing the differences from the previous works, including [1]. <sep> [1] Arora et al., A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019
The reviewers are split. Two reviewers consider the technical contribution of the paper to be insufficient, and raise concerns about comparisons with Transformers or using more standard benchmarks for GNN experiments. The other considers the experiments convincing and the method worth publishing. My own view is that this work is not ready for inclusion in the conference. In particular, I think this paper would be much stronger with either: <sep> 1: a more practical task to illustrate where this method might be applied in earnest, <sep> 2: more analysis and baselines on the synthetic data. Synthetic data can be enough for a new method if it illuminates the functioning and the benefits and drawbacks. In this paper, we have synthetic data with little analysis, and imo (concurring with R5) insufficient baselines. For example, while a vanilla Transformer probably could not do the matrix problems (with the matrices encoded naively), one might expect Transformers with sparse attention to do quite well on e.g. transpose and 90 degree rotation, especially given the training curriculum and proper positional embeddings; a convolutional network seems like a strawman. I also agree with R5 that standard benchmarks for GNN exist, and these might be appropriate (or at least there should be some discussion of why they are not). <sep> 3: some theoretical discussion of what the proposed model can do that other methods fundamentally cannot. <sep> I do think this is interesting work, and encourage the authors to revise and resubmit.
The paper presents a DKL variant with a linear kernel. Representations from several networks is combined through concatenation, making it not quite an ensemble. It's shown that the model is a universal kernel approximator. Experiments are conducted on a large number of UCI datasets. <sep> Following the discussions, the paper still has the following shortcomings: <sep> some lack of clarity in the presentation (for instance, explaining the equivalence between a multi-output learner and M different single-output learners) <sep> lack of experiments on data where deep learning is typically used (images); the UCI datasets have structured data and other ensembles like XGBoost may outperform the baselines presented in this paper <sep> difference in performance between DKL and DEKL, especially since DKL benefits from a larger model space, theoretically. maybe DEKL has better sample complexity, but does this advantage hold in the case of the large datasets that deep learning is used for?
Sequence generation models trained via maximum likelihood estimation (or variants of so called 'teacher-forcing') condition on data samples during training and on model samples for predictions. The susceptibility to this potential "mismatch" in input distribution is often referred to as exposure bias (EB). <sep> This paper stresses that most research around EB is focused on addressing it, rather than defining and/or quantifying it. Thus the submission questions the severity of EB and attempts to operationalise a testable definition for it. Myself and all the reviewers strongly support the observations and the agenda, we find the question this paper asks an important one. <sep> Despite our appreciation for this paper's relevance, we have identified a number of problems that prevent me from recommending this paper. I will comment on the two most important points: <sep> The 'operational definition' of EB in this paper is not sufficiently precise to be testable. It builds on the somewhat commonly accepted view that the effects of EB accumulate as the conditioning context grows longer, and that this causes a model to generate badly distorted sentences. This definition still leaves quite some room for interpretation (without specifying reasonable expectation about how these effects 'accumulate' and what/how bad they are, it seems difficult to design tests). We acknowledge that the submission attempts to shed light onto some of these aspects by having some 'control groups' using gold data and shuffled strings, but we did not find those sufficient (mostly in light of the next point). <sep> MT evaluation metrics (essentially, string similarity metrics), most notably (but not exclusively) BLEU, are used in this work in a setting where we cannot easily grant that they have the discriminating power that the authors expect of them. See this is not a criticism about the imperfections of BLEU (or any other automatic metric), but about the lack of evidence supporting its use against unrelated sentences. We do not find it sufficient that some recent NLG papers have made similar use of it (I, for example, would have criticised those papers on similar grounds). <sep> Overall, we believe this submission asks a relevant question, the insight about dependence on prefix is nice and might lead to a first operational definition of EB (which might be only a few refinements away from the version proposed here). The current evaluation is unconvincing and I believe the authors should be able to find more credible strategies, especially, strategies that have already gone through some scrutiny (for example, in literature around OOD detection and tests for distribution shift). <sep> Though I do not recommend this paper for acceptance, I hope the authors will find valuable feedback in the expert reviews attached.
The paper solves a PDE using an additional penalty function between the derivatives of the function. On toy examples and two PDEs it is shown that these additional terms help. <sep> Pros: - The motivation is to include derivatives in the computationa <sep> - Implementation and testing on several examples, including high-dimensional ones <sep> - Timing is included in the latest version <sep> Cons: -The loss is Sobolev norm of the residuals of the equation. <sep> - The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation. For example, for the Poisson equation, the problem is selected in such a way the solution is analytic. However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities. Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases). The L2-norm then is not the right functional for the solution to exist, not to say about the higher-order derivatives. So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution. <sep> - Convergence. There are quite a few papers on the convergence of DNN approximations to solution of PDEs. The presented methods might have converged to a local minimum. An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks. Neural Networks. 2017 Oct 1;94:103-14.
The reviewers agree that the EM perspective of Federated Learning is novel and interesting. However, a common criticism is that the connection made is rather shallow and not sufficiently developed. There look to be quite interesting potentials of the proposed framework and the specific FedSparse method, but I agree with the reviewers that both aspects need further development before they are in publishable form.
All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity. While the authors' responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance.
This paper studies the effect of the discount mismatch in actor-critics: the discount used for evaluation (often 1), the discount used for the critic and the discount used for the actor. There's notably a representation learning argument supported by a series of experiments. <sep> The initial reviews pointed out that this paper addresses very relevant research questions, sometimes in a quite original way, with a large set of experiments. However, they also raised concerns about the organization/clarity of the paper, and possible weaknesses about the experimental studies. <sep> The authors provided a rebuttal and a revision, that clarified some points and triggered additional discussions. However, if the revision improved the initial submission, the shared assessment is that the clarity and experiments themselves are still somewhat lacking. As such, the AC cannot recommend accepting this paper. <sep> Yet, this work does have interesting ideas, and the problem considered is of interest for the community and under studied. The authors are strongly encouraged to submit a revised version to a future venue.
This paper proposes a self-supervised learning method for learning representations for graph-structured data, with both local and global objectives. The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al. 19], with the InfoNCE loss [van den Oord et al. 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al. 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters. The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre-trained with it, and the results show that it largely outperforms existing graph pre-training methods. <sep> This paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept. The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self-supervised learning at both local and global level makes sense. However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre-training). The reviewers had interactive discussions with the authors, and the authors provided detailed feedback. Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings. <sep> I believe that this is a simple yet effective pre-training method for GNNs on graph-structured data. The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well-captures the graph-level similarity and also is well-separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets. However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches. The local objective is a slight modification of attribute masking strategy of [Hu et al. 19], and the global objective of clustering has been explored in self-supervised learning of CNNs for image data [Asano et al. 20]. Thus, I lean toward rejecting the paper, considering its relative novelty and quality. <sep> However, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing. I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods. The authors may consider various techniques for both local and global objectives (such as hinge loss-based contrastive loss with k-means clustering as shown in the response to R3), and suggest the proposed work as a more general framework. <sep> [Asano et al. 20] Self-Labeling via Simultaneous Clustering and Representation Learning, *CONF* 2020
This paper proposes GAN-training of a non-autoregressive generator for text. To circumvent the usual problems with non-differentiability of text GANs, the authors turn to Gumbel-Softmax parameterisation and straight-through estimation. <sep> There are a number of aspects to this submission and they are not always clearly positioned. I will concentrate on the two aspects that seem most crucial: <sep> The authors position their generator as an implicit generator, but it really isn't. If we take the continuous interpretation of the output distributions: the Gumbel-Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter. If we take the discrete interpretation of the output distribution: Gumbel-argmax is just an alternative to sampling from a Categorical distribution with known parameter. In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function. The authors do, however, train the architecture using a GAN-type objective as if the generator were implicit. <sep> In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator. Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs. In their rebuttal, the authors commented on the use of non-autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT. The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non-autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen). <sep> Other problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that. For example, ablating the non-autoregressive generator and comparing to REINFORCE. I believe these improved the submission. <sep> Still, I cannot recommend this version for publication. I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN-like objective despite the model not, strictly speaking, requiring it).
The paper deals with cross-domain few-shot learning in the case of large source-target domain shifts. <sep> The paper received mostly below-threshold reviews, with one exception (R3) whose review is addressing more general aspects, but still with some concern, especially in relation to the experimental part (to which authors did not answer). R1's review is not of much help. <sep> Clarity of the presentation and missing details seem to be recurrent issues all over the reviewers, together with remarks concerning the experimental validation, which would have required a deep revision and improvement, in particular regarding the use of more backbones, better ablation (Hebbian learner contribution, unclear initialization), processing times/computational complexity, significant comparative analysis re robust baselines. <sep> The rebuttal clarifies some of the raised remarks but there are still issues, especially regarding Hebbian learning rule and ensemble learning strategies, and about results too, so not all reviewers were convinced to raise their ratings. <sep> Overall, given the above issues, I consider the paper not yet ready for publication in *CONF* 2021.
The paper proposes to recalibrate predictive models by fitting a <sep> normalizing flow on top of the predictive model on a held out validation <sep> set using side information. At a high level this idea has some potential, <sep> especially in the multivariate setting, but there are several directions for <sep> improvement: <sep> Comparison with a broader set of baselines as suggested by the reviewers <sep> Clarity on why recalibrate with a normalizing flow especially in the 1-d case <sep> Why not any other model with explicit density? Are there other important desiderata? <sep> A motivating experiment that makes the potential value clear
All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission.
The paper proposes a variant of recurrent neural networks based on Long Short-Term Memory. Unlike the standard LSTM, the proposed mass-conserving LSTM subtracts the output hidden state of the LSTM from its current cell state, thus preserving the "mass" stored in the cell states at each step. A left-stochastic recurrent weight matrix is also used to conserve the "mass" across the time steps. Empirical experiments demonstrated the effectiveness of the proposed MC-LSTM on a range of datasets such as addition & arithmetic tasks, traffic forecast, and rainfall modeling models. <sep> Several issues were clarified during the rebuttal period in a way that satisfied the reviewers. However, some concerns still remain unanswered: <sep> This is an empirical paper that proposes a modified LSTM that brings forward a few different ideas: L1 norm, stochastic transition matrices, and subtracting the output hidden states. An ablation study is a MUST in such an applied work. It has been pointed out by other reviewers that there are many prior references on LSTMs variants. It would greatly strengthen the paper by considering more diverse baselines. There is no experiment nor discussion on how much each modification helps wrt the final accuracy. Thus it remains unclear how the results can generalize to other problems. <sep> Although the results seem convincing across various datasets that mass conservation seems to help, the datasets are non-standard benchmarks in the machine learning conferences thus there is a lack of competitive prior baselines. As the proposed LSTM has a different number of parameters compared to the standard LSTM, is it fair to compare the different architectures under the same number of neurons? What happens if we compare the architectures with the same number of parameters? And how well does the model scale as we vary the hidden size? It would be helpful to keep the contributions into perspective by using standard RNN benchmark datasets such as Penn TreeBank or Wiki-8. <sep> Overall, the basic idea seems interesting, but the lack of ablation studies significantly hurt the contribution and the positioning of the paper. Given the current submission, the paper needs further development, and non-trivial modifications, to be broadly appreciated by the machine learning community.
The paper proposes a method for inference in models with GP priors and neural network likelihoods for multi-output modelling, dealing with the problem of scalability and missing data. The paper builds upon previous work on inducing variables for scalability on GP models and inference networks for amortization (reducing the number of parameters to estimate) and dealing with missing data. <sep> There are several concerns about the paper in terms of generality/flexibility of the approach, as the proposed model shares the NN parameters across tasks and the results on the small datasets do not show improvements wrt baseline such as GPAR. The authors' comments provide somewhat satisfactory replies to these issues. Nonetheless, the major drawback of this paper is its novelty as the ideas on the paper have been explored extensively in the GP literature. Although the authors do make a case for scalability when using inference networks, there are other previous works that perhaps the authors are unaware of, for example, https://arxiv.org/abs/1905.10969 and even more sophisticated inference algorithms than can serve as truly state-of-the-art competing approaches (for example based on stochastic gradient Hamiltonian Monte Carlo, https://arxiv.org/abs/1806.05490).
The paper compares transfer learning with fine-tuning and joint training and then proposes a new approach (Merlin). Reviewers have pointed to the fact that Merlin works in a setting that is different from normal transfer learning settings (it assumes some target domain data is available during training). The authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult. Overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings. I therefore recommend to reject the paper.
This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. This would the hopefully leads to escaping sharp valleys and better generalization. Authors further provide some related theoretical results and several experiments to show effectiveness of their models. <sep> All reviewers find the proposed method well-motivated, novel and interesting. The paper is well-written and easy to follow. However, both theoretical results and empirical evaluations could be improved significantly: <sep> 1- The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. See for eg. R1's comments about this. <sep> 2- Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. In particular, there are two main areas to improve: <sep> a) Based on the Appendix D, the choice of hyper-parameters seem to be made in an arbitrary way and all models are forced to use the same hyper-parameters. This way, the choice of hyper-parameters could potentially favor one method over the other. A more principled approach is to tune hyper-parameters separately for each method. <sep> b) It looks like the choice of #epochs has been made in an arbitrary way. For all experiments, it would be much more informative to have a figure similar to the left panel of Fig. 4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not. <sep> c) Based on the current results, SALR's performance is on par with that of Entropy-SGD on CIFAR-100 and WP and there is a very small gap between them on CIFAR-10 and PTB. I highly recommend adding ImageNet results to make the empirical section stronger. The other option is to compare against other methods in fine-tuning tasks. That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine-tuning tasks. <sep> Given the above issues, my final recommendation is to reject the paper. I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. I hope authors would address the above issues as well and resubmit their work.
This paper investigates methods for gradient-based tuning of optimization hyperparameters. This is an interesting area, and the paper isn't bad. The examination of hypervariance seems relatively novel and useful. I also appreciate the point about Bayesopt sometimes working well simply due to small ranges. <sep> However, I agree with the criticisms of the reviewers. Overall this paper isn't quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable. <sep> Some minor comments: <sep> The signed-based optimizer, while simple and sensible (which is good), seem kind of ad hoc. <sep> The authors don't seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically. It's not clear that for regularization parameters that this problem exists or that your method would apply. <sep> A small nit: Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings? Perhaps some sort of sensitivity analysis would be more appropriate. Also you should reference Barack Pearlmutter's thesis which first explores these issues. I would also mention that the hypervariance is generally tiny for smaller-than-optimal learning rates, and massive for larger-than-optimal learning rates, (the chaotic regime).
This paper introduces modifications that allow to make the training of contrastive-learning-based models practical. The goal of the paper is very interesting, and the motivation clear. This paper tackles a very important issue with recent unsupervised feature learning methods. <sep> However, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work. As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude. In its current form, this paper unfortunately doesn't meet the bar of acceptance. <sep> Given the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue.
We have a very well informed reviewer who strongly feels that this paper is insufficiently novel and significant further discussion on how the paper might be raised to a publishable level with more empirical results. I will have to side with the more engaged reviewers who feel that the paper should be rejected.
While the paper contains some interesting ideas, the reviewers felt that overall the paper is not theoretical well supported, and likewise the experiments are not fully convincing. Even after the rebuttal, these concerns still persist.
This paper proposes a method to quantify transference, which is a measure of information transfer across tasks, for multi-task learning framework. Specifically, the transference is measured as the change in the loss for a specific task after performing a gradient update for another. The proposed transference measure is used to both understand the optimization dynamics of MTL and improve the MTL performance, either by grouping tasks or combining task gradients based on the transference. The method is validated on multiple datasets and is shown to bring in some performance gains over the base MTL model (PCGrad, UW-MTL). <sep> The majority of the reviewers were negative about this paper (4, 4, 5), while one reviewer gave it a positive rating (6). The reviewers in general agreed that the idea of measuring transference as the change in the loss with gradient updates is novel and intuitive. Yet, the reviewers had common concerns on the 1) weak performance improvements, and the 2) high-cost of computing the transference. While computing the transference requires additional computations with linear time complexity, which may be problematic with a large number of tasks, the performance gains using it were rather marginal (less than 0.5% over the baselines). Another common concern from the reviewers was its insufficient experimental validation, as a comparative study against existing works that perform task grouping is missing. Both the authors and reviewers actively participated in the interactive discussion. However, the reviewers found that the two critical limitations persist even after the authors' feedback, and in a subsequent internal discussion, they reached a consensus that the paper is not yet ready for publication. <sep> Thus, although the proposed method is novel and appears to be promising, it may need more developments to make it both more effective and efficient. Moreover, there should be more in-depth analysis of its time-efficiency, and other benefits (e.g. interpretability) that could be achieved with the proposed transference measure. Finally, while there exist many works on learning both hard or soft task grouping, the authors do not reference or compare against them. To name a few, [Kang et al. 11] propose how to learn the discrete task groupings, [Kumar and Daume III 12] propose to learn a soft grouping between tasks, [Lee et al. 16] propose to learn soft grouping based on asymmetric knowledge transfer direction across the tasks, and [Lee et al. 18] proposes the extension of [Lee et al. 16] to a deep learning framework. I suggest the authors to discuss and compare against the above mentioned works, and fortify the related work section by searching for more classical works on multi-task learning. <sep> [Kang et al. 11] Learning with Whom to Share in Multi-task Feature Learning, ICML 2011 <sep> [Kumar and Daume III 12] Learning Task Grouping and Overlap in Multi-task Learning, ICML 2012 <sep> [Lee et al. 16] Asymmetric Multi-task Learning based on Task Relatedness and Confidence, ICML 2016 <sep> [Lee et al. 18] Deep Asymmetric Multi-task Feature Learning, ICML 2018.
In this paper, the authors proposed a geometric graph generator that applies a WGAN model for efficient geometric interpretation. All the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks. Unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline. More analytic experiments should be designed to verify the properties of the proposed GG-GAN, especially its scalability. Although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more real-world large graphs (even if the performance may not be good, the results will be constructive for both readers and authors to understand the work).
The consensus view was that the reviewers were not convinced that the analysis done in the paper was sufficient motivated.
This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer. The authors compare against several other methods and find that their method performs well without needing to train from scratch. The reviewers thought this paper was well written and the authors were very responsive during the review period. However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing. We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (e.g. specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases.
This paper conducts a comparison between a small set of models (4 in total) for unsupervised learning. Specifically, the authors focus on comparing Bayesian Confidence Propagating Neural Networks (BCPNN), Restricted Boltzmann Machines (RBM), a recent model by Krotov & Hopfield (2019) (KH), and auto-encoders (AE). The authors compare trained weight distributions, receptive field structures, and linear classification on MNIST using the learned representations. The first two comparisons are essentially qualitative comparisons, while on classification accuracy, the authors report similar accuracy levels across the models. <sep> This paper received mixed reviews. Reviewers 4 and 5 felt it did not contribute enough for acceptance, while Reviewers 2 & 3 were more positive. However, as noted by a few of the reviewers, this paper does not appear to achieve much, and provides very limited analysis and experiments on the models. It isn't introducing any new models, nor does it make any clear distinctions between the models examined that would help the field to decide which directions to pursue. The experiments add little insight into the differences between the models that could be used to inform new work. Thus, the contribution provided here is very limited. <sep> Moreover, the motivations in this paper are confused. In general, it is important for researchers at the intersection of neuroscience and machine learning to decide what their goal is when building and or comparing models. Specifically, is the goal: (1) finding a model that may potentially explain how the brain works, or (2) finding better machine learning tools? <sep> If the goal is (1), the performance on benchmarks is less important. However, clear links to experimental data, such that experimental predictions may be possible, are very important. That's not to say that a model must be perfectly biologically realistic to be worthwhile, but it must have sufficient grounding in biology to be informative for neuroscience. However, in this manuscript, as was noted by Reviewer 4, the links to biology are tenuous. The principal claim for biological relevance for all the models considered seems to be that the update rules are local. But, this is a loose connection at best. There are many more models of unsupervised learning with far more physiological relevance that are not considered here (see e.g. Olshausen & Field, 1996, Nature; Zylberberg et al. 2011, PLoS Computational Biology; George et al., 2020, bioRxiv: https://doi.org/10.1101/2020.09.09.290601). It is true that some of these models use non-local information, but given the emerging evidence that locality is not actually even a strict property in real synaptic plasticity (see e.g. Gerstner et al., 2018, Frontiers in Neural Circuits; Williams & Holtmaat, 2018, Neuron; Banerjee et al., 2020, Nature), an obsession with rules that only use pre- and post-synaptic activity is not even clearly a desiderata for neuroscience. <sep> If the goal is (2), then performance on benchmarks, and some comparison to the SotA, is absolutely critical. Yet, this paper does none of this. Indeed, the performance achieved with the four models considered here is, as noted by Reviewer 4, very poor. In contrast, there have been numerous advances in unsupervised (or "self-supervised") learning in ML in recent years (e.g. Contrastive Predictive Coding, SimCLR, Bootstrap Your Own Latent, etc.), all of which achieve far better results than the four models considered here. Thus, the models being compared here cannot inform machine learning, as they do not appear to provide any technical advances. Of course, some models may combine goals (1) & (2), e.g. seeking increased physiological relevance while also achieving decent benchmark performance (see e.g. Sacramento et al., 2018, NeurIPS), but that is not really the situation faced here, as the models considered have little biological plausibility (as noted above) and achieve poor performance at the same time. <sep> Altogether, given these considerations, although this paper received mixed reviews, it is clearly not appropriate for acceptance at *CONF* in the Area Chair's opinion.
This paper presents a secure aggregation method to ensure byzantine robustness. The reviewers thought that the idea was interesting, but had the following concerns. <sep> Relaxing the assumptions used in the theoretical analysis as much as possible <sep> Run more extensive experiments <sep> I encourage the authors to their feedback into account when preparing the revised draft.
This paper received 4 reviews with mixed initial ratings: 4, 8, 5, 7. The main concerns of R1 and R2, who gave unfavorable scores, included limited methodological novelty beyond the data generation and insufficient empirical evaluation of state-of-the-art methods on the proposed dataset. The authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews separately: it addressed some of the concerns, but did not change the overall position of the reviewers. <sep> AC agrees with R3 and R4 that the proposed dataset and the environment may have certain practical impact and enable new research in learning CAD reconstruction. However, the contributions are indeed specific to a narrow CAD community, and R1 felt that the paper needs another round of peer reviews before acceptance, as a significant number of new results have been added during the discussion stage. After discussion with PCs, the final recommendation is to reject.
The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at *CONF*.
This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining. Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network. <sep> The pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. The rates can in fact be transferred to more poorly performing network configurations and improve performance. <sep> The cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost-effect strategy to find these. At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model. <sep> The stronger, forward-looking implication is, instead, the connection to layerwise pruning rates. Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (e.g., [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates. Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (e.g., gradient flow, or capacity) that indicates the improved eventual performance. <sep> My Recommendation is to Reject. The paper's core experiments are well-executed. However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component. Once done, that will make for a very strong paper. <sep> [1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han. EECV, 2018
While reviewers find the ideas in the paper interesting, they also raise several major concerns. <sep> In particular, R1 and R4 find the claims of "invertible" and "lossless" to be potentially misleading. <sep> The bijective property is achieve on the first stage (L-1 layers) due to a sequence of one-to-one mappings, as is done in previous work (e.g. i-RevNet) so the novelty is limited. As stated by R3, since the paper is a combination of previous methods, the writing should be substantially improved to clarify what the real, new contributions are. The interpretation of the results (e.g. Figure 4) should also be better explained.
The paper studies a high-order discretization of the ODE corresponding to Nesterov's accelerated method, as introduced by Su-Boyd-Candes. The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster. However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach. Thus, the reviews could not recommend acceptance, and I concur. The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue.
There were both positive and negative assessments of this paper by the reviewers: It was deemed a well written paper that explores cleanly rederiving the TC-VAE in the Wasserstein Autoencoder Framework and that has experiments comparing to competing approaches. However, there are two strong concerns with this paper: First, novelty appears to be strongly limited as it appears a rederivation using known approaches. Second, two reviewers were not convinced by the experimental results and do not agree with the claim that the proposed approach is better than competing methods in providing disentangled representations. I agree with this concern, in particular as assessing unsupervised disentanglement models is known to be very hard and easily leads to non-informative results (see e.g. the paper cited by the authors from Locatello et al., 2019). Overall, I recommend rejecting this paper.
This paper proposes a new mechanism, called HIRE, to improve the down-stream performance of a pre-trained Transformer on NLP tasks. Different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating. The model is evaluated on GLUE, a benchmark for natural language understanding. My major concerns are the following <sep> the gating mechanism on using intermediate sentence representation is not new, as pointed by some reviewers, although its implementation on transformers is still interesting. <sep> the empirical part is not convincing enough: a) GLUE data set is relatively simple, the authors should try something more complex, b）the improvement over baseline is rather modest, which could be achieved with simpler modification. <sep> I'd suggest to reject this paper.
The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.
This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments, comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re-submission.
The paper investigates the average stability of kernel minimal norm interpolating predictors. The main result <sep> establishes an upper bound on a particular notion of average stability for which it is well-known that it <sep> can be used to bound the generalization error. This upper bound holds for all interpolating predictors <sep> from the RKHS, but it is minimized by the minimal norm predictor. <sep> While at first glance this result looks highly interesting, a closer look reveals that the significance of the results <sep> crucially depends on the quality of the derived upper bound. Here two reviewers raised concerns, since it is <sep> by no means clear that even the optimized upper bound produces meaningful bounds on the generalization <sep> performance. The authors tried to address these concerns in their response and promised to update their <sep> paper accordingly. As a result, they added a paragraph on page 8. Unfortunately, this paragraph remains extremely <sep> vague, in particular if it comes to the more interesting case of non-linear kernels. Here, the authors briefly refer to <sep> a paper by El Karoui but no details are given. However, looking at El Karoui's paper it is anything but obvious whether <sep> the results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class <sep> of distributions. <sep> As a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion.
A line of work since 2016 has investigated learning NN-based optimisers, which produce optimisation updates by processing loss/gradient info with neural networks. This paper tries to understand the learned dynamics of these NN-based optimisers by linear approximation to the learned non-linear dynamics. Visualisation of these approximations are shown on 3 optimisation problems: linear regression, Rosenbrock function, and a toy neural network classification problem, with the hope of covering different types of objective landscapes. <sep> Reviewers agreed that the paper studies an important research question, which would interest researchers working on meta-learning learning algorithms. However there are several major concerns raised by the reviewers: (1) the example optimisation problems are toyish, and (2) the paper does not explain very well the link between the visualised behaviour and the better optimisation results, i.e. it is unclear to the reviewers why the learned dynamics lead to better optimisation results. <sep> While I am not too concerned about issue (1), I think issue (2) is a significant one, flagging that the clarity of the paper needs to be improved. Ultimately, the paper is motivated by the question "How is a learned optimizer able to outperform a well tuned baseline?", so a reader would expect some clear explanation towards answering this question. Also some reviewers are concerned about the fact that only the RNN-based optimiser in Andrychowicz et al. (2016) is analysed; since there exists other forms of learned optimisers, focusing on studying only one type of them might lead to early conclusions that are not so accurate.
The paper provides variance reduction techniques for GCN training. When training a GCN it is common to sample nodes as in SGD, but also subsample the nodes' neighbors, due to computational reasons. The entire mechanism introduces both bias and variance to the gradient estimation. The authors decompose the gradient estimate into its variance and bias error, allowing them to apply more targeted variance (and bias) reduction techniques. <sep> The results and improvement over existing GCN methods seem to be solid. The main weakness of the paper is its novelty. As pointed out in the reviews the techniques seem to be quite close to papers [5],[11] (referring to the authors posted list). <sep> It therefore boils down to the question of whether the authors simply applied existing techniques, achieving a better implementation than previous art, or did they develop a truly new algorithm that will encourage further research and deepen the understanding of GCNs. Given the decisive opinions of reviewers 1 and 4, that remained after taking the response into account, I tend to believe that the improvement provided here is either too incremental or not stated in a crisp enough manner in order to be published in its current form
The paper had three borderline reviews. While the idea of posterior sampling of a neural network is potentially useful and Langevin dynamics are a way to attempt to address that, the reviewers did not appear convinced by the experiments and what the MCMC sampling was doing wasn't really front and center there.
The meta-reviewer agrees with the reviewers that this is a marginal case. Conditioned on the quality of content and comparisons to other works: <sep> Constrained Reinforcement Learning With Learned Constraints (https://openreview.net/forum?id=akgiLNAkC7P) <sep> Parrot: Data-Driven Behavioral Priors for Reinforcement Learning (https://openreview.net/forum?id=Ysuv-WOFeKR) <sep> PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning (https://openreview.net/forum?id=BIIwfP55pp) <sep> We believe that the paper is not ready for publication yet. We would strongly encourage the authors to use the reviewers' feedback to improve the paper and resubmit to one of the upcoming conferences.
The paper proposes a learning framework for Hypergraphs. The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs. Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required. However, the proposed methods are not significantly different than that used for Graphs. Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance.
The consensus recommendation is that the paper is not ready for publication at this time.
The paper proposes a new measure of difference between two distributions using conditional transport. The paper considers an important problem. However, some major concerns remain after the discussion among the reviewers. In particular, the paper focuses on the evaluation on a toy dataset. It is unclear whether the claim carries over to large real datasets. The presentation of the paper also needs substantial improvement.
Summary: <sep> This paper introduces a method to try to learn in environments where a person specifies successful outcomes but there is no environmental reward signal. <sep> I'd personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback. Similarly, I'd be interested in how other methods of providing human prior knowledge compared. <sep> Discussion: <sep> Reviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted. <sep> Recommendation: <sep> While I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful.
The paper introduces an approach to counterfactual fairness based on data pre-processing, and compare it to other two counterfactual fairness approaches on the Adult and COMPAS datasets. <sep> The reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue. Their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion. The reviewers would have also appreciated more experiments on real-world datasets to get a more comprehensive comparison of the methods. Finally, discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited.
The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like *CONF*. <sep> The idea of combining MPC (on a 'wrong' model) with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews.
This paper is very pleasant to read. The reviewers also like the key idea discussed and find the targeted application interesting and practical. However, after reading the indeed interesting motivation, all four reviewers expected to see more from the evaluation section, including more challenging and realistic set-ups and clearer gains over standard methods. The reviewers also discuss how both the navigation problem as well as the GP constraint problem have been tackled in the past, often in combination (e.g. reference [1] by R1). Therefore, it would be needed to see additional experimental evaluation in line with those previous works.
The paper studies the robustness of binary neural networks (BNNS), showing how quantized models suffer from gradient vanishing. To solve this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near-perfect perfect success in crafting adversarial inputs for these models. The problem is interesting and important. However, the major concerns are that the technical novelty is limited raised by two Reviewers, small improvements for linear loss functions. The most related work is not compared in the experiment.
While the reviewer's noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity. I agree. (Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion. Later on page 1, you use the term "the dynamics of density" but you should not assume that the reader knows what that means. There are typos as well, e.g. "make all predictions base[d] on Equation (6)" on page 4. It would be helpful to know something about why you chose the experimental setups in Synthetic-1, 2, and 3. ) <sep> Regarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future.
The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews. <sep> However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.
This paper proposes a transferable adversarial attack method for object detection by using the relevance map. Four reviewers provided detailed reviews: 2 of them rated "Ok but not good enough - rejection", 1 rated "Marginally below" and 1 rated "Marginally above". While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc. During the rebuttal, the authors made efforts to response to all reviewers' comments. However, the major concerns remain, and the rating were not changed. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.
The focus of the paper is stochastic backpropagation for both continuous and discrete random variables. By using standard results from Fourier analysis the authors rewrite the corresponding gradients in an infinite weighted sum form ((3) and (9)), extending the results of (Rezende et al. 2014) and (Fellows. et al., 2018). The efficiency of the approach is illustrated in 2 toy examples. <sep> As summarized by the reviewers, the problem tackled is interesting. However, they also pointed out that the novelty of the approach is quite limited and its practical usefulness is not clear (it should by demonstrated against state-of-the-art baselines, on realistic benchmarks).
The authors propose an RL-based approach, "Rewriting-by Generating (RBG)", to solve large-scale capacitated vehicle routing problems (CVRPs): such problems are NP-hard in general and are ubiquitous. The RL agent consists of a "Generator" and "Rewriter". In generation, the graph is sub-divided into several regions and in each region, an RL algorithm runs to get the best (or near-optimal) route. The rewriter then patches these near-optimal sub-solutions together using "hierarchical RL". <sep> The paper is generally well-written. <sep> One main concern is related to generalizability: the authors respond that their approach can work for other NP-hard combinatorial-optimization problems such as knapsack. The authors are encouraged to do a systematic study of several such (related) problems where their approach can work. It was also a concern that the overall approach of partitioning the input instance and rewriting the CVRP solution by merging regions and recomputing routes, is also employed by commercial OR solvers. The authors are encouraged to do a careful comparison (and perhaps melding) with such available solvers, to get a hybrid "OR + ML" improvement. It is also suggested that the authors include several different constraints from real-world VRP (e.g., heterogeneous vehicle costs, costs of missed shipment, route limits, upper-bounded number of vehicles etc.).
Reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, Poisson neuron, and the universal approximation properties. However, there are concerns, especially by R4 and R5, that the presentation is confusing, lacks clarity, and should be substantially improved. <sep> Note: Theorem 1.7 in (Helgason, 1970) is proved explicitly for the case n=2, not for general n as claimed in (9). Thus the Laplacian eigenspace motivation needs to be re-written/re-examined.
A method is proposed for removing prior knowledge, presented as a <sep> distance matrix, from low-dimensional embeddings, to focus them on <sep> what is new. <sep> The task of visualizing novely in data is interesting and good <sep> solutions would potentially be highly useful. <sep> The proposed method essentially substracts a distance matrix from <sep> another. While this is sensible, it is not completely clear in what <sep> sense this is the right solution for what the embeddings will be <sep> used for. <sep> In final discussions among the reviews, the main remaining concerns <sep> were considered severe: comparisons to other methods being limited, <sep> and possible problems in one of the experiments.
This paper studies the role of "noise injection" in GANs with tools from Riemannian geometry, and derives a new noise injection approach that aims to learn a fuzzy coordinate system to model non-Euclidean geometry. The new noise injection approach is shown to improve over StyleGANv2 noise injection on lower-resolution 128x128 FFHQ, LSUN, and 32x32 CIFAR-10 images. <sep> Some reviewers found the experimental results a "considerable improvement on DCGAN and StyleGANv2" (R3), "extensive and convincing" (R2), while others had concerns around the experimental setup using lower resolution images (R1, R4). While reviewers were mostly positive about the experimental wins of the paper, there was confusion (R3) and several concerns (R4) around the theory and the relationship between the theory and the practical noise injection algorithm. I additionally had several concerns around the presentation and relation to prior work on generative models. Thus in the current state I cannot recommend this paper for acceptance. Below I highlight concerns that should be addressed in future revisions. <sep> My biggest concern is the tremendous gap between the theoretical claims and the practical implementation. When training a GAN with the new form of noise injection, does it learn the skeleton and fuzzy equivalence relationships you claim? This paper is missing any kind of toy experimenting showing that training a GAN with fuzzy reparameterization discovers these relationships or coordinates. Such an experiment would greatly strengthen the paper and help to answer the question of why this new method works (i.e. it's not just more parameters, a slightly better architecture, or better hyperprameters as mentioned by R3 and R4). There's also no discussion of what happens theoretically when you have multiple layers of fuzzy reparameterization, and the claims that StyleGAN2's noise injection limits to Euclidean geometry is false in this case (and thus StyleGAN2's noise injection can also overcome the "adversarial dimension trap"). <sep> Theoretical setting: As mentioned by R4, there is much prior work on the difficulties in fitting a lower-dimensional model manifold to a higher-dimensional data manifold (e.g. WGAN). Theorem 1 highlights the impossibility of exactly fitting the data manifold with (smooth) neural networks, but the resulting solutions of increasing the dimensionality of the latent space is well-known and commonly used (e.g. StyleGAN). This paper also doesn't discuss the alternative of approximately fitting the data manifold with a lower-dimensional structure, which is what is often studied in practice. <sep> Clarity: The term "noise injection" is overloaded in the literature, and the current presentation of the paper does not sufficiently describe the method. There's also no discussion of "instance noise" that is another solution to this problem that adds noise to inputs of the discriminator to yield finite f-divergences (Sonderby et al., 2016, Roth et al., 2017). The work on instance noise is very related to the approach here, but only adds noise to the output of the generator, not at all levels. <sep> There's also no discussion of how adding noise is just expanding the generative model with additional latent variables, a standard approach that is often discussed in the context of hierarchical generative models. The authors mention the relation to reparameterization trick in VAEs, but argue it is doing something fundamentally different. However, modern VAE architectures (IAF-VAE, Very Deep VAE), use a very similar form of modulation at multiple levels in the hierarchy. <sep> Experiments: There are no error bars in experimental results, and many results are presented in a new experimental setting defined by the authors (lower resolution than prior work even if using prior code). Rerunning experiments in more standard settings on full resolution images would greatly improve the confidence that the new noise injection strategy is effective.
This work develops a weight-quantization method for deep neural networks that is suitable for a type of analog hardware system known as crossbar-enabled analog computing-in-memory (CACIM). The goal of this work is to train models on GPUs in such a way that they retain their predictive accuracy during inference when deployed on the analogue hardware system. <sep> Pros: <sep> Good adaptation of quantization methods to the CACIM system <sep> Simple method <sep> Validation of the proposed method on multiple datasets and models <sep> Cons: <sep> Lack of novelty: the proposed method is a simple combination of two popular methods, LLoyd's quantization and noise-aware training <sep> All reviewers appreciate the simplicity of the method and the good fit to the hardware. The authors responded to all reviews and two reviewers acknowledged the authors' response. The authors acknowledge some reviewer observations (motivation of quantization as reducing analogue noise, lack of experiments on the actual CACIM system), and the authors added an experimental evaluation on the actual physical CACIM system showing that their method performs well. <sep> Overall the work is well-executed and the proposed method is a good fit to the CACIM system. However, the proposed quantization method is a straightforward adaptation of popular quantization methods.
We thank the authors for their detailed responses to reviewers, and for engaging in a constructive discussions. <sep> As explained by the reviewers, the paper is clearly written and the method is novel. However, the novelty is to combine existing ideas and techniques to define an objective function that allows to incorporate cluster assignment constraints, which was considered incremental. Regarding quality, the discussion highlighted some possible improvements that the authors propose to do in a future version of the paper, and we encourage them to follow that direction. Regarding significance, although the experimental results are promising there were some concerns that the improvement over existing techniques is marginal, and that more experiments leading to a clearer message would be useful. <sep> In summary, this is not a bad paper, but it is below the standards of *CONF* in its current form.
The paper's initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline. <sep> The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers' and authors' remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty -- as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper -- even a two time-step trajectory with normal rewards per state transition can exhibit a mixture-of-Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method. <sep> I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.
The paper studies the effectiveness of few-shot learning techniques in settings where the training labels are imbalanced. While addressing an interesting practical problem, reviewers raised concerns about the paper's technical depth, insufficient distinction to existing techniques for coping with label imbalance, and limited qualitative conclusions from the results. The authors incorporated some of these comments in their revision, but a more comprehensive update on the latter two points appears appropriate.
Overview: <sep> This paper introduces a maximum mutual information method for helping to coordinate RL agents without communication. <sep> Discussion: <sep> Some reviewers leaned towards accept, but I found the two reviewers recommending rejecting to be more convincing. <sep> Recommendation: <sep> This is an important research topic and I'm glad this paper is focusing on the problem. Hopefully the reviews will help improve a future version of this paper. I agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward. <sep> In addition, I think the setting needs to be better motivated. This is a centralized training with decentralized execution (CTDE) setting, and this paper helps the agents coordinate. In CTDE, the agents work in the environment and then pool their information to train before deploying on the next episode. I don't understand why, e.g., in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object (the episode ends), and then cannot communicate once the next episode starts.
This paper begins to formalize a connection between value decomposition and difference rewards. Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper. <sep> Given the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results. As these are representative of the potential audience at *CONF*, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community. <sep> The empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper "Qplex: Duplex dueling multi-agent q-learning." Given the theoretical connection to difference rewards (e.g. COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications.
This paper present novel formulations to address the problem of unbalanced Gromov. The Conic formulation is very interesting but stays theoretical until optimization algorithms are available. The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers. Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs. <sep> The paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results. The detailed a very well written response to the reviewers comment has been appreciated. But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper. This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community. Despite acknowledging the theoretical contributions of the paper, the AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation, ...).
The paper studies an interesting problem motivated by VLSI design. The reviewers agree that there are interesting aspects of the RC algorithm. Nevertheless, the paper could be improved by a clearer characterization/apples-to-apples comparison to baselines, particularly regarding computation cost, use of parallelism, as well as a more thorough contrast to state of the art in general. Given the contribution is experimental, and this is a well studied problem, it is important to establish whether the solution is indeed best-in-class; cost due to training should be taken into account, and minimized to the extend possible. Going beyond the baselines considered here, as well as reviewing possible theoretical connections to other problems and guarantees, would also strengthen the paper.
The paper discusses the problem of how to augment cross-modal retrieval for the task of multi-modal classification -- it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering. However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains.
The paper presents a method for meta-learning the loss function. The analysis mainly concerns the recently proposed TaylorGLO method on the (slightly less recent) Baikal loss. There was no consensus on this paper, but no reviewer was willing to fight for acceptance either. I found the paper not self-contained, with important non-standard elements undefined, starting with the Baikal loss, notations that are not defined in the main text, and a nomenclature that is also unusual with important terms such as "attractor" or "invariant" used in meanings that are non-standard in optimization or machine learning. <sep> Regarding content, most of the analyses refer to properties of the Baikal loss (not presented in the main text) that are deemed to be positive, without any theoretical support (Theorems 1 and 2). The inability to overfit is here posed as an obvious quality of a training loss. Then, a way to prevent the failure of the meta-training algorithm is presented in Theorem 3. Finally, an experiment is provided, showing that the proposed meta-training algorithm performs better than "vanilla" training with respect to adversarial attacks with FGSM. There is no comparison with other defense mechanisms and no analysis explains the results. Overall, although some interesting aspects may be developedin this paper, they are currently not well served by writing or the experimental evidences, so I recommend rejection.
This paper clearly has great ideas and reviewers appreciated that. However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue. We don't know if the reported accuracy is a respectable one (in the public domain). Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility. <sep> We recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non-trivial.
The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts. This is a simple yet intuitive extension: self-attention is augmented with an expiration value prediction. Experiments were carried out on NLP and RL tasks. <sep> Overall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short.
There was quite a bit of internal discussion on this paper. To summarize: <sep> The idea is very neat and interesting and likely to work <sep> The paper is likely to inspire future work <sep> There are still serious doubts about the experimental evaluation that is not entirely up to par with current standards <sep> The reviewers were not convinced 100% by the arguments about the 'custom' environments <sep> The reviewers were not convinced 100% that the baselines were given their best shot <sep> While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like *CONF*.
This paper presents a method for relational inference in multi-agent/multi-object trajectory prediction tasks. Different from the neural relational inference (NRI) model [1], the presented method is able to model time-varying relations. Experimental results on physics simulations and sports games (basketball) show benefits over variants of the NRI model. <sep> The reviewers agree that the presented method is mostly solid, that the experiments are insightful, and that this is generally a well-written paper. The authors, however, have apparently overlooked recent related work [2] (dNRI) that proposes a very similar model. In the light of dNRI, it is difficult to argue for the novelty of the presented approach, and the paper needs to undergo a revision in order to more clearly differentiate it from the dNRI model, and to resolve the other concerns raised by the reviewers. <sep> [1] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) <sep> [2] Graber et al., Dynamic Neural Relational Inference (CVPR 2020)
Although the reviewers found the paper well-written that analyzes a relatively popular algorithm (TD(0) version of A3C), there are concerns regarding the novelty of the convergence results given those for A2C, the comparison of the results with those for A2C, and the sufficiency of the experiments. Although the authors addressed some of these issues/comments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature. I would suggest that the authors take the reviewers' comments into account, have a more comprehensive discussion about the relation of their results with those in the literature (two-time scale algorithms), and prepare their work for future conferences.
This paper introduces a dataset and a trained evaluation metric for evaluating discourse phenomena for MT. Several context-aware MT models are compared against a sentence level baseline. The paper develops metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. Data is released for three language pairs (all using English as the target language). <sep> First, I'd like to point out that creating datasets and benchmarks for analyzing/evaluating discourse-level errors in machine translation is an extremely valuable contribution. This paper is addressing a very relevant problem and even though there is no new model/method/algorithm being proposed, this work fits this conference - it is my opinion that the community should welcome and value more than it currently does the efforts spent in creating high quality datasets that can help make progress in the field. <sep> There was substantial discussion among reviewers about this paper. <sep> The main weaknesses raised by the reviewers were: <sep> Limited information about the process to create the anaphora test, which was a contribution of prior work (Jwalapuram et al. (2019) - this was addressed in the updated version; but the anaphora challenge sets seem to be only a minor update over previous work. <sep> All language pairs use English as the target language, and it is not simple to extend this approach beyond English target languages. <sep> Lack of detail on how BLEU scores were computed (tokenised? true cased? My recommendation is to use sacrebleu) - this was clarified in the rebuttal. <sep> The evaluated NMT models all date from 2018 or earlier. <sep> Two of the 4 benchmarks (anaphora and coherence) are evaluated by neural models trained on WMT outputs, which makes the interpretation of scores is opaque, and their validity is unclear. <sep> While the creation of a benchmark for discourse evaluation of MT is a laudable effort as mentioned above, it is my opinion that due to some of the weaknesses above the current version of this work is not yet ready for publication. However, I strongly encourage the authors to improve upon these points and resubmit their work to another venue. I list some suggestions below to improve this paper. <sep> My biggest concern with the current version is the last weakness above. As pointed out by a reviewer, the framework of Jwalapuram et al. (2019) provides empirical support for the model's sensitivity (if there is a pronoun error, does the metric pick it up?). But they don't necessarily capture model specificity (if the metric ranks one output higher, can we be confident that this is because of a pronoun translation error?). For the coherence metric, authors make an argument that their metric is sensitive to coherence issue, but concerns remain about whether it is sufficiently specific to these issues. In the rebuttal, authors argue that BLEURT is sentence-level, but they could easily aggregate sentence-level judgments and report correlation between BLEURT and human coherence judgments to show that their metric correlates better with human coherence judgments than BLEURT or even just BLEU. Besides BLEURT, I would add there are other recently proposed metrics that may capture discourse phenomena (neural metrics trained against MQM annotations or sentence-level human assessments with document context) and should be compared against: check COMET [1] or PRISM [2] (the latter is sentence-based but could be adapted for paragraphs or documents). <sep> There is also prior work comparing various context-aware machine translation approaches against a sentence-level baseline, some with negative findings [3,4,5]. I suggest the authors look at this related work in future iterations of their paper. <sep> [1] https://arxiv.org/pdf/2009.09025.pdf <sep> [2] https://arxiv.org/pdf/2004.14564.pdf <sep> [3] https://www.aclweb.org/anthology/2020.eamt-1.24.pdf <sep> [4] https://arxiv.org/pdf/1910.00294.pdf <sep> [5] https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf
All three reviewers expressed consistent concerns on this submission in their reviews. In addition, none of them enthusiastically supported this work during discussion. It is clear this submission does not make the bar of *CONF*. Thus a reject is recommended.
The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output. <sep> Pros: <sep> Handling less structured data is surely an important problem in machine learning and is much less explored. <sep> The paper is well written, easily understandable even with a fast browsing. <sep> The experimental results show some improvement. <sep> Cons: <sep> The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4. <sep> By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4's comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental. <sep> Although the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper.
The paper proposes to speed-up self-supervised learning for semi-supervised learning by combining self-supervised pretraining and supervised fine-tuning into a single objective. The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses. Most reviewers are concerned about the novelty of the approach and the significance of empirical results. I agree with both concerns. I appreciate the comparison between log⁡∑exp and ∑log⁡exp, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower). I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper.
This paper proposes to automatically determine when the SGD step-size should be decreased, by running two "threads" of SGD for a bunch of iterations, divide those into windows, and then look at the average inner-product of the gradients in the two threads in each window. If the inner-product tends to be high, that indicates that there is still "signal" in the gradient and it should not be decreased. If it is low, that indicates that the gradient is mostly "noise". In the latter case, the learning rate is decreased by a factor of gamma and the length of the next phase is increased by gamma. <sep> Theorem 3.1 essentially assumes smoothness, a bounded fourth moment for the stochastic gradient, and that the stochastic gradient error is not too far from isotropic. Then it shows that if the step-size is set small enough, the standard deviation of the diagnostic (Q_i) can be upper-bounded in terms of the expected value of Q_i. It follows that the probability of Q_i being negative cannot be too large (bounded in terms of the step size eta and the length of the windows l). <sep> Theorem 3.2 adds the assumption of strong convexity and weakens the assumption on the gradient to a bounded second moment. Then it upper-bounds the expected value of the diagnostic in terms of its standard deviation. <sep> Proposition 3.4 gives a proof of convergence. As far as I can tell the proof is essentially that the learning rate decay can't be much worse than what would happen if the diagnostic always set to decrease. In particular: (1) It's impossible for the learning rate to decay too quickly, since the length of each phase is increased by gamma whenever the learning rate is decreased by gamma. (This is a "non-adaptive result.) (2) The learning rate will eventually decay with probability 1. <sep> Various concerns were brought up by the reviewers. Perhaps the most strongly voiced concern was that the proposed method is a heuristic rather than a method with a rigorous guarantee. For my part I am in agreement with the authors and other reviewers that heuristic methods for decreasing the learning rate are worthy of study given the large practical importance of this problem. <sep> I concur with the concern raised by some reviewers that the theoretical component of the paper may not have little explanatory value for the results that are given. The assumption of strong convexity is not a major concern to me. (Though not true it can still give intuition.) More concerning is that theory essentially takes a fixed step-size scheme (repeatedly decrease the step size by gamma and increasing the length of a phase by gamma) and then shows that the diagnostic can't be too much worse. This isn't in keeping with the motivation of being adaptive. <sep> The reviewers were also concerned about the explanation of better results due to less overfitting. This may be true, but the theory makes no mention of overfitting. <sep> There was a consensus that the experimental results were promising, though some minor issues were raised. <sep> While the direction explored in the paper has value, there are enough open questions about the relationship of the theory to the experimental results to warrant another round of review. <sep> Small thoughts, not significant to acceptance: <sep> The current heuristic runs two separate threads and looks at the inner-product of those gradients. An alternative to this would be to run a single thread along with a "ghost" thread that computes a different gradient at each iteration. It would be great to comment on the difference and why one might be superior to the other. A more radical alternative would be to run a single thread, but then compute the diagnostic on each half of the minibatch. A more radical alternative still would be to analytically do that splitting many times and average the results. This seems like it might simultaneously reduce the variance of the diagnostic and also reduce the computational cost. <sep> The current heuristic runs two threads. Is there a tradeoff if you run more? <sep> The statement of theorems could be more user-friendly. To understand Thm 3.1, I needed to search o find the definitions of: eta, l, i, w, Q_i. With a small amount of effort this could be re-written to remind the reader that w is the number of windows, l is the length, eta is the stepsize, etc. It is particularly unfortunate that sd() is never formally defined (only by reading the appendix did I discover that this was the standard deviation.) <sep> The fact that the length of threads is always increased by a factor of gamma whenever the step size is reduced by gamma seems contrary to the spirit of the proposed diagnostic. After all, this "bakes in" a kind of "fastest possible" decay schedule. If the diagnostic were fully reliable, shouldn't this not be necessary? The decision to add this doe not get nearly enough discussion in the paper in my view. <sep> I think it might be clearer to re-state theorem 3.1 including the Chebyshev result after it.
The paper is about an approach that combines successor representation with marginalized importance sampling. <sep> Although the reviewers acknowledge that the paper has some merits (interesting idea, good discussion, extensive experimental analysis) and the authors' responses have solved most of the reviewers' issues, the paper is borderline and the reviewers did not reach a consensus about its acceptance. In particular, the reviewers feel that the contributions of this paper are not significant enough. <sep> I encourage the authors to modify their paper by taking into consideration the suggestions provided by the reviewers and try to submit it to one of the forthcoming machine learning conferences.
The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers. <sep> At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function ℓ is minimized and why minimizing it makes sense (assuming, e.g., that ℓ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes xt when the procedure is started from x). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1) <sep> Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where Bp and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that xT, which is the final outcome of the algorithm, remains in the Lp ball). Another example is the Discrete Approximate CorrAttackFlip paragraph which requires that every coordinate of x should be changed by ±ϵ. It is also not clear what "dividing the image into several blocks" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously x was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same ϵ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper. <sep> The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.
The paper investigates the relationship between data augmentations used during training and their effect on the accuracy when evaluated on unseen corruptions at test-time. The paper proposes a metric called minimal sample distance (MSD) to measure the similarity between augmentations during training time and corruptions at test time. <sep> The reviewers agree that the paper aims to solve an important problem and the paper has some interesting findings. However, the current version has a few shortcomings: <sep> Some of the claims about "overfitting" are confusing, especially for data augmentations that use ops similar to those in ImageNet-C. This is already known and which is why some papers uses a subset of operations (e.g. AugMix uses a subset of AutoAugment operations). <sep> The main take-home message and novelty is unclear: The initial version titled ("Is Robustness Robust?") seemed to argue that we may be overfitting to Imagenet-C, but the rebuttal and the updated version revised some of the claims (see response to R3 and R4). In light of the revision, I'm not sure how the main take-home messages differ from existing papers such as Yin et al. 2019 or "Many faces of robustness". <sep> One of the main differences is quantification of the distribution similarity, however, as pointed out by R2, this analysis does not explain when stylized corruptions would help, so the current version of the paper feels a bit incomplete to me. <sep> I recommend the authors to revise the draft based on reviewer feedback and resubmit the paper to another venue.
The paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of train/val. Hence, the method uses adversarial train/val splits during training. The paper is reviewed by three expert reviews and none of them championed the paper to be accepted. I carefully checked the reviews and the authors' response and agree with the reviewers. Specifically: <sep> R#1: Argues that the paper is not ready for publication. Also argues the optimization problem is only a motivation as it is not directly solved. This is an important issue and it needs to be addressed in a conclusive manner. <sep> R#2: Argues empirical studies do not show the value of train/val splitting. I partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method. <sep> R#3: Argues the contribution is not enough for publication. The paper is clearly novel but the contribution and novelty is not presented in a clear manner. Moreover, the empirical study does not complement the novelty. Hence, I disagree with the comment. <sep> Overall, I believe the paper proposes an interesting idea. However, the presentation and empirical studies need to be improved significantly. I recommend authors to address these issues and submit to the next conference.
This paper proposes a method for automatically discovering graph algorithms using GNNs. In general, the reviewers find the paper well-written, and the problem and the approach interesting. However, there is a concern on the practical usefulness of proposed method as shown in the following comments: "My main concerns are on Q2, i.e., the practical usefulness of the algorithm"[R1]; "It sounds like the proposed model is hard to generalize to different datasets" [R3]; "The proposed explainer does not generate practically useful outputs for discovering new algorithms"[R4].
The authors consider local 'why' or 'abductive' explanations for a model and a given class, which identify a minimal subset of features such that they're sufficient to imply that the model predicts the class; and 'why not' or 'contrastive' explanations, which identify a minimal subset s.t. they're sufficient to imply that the model predicts a different class. The two types of explanation are related using earlier work on minimal hitting sets going back to Reiter (1987). <sep> Reviewers were divided in their opinions. R4 was very positive but with little detail and only medium confidence, then did not participate in discussion. R2 was the only reviewer with high confidence, leaning against acceptance. The paper relies on FOL which was hard for reviewers to grapple with, and may make it challenging for readers. The presentation could be improved by clearly linking to existing work and demonstrating why the new approach is important.
This paper introduces a method for hierarchical classification with deep networks. The idea is interesting, and as far as I know novel: namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers. The idea of placing spheres (with a fixed radius) around each classifier and forcing the child-classifiers to lie on these spheres is quite clever. <sep> The reviewers have pointed out some concerns with this paper. Some had to do with terminology (which the authors should fix but which is no big deal), but the main weakness are the experimental results and the ablation study. The reviewers were not convinced that the optimization in the Euclidean space wouldn't be sufficient. A more thorough ablation study could help here. <sep> This is the kind of paper that I really want to see published eventually, but right now isn't quite ready yet. If you make one more iteration (in particular adding a stronger ablation study) it should be a strong submission to the next conference. Good luck!
In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at *CONF*.
The paper discusses an extension of BERT for learning user representations based on activity patterns in a self-supervised setting. All reviewers have concerns about the validity of the claims and the significance of the experimental results. Overall, I agree with the reviewers that the paper needs more work to be published at *CONF*. I recommend rejection.
The reviewers found this to be an interesting and clearly-written paper, but broadly agreed that it is not yet ready for acceptance. In particular, multiple reviewers felt that the experiments don't show clear benefits of the proposed SVAE approach when compared to the VAEVAE and other baselines; nor do they sufficiently back up the central claim regarding relative benefit of PoE vs MoE for either "AND" or "OR" relations. Hopefully the comments and suggestions from the reviewers, particularly regarding framing and experimental validation, will help in revising the paper.
The authors propose a low-bit floating point quantization method to reduce energy and time consumption for deep learning training. Dynamic quantization and MLS tensor arithmetic are used to enhance the effectiveness of MLS. The motivation is clear and the efficient training is an important problem to address. However, the effectiveness of proposed method is not well justified and experimental results are less convincing. In addition, the clarify of paper still needs to be further improved.
Overall, all reviewers generally agree that the idea of using visual similarity to unsupervised alignment of multiple languages is interesting and the proposed method and dataset are well-designed, while three of them raised some concerns related to the retrieval nature of the method. In particular, discussions about its place as a study of machine translation and comparison with other cross-lingual retrieval baselines were the main issues. Although authors made great effort to address reviewers' concerns points and did clarify some of them, unfortunately the reviewers were not fully convinced by the response, and one reviewer decided to downgrade the initial score. After all, three reviewers rate the paper as 'below the acceptance threshold'. Based on their opinions, I decided to recommend rejection. <sep> I think the entire picture of the work and the logic flow could be much clearer by discussing in a top down manner why this idea should be implemented with a retrieval-based approach, rather than superficially adding "using retrieval" to some sentences.
The paper proposes a multi-scale spatial-temporal joint graph convolution for spatiotemporal forecastings. Many reviewers have concerns regarding novelty, baseline comparisons, and writing clarity of the draft.
In this paper, the authors study the behavior of the Lookahead dynamics of Zhang et al. (2019) in bilinear zero-sum games. These dynamics work as follows: given a base algorithm for solving the game (such as gradient descent-ascent or extra-gradient), the Lookahead dynamics perform k iterations of the base algorithm followed by an exponential moving average step with weight α. The authors then provide a range of sufficient conditions for the eigenvalues of the matrix defining the game under which the Lookahead dynamics become more stable and converge faster than the base method. <sep> This paper received four reviews and generated a very lively discussion between the authors and reviewers. Reviewer 4 was enthusiastic about the paper; the other three initially recommended rejection. During the discussion phase, the authors revised their paper extensively, and Reviewer 3 increased their score to an "accept" recommendation as a result. In the end, the reviewers were evenly split, and I also struggled a lot to reach a recommendation decision. <sep> On the plus side, the paper treats an interesting problem: prior empirical evidence suggests that the Lookahead dynamics can improve the training of some adversarial machine learning models, so a theoretical study is very welcome and of clear value. On the other hand, the setting treated by the paper (bilinear min-max games) is somewhat restrictive, and the authors' theoretical conclusions do not always admit as clear an interpretation as one would like. <sep> The issues that ended up playing the most important role in my recommendation were as follows: <sep> The Lookahead dynamics with period k involve k gradient evaluations, so their rate of convergence should be compared at a k:1 ratio to GD and EG (with an additional 2:1 ratio between GD and EG to put things on an even scale). To a certain degree, this k:1 ratio is present in the last part of Lemma 3; however, the exact acceleration achieved by the "shrinkage" of the spectral radius is not clear. This can also be seen in the semi-log plots provided by the authors, where the corresponding slopes of GD/EGD methods should be multiplied by k when compared to the respective LA variants. In this regard, a comparison with the values of k provided in Appendix D reveal that the performance of the Lookahead variants in terms of gradient queries is very similar (if not worse) to the non-LA variants. This is a cause of concern because, if LA does not accelerate convergence in simple bilinear games, it is not credible to expect faster convergence in more complicated problems. During the AC/reviewer discussion of this point, Reviewer 3 pointed out that this might be due to a suboptimal tuning of α (i.e., that it was not chosen "small enough"), and went out to note that this echoes the arguments of other reviewers that the characterization of acceleration may be problematic and not significant (even if it takes place). <sep> Another major concern has to do with the stabilization provided by the Lookahead dynamics: using a benchmark game proposed in a recent paper by Hsieh et al. (2020), the authors showed that the Lookahead dynamics converge to a point which is unstable under GDA/EG (and hence avoided). This is fully consistent with the authors' theoretical analysis, but it also highlights an important problem with the Lookahead optimizer: if k and α are tuned to suitable values for stabilization, the algorithm converges to a non-desirable critical point (a max-min instead of a min-max solution). This is a major cause of concern because it shows that the algorithm may, in general, converge to highly suboptimal states. <sep> The above create an inconsistency in the main story of the paper. In fact, it seems to me that the authors' results form more of a "cautionary tale in hiding": even in very simple bilinear problems, the lookahead step may not provide acceleration and, even worse, it could converge to highly undesirable critical points. I find this "negative" contribution quite valuable from a theoretical standpoint, and I believe that a thoroughly revised paper along these lines would be of interest in the top venues of the community (though a more theoretical outlet like COLT might be more appropriate). However, this would require a drastic rewrite of the paper, to the extent that it should be treated as a new submission. <sep> In view of all this, I am recommending a rejection at this stage. I insist however that this should not be seen as a critique for the mathematical analysis of the authors (which was appreciated by the reviewers), but as a recommendation to reframe the paper's narrative to bring it in line with the algorithm's observed behavior. I strongly encourage the authors to resubmit at the next top-tier opportunity.
The authors present a model-based method for cooperative multi-agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability. <sep> Overall, all reviewers found this work to be of great interest and the combination of planning + communication novel. However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines. The authors have since clarified several aspects in their paper and also included a new RL environment. <sep> However, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing. I would like to echo though reviewers' suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue.
This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. <sep> Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters. <sep> The reviewers finds the paper technically solid but difficult to read and with a limited contribution. <sep> The AC carefully reads the paper and discussions. Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019). Therefore, the AC recommends rejection.
This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers' judgements, this is a good submission but hasn't reached the bar of *CONF*.
This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature. <sep> Pros <sep> Clear writing. <sep> Detailed hyperparameters to aid reproducibility. <sep> Straightforward model. <sep> Cons <sep> Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017. <sep> Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive. <sep> Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence.
The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing: <sep> i) a simple to implement (local) node aggregation approach; <sep> ii) some theoretical support to the proposed approach; <sep> iii) empirical evidence that the proposed approach could be effective. <sep> Notwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact: <sep> i) reference to literature is missing some important recent contributions to the addressed problem (e.g. Gated Graph Sequence Neural Networks, GNNExplainer); if possibile, also experimental comparisons vs those approaches is desirable; <sep> ii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance; <sep> iii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method. <sep> In summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability.
The paper received mixed reviews that overall lean negative. <sep> The main concern shared by reviewers is the novelty of the findings. Although the paper presents a systematic study that certainly has value, reviewers do not find sufficient insights from the analysis. The ACs agree with the reviewers that the paper is below the bar for acceptance.
The paper presents an evolutionary optimization framework for training discrete VAEs, which is different to the standard way of training VAEs. One of the main criticism of the paper was the choice of experiments, but the authors addressed this point by adding an inpainting benchmark. <sep> Unfortunately, the reviewers' scores are borderline, and one of the reviewers pointed out the lack of scalability (more precisely, linear scalability with the number of observations) and cannot recommend acceptance based on the limited application impact. Given the large number of *CONF* submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
The paper considers the problem of private data sharing under local differential privacy. <sep> (1) it assumes having access to a public unlabeled dataset for learning a VAE, so it reduces the dimensionality in a more meaningful way than simply running PCA. (2) the LDP guarantee is coming from the standard Laplace mechanism and Randomized Responses. (3) then the authors propose how to learn a model based on the privately released (encoded) data which exploits the knowledge of the noise distribution. <sep> None of these components are new as far as I know, nor were they new in the context of differential privacy. For example, the use of a publicly available data for DP was considered in: <sep> Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure <sep> vs. approximate differential privacy. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 363–378. Springer, 2013. <sep> (they called it Semi-Private Learning...) <sep> Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017). Semi-supervised knowledge transfer for deep learning from private training data. In *CONF*-17. <sep> The idea of integrating out the noise by leveraging the known noise structure were considered in: <sep> Williams, O., & McSherry, F. (2010). Probabilistic inference and differential privacy. Advances in Neural Information Processing Systems, 23, 2451-2459. <sep> Balle, B., & Wang, Y. X. (2018). Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising. In International Conference on Machine Learning (pp. 394-403). <sep> And many subsequent work. <sep> The contribution of this work is in combining these known pieces (without citing some of the earlier work) to achieve a reasonably strong set of experimental results (for LDP standard). I believe this is the first experimental study that uses VAE for the dimension reduction, however, this alone is not sufficient to carry the paper in my opinion; especially since the setting is now much easier, with access to a public dataset. <sep> The reviewers question the experiments are baselines are usually not using a public dataset as well as the practicality of the proposed method. Also, connections to some of the existing work on private data release (a.k.a., private synthetic data generation) were note clarified. For these reasons, there were not sufficient support among the reviewers to push the paper through. <sep> The authors are encouraged to revise the paper according to the suggestions and resubmit in the next appropriate venue.
This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap. It raised quite a lot of discussion, which finally went in not very constructive way. The reviewers generally agree that the paper has potential, but the actual contribution is limited. <sep> Pros: <sep> The idea that top eigenspaces between different models have high overlap is interesting <sep> The explanation that these structures can be explained by Kronecker-product approximation of the Hessian. <sep> Cons: <sep> The connection to PAC-Bayes is unclear and seems artificial. <sep> Many of the related work is missing <sep> The models and datasets are too simple, and general conclusions can not be made on such kind of models. Much more testing is needed to verify the claims, including state-of-the art architectures and datasets.
In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning. Mixing observational and experimental data is a well-studied problem, and it is well-known how to incorporate interventions into e.g. the likelihood function, along with theoretical guarantees and identifiability. Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited.
The paper proposed a new way for training models that stack the same basic block for multiple times -- share the weights first and then untie the weights. Ablation study shows that the proposed algorithm has marginal improvement over the baseline. The authors also provide some theoretical justifications to how the proposed idea works. <sep> The proposed idea is straightforward and intuitive. Weight sharing has been used in previous works, and what's new in this paper is to unshare the weights in the middle (with a heuristic rule). The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy. However the experimental supports are somehow weak and incomplete. For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K). It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps. <sep> Furthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training. <sep> The reviewers conducted some lengthy discussions after the author rebuttal was available. As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation.
The paper studies benchmarking of bias mitigation methods. The authors propose a synthetic dataset of images (alike colored-MNIST) that enables a controlled setup over different types of correlations between a binary sensitive attribute, dataset features, and a binary outcome label. The authors have evaluated 2K models that are the variants of three recently proposed debiasing methods using fair representation learning across various settings. <sep> While the reviewers acknowledged the importance of benchmarking fair learning methods in a systematic controlled setting, they have raised several concerns: <sep> (1) the proposed benchmark is too abstract/unrealistic (R4, R2, R3); it is not clear whether the findings from this benchmark can be generalized to real-world data with real sensitive features, (2) the proposed benchmark is limited to pseudo sensitive attributes (R1) that are binary (R1, R2), (3) the paper lacks in-depth analysis on why certain methods work under certain conditions (R3). Among these, (2) did not have a major impact on the decision, but would be helpful to address in a subsequent revision, (3) was partially addressed in the rebuttal. However, (1) makes it very difficult to assess the benefits of the proposed benchmark, and was viewed by AC as a critical issue. <sep> The authors provided a detailed rebuttal addressing multiple of the reviewers' concerns. AC can confirm that all four reviewers have read the author responses and have contributed to the discussion. A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. See R1 post-rebuttal encouragement and suggestions how to strengthen the work. We hope the detailed reviews are useful for improving the paper.
This paper received 2 borderline accepts, 1 accept, and 1 reject. <sep> This paper was discussed on the forum and no consensus was reached. The two reviewers who rated the paper as borderline accept emphasized that the biological claims are overblown, that the intellectual contributions (the initialization scheme and partial training) are incremental from a statistical learning perspective, and that the potential applications for the future (like alternate learning rules) are too speculative. I agree with both of these reviewers (and the negative reviewer) that the biological rationale is problematic and the approach is not credible as a model of biology. It is not evaluated as a computer vision model either. And I completely agree with the point raised by several reviewers that there is simply no data about how many synaptic updates to target. Hence, statements regarding % of total synaptic updates and % of brain matches seem empty without a precise target. For all these reasons, I recommend this paper be rejected.
Overview This paper applies RL to automated theorem proving to eliminate the need for human-written proofs as training data. The method uses TF-IDF for premise selections. The experiments compared with supervised baseline demonstrate some good performance. <sep> Pro The paper provides a side-by-side comparison of the effect of the availability of human proofs on the final theorem proving. <sep> The experiments compared with supervised baseline show that the proposed method has good performance even without human knowledge. The prosed TF-IDF selection algorithm addresses a challenging issue in exploration of RL. <sep> Con The reviewers primarily concern about the novelty of the methods. It appears the method is not new since there exist a body of work leveraging RL to learn theorem provers. The tasks are also not novel. After rebuttal, the reviewers are not convinced that the novelty is significant enough for *CONF*. The reviewers are also concerned that the proposed method might not be easily generalized to other tasks. <sep> Recommendation Although the proposed method and experiment demonstrate some merits, there is a lack of novelty in terms of approaches. Since existing results already consider similar methods and similar tasks, it would make the paper stronger if thorough experimental comparisons are performed.
Dear authors, <sep> the paper contains many interesting and novel ideas. Indeed, tuning step-size is very time and energy-consuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ML models. <sep> The paper contains many weaknesses as noted by reviewers. I know that you have addressed many of them one of the reviewers is still concerned about the other issues involving Theorem 1 and the assumption of the bounded preconditioner. <sep> He thinks the preconditioner bound is troublesome. In the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to NOT be bounded below. It seems that the analysis might actually improve if the authors abandoned AMSGrad/Adam and instead just considered SGD for which the preconditioner assumption is not an assumption but just a property of the algorithm. <sep> Thank you
This paper proposes a benchmark suite of offline model-based optimization problems. This benchmark includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from the results. They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area, and provides a test bed for algorithms that try to solve this challenge. However, most reviewers agreed that a more in-depth analysis and insightful explorations for the RL experiment results will help readers understand why their method has superiority even without trajectory data, and that the paper needs another revision before being accepted. Therefore, I recommend rejection although all reviewers agreed that the tasks is very interesting and a good start.
The authors address the problem of learning environment-invariant representations in the case where environments are observed sequentially. <sep> This is done by using a variational Bayesian and bilevel framework. <sep> The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection. <sep> R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer's <sep> suggestions to improve the paper. <sep> R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper. <sep> The authors should improve the work taking into account the reviewrs' comments.
The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning. vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by "action backtracking" and a forward model, with the intrinsic reward coming from the reward map. The authors might be interested in related work on few shot imitation learning: e.g., "One shot imitation learning", Duan et al, 2017, "Watch, try learn: meta-learning from demonstrations and rewards", Zhou et al 2019. As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re-visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions).
The paper present a new learning-based approach` to solve the Maximum Common Subgraph problem. All the reviewers find the idea of using GCN and RL to guide the branch and bound interesting although, even after reading the rebuttal, there are some important concerns about the paper. <sep> The main issue raised by many reviewers are on scalability of the methods and motivation of the problem. It would be nice to add a scalability experiments on large networks(>1M nodes) to show that the method could potentially scale. In fact, the original motivation based on drug discovery, chemoinformatics etc. application is a bit weak because in those area domain specific heuristic should work better. <sep> Overall, the paper is interesting but it does not meet the high publication bar of *CONF*.
Problem Significance This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs. The AC agrees on the importance of a fine-grained characterization of outliers given the large OOD uncertainty space. <sep> Technical contribution The key idea of the paper is to combine the predictions from multiple existing OOD detection methods. While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity. The AC agrees with reviewers that the paper is not ready yet for *CONF* publication, and can be further strengthened by: <sep> (R1) reporting the computational cost for the integrated approach. The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline. The cumulative time for calculating all four scores may be non-negligible. Authors are encouraged to analyze the performance tradeoff in a future revision. <sep> (R2 & R3) discussing the effect of hyper-parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune. <sep> (R3) comparing with more recent development on OOD detection and move the new results to the main paper. The AC also thinks it's worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches. <sep> (R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically. Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10-CIFAR100 pair). This may deem further careful examination to understand what value other components are adding, and in what circumstance. <sep> (R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high-dimensional space would be valuable. The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs. Moreover, it's important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs. For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization. <sep> Recent work by Hsu et al. [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors. <sep> Recommendation Three knowledgeable reviewers have indicated rejection. The AC discounted R4's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion. <sep> [1] Richard Harang, Ethan M. Rudd. Towards Principled Uncertainty Estimation for Deep Neural Networks <sep> [2] Hsu et al. Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data
This paper studies the relationship between adversarial transferability and knowledge transferability. It develops two metrics to measure adversarial transferability and a theoretical framework to justify the positive correlation between adversarial transferability and knowledge transferability. Synthetic experiments show that adversarial transferability measured by the proposed metrics indicates knowledge transferability. <sep> While the paper studies an interesting and fundamental problem, with a sound theoretical analysis and a clear presentation, reviewers still have several reservations to directly accept it. <sep> Lack of interpretation. How this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer? <sep> Lack of inspiration. How the insights can lead to better transfer techniques, apply to practical applications, and foster future research? <sep> Lack of justification. Why such definitions of metrics are the intrinsic ways of measuring adversarial transferability? How well do they correlate with the practical experience with advanced attack, defense, and transfer methods? <sep> AC believes the endeavor made by this paper towards a fundamental problem is highly necessary to our field. But given the above reservations, AC would encourage the authors to further strengthen their work to make it more inspiring and useful.
Reviewers found the construction is very clever and the empirical results are interesting. However, a more thorough theoretical explanation is needed for acceptance.
This paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework. The positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines. Following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model. As one reviewer discusses, the authors are attempting to use RNNs to impute missing infection status labels when the missingness mechanism is assumed to be (i) not at random, (ii) playing out over time (as it is unclear whether Y^t is assumed (conditionally) independent of Y^t' with t' << t), and (iii) subject to interference (whether someone is tested is the 'treatment' here since it's a missingess problem and one person's propensity to be tested could causally affect another person's downstream infection status since apparently no Markov independence is assumed. There is also consensus that the writing quality can be greatly improved. Overall this work contains some ideas with potential in a thorough revision
The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non-target class. The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written. <sep> Unfortunately, it seems that some of the main claims, such as DNNs trained on clean data "learn abstract shapes along with some texture", resort to qualitative evaluation of the few examples shown in the paper. Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors' claims. In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated. <sep> As these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point. That said, the approach looks very promising and I hope the authors continue to pursue this idea.
Reviewers have different views on the paper and after going through the reviews, replies and the papers, we believe that <sep> there is room for improvement here. <sep> While the part related to indefinite symmetric kernels, and general similarity functions seems to be well covered, as <sep> well as the part on Transformers, the relation with learning in RKBS and Transformer is far from being clear and Reviewer 4 makes a strong point on this. For instance, <sep> what is the goal of the section 5 and Definition 1 . Indeed it is not clear here if the point of the authors is to learn the kernel parameters in equation 9 or to learn to predict the output of a transformer. If it is the latter, the connection with the first part is unclear. <sep> In Equation 11, I can understand that x and y are the sequences t and s but what is z_ij and how it is obtained? So again, the learning problem drops in without justification and it is not explained how it can be solved. The theoretical results involving the representer theorem is nice though. <sep> The experiment does not seem very related to the learning problem in Equation 11 introduced by the authors.it seems to me that they are just trying different kernels on top of the dot product.
Overall, the paper makes some interesting and intuitive observations regarding the autoencoders with a cycle consistency, and aims at achieving controllable synthesis via a disentangled representation. However, the overall consensus was that the manuscript needs further iterations: <sep> In particular: <sep> The ideas should be made more precise using mathematical arguments, as it stands some ideas are (e.g. DEAE and UDV) disconnected. <sep> The scope needs to be clarified, e.g. respective contributions of GSL-AE and DEAE, use of label information <sep> More numerical/quantitative evaluations, the current experimentation is not convincing enough, needed for better justification (spurious and not convincing experimentations) <sep> The English of the manuscript could be improved as it occasionally hampers the flow.
This paper proposes to learn representations in an unsupervised manner using a generative model in which observations are generated by combining independent causal mechanisms (ICMs), in combination with a global mechanism. The authors introduce an unconventional mixture prior for the shared and independent components of the representation and train an encoder, discriminator and generator using a Wasserstein GAN with additional terms that enforce consistency in the data and latent space. Experiments consider variations of MNIST and Fashion-MNIST and perform comparisons against a standard VAE, a β-VAE, and the Ada-GVAE. <sep> Reviewers are broadly in agreement that this submission is not ready for publication in its current form. R4 in particular has left very detailed comments regarding clarity. The authors were able to in part address these comments, and R4 raised their score in response. That said, from a read of the manuscript in its latest form, the metareviewer (who is very familiar with literature on disentangled representations) is inclined to agree with the reviewers that this is work that has value, but is very difficult to follow in its current form. The metareviewer would like to suggest that the authors regroup, think carefully about how to improve clarity (in addition to addressing concrete points raised in reviews) and resubmit to a different venue.
The paper proposes to effectively learn representation of 3D data (point clouds/meshes) using a spherical GNN architecture over concentric spherical maps. A method for converting point clouds to concentric spherical images is also proposed. Evaluation is done via 3D classification tasks on rotated data. <sep> Strengths: <sep> Interesting novel method for learning 3D representations <sep> Technically sound <sep> Performs similarly to spherical CNNs and other STOA on the Modelnet40 dataset <sep> Weaknesses: <sep> Presentation of the work needs to be further improved such that it is easier for others to reproduce <sep> More in-depth experiments are needed to justify how much Spherical GNN improves over other STOA, particular given how classification accuracy is very similar to STOA.
This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers: <sep> The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides. <sep> There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works. <sep> The experimental part is weak. It only involves small data set and very simple networks. <sep> Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research.
This paper proposes routing strategies for multilingual NMT. The motivation is to train a single mixture model that can serve the training and prediction of multiple models. Several strategies are proposed: token-level, sentence-level and task-level. This is a simple and straightforward approach (which is fine). The main concerns from the reviewers regard novelty and missing comparisons. In their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work. However, the author's response did not address enough some of other reviewers' concerns regarding comparison with other approaches, and the lack of novelty persists (mixture models for multi-task learning have been previously proposed in the literature), which makes me lean towards rejection. I suggest the authors address these aspects in future iterations of their work.
Overview: <sep> The paper tries to answer which mutual information (MI) objective is sufficient for representation learning (repL) in reinforcement learning (RL). Three common objectives are considered: forward, state, and inverse. The paper shows that only the forward objective is sufficient for learning, i.e., sufficient for learning of optimal policy/value function. The authors also demonstrate this phenomena using empirical experiments. <sep> Quality, Clarity, Originality and Significance: <sep> All the reviewers believe this paper is novel in terms of methodology, i.e., evaluate the sufficiency of the repL in terms of down stream tasks. However, there is a lack of clarity in the experiment sections. The authors have provided more details in the rebuttal phase. The reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field. An unofficial review pointed out there is a mistake in the proof of the paper. The authors later also confirmed the flaw and claimed it is fixed. <sep> Recommendation: <sep> The paper is indeed interesting and novel. However, the impact to the practice community might not be significant. That being said, the paper should warrant publication eventually. However, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws. The reviewers are concerned about this. Overall I believe that the paper is not in a state to be published yet.
The paper proposes to explain the representation for layer-aware neural sequence encoders with multi-order-graph (MoG). Based on the MoG explanation, it further proposes Graph-Transformer as a graph-based self-attention network empowered Transformer. As commented by the authors, a main purpose of Graph-Transformer is to show an example application of the MoG explanation. <sep> During the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524-561 in the attached code: "supplement/fairseq-0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py". The AC had made the following comments to the referees: "Whether the performance gain of Graph-Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high-level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x = x - beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output." <sep> Reviewer 2 responded to the AC's concern: "After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self-attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi-head attentions (line 539-541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539-541) is not very convincing. The link is weak. We agree with your comments." <sep> To summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an *CONF* publication. Therefore, the AC recommends Reject.
This paper studies the problem of uncertainty estimation under distribution shift. The proposed approach (PAD) addresses this under-estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under-estimation at those augmented datapoints. Results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches. <sep> All the reviewer agreed that the experiments are well conducted and the empirical results are very promising. However, they also had a shared concern on the justification of the approach. Reviewers are less willing to accept a paper merely for commending its empirical performance. <sep> I share the above concern as the reviewers, and I personally found the presentation of the approach a bit rush and disconnected from the motivation. For example, the current presentation feels like the method is motivated by BNNs but it is not clear to me how the proposed objective connects to the motivation. Also no derivation of the objective is included in either main text or appendix. <sep> In revision, I would suggest a focus on improving the clarity and theoretical justification of the proposed objective function.
This paper proposed a novel Adversarial Deep Metric Learning approaches. The reviews pointed out the paper proposes an interesting idea and it is among the rare works that address directly robust metric learning which an important topic for efficient metric learning. <sep> Some concerns were raised about the analysis and the lack of comparisons notably with other types of adversarial attacks. <sep> The authors provide a rebuttal where they addressed some concerns raised by reviewers with some precisions on the work, its positioning with respect to other related papers and additional comparisons notably with other types of attacks. <sep> A minor remark: there is a typo in Eq(13), where the z in the loss function is actually not defined and should be included in the max function. <sep> That being said, the contribution is still limited in considering only the infinite norm, analysis and comparisons to prior work remain weak. The paper does not meet the requirements for acceptance to *CONF* in its current form. <sep> I have then to propose rejection.
This paper analyzes some design choices for neural processes, paying particular attention to their small-data performance, uncertainty, and posterior contraction. This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8. However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper's broader recommendations. As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper---while having some interesting ideas---needs a bit more precision and breadth to its experiments.
This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows. The paper provides a hardness result for arbitrary conditional queries. Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the conditioning is relaxed. <sep> There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced. After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done. Novel computational complexity results as such are not really in the scope of *CONF*. There's nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed. <sep> Like R4, I do not follow how this hardness result is meant to motivate the smoothing that's applied. The paper is unambiguous that the goal is to do conditional inference. A hardness result is presented for conditional inference, and so a relaxed surrogate is presented. This has a minor problem that it's not clear the relaxed problem avoids the complexity boundary of the original one. There's a larger problem, though. The hardness result has not been sidestepped! The goal is still to solve conditional inference. The algorithm that's presented is still an approximate algorithm for conditional inference. R4 suggests that other approximation algorithms should be compared to. The authors responded to this point, but I am not able to understand the response. For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (e.g. without smoothing) <sep> None of the above is to say that the smoothing approach is bad. It may very well be. However, I think that either the existing argument should be clarified or a different argument should be given. <sep> Finally here are two minor points (These weren't raised by reviewers and aren't significant for acceptance of the paper. I'm just bringing them up in case they are useful.) <sep> Is Eq. 3 (proof in Appendix B.1) not just an example of the invariance of the KL-divergence under diffeomorphisms? <sep> Proof in appendix B.2 appears to just a special case of the standard chain rule of KL-divergence (e.g. as covered in Cover and Thomas)
The paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization. The starting point is the Lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy. The paper proves that the algorithm converges, and evaluates it experimentally in MuJoCo domains. <sep> The main issues raised by the reviewers were related to the proofs (see especially R1) and experimental evaluation (R4). The authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication. Thus, I'm recommending rejection.
This work provides evidence against the hypothesis that ResNets implement iterative inference, or that iterative convergent computation is a good inductive bias to have in these models. The reviewers indicate that they think this hypothesis is interesting and relevant to the *CONF* community, but they do not find the current work sufficiently convincing. Both theoretically and experimentally the paper does not fully demonstrate the claim that iterative inference is not useful in ResNets, and the reviewers are unanimous in their recommendation to reject the paper until the evidence for this claim is strengthened.
Dear authors, <sep> Improving the theoretical understanding of powerful algorithms is an important contribution to our field. Nevertheless, most of the reviewers are inclined to reject the paper. I somehow have to agree with them as e.g., adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the *CONF* community. I would encourage you to chose maybe another venue. <sep> Thanks
The paper presents a multi-agent RL algorithm where the rewards of the other agents are only known up to some accuracy. The setting is somewhat restrictive, in the sense that the transition is assumed to be known. It would perhaps have been more interesting for the paper to also consider unknown transitions, so as to bring it closer to work in single-agent reinforcement learning. It also seems to not be making a very good job of linking the related work to the contribution of this paper (even after looking at the appendix). <sep> Authors briefly say in the introduction <sep> "Alternative frameworks improve robustness, e.g., to changes in environment dynamics, observation or action spaces (Pinto et al., 2017; Li et al., 2019; Tessler et al., 2019), but do not address reality gaps due to reward function mismatches, as they use inappropriate metrics on the space of adversarial perturbations" <sep> Authors should try and better explain the differences with those papers. Do they consider changes in dynamics rather than the reward? It appears that the former is more general than the latter. Couldn't the authors compare with them with an appropriate experiment? <sep> It is also hard to see how this exactly connects with a reality gap. What is the 'training' environment? What is the 'testing' environment? This is simply a robust optimisation algorithm applied to multi-agent games with partially unknown reward functions. <sep> In addition the experiments themselves are not explained clearly. <sep> On the plus side, I think the algorithmic details and experimental are interesting. If there was a better explanation and discussion/comparison with related work, then it would have been a good paper. Authors are encouraged to make a stronger effort to compare with other methods both in terms of the algorithm and experimentally.
Knowledge distillation (KD) has been widely used in practice for deployment. In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co-train the teacher and student models. The proposed method is novel and interesting. Empirical results showed that the proposed method can perform better than several KD variants. However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal. Besides this, a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments. In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task. On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model, and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer. [1] also contains a theoretic analysis (mean-field analysis based) to support their method. Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient. <sep> [1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020.
This paper studies various graph measures in depth. The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing. But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions.
The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis. All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results. Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain. After consolidating the reviews and rebuttal, the area chair finds the reviewer's argument convincing and would not recommend acceptance of the paper.
